<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 87]
- [cs.CL](#cs.CL) [Total: 23]
- [cs.AI](#cs.AI) [Total: 12]
- [cs.LG](#cs.LG) [Total: 45]
- [cs.RO](#cs.RO) [Total: 28]
- [eess.SY](#eess.SY) [Total: 11]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [What Happens When: Learning Temporal Orders of Events in Videos](https://arxiv.org/abs/2512.08979)
*Daechul Ahn,Yura Choi,Hyeonbeom Choi,Seongwon Cho,San Kim,Jonghyun Choi*

Main category: cs.CV

TL;DR: 该论文提出了VECTOR基准来评估视频大语言模型的事件时序理解能力，发现现有模型在此方面存在不足，并提出了MECOT方法通过事件级指令微调和思维链提示来提升时序理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有视频大语言模型在视频理解方面表现出色，但其对多个事件时序顺序的准确捕捉能力尚未得到充分探索。研究发现即使视频帧被打乱，模型在现有基准上仍表现良好，这表明模型可能依赖典型场景的先验知识而非准确的时序处理来回答问题。

Method: 提出了VECTOR基准来专门评估模型识别事件时序顺序的能力。为提升时序理解，提出了MECOT方法：1）在详细的事件级视频描述上进行模型训练；2）在推理时使用思维链提示来增强时序意识。

Result: 在VECTOR基准上，各种视频大语言模型经常无法理解事件的顺序。MECOT方法在VECTOR基准上超越了先前方法，同时在现有视频基准上也提升了性能，表明其时序理解的有效性。

Conclusion: 视频大语言模型在事件时序理解方面存在不足，提出的VECTOR基准能有效评估这一能力，而MECOT方法通过事件级指令微调和思维链提示能显著提升模型的时序理解性能。

Abstract: Video Large Multimodal Models (VLMMs) have shown impressive performance in video understanding, yet their ability to accurately capture the temporal order of multiple events remains underexplored. We interestingly observe that, even when video frames are scrambled, models perform very well on the existing benchmarks by comprehensive experiments. This implies that VLMMs may not necessarily rely on accurate sequential processing of visual events, but instead depend on prior knowledge of typical scenarios to answer the question. To benchmark temporal understanding capabilities in VLMMs, we propose VECTOR, designed to explicitly assess a model's ability to identify the temporal order of events. On this benchmark, we observe that various VLMMs often fail to understand the orders of events. To address this, we propose MECOT (Multi-Event instruction fine-tuning with Chain-of-Thought), which (1) trains models on detailed, event-by-event video descriptions and (2) using chain-of-thought prompts at inference to enhance temporal awareness. MECOT outperforms prior arts on VECTOR as well as improving performance on existing video benchmarks, implying effectiveness of temporal understanding. We release our code, model and datasets.

</details>


### [2] [Mitigating Bias with Words: Inducing Demographic Ambiguity in Face Recognition Templates by Text Encoding](https://arxiv.org/abs/2512.08981)
*Tahar Chettaoui,Naser Damer,Fadi Boutros*

Main category: cs.CV

TL;DR: 提出UTIE方法，利用视觉语言模型的多模态对齐能力，通过向人脸嵌入中添加其他人口群体的文本特征来减少人口统计偏见，在保持或提高人脸验证准确性的同时降低偏见指标。


<details>
  <summary>Details</summary>
Motivation: 人脸识别系统存在人口统计偏见，主要原因是人脸嵌入中身份相关特征与人口统计特定信息纠缠在一起。这种偏见在大型多元文化城市中尤为关键，尤其是在生物识别技术成为智慧城市基础设施重要组成部分的背景下。这种纠缠导致人口统计属性在嵌入空间中掩盖了身份线索，造成不同人口群体间验证性能的差异。

Method: 提出统一文本-图像嵌入（UTIE）策略，利用视觉语言模型（VLM）的零样本能力和跨模态语义对齐特性。通过向每个人口群体的人脸嵌入中添加从其他人口群体提取的文本衍生人口统计特征，从而丰富人脸嵌入信息，鼓励嵌入空间中对人口统计属性保持更中性的表示，强调身份相关特征。

Result: 在RFW和BFW两个人脸识别偏见评估基准上，使用CLIP、OpenCLIP和SigLIP三种视觉语言模型进行实验。结果显示UTIE方法能够持续降低偏见指标，同时在许多情况下保持甚至提高了人脸验证的准确性。

Conclusion: UTIE方法通过利用视觉语言模型的跨模态对齐能力，有效减少了人脸识别系统中的人口统计偏见，实现了更公平的验证性能，同时不损害整体识别准确性。

Abstract: Face recognition (FR) systems are often prone to demographic biases, partially due to the entanglement of demographic-specific information with identity-relevant features in facial embeddings. This bias is extremely critical in large multicultural cities, especially where biometrics play a major role in smart city infrastructure. The entanglement can cause demographic attributes to overshadow identity cues in the embedding space, resulting in disparities in verification performance across different demographic groups. To address this issue, we propose a novel strategy, Unified Text-Image Embedding (UTIE), which aims to induce demographic ambiguity in face embeddings by enriching them with information related to other demographic groups. This encourages face embeddings to emphasize identity-relevant features and thus promotes fairer verification performance across groups. UTIE leverages the zero-shot capabilities and cross-modal semantic alignment of Vision-Language Models (VLMs). Given that VLMs are naturally trained to align visual and textual representations, we enrich the facial embeddings of each demographic group with text-derived demographic features extracted from other demographic groups. This encourages a more neutral representation in terms of demographic attributes. We evaluate UTIE using three VLMs, CLIP, OpenCLIP, and SigLIP, on two widely used benchmarks, RFW and BFW, designed to assess bias in FR. Experimental results show that UTIE consistently reduces bias metrics while maintaining, or even improving in several cases, the face verification accuracy.

</details>


### [3] [Consist-Retinex: One-Step Noise-Emphasized Consistency Training Accelerates High-Quality Retinex Enhancement](https://arxiv.org/abs/2512.08982)
*Jian Xu,Wei Chen,Shigui Li,Delu Zeng,John Paisley,Qibin Zhao*

Main category: cs.CV

TL;DR: Consist-Retinex：首个将一致性模型应用于Retinex低光增强的框架，通过双目标一致性损失和自适应噪声强调采样策略，实现单步采样的SOTA性能，训练成本仅为基准的1/8。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在基于Retinex的低光图像增强中取得了显著成功，但需要数百次迭代采样步骤，严重限制了实际部署。虽然一致性模型为无条件合成提供了一步生成的可能性，但在条件增强任务中的应用尚未探索。

Method: 提出Consist-Retinex框架，包含两个核心创新：1）双目标一致性损失，结合时间一致性和在随机时间采样下的真实值对齐，提供全谱监督以实现稳定收敛；2）自适应噪声强调采样策略，优先训练对一步条件生成至关重要的大噪声区域。

Result: 在VE-LOL-L数据集上，Consist-Retinex通过单步采样实现了最先进的性能（PSNR：25.51 vs. 23.41，FID：44.73 vs. 49.59，相比Diff-Retinex++），同时训练预算仅为1000步Diff-Retinex基线的1/8。

Conclusion: Consist-Retinex成功将一致性建模应用于Retinex低光增强，解决了条件增强与无条件生成在训练动态上的根本差异，实现了高效的单步增强，为实际部署提供了可行的解决方案。

Abstract: Diffusion models have achieved remarkable success in low-light image enhancement through Retinex-based decomposition, yet their requirement for hundreds of iterative sampling steps severely limits practical deployment. While recent consistency models offer promising one-step generation for \textit{unconditional synthesis}, their application to \textit{conditional enhancement} remains unexplored. We present \textbf{Consist-Retinex}, the first framework adapting consistency modeling to Retinex-based low-light enhancement. Our key insight is that conditional enhancement requires fundamentally different training dynamics than unconditional generation standard consistency training focuses on low-noise regions near the data manifold, while conditional mapping critically depends on large-noise regimes that bridge degraded inputs to enhanced outputs. We introduce two core innovations: (1) a \textbf{dual-objective consistency loss} combining temporal consistency with ground-truth alignment under randomized time sampling, providing full-spectrum supervision for stable convergence; and (2) an \textbf{adaptive noise-emphasized sampling strategy} that prioritizes training on large-noise regions essential for one-step conditional generation. On VE-LOL-L, Consist-Retinex achieves \textbf{state-of-the-art performance with single-step sampling} (\textbf{PSNR: 25.51 vs. 23.41, FID: 44.73 vs. 49.59} compared to Diff-Retinex++), while requiring only \textbf{1/8 of the training budget} relative to the 1000-step Diff-Retinex baseline.

</details>


### [4] [HSCP: A Two-Stage Spectral Clustering Framework for Resource-Constrained UAV Identification](https://arxiv.org/abs/2512.08983)
*Maoyu Wang,Yao Lu,Bo Zhou,Zhuangzhi Chen,Yun Lin,Qi Xuan,Guan Gui*

Main category: cs.CV

TL;DR: HSCP：一种结合层剪枝和通道剪枝的层次化谱聚类剪枝框架，用于无人机射频指纹识别，在保持高精度的同时实现极端压缩和高效推理。


<details>
  <summary>Details</summary>
Motivation: 传统无人机识别方法在复杂环境中难以提取可靠信号特征且无法满足实时性要求。基于深度学习的RFFI方法虽然提高了识别精度，但模型大、计算需求高，难以部署在资源受限的边缘设备上。现有剪枝技术难以同时优化压缩率、硬件加速和识别精度。

Method: 提出HSCP层次化谱聚类剪枝框架：第一阶段使用基于中心核对齐(CKA)的谱聚类识别并移除冗余层；第二阶段将相同策略应用于通道维度以消除更细粒度的冗余；最后采用噪声鲁棒微调策略确保模型鲁棒性。

Result: 在UAV-M100基准测试中，HSCP优于现有通道和层剪枝方法。在ResNet18上实现了86.39%的参数减少和84.44%的FLOPs减少，同时比未剪枝基线提高了1.49%的准确率，即使在低信噪比环境中也保持优越的鲁棒性。

Conclusion: HSCP框架通过结合层剪枝和通道剪枝，实现了极端压缩、高性能和高效推理，解决了无人机射频指纹识别在边缘设备部署中的模型大小和计算需求问题。

Abstract: With the rapid development of Unmanned Aerial Vehicles (UAVs) and the increasing complexity of low-altitude security threats, traditional UAV identification methods struggle to extract reliable signal features and meet real-time requirements in complex environments. Recently, deep learning based Radio Frequency Fingerprint Identification (RFFI) approaches have greatly improved recognition accuracy. However, their large model sizes and high computational demands hinder deployment on resource-constrained edge devices. While model pruning offers a general solution for complexity reduction, existing weight, channel, and layer pruning techniques struggle to concurrently optimize compression rate, hardware acceleration, and recognition accuracy. To this end, in this paper, we introduce HSCP, a Hierarchical Spectral Clustering Pruning framework that combines layer pruning with channel pruning to achieve extreme compression, high performance, and efficient inference. In the first stage, HSCP employs spectral clustering guided by Centered Kernel Alignment (CKA) to identify and remove redundant layers. Subsequently, the same strategy is applied to the channel dimension to eliminate a finer redundancy. To ensure robustness, we further employ a noise-robust fine-tuning strategy. Experiments on the UAV-M100 benchmark demonstrate that HSCP outperforms existing channel and layer pruning methods. Specifically, HSCP achieves $86.39\%$ parameter reduction and $84.44\%$ FLOPs reduction on ResNet18 while improving accuracy by $1.49\%$ compared to the unpruned baseline, and maintains superior robustness even in low signal-to-noise ratio environments.

</details>


### [5] [RAG-HAR: Retrieval Augmented Generation-based Human Activity Recognition](https://arxiv.org/abs/2512.08984)
*Nirhoshan Sivaroopan,Hansi Karunarathna,Chamara Madarasingha,Anura Jayasumana,Kanchana Thilakarathna*

Main category: cs.CV

TL;DR: RAG-HAR是一个无需训练的检索增强框架，利用大语言模型进行人类活动识别，通过轻量级统计描述符和向量数据库检索实现高效准确的活动识别，并在多个基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法需要针对特定数据集进行训练、大量标注数据和大量计算资源，这限制了人类活动识别在实际应用中的可扩展性和实用性。

Method: RAG-HAR采用检索增强框架：1) 计算轻量级统计描述符；2) 从向量数据库中检索语义相似的样本；3) 利用这些上下文证据进行基于LLM的活动识别；4) 通过提示优化和LLM生成的活动描述符增强上下文信息。

Result: 在六个不同的人类活动识别基准测试中达到了最先进的性能，最重要的是无需模型训练或微调，能够识别和有意义地标记多个未见的人类活动。

Conclusion: RAG-HAR通过训练免费的检索增强框架，克服了传统深度学习方法在人类活动识别中的局限性，展示了强大的鲁棒性和实际应用价值，能够扩展到未知行为的识别。

Abstract: Human Activity Recognition (HAR) underpins applications in healthcare, rehabilitation, fitness tracking, and smart environments, yet existing deep learning approaches demand dataset-specific training, large labeled corpora, and significant computational resources.We introduce RAG-HAR, a training-free retrieval-augmented framework that leverages large language models (LLMs) for HAR. RAG-HAR computes lightweight statistical descriptors, retrieves semantically similar samples from a vector database, and uses this contextual evidence to make LLM-based activity identification. We further enhance RAG-HAR by first applying prompt optimization and introducing an LLM-based activity descriptor that generates context-enriched vector databases for delivering accurate and highly relevant contextual information. Along with these mechanisms, RAG-HAR achieves state-of-the-art performance across six diverse HAR benchmarks. Most importantly, RAG-HAR attains these improvements without requiring model training or fine-tuning, emphasizing its robustness and practical applicability. RAG-HAR moves beyond known behaviors, enabling the recognition and meaningful labelling of multiple unseen human activities.

</details>


### [6] [An Efficient Test-Time Scaling Approach for Image Generation](https://arxiv.org/abs/2512.08985)
*Vignesh Sundaresha,Akash Haridas,Vikram Appia,Lav Varshney*

Main category: cs.CV

TL;DR: 提出Verifier-Threshold方法，通过自动重新分配测试时计算资源，在图像生成任务中实现2-4倍的计算时间减少


<details>
  <summary>Details</summary>
Motivation: 现有方法在分配非均匀推理计算预算时依赖贪心算法，导致计算资源分配效率低下，需要更有效的测试时计算分配策略

Method: 提出Verifier-Threshold方法，自动重新分配测试时计算资源，优化扩散和流模型的噪声样本搜索过程

Result: 在GenEval基准测试上，在相同性能水平下，相比最先进方法实现了2-4倍的计算时间减少

Conclusion: Verifier-Threshold方法能够有效优化测试时计算分配，显著提升图像生成模型的推理效率

Abstract: Image generation has emerged as a mainstream application of large generative AI models. Just as test-time compute and reasoning have helped language models improve their capabilities, similar benefits have also been observed with image generation models. In particular, searching over noise samples for diffusion and flow models has shown to scale well with test-time compute. While recent works have explored allocating non-uniform inference-compute budgets across different denoising steps, they rely on greedy algorithms and allocate the compute budget ineffectively. In this work, we study this problem and propose solutions to fix it. We propose the Verifier-Threshold method which automatically reallocates test-time compute and delivers substantial efficiency improvements. For the same performance on the GenEval benchmark, we achieve a 2-4x reduction in computational time over the state-of-the-art method.

</details>


### [7] [Explainable Fundus Image Curation and Lesion Detection in Diabetic Retinopathy](https://arxiv.org/abs/2512.08986)
*Anca Mihai,Adrian Groza*

Main category: cs.CV

TL;DR: 提出一个用于糖尿病视网膜病变AI模型训练的质量控制框架，通过特征分类器过滤不合格图像，使用深度学习辅助标注，并通过标注者一致性评估确定标注数据的可用性。


<details>
  <summary>Details</summary>
Motivation: 糖尿病视网膜病变的早期诊断对防止视力丧失至关重要。虽然AI可以帮助临床医生识别病变，但模型需要高质量的标注数据集。由于视网膜结构复杂，图像采集和人工标注过程中可能出现错误，因此需要确保用于评估和AI训练的数据具有高标准。

Method: 提出一个三阶段质量控制框架：1）使用基于可解释特征的分类器过滤不合格图像，特征通过图像处理和对比学习提取；2）图像增强后使用深度学习辅助进行标注；3）通过推导的公式计算标注者间一致性，确定标注数据的可用性。

Result: 论文提出了一个完整的质量控制框架，但没有提供具体的实验结果数据。框架设计涵盖了从图像筛选到标注质量评估的全过程。

Conclusion: 该质量控制框架能够确保只有高质量的数据用于糖尿病视网膜病变AI模型的评估和训练，从而提高模型的可靠性和临床实用性。

Abstract: Diabetic Retinopathy (DR) affects individuals with long-term diabetes. Without early diagnosis, DR can lead to vision loss. Fundus photography captures the structure of the retina along with abnormalities indicative of the stage of the disease. Artificial Intelligence (AI) can support clinicians in identifying these lesions, reducing manual workload, but models require high-quality annotated datasets. Due to the complexity of retinal structures, errors in image acquisition and lesion interpretation of manual annotators can occur. We proposed a quality-control framework, ensuring only high-standard data is used for evaluation and AI training. First, an explainable feature-based classifier is used to filter inadequate images. The features are extracted both using image processing and contrastive learning. Then, the images are enhanced and put subject to annotation, using deep-learning-based assistance. Lastly, the agreement between annotators calculated using derived formulas determines the usability of the annotations.

</details>


### [8] [3DID: Direct 3D Inverse Design for Aerodynamics with Physics-Aware Optimization](https://arxiv.org/abs/2512.08987)
*Yuze Hao,Linchao Zhu,Yi Yang*

Main category: cs.CV

TL;DR: 提出3DID框架，通过连续潜在表示与物理感知优化策略直接导航3D设计空间，实现高质量3D几何生成


<details>
  <summary>Details</summary>
Motivation: 传统逆设计方法在3D领域中面临设计空间指数增长问题，现有深度学习方法多采用2D投影或微调现有3D形状，牺牲了体积细节并限制了设计探索，无法实现真正的从零开始3D设计

Method: 提出3DID框架：1) 学习统一的物理-几何嵌入，在连续潜在空间中紧凑捕捉形状和物理场数据；2) 采用两阶段物理感知优化策略：第一阶段使用梯度引导扩散采样器探索全局潜在流形，第二阶段进行目标驱动、拓扑保持的细化

Result: 3DID能够生成高保真度的3D几何体，在解决方案质量和设计多样性方面均优于现有方法

Conclusion: 该框架通过直接导航3D设计空间，克服了现有方法的局限性，实现了真正的从零开始3D逆设计，在质量和多样性方面均有显著提升

Abstract: Inverse design aims to design the input variables of a physical system to optimize a specified objective function, typically formulated as a search or optimization problem. However, in 3D domains, the design space grows exponentially, rendering exhaustive grid-based searches infeasible. Recent advances in deep learning have accelerated inverse design by providing powerful generative priors and differentiable surrogate models. Nevertheless, current methods tend to approximate the 3D design space using 2D projections or fine-tune existing 3D shapes. These approaches sacrifice volumetric detail and constrain design exploration, preventing true 3D design from scratch. In this paper, we propose a 3D Inverse Design (3DID) framework that directly navigates the 3D design space by coupling a continuous latent representation with a physics-aware optimization strategy. We first learn a unified physics-geometry embedding that compactly captures shape and physical field data in a continuous latent space. Then, we introduce a two-stage strategy to perform physics-aware optimization. In the first stage, a gradient-guided diffusion sampler explores the global latent manifold. In the second stage, an objective-driven, topology-preserving refinement further sculpts each candidate toward the target objective. This enables 3DID to generate high-fidelity 3D geometries, outperforming existing methods in both solution quality and design versatility.

</details>


### [9] [Deterministic World Models for Verification of Closed-loop Vision-based Systems](https://arxiv.org/abs/2512.08991)
*Yuang Geng,Zhuoyang Zhou,Zhongzheng Zhang,Siyuan Pan,Hoang-Dung Tran,Ivan Ruchkin*

Main category: cs.CV

TL;DR: 提出确定性世界模型(DWM)替代传统生成模型，消除随机潜变量以减少过近似误差，提升视觉控制系统验证精度


<details>
  <summary>Details</summary>
Motivation: 视觉控制系统验证面临图像高维度和视觉环境建模困难，现有生成模型依赖随机潜变量导致不必要的过近似误差

Method: 提出确定性世界模型(DWM)，将系统状态直接映射到生成图像，消除不可解释潜变量；采用双目标损失函数（像素级重建精度+控制差异损失）；集成到基于Star的可达性分析验证流程，使用保形预测推导统计边界

Result: 在标准基准测试中，相比潜变量基线方法，该方法产生更紧密的可达集和更好的验证性能

Conclusion: 确定性世界模型能有效提升视觉控制系统验证精度，通过消除随机潜变量和结合保形预测，为实际视觉系统提供更严格的轨迹偏差统计边界

Abstract: Verifying closed-loop vision-based control systems remains a fundamental challenge due to the high dimensionality of images and the difficulty of modeling visual environments. While generative models are increasingly used as camera surrogates in verification, their reliance on stochastic latent variables introduces unnecessary overapproximation error. To address this bottleneck, we propose a Deterministic World Model (DWM) that maps system states directly to generative images, effectively eliminating uninterpretable latent variables to ensure precise input bounds. The DWM is trained with a dual-objective loss function that combines pixel-level reconstruction accuracy with a control difference loss to maintain behavioral consistency with the real system. We integrate DWM into a verification pipeline utilizing Star-based reachability analysis (StarV) and employ conformal prediction to derive rigorous statistical bounds on the trajectory deviation between the world model and the actual vision-based system. Experiments on standard benchmarks show that our approach yields significantly tighter reachable sets and better verification performance than a latent-variable baseline.

</details>


### [10] [Demo: Generative AI helps Radiotherapy Planning with User Preference](https://arxiv.org/abs/2512.08996)
*Riqiang Gao,Simon Arberet,Martin Kraus,Han Liu,Wilko FAR Verbakel,Dorin Comaniciu,Florin-Cristian Ghesu,Ali Kamen*

Main category: cs.CV

TL;DR: 提出一种基于用户偏好风味的3D剂量预测生成模型，不依赖参考计划作为训练真值，避免模型偏向特定规划风格或机构偏好


<details>
  <summary>Details</summary>
Motivation: 放疗规划过程复杂且机构间差异大，现有深度学习方法依赖参考计划作为真值训练，容易偏向特定规划风格或机构偏好，需要更灵活个性化的解决方案

Method: 引入新颖的生成模型，仅基于用户定义偏好风味预测3D剂量分布，这些可定制偏好让规划者能优先考虑危及器官和计划靶区之间的权衡，提供更大灵活性和个性化

Result: 比较评估表明，该方法在某些场景下在适应性和计划质量方面超越了Varian RapidPlan模型，能够无缝集成到临床治疗规划系统中，帮助用户高效生成高质量计划

Conclusion: 提出的基于用户偏好的生成模型为放疗规划提供了更灵活、个性化的解决方案，避免了传统方法对特定规划风格的依赖，在临床应用中具有实用价值

Abstract: Radiotherapy planning is a highly complex process that often varies significantly across institutions and individual planners. Most existing deep learning approaches for 3D dose prediction rely on reference plans as ground truth during training, which can inadvertently bias models toward specific planning styles or institutional preferences. In this study, we introduce a novel generative model that predicts 3D dose distributions based solely on user-defined preference flavors. These customizable preferences enable planners to prioritize specific trade-offs between organs-at-risk (OARs) and planning target volumes (PTVs), offering greater flexibility and personalization. Designed for seamless integration with clinical treatment planning systems, our approach assists users in generating high-quality plans efficiently. Comparative evaluations demonstrate that our method can surpasses the Varian RapidPlan model in both adaptability and plan quality in some scenarios.

</details>


### [11] [Diffusion Model Regularized Implicit Neural Representation for CT Metal Artifact Reduction](https://arxiv.org/abs/2512.08999)
*Jie Wen,Chenhe Du,Xiao Wang,Yuyao Zhang*

Main category: cs.CV

TL;DR: 提出基于扩散模型正则化隐式神经表示的金属伪影减少方法，解决监督方法数据依赖和无监督方法物理约束不足的问题


<details>
  <summary>Details</summary>
Motivation: 现有监督金属伪影减少方法依赖有限配对数据导致性能不稳定，而无监督方法未能有效整合CT物理几何约束且传统正则化无法充分利用先验知识

Method: 提出扩散模型正则化隐式神经表示框架，隐式神经表示整合物理约束保证数据保真度，预训练扩散模型提供先验知识正则化解决方案

Result: 在模拟和临床数据上的实验结果表明该方法有效且具有良好泛化能力，具备临床应用潜力

Conclusion: 提出的框架成功解决了现有金属伪影减少方法的局限性，通过结合物理约束和扩散模型先验知识，为临床环境提供了有前景的解决方案

Abstract: Computed tomography (CT) images are often severely corrupted by artifacts in the presence of metals. Existing supervised metal artifact reduction (MAR) approaches suffer from performance instability on known data due to their reliance on limited paired metal-clean data, which limits their clinical applicability. Moreover, existing unsupervised methods face two main challenges: 1) the CT physical geometry is not effectively incorporated into the MAR process to ensure data fidelity; 2) traditional heuristics regularization terms cannot fully capture the abundant prior knowledge available. To overcome these shortcomings, we propose diffusion model regularized implicit neural representation framework for MAR. The implicit neural representation integrates physical constraints and imposes data fidelity, while the pre-trained diffusion model provides prior knowledge to regularize the solution. Experimental results on both simulated and clinical data demonstrate the effectiveness and generalization ability of our method, highlighting its potential to be applied to clinical settings.

</details>


### [12] [A Physics-Constrained, Design-Driven Methodology for Defect Dataset Generation in Optical Lithography](https://arxiv.org/abs/2512.09001)
*Yuehua Hu,Jiyeong Kong,Dong-yeol Shin,Jaekyun Kim,Kyung-Tae Kang*

Main category: cs.CV

TL;DR: 该研究提出了一种生成大规模、物理有效的缺陷数据集的新方法，用于半导体制造中的AI缺陷检测，解决了训练数据稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 微纳制造中AI缺陷检测的有效性受到高质量、物理基础训练数据稀缺的限制，特别是半导体光刻缺陷数据难以获取，导致公开数据集短缺。

Method: 提出一种新方法：1）使用可控的物理约束数学形态学操作（腐蚀和膨胀）从原始设计布局中合成缺陷布局；2）通过高保真数字微镜器件光刻制造物理样本；3）比较合成缺陷样本和缺陷参考的光学显微图像，创建一致的缺陷标注。

Result: 构建了包含3,530张光学显微图像、13,365个标注缺陷实例的综合数据集，涵盖桥接、毛刺、收缩和污染四类缺陷。Mask R-CNN在桥接、毛刺和收缩类上的AP@0.5分别为0.980、0.965和0.971，相比Faster R-CNN的0.740、0.719和0.717，平均提升约34%。污染类提升约42%。

Conclusion: 提出的生成具有像素级标注缺陷数据集的方法对于半导体制造中基于AI的测量/检测是可行的，能够实现稳健的缺陷检测性能。

Abstract: The efficacy of Artificial Intelligence (AI) in micro/nano manufacturing is fundamentally constrained by the scarcity of high-quality and physically grounded training data for defect inspection. Lithography defect data from semiconductor industry are rarely accessible for research use, resulting in a shortage of publicly available datasets. To address this bottleneck in lithography, this study proposes a novel methodology for generating large-scale, physically valid defect datasets with pixel-level annotations. The framework begins with the ab initio synthesis of defect layouts using controllable, physics-constrained mathematical morphology operations (erosion and dilation) applied to the original design-level layout. These synthesized layouts, together with their defect-free counterparts, are fabricated into physical samples via high-fidelity digital micromirror device (DMD)-based lithography. Optical micrographs of the synthesized defect samples and their defect-free references are then compared to create consistent defect delineation annotations. Using this methodology, we constructed a comprehensive dataset of 3,530 Optical micrographs containing 13,365 annotated defect instances including four classes: bridge, burr, pinch, and contamination. Each defect instance is annotated with a pixel-accurate segmentation mask, preserving full contour and geometry. The segmentation-based Mask R-CNN achieves AP@0.5 of 0.980, 0.965, and 0.971, compared with 0.740, 0.719, and 0.717 for Faster R-CNN on bridge, burr, and pinch classes, representing a mean AP@0.5 improvement of approximately 34%. For the contamination class, Mask R-CNN achieves an AP@0.5 roughly 42% higher than Faster R-CNN. These consistent gains demonstrate that our proposed methodology to generate defect datasets with pixel-level annotations is feasible for robust AI-based Measurement/Inspection (MI) in semiconductor fabrication.

</details>


### [13] [A Survey of Body and Face Motion: Datasets, Performance Evaluation Metrics and Generative Techniques](https://arxiv.org/abs/2512.09005)
*Lownish Rai Sookha,Nikhil Pakhale,Mudasir Ganaie,Abhinav Dhall*

Main category: cs.CV

TL;DR: 这是一篇关于身体和面部动作生成的综述论文，涵盖了核心概念、表示技术、生成方法、数据集和评估指标，旨在提升虚拟角色在对话场景中的真实感、连贯性和表现力。


<details>
  <summary>Details</summary>
Motivation: 身体和面部动作在交流中扮演着重要角色，传达着参与者的关键信息。虽然生成建模和多模态学习的发展使得从语音、对话上下文和视觉线索生成动作成为可能，但由于语言/非语言线索与个体人格特质之间的复杂相互作用，生成富有表现力和连贯性的面部和身体动态仍然具有挑战性。

Method: 这是一篇综述论文，采用文献综述的方法，系统性地回顾了身体和面部动作生成领域的核心概念、表示技术、生成方法、数据集和评估指标。作者对现有研究进行了全面梳理和分析。

Result: 论文提供了身体和面部动作生成领域的全面综述，涵盖了该领域的关键技术和方法。作者还创建了一个资源网站（https://lownish23csz0010.github.io/mogen/）来提供详细资源。据作者所知，这是第一篇同时涵盖身体和面部动作生成的综合性综述。

Conclusion: 论文强调了未来研究方向，旨在增强虚拟角色在对话场景中的真实感、连贯性和表现力。通过系统梳理现有技术和方法，为研究人员提供了该领域的全面概览和发展方向。

Abstract: Body and face motion play an integral role in communication. They convey crucial information on the participants. Advances in generative modeling and multi-modal learning have enabled motion generation from signals such as speech, conversational context and visual cues. However, generating expressive and coherent face and body dynamics remains challenging due to the complex interplay of verbal / non-verbal cues and individual personality traits. This survey reviews body and face motion generation, covering core concepts, representations techniques, generative approaches, datasets and evaluation metrics. We highlight future directions to enhance the realism, coherence and expressiveness of avatars in dyadic settings. To the best of our knowledge, this work is the first comprehensive review to cover both body and face motion. Detailed resources are listed on https://lownish23csz0010.github.io/mogen/.

</details>


### [14] [Towards Lossless Ultimate Vision Token Compression for VLMs](https://arxiv.org/abs/2512.09010)
*Dehua Zheng,Mouxiao Huang,Borui Jiang,Hailin Hu,Xinghao Chen*

Main category: cs.CV

TL;DR: LUVC框架通过视觉编码器的迭代合并和LLM的低通滤波剪枝，实现视觉token的无损压缩，在保持精度的同时加速VLM推理2倍


<details>
  <summary>Details</summary>
Motivation: 高分辨率图像和视频的token表示存在大量冗余，导致视觉语言模型计算效率低、延迟高。现有基于注意力/相似度的压缩算法存在位置偏差或类别不平衡问题，且无法泛化到浅层LLM层

Method: 1. 在视觉编码器中扩展token压缩，采用空间轴正交的有效迭代合并方案；2. 在LLM中集成频谱剪枝单元，使用无注意力/相似度的低通滤波器逐步剪枝冗余视觉token；3. 提出LUVC框架，系统压缩视觉token直到LLM最后一层完全消除，使高维视觉特征逐步融入多模态查询

Result: LUVC实现了语言模型推理2倍加速，精度下降可忽略不计。无需训练的特性使其能够立即部署到多个VLM中

Conclusion: LUVC框架通过系统性的视觉token压缩，有效解决了VLM的计算效率和延迟问题，在保持模型精度的同时显著提升推理速度

Abstract: Visual language models encounter challenges in computational efficiency and latency, primarily due to the substantial redundancy in the token representations of high-resolution images and videos. Current attention/similarity-based compression algorithms suffer from either position bias or class imbalance, leading to significant accuracy degradation. They also fail to generalize to shallow LLM layers, which exhibit weaker cross-modal interactions. To address this, we extend token compression to the visual encoder through an effective iterative merging scheme that is orthogonal in spatial axes to accelerate the computation across the entire VLM. Furthermoer, we integrate a spectrum pruning unit into LLM through an attention/similarity-free low-pass filter, which gradually prunes redundant visual tokens and is fully compatible to modern FlashAttention. On this basis, we propose Lossless Ultimate Vision tokens Compression (LUVC) framework. LUVC systematically compresses visual tokens until complete elimination at the final layer of LLM, so that the high-dimensional visual features are gradually fused into the multimodal queries. The experiments show that LUVC achieves a 2 speedup inference in language model with negligible accuracy degradation, and the training-free characteristic enables immediate deployment across multiple VLMs.

</details>


### [15] [An Automated Tip-and-Cue Framework for Optimized Satellite Tasking and Visual Intelligence](https://arxiv.org/abs/2512.09670)
*Gil Weissman,Amir Ivry,Israel Cohen*

Main category: cs.CV

TL;DR: 本文提出了一种完全自动化的Tip-and-Cue框架，用于卫星成像任务分配和调度，通过外部数据源生成提示，自动创建成像任务，优化多卫星调度，并使用AI模型处理图像生成结构化报告。


<details>
  <summary>Details</summary>
Motivation: 随着卫星星座的普及、任务分配延迟的减少以及传感器能力的多样化，自动化地球观测的机会不断扩大。需要开发能够自动处理卫星成像任务分配和调度的系统，以充分利用这些机会。

Method: 提出完全自动化的Tip-and-Cue框架：1) 从外部数据源或先前卫星图像分析生成提示，识别时空目标并确定优先级；2) 根据提示创建成像任务，考虑传感器约束、时间要求和效用函数；3) 使用连续效用函数优化多卫星调度；4) 使用基于AI的模型（包括目标检测器和视觉语言模型）处理图像；5) 生成结构化视觉报告。

Result: 通过海上船舶跟踪场景验证了框架的有效性，利用自动识别系统(AIS)数据进行轨迹预测、目标观测和可操作输出生成。该系统可扩展到更广泛的应用，如智慧城市监控和灾害响应。

Conclusion: 该框架为卫星成像任务分配和调度提供了一种完全自动化的解决方案，能够有效利用卫星资源，生成可解释的结构化报告，并支持下游任务的新见解识别，具有广泛的应用前景。

Abstract: The proliferation of satellite constellations, coupled with reduced tasking latency and diverse sensor capabilities, has expanded the opportunities for automated Earth observation. This paper introduces a fully automated Tip-and-Cue framework designed for satellite imaging tasking and scheduling. In this context, tips are generated from external data sources or analyses of prior satellite imagery, identifying spatiotemporal targets and prioritizing them for downstream planning. Corresponding cues are the imaging tasks formulated in response, which incorporate sensor constraints, timing requirements, and utility functions. The system autonomously generates candidate tasks, optimizes their scheduling across multiple satellites using continuous utility functions that reflect the expected value of each observation, and processes the resulting imagery using artificial-intelligence-based models, including object detectors and vision-language models. Structured visual reports are generated to support both interpretability and the identification of new insights for downstream tasking. The efficacy of the framework is demonstrated through a maritime vessel tracking scenario, utilizing Automatic Identification System (AIS) data for trajectory prediction, targeted observations, and the generation of actionable outputs. Maritime vessel tracking is a widely researched application, often used to benchmark novel approaches to satellite tasking, forecasting, and analysis. The system is extensible to broader applications such as smart-city monitoring and disaster response, where timely tasking and automated analysis are critical.

</details>


### [16] [ConceptPose: Training-Free Zero-Shot Object Pose Estimation using Concept Vectors](https://arxiv.org/abs/2512.09056)
*Liming Kuang,Yordanka Velikova,Mahdi Saleh,Jan-Nico Zaech,Danda Pani Paudel,Benjamin Busam*

Main category: cs.CV

TL;DR: ConceptPose：首个无需训练、无需模型的零样本6DoF物体姿态估计框架，利用视觉语言模型创建开放词汇3D概念地图，通过3D-3D对应实现精确姿态估计


<details>
  <summary>Details</summary>
Motivation: 传统物体姿态估计方法需要大量数据集特定训练，而大规模视觉语言模型展现出强大的零样本能力。本研究旨在将这两个领域结合，开发无需训练和模型依赖的姿态估计方法

Method: 利用视觉语言模型创建开放词汇3D概念地图，每个点通过显著性图获得概念向量标签。通过建立概念地图之间的鲁棒3D-3D对应关系，实现6DoF相对姿态的精确估计

Result: 在常见零样本相对姿态估计基准测试中达到最先进水平，ADD(-S)分数比现有方法提升超过62%，包括那些使用大量数据集特定训练的方法

Conclusion: ConceptPose成功实现了无需对象或数据集特定训练的物体姿态估计，通过视觉语言模型的零样本能力与3D概念地图的结合，为姿态估计领域提供了新的训练自由范式

Abstract: Object pose estimation is a fundamental task in computer vision and robotics, yet most methods require extensive, dataset-specific training. Concurrently, large-scale vision language models show remarkable zero-shot capabilities. In this work, we bridge these two worlds by introducing ConceptPose, a framework for object pose estimation that is both training-free and model-free. ConceptPose leverages a vision-language-model (VLM) to create open-vocabulary 3D concept maps, where each point is tagged with a concept vector derived from saliency maps. By establishing robust 3D-3D correspondences across concept maps, our approach allows precise estimation of 6DoF relative pose. Without any object or dataset-specific training, our approach achieves state-of-the-art results on common zero shot relative pose estimation benchmarks, significantly outperforming existing methods by over 62% in ADD(-S) score, including those that utilize extensive dataset-specific training.

</details>


### [17] [SIP: Site in Pieces- A Dataset of Disaggregated Construction-Phase 3D Scans for Semantic Segmentation and Scene Understanding](https://arxiv.org/abs/2512.09062)
*Seongyong Kim,Yong Kwon Cho*

Main category: cs.CV

TL;DR: SIP数据集是针对建筑工地3D感知的真实LiDAR数据集，包含室内外场景，使用地面激光扫描仪采集，具有建筑环境专用分类体系，解决了现有数据集与真实工地条件不匹配的问题。


<details>
  <summary>Details</summary>
Motivation: 现有公共3D感知数据集主要基于密集融合扫描，具有均匀采样和完整可见性，无法反映真实建筑工地的实际约束条件。工地数据通常受安全要求、有限访问和持续作业限制，只能获取孤立单站LiDAR视图，导致径向密度衰减、几何碎片化和视角依赖可见性等特征，这些在现有数据集中代表性不足。

Method: 创建SIP数据集，使用地面激光扫描仪采集室内外场景，采用专门针对建筑环境设计的分类体系进行点级标注：A.建筑环境、B.施工操作、C.工地周边。数据集包含结构组件和细长临时物体（如脚手架、MEP管道、剪式升降机），扫描协议、标注工作流程和质量控制程序为数据集建立了统一基础。

Result: SIP数据集公开可用，配有Git仓库支持，提供可适配的类别配置，便于在现代3D深度学习框架中采用。数据集保留了真实世界传感特性，能够支持稳健的基准测试。

Conclusion: 通过提供保留真实传感特性的现场数据，SIP实现了稳健的基准测试，有助于推进面向建筑的3D视觉任务发展，填补了现有数据集与真实工地条件之间的差距。

Abstract: Accurate 3D scene interpretation in active construction sites is essential for progress monitoring, safety assessment, and digital twin development. LiDAR is widely used in construction because it offers advantages over camera-based systems, performing reliably in cluttered and dynamically changing conditions. Yet most public datasets for 3D perception are derived from densely fused scans with uniform sampling and complete visibility, conditions that do not reflect real construction sites. Field data are often collected as isolated single-station LiDAR views, constrained by safety requirements, limited access, and ongoing operations. These factors lead to radial density decay, fragmented geometry, and view-dependent visibility-characteristics that remain underrepresented in existing datasets. This paper presents SIP, Site in Pieces, a dataset created to reflect the practical constraints of LiDAR acquisition during construction. SIP provides indoor and outdoor scenes captured with a terrestrial LiDAR scanner and annotated at the point level using a taxonomy tailored to construction environments: A. Built Environment, B. Construction Operations, and C. Site Surroundings. The dataset includes both structural components and slender temporary objects such as scaffolding, MEP piping, and scissor lifts, where sparsity caused by occlusion and fragmented geometry make segmentation particularly challenging. The scanning protocol, annotation workflow, and quality control procedures establish a consistent foundation for the dataset. SIP is openly available with a supporting Git repository, offering adaptable class configurations that streamline adoption within modern 3D deep learning frameworks. By providing field data that retain real-world sensing characteristics, SIP enables robust benchmarking and contributes to advancing construction-oriented 3D vision tasks.

</details>


### [18] [KD-OCT: Efficient Knowledge Distillation for Clinical-Grade Retinal OCT Classification](https://arxiv.org/abs/2512.09069)
*Erfan Nourbakhsh,Nasrin Sanjari,Ali Nourbakhsh*

Main category: cs.CV

TL;DR: 提出KD-OCT知识蒸馏框架，将大型ConvNeXtV2-Large教师模型压缩为轻量级EfficientNet-B2学生模型，用于OCT图像中正常、玻璃膜疣和脉络膜新生血管的分类，在保持高诊断性能的同时实现实时部署。


<details>
  <summary>Details</summary>
Motivation: 年龄相关性黄斑变性和脉络膜新生血管相关疾病是全球视力丧失的主要原因，光学相干断层扫描是早期检测和管理的关键工具。然而，像ConvNeXtV2-Large这样的先进深度学习模型在临床部署中面临计算需求大的挑战，需要开发既能保持高诊断性能又能实现实时部署的高效模型。

Method: 提出KD-OCT知识蒸馏框架，使用增强版ConvNeXtV2-Large作为教师模型（采用高级数据增强、随机权重平均和焦点损失），通过实时蒸馏将知识压缩到轻量级EfficientNet-B2学生模型中。采用结合软教师知识转移和硬真实监督的混合损失函数，在Noor眼科医院数据集上进行患者级交叉验证评估。

Result: KD-OCT在效率-准确性平衡方面优于可比的多尺度或特征融合OCT分类器，实现了接近教师模型的性能，同时显著减少了模型大小和推理时间。尽管经过压缩，学生模型仍超越大多数现有框架，便于AMD筛查的边缘部署。

Conclusion: KD-OCT框架成功地将高性能教师模型压缩为轻量级学生模型，在保持高诊断准确性的同时大幅降低计算需求，为AMD筛查的临床实时部署提供了可行解决方案，促进了边缘计算在眼科诊断中的应用。

Abstract: Age-related macular degeneration (AMD) and choroidal neovascularization (CNV)-related conditions are leading causes of vision loss worldwide, with optical coherence tomography (OCT) serving as a cornerstone for early detection and management. However, deploying state-of-the-art deep learning models like ConvNeXtV2-Large in clinical settings is hindered by their computational demands. Therefore, it is desirable to develop efficient models that maintain high diagnostic performance while enabling real-time deployment. In this study, a novel knowledge distillation framework, termed KD-OCT, is proposed to compress a high-performance ConvNeXtV2-Large teacher model, enhanced with advanced augmentations, stochastic weight averaging, and focal loss, into a lightweight EfficientNet-B2 student for classifying normal, drusen, and CNV cases. KD-OCT employs real-time distillation with a combined loss balancing soft teacher knowledge transfer and hard ground-truth supervision. The effectiveness of the proposed method is evaluated on the Noor Eye Hospital (NEH) dataset using patient-level cross-validation. Experimental results demonstrate that KD-OCT outperforms comparable multi-scale or feature-fusion OCT classifiers in efficiency- accuracy balance, achieving near-teacher performance with substantial reductions in model size and inference time. Despite the compression, the student model exceeds most existing frameworks, facilitating edge deployment for AMD screening. Code is available at https://github.com/erfan-nourbakhsh/KD- OCT.

</details>


### [19] [Adaptive Thresholding for Visual Place Recognition using Negative Gaussian Mixture Statistics](https://arxiv.org/abs/2512.09071)
*Nick Trinh,Damian Lyons*

Main category: cs.CV

TL;DR: 该论文提出了一种基于"负"高斯混合统计的自动阈值选择方法，用于视觉地点识别中的匹配决策，解决了手动设置阈值难以适应不同视觉场景的问题。


<details>
  <summary>Details</summary>
Motivation: 视觉地点识别（VPR）是相机建图和导航应用中的重要技术，但同一地点的图像可能因季节变化、天气光照、环境结构变化以及行人车辆等瞬态因素而差异很大。现有方法使用召回率@K和ROC曲线等指标评估图像描述符，但在实际机器人应用中，确定哪些匹配足够好通常简化为手动设置阈值，而手动选择阈值难以适应各种视觉场景。

Method: 通过分析地点的"负"高斯混合统计（表示"非此地"的图像统计信息）来自动选择VPR阈值。该方法利用负样本的统计特性来确定合适的匹配阈值。

Result: 该方法能够为多种图像数据库和图像描述符选择出工作良好的阈值，提高了VPR系统在不同视觉场景下的适应性和鲁棒性。

Conclusion: 基于负高斯混合统计的自动阈值选择方法有效解决了VPR中手动设置阈值的局限性，为机器人视觉导航系统提供了更可靠的匹配决策机制。

Abstract: Visual place recognition (VPR) is an important component technology for camera-based mapping and navigation applications. This is a challenging problem because images of the same place may appear quite different for reasons including seasonal changes, weather illumination, structural changes to the environment, as well as transient pedestrian or vehicle traffic. Papers focusing on generating image descriptors for VPR report their results using metrics such as recall@K and ROC curves. However, for a robot implementation, determining which matches are sufficiently good is often reduced to a manually set threshold. And it is difficult to manually select a threshold that will work for a variety of visual scenarios. This paper addresses the problem of automatically selecting a threshold for VPR by looking at the 'negative' Gaussian mixture statistics for a place - image statistics indicating not this place. We show that this approach can be used to select thresholds that work well for a variety of image databases and image descriptors.

</details>


### [20] [AgentComp: From Agentic Reasoning to Compositional Mastery in Text-to-Image Models](https://arxiv.org/abs/2512.09081)
*Arman Zarei,Jiacheng Pan,Matthew Gwilliam,Soheil Feizi,Zhenheng Yang*

Main category: cs.CV

TL;DR: AgentComp框架通过LLM驱动的智能体自动构建组合性数据集，并采用智能体偏好优化方法微调文生图模型，显著提升组合生成能力，在T2I-CompBench上取得SOTA结果且不损失图像质量。


<details>
  <summary>Details</summary>
Motivation: 当前文生图模型在组合性方面存在局限，难以准确捕捉对象关系、属性绑定和细粒度细节。主要问题是模型没有经过明确训练来区分组合相似的提示和图像，导致输出接近预期描述但在细粒度细节上存在偏差。

Method: AgentComp框架利用配备图像生成、编辑和VQA工具的大语言模型，通过其推理和工具使用能力自主构建组合性数据集。然后采用智能体偏好优化方法微调文生图模型，使模型能更好地区分组合相似的样本。

Result: 在T2I-CompBench等组合性基准测试中取得最先进结果，且不损害图像质量（这是先前方法的常见缺点）。甚至能泛化到未明确训练的其他能力，如文本渲染。

Conclusion: AgentComp通过明确训练模型区分组合变化并增强推理能力，有效解决了文生图模型的组合性问题，在保持图像质量的同时显著提升了组合生成性能。

Abstract: Text-to-image generative models have achieved remarkable visual quality but still struggle with compositionality$-$accurately capturing object relationships, attribute bindings, and fine-grained details in prompts. A key limitation is that models are not explicitly trained to differentiate between compositionally similar prompts and images, resulting in outputs that are close to the intended description yet deviate in fine-grained details. To address this, we propose AgentComp, a framework that explicitly trains models to better differentiate such compositional variations and enhance their reasoning ability. AgentComp leverages the reasoning and tool-use capabilities of large language models equipped with image generation, editing, and VQA tools to autonomously construct compositional datasets. Using these datasets, we apply an agentic preference optimization method to fine-tune text-to-image models, enabling them to better distinguish between compositionally similar samples and resulting in overall stronger compositional generation ability. AgentComp achieves state-of-the-art results on compositionality benchmarks such as T2I-CompBench, without compromising image quality$-$a common drawback in prior approaches$-$and even generalizes to other capabilities not explicitly trained for, such as text rendering.

</details>


### [21] [Explaining the Unseen: Multimodal Vision-Language Reasoning for Situational Awareness in Underground Mining Disasters](https://arxiv.org/abs/2512.09092)
*Mizanur Rahman Jewel,Mohamed Elmahallawy,Sanjay Madria,Samuel Frimpong*

Main category: cs.CV

TL;DR: MDSE是一个用于地下矿井灾难场景的多模态视觉-语言框架，能自动生成详细文本描述，提升灾后环境感知能力


<details>
  <summary>Details</summary>
Motivation: 地下矿井灾难会产生黑暗、灰尘和坍塌，严重影响视觉感知，使人类和传统系统难以获取环境信息，需要一种能自动解释灾后场景的系统来提升应急响应能力

Method: 提出MDSE框架，包含三个创新：1) 上下文感知交叉注意力机制，用于在严重退化条件下对齐视觉和文本特征；2) 分割感知的双路径视觉编码，融合全局和区域特定嵌入；3) 资源高效的基于Transformer的语言模型，以最小计算成本生成表达性描述

Result: 在UMD数据集和相关基准测试中，MDSE显著优于最先进的图像描述模型，能生成更准确、上下文相关的描述，在模糊环境中捕捉关键细节

Conclusion: MDSE通过创新的多模态框架有效解决了地下矿井灾难场景的视觉解释问题，提升了应急响应中的环境感知能力，并创建了首个真实地下灾难场景数据集

Abstract: Underground mining disasters produce pervasive darkness, dust, and collapses that obscure vision and make situational awareness difficult for humans and conventional systems. To address this, we propose MDSE, Multimodal Disaster Situation Explainer, a novel vision-language framework that automatically generates detailed textual explanations of post-disaster underground scenes. MDSE has three-fold innovations: (i) Context-Aware Cross-Attention for robust alignment of visual and textual features even under severe degradation; (ii) Segmentation-aware dual pathway visual encoding that fuses global and region-specific embeddings; and (iii) Resource-Efficient Transformer-Based Language Model for expressive caption generation with minimal compute cost. To support this task, we present the Underground Mine Disaster (UMD) dataset--the first image-caption corpus of real underground disaster scenes--enabling rigorous training and evaluation. Extensive experiments on UMD and related benchmarks show that MDSE substantially outperforms state-of-the-art captioning models, producing more accurate and contextually relevant descriptions that capture crucial details in obscured environments, improving situational awareness for underground emergency response. The code is at https://github.com/mizanJewel/Multimodal-Disaster-Situation-Explainer.

</details>


### [22] [Food Image Generation on Multi-Noun Categories](https://arxiv.org/abs/2512.09095)
*Xinyue Pan,Yuhao Chen,Jiangpeng He,Fengqing Zhu*

Main category: cs.CV

TL;DR: 提出FoCULR方法解决多名词食物类别图像生成问题，通过融入食物领域知识和早期引入核心概念，改善生成模型对复合名词语义的理解和空间布局


<details>
  <summary>Details</summary>
Motivation: 多名词食物类别（如"egg noodle"）在图像生成中容易导致误解，生成模型会将复合名词错误理解为分离的实体，产生不正确的成分或对象。这在真实世界数据集中很常见，占UEC-256等基准测试的大部分条目

Method: 提出FoCULR（Food Category Understanding and Layout Refinement）方法，通过融入食物领域知识和在生成过程早期引入核心概念，改善文本编码器对多名词类别的理解，纠正多名词关系的误解，优化空间布局

Result: 实验结果表明，这些技术的集成提高了食物领域的图像生成性能

Conclusion: FoCULR方法通过融入领域知识和改进多名词理解，有效解决了食物图像生成中复合名词类别带来的挑战

Abstract: Generating realistic food images for categories with multiple nouns is surprisingly challenging. For instance, the prompt "egg noodle" may result in images that incorrectly contain both eggs and noodles as separate entities. Multi-noun food categories are common in real-world datasets and account for a large portion of entries in benchmarks such as UEC-256. These compound names often cause generative models to misinterpret the semantics, producing unintended ingredients or objects. This is due to insufficient multi-noun category related knowledge in the text encoder and misinterpretation of multi-noun relationships, leading to incorrect spatial layouts. To overcome these challenges, we propose FoCULR (Food Category Understanding and Layout Refinement) which incorporates food domain knowledge and introduces core concepts early in the generation process. Experimental results demonstrate that the integration of these techniques improves image generation performance in the food domain.

</details>


### [23] [GimbalDiffusion: Gravity-Aware Camera Control for Video Generation](https://arxiv.org/abs/2512.09112)
*Frédéric Fortier-Chouinard,Yannick Hold-Geoffroy,Valentin Deschaintre,Matheus Gadelha,Jean-François Lalonde*

Main category: cs.CV

TL;DR: GimbalDiffusion是一个文本到视频生成框架，通过物理世界坐标和重力参考实现精确的相机控制，解决了现有方法在相机运动控制方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有文本到视频生成方法在相机运动控制方面存在局限，通常使用相对或模糊的表示来编码相机轨迹，缺乏明确的几何控制能力。需要一种能够基于物理世界坐标实现精确相机控制的方法。

Method: 1. 使用重力作为全局参考，在绝对坐标系中定义相机轨迹；2. 利用全景360度视频构建多样化的相机轨迹；3. 引入空俯仰条件标注策略，减少模型对文本内容的依赖；4. 重新平衡SpatialVID-HQ数据集建立相机感知视频生成基准。

Result: 该方法实现了精确、可解释的相机参数控制，无需初始参考帧。能够生成超出传统视频数据中主要直线、前向轨迹的多样化相机运动，提高了文本到视频模型的相机控制能力和鲁棒性。

Conclusion: GimbalDiffusion框架通过基于物理世界坐标的相机控制、全景视频数据利用和空俯仰条件标注等创新，显著提升了文本到视频生成中相机控制的精确性和鲁棒性，为生成框架内的重力对齐相机操作提供了有效解决方案。

Abstract: Recent progress in text-to-video generation has achieved remarkable realism, yet fine-grained control over camera motion and orientation remains elusive. Existing approaches typically encode camera trajectories through relative or ambiguous representations, limiting explicit geometric control. We introduce GimbalDiffusion, a framework that enables camera control grounded in physical-world coordinates, using gravity as a global reference. Instead of describing motion relative to previous frames, our method defines camera trajectories in an absolute coordinate system, allowing precise and interpretable control over camera parameters without requiring an initial reference frame. We leverage panoramic 360-degree videos to construct a wide variety of camera trajectories, well beyond the predominantly straight, forward-facing trajectories seen in conventional video data. To further enhance camera guidance, we introduce null-pitch conditioning, an annotation strategy that reduces the model's reliance on text content when conflicting with camera specifications (e.g., generating grass while the camera points towards the sky). Finally, we establish a benchmark for camera-aware video generation by rebalancing SpatialVID-HQ for comprehensive evaluation under wide camera pitch variation. Together, these contributions advance the controllability and robustness of text-to-video models, enabling precise, gravity-aligned camera manipulation within generative frameworks.

</details>


### [24] [Integrated Pipeline for Coronary Angiography With Automated Lesion Profiling, Virtual Stenting, and 100-Vessel FFR Validation](https://arxiv.org/abs/2512.09134)
*Georgy Kopanitsa,Oleg Metsker,Alexey Yakovlev*

Main category: cs.CV

TL;DR: AngioAI-QFR是一个端到端的血管造影分析系统，结合深度学习检测狭窄、分割管腔、提取中心线和直径，提供相对血流容量分析和虚拟支架置入，自动计算QFR，在100条血管中与FFR相关性高（r=0.89），诊断性能优异（AUC=0.93）。


<details>
  <summary>Details</summary>
Motivation: 冠状动脉造影是评估冠心病的主要工具，但视觉评估狭窄存在变异性且与缺血相关性有限。基于导丝的FFR改善了病变选择但未系统使用。现有的血管造影衍生指标如QFR虽然提供无导丝生理学评估，但许多工具工作流程繁琐，且与自动化解剖分析和虚拟PCI规划分离。

Method: 开发了AngioAI-QFR端到端血管造影分析系统，结合深度学习狭窄检测、管腔分割、中心线和直径提取、每毫米相对血流容量分析，以及虚拟支架置入并自动重新计算血管造影衍生的QFR。在100条连续血管中使用侵入性FFR作为参考进行评估。

Result: 狭窄检测精度0.97，管腔分割Dice系数0.78。AngioAI-QFR与FFR强相关（r=0.89，MAE 0.045）。检测FFR≤0.80的AUC为0.93，灵敏度0.88，特异度0.86。93%的血管完全自动完成分析，中位结果时间41秒。RFC分析能区分局灶性和弥漫性血流容量损失，虚拟支架预测局灶性病变的QFR改善更大。

Conclusion: AngioAI-QFR提供了一个实用、近实时的分析流程，统一了计算机视觉、功能分析和虚拟PCI，实现了自动化的血管造影衍生生理学评估。

Abstract: Coronary angiography is the main tool for assessing coronary artery disease, but visual grading of stenosis is variable and only moderately related to ischaemia. Wire based fractional flow reserve (FFR) improves lesion selection but is not used systematically. Angiography derived indices such as quantitative flow ratio (QFR) offer wire free physiology, yet many tools are workflow intensive and separate from automated anatomy analysis and virtual PCI planning. We developed AngioAI-QFR, an end to end angiography only pipeline combining deep learning stenosis detection, lumen segmentation, centreline and diameter extraction, per millimetre Relative Flow Capacity profiling, and virtual stenting with automatic recomputation of angiography derived QFR. The system was evaluated in 100 consecutive vessels with invasive FFR as reference. Primary endpoints were agreement with FFR (correlation, mean absolute error) and diagnostic performance for FFR <= 0.80. On held out frames, stenosis detection achieved precision 0.97 and lumen segmentation Dice 0.78. Across 100 vessels, AngioAI-QFR correlated strongly with FFR (r = 0.89, MAE 0.045). The AUC for detecting FFR <= 0.80 was 0.93, with sensitivity 0.88 and specificity 0.86. The pipeline completed fully automatically in 93 percent of vessels, with median time to result 41 s. RFC profiling distinguished focal from diffuse capacity loss, and virtual stenting predicted larger QFR gain in focal than in diffuse disease. AngioAI-QFR provides a practical, near real time pipeline that unifies computer vision, functional profiling, and virtual PCI with automated angiography derived physiology.

</details>


### [25] [GTAvatar: Bridging Gaussian Splatting and Texture Mapping for Relightable and Editable Gaussian Avatars](https://arxiv.org/abs/2512.09162)
*Kelian Baert,Mae Younes,Francois Bourel,Marc Christie,Adnane Boukhayma*

Main category: cs.CV

TL;DR: 该论文提出了一种结合2D高斯泼溅精度与UV纹理映射直观性的方法，用于从单目视频重建可编辑的头像材质纹理，支持重光照和外观编辑。


<details>
  <summary>Details</summary>
Motivation: 高斯泼溅方法虽然能准确重建逼真头像，但缺乏传统三角网格方法的直观可编辑性。现有方法在编辑性和纹理映射方面存在不足，需要一种既能保持重建精度又能提供直观编辑控制的方法。

Method: 将每个规范高斯基元的局部坐标系嵌入到模板网格UV空间的补丁中，从单目视频在传统UV域上重建连续可编辑的材质头部纹理。采用高效的基于物理的反射模型实现重光照和内在材质贴图编辑。

Result: 通过与最先进方法的广泛比较，证明了重建的准确性、重光照结果的质量，以及通过纹理映射提供直观控制修改头像外观和几何形状的能力，无需额外优化。

Conclusion: 该方法成功结合了高斯泼溅的精度和UV纹理映射的直观性，实现了从单目视频重建可编辑头像材质纹理，为头像编辑提供了新的解决方案。

Abstract: Recent advancements in Gaussian Splatting have enabled increasingly accurate reconstruction of photorealistic head avatars, opening the door to numerous applications in visual effects, videoconferencing, and virtual reality. This, however, comes with the lack of intuitive editability offered by traditional triangle mesh-based methods. In contrast, we propose a method that combines the accuracy and fidelity of 2D Gaussian Splatting with the intuitiveness of UV texture mapping. By embedding each canonical Gaussian primitive's local frame into a patch in the UV space of a template mesh in a computationally efficient manner, we reconstruct continuous editable material head textures from a single monocular video on a conventional UV domain. Furthermore, we leverage an efficient physically based reflectance model to enable relighting and editing of these intrinsic material maps. Through extensive comparisons with state-of-the-art methods, we demonstrate the accuracy of our reconstructions, the quality of our relighting results, and the ability to provide intuitive controls for modifying an avatar's appearance and geometry via texture mapping without additional optimization.

</details>


### [26] [WonderZoom: Multi-Scale 3D World Generation](https://arxiv.org/abs/2512.09164)
*Jin Cao,Hong-Xing Yu,Jiajun Wu*

Main category: cs.CV

TL;DR: WonderZoom提出了一种从单张图像生成多尺度3D场景的新方法，解决了现有3D生成模型只能合成单一尺度内容的限制。


<details>
  <summary>Details</summary>
Motivation: 现有3D世界生成模型局限于单一尺度合成，无法在不同粒度上生成连贯的场景内容。根本挑战在于缺乏能够生成和渲染空间尺寸差异巨大的内容的尺度感知3D表示。

Method: 通过两个关键创新：(1) 尺度自适应高斯面元用于多尺度3D场景的生成和实时渲染；(2) 渐进细节合成器迭代生成更精细尺度的3D内容。

Result: 实验表明WonderZoom在质量和对齐性上显著优于最先进的视频和3D模型，能够从单张图像创建多尺度3D世界。用户可"放大"3D区域并自回归合成从景观到微观特征的精细细节。

Conclusion: WonderZoom实现了从单张图像生成多尺度3D场景的能力，突破了现有3D生成模型的尺度限制，为多尺度3D世界创建提供了新方法。

Abstract: We present WonderZoom, a novel approach to generating 3D scenes with contents across multiple spatial scales from a single image. Existing 3D world generation models remain limited to single-scale synthesis and cannot produce coherent scene contents at varying granularities. The fundamental challenge is the lack of a scale-aware 3D representation capable of generating and rendering content with largely different spatial sizes. WonderZoom addresses this through two key innovations: (1) scale-adaptive Gaussian surfels for generating and real-time rendering of multi-scale 3D scenes, and (2) a progressive detail synthesizer that iteratively generates finer-scale 3D contents. Our approach enables users to "zoom into" a 3D region and auto-regressively synthesize previously non-existent fine details from landscapes to microscopic features. Experiments demonstrate that WonderZoom significantly outperforms state-of-the-art video and 3D models in both quality and alignment, enabling multi-scale 3D world creation from a single image. We show video results and an interactive viewer of generated multi-scale 3D worlds in https://wonderzoom.github.io/

</details>


### [27] [Learning Patient-Specific Disease Dynamics with Latent Flow Matching for Longitudinal Imaging Generation](https://arxiv.org/abs/2512.09185)
*Hao Chen,Rui Yin,Yifan Chen,Qi Chen,Chao Li*

Main category: cs.CV

TL;DR: 该论文提出Δ-LFM框架，通过流匹配方法建模疾病进展的连续单调动态，并学习患者特定的潜在对齐，使疾病轨迹沿特定轴单调增加，从而创建一致且语义有意义的潜在空间。


<details>
  <summary>Details</summary>
Motivation: 疾病进展建模对早期诊断和个性化治疗至关重要。现有生成方法存在关键不匹配：疾病动态本质上是连续单调的，但潜在表示往往分散且缺乏语义结构，扩散模型通过随机去噪过程破坏了连续性。

Method: 提出Δ-LFM框架：1) 将疾病动态视为速度场，利用流匹配对齐患者数据的时间演化；2) 学习患者特定的潜在对齐，强制患者轨迹沿特定轴排列，其幅度随疾病严重程度单调增加。

Result: 在三个纵向MRI基准测试中，Δ-LFM表现出强大的实证性能，更重要的是提供了一个解释和可视化疾病动态的新框架。

Conclusion: Δ-LFM通过流匹配和患者特定潜在对齐，成功建模了疾病进展的连续单调动态，创建了语义有意义的潜在空间，为疾病动态解释和可视化提供了新框架。

Abstract: Understanding disease progression is a central clinical challenge with direct implications for early diagnosis and personalized treatment. While recent generative approaches have attempted to model progression, key mismatches remain: disease dynamics are inherently continuous and monotonic, yet latent representations are often scattered, lacking semantic structure, and diffusion-based models disrupt continuity with random denoising process. In this work, we propose to treat the disease dynamic as a velocity field and leverage Flow Matching (FM) to align the temporal evolution of patient data. Unlike prior methods, it captures the intrinsic dynamic of disease, making the progression more interpretable. However, a key challenge remains: in latent space, Auto-Encoders (AEs) do not guarantee alignment across patients or correlation with clinical-severity indicators (e.g., age and disease conditions). To address this, we propose to learn patient-specific latent alignment, which enforces patient trajectories to lie along a specific axis, with magnitude increasing monotonically with disease severity. This leads to a consistent and semantically meaningful latent space. Together, we present $Δ$-LFM, a framework for modeling patient-specific latent progression with flow matching. Across three longitudinal MRI benchmarks, $Δ$-LFM demonstrates strong empirical performance and, more importantly, offers a new framework for interpreting and visualizing disease dynamics.

</details>


### [28] [Efficient Feature Compression for Machines with Global Statistics Preservation](https://arxiv.org/abs/2512.09235)
*Md Eimran Hossain Eimon,Hyomin Choi,Fabien Racapé,Mateen Ulhaq,Velibor Adzic,Hari Kalva,Borko Furht*

Main category: cs.CV

TL;DR: 本文提出了一种用于AI模型分拆推理的特征数据压缩方法，通过Z-score归一化有效恢复压缩特征，在MPEG FCM编解码标准中实现比特率降低和任务精度提升


<details>
  <summary>Details</summary>
Motivation: 在AI模型分拆推理中，需要在两部分之间传输中间特征数据，有效的特征数据压缩变得至关重要。现有方法在压缩效率和任务精度方面存在改进空间。

Method: 采用Z-score归一化方法在解码端高效恢复压缩的特征数据，并将该方法集成到MPEG正在开发的FCM编解码标准中，替代现有的缩放方法。同时提出了一种简化方法以进一步减少特定情况下的开销。

Result: 实验表明，该方法平均降低17.09%的比特率，在目标跟踪任务中最高可降低65.69%的比特率，且不牺牲任务精度。同时减少了开销比特并提高了端任务精度。

Conclusion: 提出的基于Z-score归一化的特征数据压缩方法在MPEG FCM标准中表现优异，显著降低了比特率开销，同时保持或提高了AI任务的准确性，为分拆推理提供了有效的特征压缩解决方案。

Abstract: The split-inference paradigm divides an artificial intelligence (AI) model into two parts. This necessitates the transfer of intermediate feature data between the two halves. Here, effective compression of the feature data becomes vital. In this paper, we employ Z-score normalization to efficiently recover the compressed feature data at the decoder side. To examine the efficacy of our method, the proposed method is integrated into the latest Feature Coding for Machines (FCM) codec standard under development by the Moving Picture Experts Group (MPEG). Our method supersedes the existing scaling method used by the current standard under development. It both reduces the overhead bits and improves the end-task accuracy. To further reduce the overhead in certain circumstances, we also propose a simplified method. Experiments show that using our proposed method shows 17.09% reduction in bitrate on average across different tasks and up to 65.69% for object tracking without sacrificing the task accuracy.

</details>


### [29] [CS3D: An Efficient Facial Expression Recognition via Event Vision](https://arxiv.org/abs/2512.09592)
*Zhe Wang,Qijin Song,Yucen Peng,Weibang Bai*

Main category: cs.CV

TL;DR: CS3D框架通过分解3D卷积降低计算复杂度和能耗，结合软脉冲神经元和时空注意力机制提高表情识别精度，在多个数据集上优于RNN、Transformer和C3D方法，能耗仅为原始C3D的21.97%


<details>
  <summary>Details</summary>
Motivation: 事件相机在表情识别中具有高时间分辨率、低延迟等优势，但传统深度学习模型能耗高，难以部署在边缘设备上，限制了事件视觉方法在实际应用中的推广

Method: 提出CS3D框架：1）分解3D卷积方法降低计算复杂度和能耗；2）使用软脉冲神经元增强信息保留能力；3）引入时空注意力机制提升表情检测精度

Result: 在多个数据集上，CS3D方法比RNN、Transformer和C3D架构获得更高准确率，同时能耗仅为原始C3D方法的21.97%

Conclusion: CS3D框架有效解决了事件相机表情识别中的能耗问题，在保持高精度的同时显著降低计算成本，适合部署在边缘计算设备上，为人机交互应用提供实用解决方案

Abstract: Responsive and accurate facial expression recognition is crucial to human-robot interaction for daily service robots. Nowadays, event cameras are becoming more widely adopted as they surpass RGB cameras in capturing facial expression changes due to their high temporal resolution, low latency, computational efficiency, and robustness in low-light conditions. Despite these advantages, event-based approaches still encounter practical challenges, particularly in adopting mainstream deep learning models. Traditional deep learning methods for facial expression analysis are energy-intensive, making them difficult to deploy on edge computing devices and thereby increasing costs, especially for high-frequency, dynamic, event vision-based approaches. To address this challenging issue, we proposed the CS3D framework by decomposing the Convolutional 3D method to reduce the computational complexity and energy consumption. Additionally, by utilizing soft spiking neurons and a spatial-temporal attention mechanism, the ability to retain information is enhanced, thus improving the accuracy of facial expression detection. Experimental results indicate that our proposed CS3D method attains higher accuracy on multiple datasets compared to architectures such as the RNN, Transformer, and C3D, while the energy consumption of the CS3D method is just 21.97\% of the original C3D required on the same device.

</details>


### [30] [OmniPSD: Layered PSD Generation with Diffusion Transformer](https://arxiv.org/abs/2512.09247)
*Cheng Liu,Yiren Song,Haofan Wang,Mike Zheng Shou*

Main category: cs.CV

TL;DR: OmniPSD是一个基于Flux生态系统的统一扩散框架，能够通过上下文学习实现文本到PSD生成和图像到PSD分解，解决了生成具有透明通道的分层PSD文件的挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散模型在图像生成和编辑方面取得了显著进展，但生成或重建具有透明alpha通道的分层PSD文件仍然极具挑战性。现有的方法难以处理这种复杂的结构化透明图层。

Method: OmniPSD采用统一扩散框架：1）文本到PSD生成：将多个目标图层空间排列到单个画布上，通过空间注意力学习它们的组合关系；2）图像到PSD分解：执行迭代的上下文编辑，逐步提取和擦除文本和前景组件；3）使用RGBA-VAE作为辅助表示模块来保持透明度而不影响结构学习。

Result: 在新建的RGBA分层数据集上的广泛实验表明，OmniPSD实现了高保真度生成、结构一致性和透明度感知，为分层设计生成和分解提供了新范式。

Conclusion: OmniPSD通过扩散变换器为分层设计生成和分解提供了新的解决方案，能够生成语义连贯、层次结构分明的可编辑PSD图层，在保持透明度的同时实现了高质量的结构化生成。

Abstract: Recent advances in diffusion models have greatly improved image generation and editing, yet generating or reconstructing layered PSD files with transparent alpha channels remains highly challenging. We propose OmniPSD, a unified diffusion framework built upon the Flux ecosystem that enables both text-to-PSD generation and image-to-PSD decomposition through in-context learning. For text-to-PSD generation, OmniPSD arranges multiple target layers spatially into a single canvas and learns their compositional relationships through spatial attention, producing semantically coherent and hierarchically structured layers. For image-to-PSD decomposition, it performs iterative in-context editing, progressively extracting and erasing textual and foreground components to reconstruct editable PSD layers from a single flattened image. An RGBA-VAE is employed as an auxiliary representation module to preserve transparency without affecting structure learning. Extensive experiments on our new RGBA-layered dataset demonstrate that OmniPSD achieves high-fidelity generation, structural consistency, and transparency awareness, offering a new paradigm for layered design generation and decomposition with diffusion transformers.

</details>


### [31] [GLACIA: Instance-Aware Positional Reasoning for Glacial Lake Segmentation via Multimodal Large Language Model](https://arxiv.org/abs/2512.09251)
*Lalit Maurya,Saurabh Kaushik,Beth Tellman*

Main category: cs.CV

TL;DR: GLACIA是一个结合大语言模型与分割能力的冰川湖监测框架，不仅能生成准确的分割掩码，还能提供空间推理输出，支持自然语言交互，提升灾害预防的直观性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有基于CNN和ViT的冰川湖分割方法局限于像素级预测，缺乏高层全局场景语义和人类可解释的推理能力，难以支持直观的灾害预防和决策制定。

Method: 提出GLACIA框架，首次将大语言模型与分割能力结合，构建Glacial Lake Position Reasoning数据集管道，提供多样化的空间基础问答对，解决遥感数据中实例感知位置推理数据的缺乏问题。

Result: GLACIA在mIoU指标上达到87.30，超越了基于CNN的方法（78.55-79.01）、ViT方法（69.27-81.75）、地理基础模型（76.37-87.10）和基于推理的分割方法（60.12-75.66）。

Conclusion: GLACIA通过自然语言交互支持更高效和可解释的决策制定，在快速变化的冰川环境中为直观的灾害准备和明智的政策制定提供了有效工具。

Abstract: Glacial lake monitoring bears great significance in mitigating the anticipated risk of Glacial Lake Outburst Floods. However, existing segmentation methods based on convolutional neural networks (CNNs) and Vision Transformers (ViTs), remain constrained to pixel-level predictions, lacking high-level global scene semantics and human-interpretable reasoning. To address this, we introduce GLACIA (\textbf{G}lacial \textbf{LA}ke segmentation with \textbf{C}ontextual \textbf{I}nstance \textbf{A}wareness), the first framework that integrates large language models with segmentation capabilities to produce both accurate segmentation masks and corresponding spatial reasoning outputs. We construct the Glacial Lake Position Reasoning (GLake-Pos) dataset pipeline, which provides diverse, spatially grounded question-answer pairs designed to overcome the lack of instance-aware positional reasoning data in remote sensing. Comparative evaluation demonstrate that GLACIA (mIoU: 87.30) surpasses state-of-the-art method based on CNNs (mIoU: 78.55 - 79.01), ViTs (mIoU: 69.27 - 81.75), Geo-foundation models (mIoU: 76.37 - 87.10), and reasoning based segmentation methods (mIoU: 60.12 - 75.66). Our approach enables intuitive disaster preparedness and informed policy-making in the context of rapidly changing glacial environments by facilitating natural language interaction, thereby supporting more efficient and interpretable decision-making. The code is released on https://github.com/lalitmaurya47/GLACIA

</details>


### [32] [Rethinking Chain-of-Thought Reasoning for Videos](https://arxiv.org/abs/2512.09616)
*Yiwu Zhong,Zi-Yuan Hu,Yin Li,Liwei Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种高效的视频推理框架，通过压缩视觉标记和生成简洁推理轨迹，在保持竞争力的同时大幅提升推理效率，挑战了传统长链思维的必要性。


<details>
  <summary>Details</summary>
Motivation: 现有基于链式思维（CoT）的多模态大语言模型在视频推理中通常依赖冗长的推理链和大量视觉标记，导致效率低下。作者通过基准研究发现，简洁推理结合少量视觉标记可能足以实现有效的视频推理。

Method: 设计并验证了一个高效的后训练和推理框架，使视频MLLM能够在压缩的视觉标记上操作，并在回答问题前生成简洁的推理轨迹。该框架不需要人工CoT标注或监督微调。

Result: 生成的模型显著提高了推理效率，在多样化基准测试中表现出竞争力，避免了传统长链推理的依赖。

Conclusion: 研究表明，类似人类的冗长链式思维推理对于通用视频推理可能并非必要，简洁推理既能保持有效性又能提高效率。

Abstract: Chain-of-thought (CoT) reasoning has been highly successful in solving complex tasks in natural language processing, and recent multimodal large language models (MLLMs) have extended this paradigm to video reasoning. However, these models typically build on lengthy reasoning chains and large numbers of input visual tokens. Motivated by empirical observations from our benchmark study, we hypothesize that concise reasoning combined with a reduced set of visual tokens can be sufficient for effective video reasoning. To evaluate this hypothesis, we design and validate an efficient post-training and inference framework that enhances a video MLLM's reasoning capability. Our framework enables models to operate on compressed visual tokens and generate brief reasoning traces prior to answering. The resulting models achieve substantially improved inference efficiency, deliver competitive performance across diverse benchmarks, and avoid reliance on manual CoT annotations or supervised fine-tuning. Collectively, our results suggest that long, human-like CoT reasoning may not be necessary for general video reasoning, and that concise reasoning can be both effective and efficient. Our code will be released at https://github.com/LaVi-Lab/Rethink_CoT_Video.

</details>


### [33] [ROI-Packing: Efficient Region-Based Compression for Machine Vision](https://arxiv.org/abs/2512.09258)
*Md Eimran Hossain Eimon,Alena Krause,Ashan Perera,Juan Merlos,Hari Kalva,Velibor Adzic,Borko Furht*

Main category: cs.CV

TL;DR: ROI-Packing是一种针对机器视觉任务的高效图像压缩方法，通过优先处理对任务精度关键的兴趣区域并高效打包，同时丢弃不相关数据，无需重新训练或微调下游任务模型即可实现显著压缩效率。


<details>
  <summary>Details</summary>
Motivation: 传统图像压缩方法通常针对人类视觉优化，而机器视觉任务对图像内容的需求不同。现有方法要么需要重新训练下游模型，要么压缩效率有限。需要一种无需模型调整、能针对机器视觉任务优化的高效压缩方法。

Method: ROI-Packing方法识别对下游任务精度关键的兴趣区域，优先处理这些区域并进行高效打包，同时丢弃对任务不重要的数据。该方法不需要重新训练或微调下游任务模型，直接对图像进行压缩处理。

Result: 在五个数据集和两个流行任务（目标检测和实例分割）上的综合评估显示：相比MPEG标准化的最新VVC编解码器，ROI-Packing在保持任务精度不变的情况下实现了高达44.10%的比特率降低，同时在相同比特率下任务精度提高了8.88%。

Conclusion: ROI-Packing是一种高效且实用的机器视觉图像压缩方法，通过针对任务关键区域进行优化，在无需修改下游模型的情况下实现了显著的压缩效率提升和精度改进，为机器视觉系统的实际部署提供了有价值的解决方案。

Abstract: This paper introduces ROI-Packing, an efficient image compression method tailored specifically for machine vision. By prioritizing regions of interest (ROI) critical to end-task accuracy and packing them efficiently while discarding less relevant data, ROI-Packing achieves significant compression efficiency without requiring retraining or fine-tuning of end-task models. Comprehensive evaluations across five datasets and two popular tasks-object detection and instance segmentation-demonstrate up to a 44.10% reduction in bitrate without compromising end-task accuracy, along with an 8.88 % improvement in accuracy at the same bitrate compared to the state-of-the-art Versatile Video Coding (VVC) codec standardized by the Moving Picture Experts Group (MPEG).

</details>


### [34] [MedForget: Hierarchy-Aware Multimodal Unlearning Testbed for Medical AI](https://arxiv.org/abs/2512.09867)
*Fengli Wu,Vaidehi Patil,Jaehong Yoon,Yue Zhang,Mohit Bansal*

Main category: cs.CV

TL;DR: MedForget是一个用于医疗AI系统的层次感知多模态遗忘测试平台，旨在评估预训练多模态大语言模型在医疗数据上的选择性遗忘效果，以解决医疗隐私法规下的合规挑战。


<details>
  <summary>Details</summary>
Motivation: 医疗AI系统中部署的预训练多模态大语言模型使用敏感患者数据进行训练，这引发了HIPAA和GDPR等法规下的隐私和合规挑战，特别是"被遗忘权"的要求。现有的遗忘方法在复杂医疗环境中的有效性尚未得到充分探索。

Method: 提出了MedForget测试平台，将医院数据建模为嵌套层次结构（机构->患者->研究->部分），包含3840个多模态实例（图像、问题、答案）。使用四种最先进的遗忘方法在三个任务（生成、分类、完形填空）上进行实验，并引入重建攻击来测试遗忘效果。

Result: 实验表明现有方法难以实现完全、层次感知的遗忘而不降低诊断性能。粗粒度遗忘的模型对重建攻击有较强抵抗力，而细粒度遗忘的模型则容易受到重建攻击的影响。

Conclusion: MedForget为构建合规医疗AI系统提供了一个实用的、符合HIPAA标准的测试平台，揭示了医疗数据遗忘的复杂性，并为开发更有效的遗忘方法提供了基础。

Abstract: Pretrained Multimodal Large Language Models (MLLMs) are increasingly deployed in medical AI systems for clinical reasoning, diagnosis support, and report generation. However, their training on sensitive patient data raises critical privacy and compliance challenges under regulations such as HIPAA and GDPR, which enforce the "right to be forgotten". Unlearning, the process of tuning models to selectively remove the influence of specific training data points, offers a potential solution, yet its effectiveness in complex medical settings remains underexplored. To systematically study this, we introduce MedForget, a Hierarchy-Aware Multimodal Unlearning Testbed with explicit retain and forget splits and evaluation sets containing rephrased variants. MedForget models hospital data as a nested hierarchy (Institution -> Patient -> Study -> Section), enabling fine-grained assessment across eight organizational levels. The benchmark contains 3840 multimodal (image, question, answer) instances, each hierarchy level having a dedicated unlearning target, reflecting distinct unlearning challenges. Experiments with four SOTA unlearning methods on three tasks (generation, classification, cloze) show that existing methods struggle to achieve complete, hierarchy-aware forgetting without reducing diagnostic performance. To test whether unlearning truly deletes hierarchical pathways, we introduce a reconstruction attack that progressively adds hierarchical level context to prompts. Models unlearned at a coarse granularity show strong resistance, while fine-grained unlearning leaves models vulnerable to such reconstruction. MedForget provides a practical, HIPAA-aligned testbed for building compliant medical AI systems.

</details>


### [35] [MoRel: Long-Range Flicker-Free 4D Motion Modeling via Anchor Relay-based Bidirectional Blending with Hierarchical Densification](https://arxiv.org/abs/2512.09270)
*Sangwoon Kwak,Weeyoung Kwon,Jun Young Jeong,Geonho Kim,Won-Sik Cheong,Jihyong Oh*

Main category: cs.CV

TL;DR: MoRel提出了一种基于锚点中继双向混合的4D高斯溅射框架，用于高效建模长时程动态场景，解决了内存爆炸、时间闪烁和遮挡处理等问题。


<details>
  <summary>Details</summary>
Motivation: 现有4D高斯溅射方法在处理长时程动态视频时面临内存爆炸、时间闪烁和遮挡处理失败等挑战，需要一种能保持时间一致性且内存高效的建模方法。

Method: 提出锚点中继双向混合机制，在关键帧时间索引处渐进构建局部规范锚点空间，在锚点级别建模帧间变形，通过可学习不透明度控制自适应混合双向变形。还引入特征方差引导的分层致密化方案。

Result: 构建了SelfCap_LR长时程4D运动数据集，MoRel实现了时间一致、无闪烁的长时程4D重建，同时保持有界内存使用，在动态高斯表示中展示了可扩展性和效率。

Conclusion: MoRel通过锚点中继双向混合机制有效解决了长时程动态场景建模中的内存和时间一致性问题，为动态高斯表示提供了可扩展且高效的解决方案。

Abstract: Recent advances in 4D Gaussian Splatting (4DGS) have extended the high-speed rendering capability of 3D Gaussian Splatting (3DGS) into the temporal domain, enabling real-time rendering of dynamic scenes. However, one of the major remaining challenges lies in modeling long-range motion-contained dynamic videos, where a naive extension of existing methods leads to severe memory explosion, temporal flickering, and failure to handle appearing or disappearing occlusions over time. To address these challenges, we propose a novel 4DGS framework characterized by an Anchor Relay-based Bidirectional Blending (ARBB) mechanism, named MoRel, which enables temporally consistent and memory-efficient modeling of long-range dynamic scenes. Our method progressively constructs locally canonical anchor spaces at key-frame time index and models inter-frame deformations at the anchor level, enhancing temporal coherence. By learning bidirectional deformations between KfA and adaptively blending them through learnable opacity control, our approach mitigates temporal discontinuities and flickering artifacts. We further introduce a Feature-variance-guided Hierarchical Densification (FHD) scheme that effectively densifies KfA's while keeping rendering quality, based on an assigned level of feature-variance. To effectively evaluate our model's capability to handle real-world long-range 4D motion, we newly compose long-range 4D motion-contained dataset, called SelfCap$_{\text{LR}}$. It has larger average dynamic motion magnitude, captured at spatially wider spaces, compared to previous dynamic video datasets. Overall, our MoRel achieves temporally coherent and flicker-free long-range 4D reconstruction while maintaining bounded memory usage, demonstrating both scalability and efficiency in dynamic Gaussian-based representations.

</details>


### [36] [LongT2IBench: A Benchmark for Evaluating Long Text-to-Image Generation with Graph-structured Annotations](https://arxiv.org/abs/2512.09271)
*Zhichao Yang,Tianjiao Gu,Jianjie Wang,Feiyu Lin,Xiangfei Sheng,Pengfei Chen,Leida Li*

Main category: cs.CV

TL;DR: 本文提出了LongT2IBench，一个包含14K长文本-图像对和图结构人工标注的数据集，用于评估长文本到图像生成的对齐质量，并开发了LongT2IExpert评估器，通过层次化对齐思维链指令调优实现定量评分和结构化解释。


<details>
  <summary>Details</summary>
Motivation: 随着长文本到图像生成技术的普及，迫切需要自动且可解释的模型来评估长提示场景下的图文对齐质量。现有T2I对齐基准主要关注短提示场景，仅提供MOS或Likert量表标注，限制了长T2I评估器的发展，特别是在对齐可解释性方面。

Method: 1) 构建LongT2IBench数据集：包含14K长文本-图像对，采用Generate-Refine-Qualify标注协议将长提示转换为包含实体、属性和关系的文本图结构；2) 提出LongT2IExpert评估器：通过层次化对齐思维链指令调优多模态大语言模型，使其能够提供定量评分和结构化解释。

Result: LongT2IBench提供了细粒度的图文对齐标注，可转换为对齐分数和解释。LongT2IExpert在广泛的实验和比较中表现出色，在长文本到图像生成的对齐评估和解释方面具有优越性。

Conclusion: 该研究填补了长文本到图像生成评估的空白，通过图结构标注和层次化对齐思维链方法，实现了更准确、可解释的长T2I对齐评估，为相关模型开发提供了重要基准和工具。

Abstract: The increasing popularity of long Text-to-Image (T2I) generation has created an urgent need for automatic and interpretable models that can evaluate the image-text alignment in long prompt scenarios. However, the existing T2I alignment benchmarks predominantly focus on short prompt scenarios and only provide MOS or Likert scale annotations. This inherent limitation hinders the development of long T2I evaluators, particularly in terms of the interpretability of alignment. In this study, we contribute LongT2IBench, which comprises 14K long text-image pairs accompanied by graph-structured human annotations. Given the detail-intensive nature of long prompts, we first design a Generate-Refine-Qualify annotation protocol to convert them into textual graph structures that encompass entities, attributes, and relations. Through this transformation, fine-grained alignment annotations are achieved based on these granular elements. Finally, the graph-structed annotations are converted into alignment scores and interpretations to facilitate the design of T2I evaluation models. Based on LongT2IBench, we further propose LongT2IExpert, a LongT2I evaluator that enables multi-modal large language models (MLLMs) to provide both quantitative scores and structured interpretations through an instruction-tuning process with Hierarchical Alignment Chain-of-Thought (CoT). Extensive experiments and comparisons demonstrate the superiority of the proposed LongT2IExpert in alignment evaluation and interpretation. Data and code have been released in https://welldky.github.io/LongT2IBench-Homepage/.

</details>


### [37] [LoGoColor: Local-Global 3D Colorization for 360° Scenes](https://arxiv.org/abs/2512.09278)
*Yeonjin Chang,Juhwan Cho,Seunghyeon Seo,Wonsik Shin,Nojun Kwak*

Main category: cs.CV

TL;DR: LoGoColor提出了一种新的3D着色方法，通过消除指导平均过程，使用"局部-全局"方法在复杂360°场景中保持颜色多样性，解决了现有方法因2D图像模型不一致导致的颜色单调问题。


<details>
  <summary>Details</summary>
Motivation: 现有的3D着色方法通过蒸馏2D图像着色模型，但存在固有的不一致性问题，导致训练过程中颜色被平均化，产生单调和过度简化的结果，特别是在复杂的360°场景中。作者希望保持颜色多样性，避免平均化过程。

Method: 提出LoGoColor管道，采用"局部-全局"方法：将场景划分为子场景，使用微调的多视图扩散模型明确处理子场景间和子场景内的一致性，消除指导平均过程，生成一致着色的训练视图。

Result: 该方法在复杂360°场景上实现了定量和定性上更一致、更合理的3D着色，比现有方法表现更好，并通过新颖的颜色多样性指数验证了其优越的颜色多样性。

Conclusion: LoGoColor通过消除指导平均过程并采用局部-全局一致性方法，成功解决了3D着色中的颜色多样性问题，在复杂场景中实现了更一致和丰富的着色效果。

Abstract: Single-channel 3D reconstruction is widely used in fields such as robotics and medical imaging. While this line of work excels at reconstructing 3D geometry, the outputs are not colored 3D models, thus 3D colorization is required for visualization. Recent 3D colorization studies address this problem by distilling 2D image colorization models. However, these approaches suffer from an inherent inconsistency of 2D image models. This results in colors being averaged during training, leading to monotonous and oversimplified results, particularly in complex 360° scenes. In contrast, we aim to preserve color diversity by generating a new set of consistently colorized training views, thereby bypassing the averaging process. Nevertheless, eliminating the averaging process introduces a new challenge: ensuring strict multi-view consistency across these colorized views. To achieve this, we propose LoGoColor, a pipeline designed to preserve color diversity by eliminating this guidance-averaging process with a `Local-Global' approach: we partition the scene into subscenes and explicitly tackle both inter-subscene and intra-subscene consistency using a fine-tuned multi-view diffusion model. We demonstrate that our method achieves quantitatively and qualitatively more consistent and plausible 3D colorization on complex 360° scenes than existing methods, and validate its superior color diversity using a novel Color Diversity Index.

</details>


### [38] [FoundIR-v2: Optimizing Pre-Training Data Mixtures for Image Restoration Foundation Model](https://arxiv.org/abs/2512.09282)
*Xiang Chen,Jinshan Pan,Jiangxin Dong,Jian Yang,Jinhui Tang*

Main category: cs.CV

TL;DR: FoundIR-v2是一个基于扩散的图像修复基础模型，通过数据均衡调度和MoE驱动的调度器，实现了在50多个子任务上的全面性能提升


<details>
  <summary>Details</summary>
Motivation: 研究发现不同修复任务的数据混合比例是决定全能图像修复模型整体性能的关键因素，需要优化数据比例以实现平衡的数据集组成

Method: 提出数据均衡调度范式动态优化不同任务训练数据的混合比例，并引入MoE驱动的调度器为每个修复任务灵活分配任务自适应的扩散先验

Result: 模型能够处理超过50个子任务，覆盖更广泛的真实场景，并在性能上优于现有最先进方法

Conclusion: 通过数据均衡调度和MoE驱动的任务自适应扩散先验分配，FoundIR-v2实现了在多样化图像修复任务上的全面优异性能

Abstract: Recent studies have witnessed significant advances in image restoration foundation models driven by improvements in the scale and quality of pre-training data. In this work, we find that the data mixture proportions from different restoration tasks are also a critical factor directly determining the overall performance of all-in-one image restoration models. To this end, we propose a high-capacity diffusion-based image restoration foundation model, FoundIR-v2, which adopts a data equilibrium scheduling paradigm to dynamically optimize the proportions of mixed training datasets from different tasks. By leveraging the data mixing law, our method ensures a balanced dataset composition, enabling the model to achieve consistent generalization and comprehensive performance across diverse tasks. Furthermore, we introduce an effective Mixture-of-Experts (MoE)-driven scheduler into generative pre-training to flexibly allocate task-adaptive diffusion priors for each restoration task, accounting for the distinct degradation forms and levels exhibited by different tasks. Extensive experiments demonstrate that our method can address over 50 sub-tasks across a broader scope of real-world scenarios and achieves favorable performance against state-of-the-art approaches.

</details>


### [39] [Traffic Scene Small Target Detection Method Based on YOLOv8n-SPTS Model for Autonomous Driving](https://arxiv.org/abs/2512.09296)
*Songhan Wu*

Main category: cs.CV

TL;DR: 提出改进的YOLOv8n-SPTS模型，通过SPD-Conv、SPPFCSPC和TSFP三个创新提升自动驾驶中小目标检测性能，在VisDrone2019-DET数据集上取得最佳指标。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶动态感知中的小目标识别是关键问题，现有算法因小目标信息缺失、尺度不平衡和遮挡导致检测性能不佳。

Method: 1) 用SPD-Conv模块替换传统卷积，保留细粒度信息；2) 引入SPPFCSPC模块增强多尺度特征融合；3) 设计TSFP结构，增加160*160小目标检测头，移除冗余大目标检测头。

Result: 在VisDrone2019-DET数据集上，YOLOv8n-SPTS模型在精度(61.9%)、召回率(48.3%)、mAP@0.5(52.6%)和mAP@0.5:0.95(32.6%)均排名第一，显著降低遮挡和密集场景中小目标的漏检率。

Conclusion: 提出的YOLOv8n-SPTS模型通过三个关键创新有效解决了自动驾驶中小目标检测的挑战，在精度和效率上取得平衡，为动态感知系统提供了可靠解决方案。

Abstract: This paper focuses on the key issue in autonomous driving: small target recognition in dynamic perception. Existing algorithms suffer from poor detection performance due to missing small target information, scale imbalance, and occlusion. We propose an improved YOLOv8n-SPTS model, which enhances the detection accuracy of small traffic targets through three key innovations: First, optimizing the feature extraction module. In the Backbone Bottleneck structure of YOLOv8n, 4 traditional convolution modules are replaced with Space-to-Depth Convolution (SPD-Conv) modules. This module retains fine-grained information through space-to-depth conversion, reduces information loss, and enhances the ability to capture features of low-resolution small targets. Second, enhancing feature fusion capability. The Spatial Pyramid Pooling - Fast Cross Stage Partial Connection (SPPFCSPC) module is introduced to replace the original SPPF module, integrating the multi-scale feature extraction from Spatial Pyramid Pooling (SPP) and the feature fusion mechanism of Cross Stage Partial Connection (CSP), thereby improving the model's contextual understanding of complex scenes and multi-scale feature expression ability. Third, designing a dedicated detection structure for small targets. A Triple-Stage Feature Pyramid (TSFP) structure is proposed, which adds a 160*160 small target detection head to the original detection heads to fully utilize high-resolution features in shallow layers; meanwhile, redundant large target detection heads are removed to balance computational efficiency. Comparative experiments on the VisDrone2019-DET dataset show that YOLOv8n-SPTS model ranks first in precision (61.9%), recall (48.3%), mAP@0.5 (52.6%), and mAP@0.5:0.95 (32.6%). Visualization results verify that the miss rate of small targets such as pedestrians and bicycles in occluded and dense scenes is significantly reduced.

</details>


### [40] [VABench: A Comprehensive Benchmark for Audio-Video Generation](https://arxiv.org/abs/2512.09299)
*Daili Hua,Xizhi Wang,Bohan Zeng,Xinyi Huang,Hao Liang,Junbo Niu,Xinlong Chen,Quanqing Xu,Wentao Zhang*

Main category: cs.CV

TL;DR: VABench是一个全面的音视频同步生成基准测试框架，填补了现有视频生成基准在音视频同步评估方面的空白，包含三种任务类型和15个评估维度。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成基准虽然提供了全面的视觉质量评估指标，但缺乏对音视频生成模型（特别是同步音视频输出）的可靠评估方法，需要建立一个专门的基准测试框架来系统评估同步音视频生成能力。

Method: 提出VABench基准框架，包含三种主要任务类型：文本到音视频（T2AV）、图像到音视频（I2AV）和立体声音视频生成；建立两个主要评估模块，覆盖15个维度，包括成对相似性（文本-视频、文本-音频、视频-音频）、音视频同步、唇语一致性以及精心设计的音视频问答对等。

Result: VABench覆盖七个主要内容类别：动物、人声、音乐、环境声音、同步物理声音、复杂场景和虚拟世界；提供了系统化的评估结果分析和可视化，旨在为评估具有同步音频能力的视频生成模型建立新标准。

Conclusion: VABench填补了音视频同步生成评估的空白，通过全面的多维度评估框架，有望推动该领域的全面进步，并为评估同步音视频生成模型建立新的标准。

Abstract: Recent advances in video generation have been remarkable, enabling models to produce visually compelling videos with synchronized audio. While existing video generation benchmarks provide comprehensive metrics for visual quality, they lack convincing evaluations for audio-video generation, especially for models aiming to generate synchronized audio-video outputs. To address this gap, we introduce VABench, a comprehensive and multi-dimensional benchmark framework designed to systematically evaluate the capabilities of synchronous audio-video generation. VABench encompasses three primary task types: text-to-audio-video (T2AV), image-to-audio-video (I2AV), and stereo audio-video generation. It further establishes two major evaluation modules covering 15 dimensions. These dimensions specifically assess pairwise similarities (text-video, text-audio, video-audio), audio-video synchronization, lip-speech consistency, and carefully curated audio and video question-answering (QA) pairs, among others. Furthermore, VABench covers seven major content categories: animals, human sounds, music, environmental sounds, synchronous physical sounds, complex scenes, and virtual worlds. We provide a systematic analysis and visualization of the evaluation results, aiming to establish a new standard for assessing video generation models with synchronous audio capabilities and to promote the comprehensive advancement of the field.

</details>


### [41] [From SAM to DINOv2: Towards Distilling Foundation Models to Lightweight Baselines for Generalized Polyp Segmentation](https://arxiv.org/abs/2512.09307)
*Shivanshu Agnihotri,Snehashis Majhi,Deepak Ranjan Nayak,Debesh Jha*

Main category: cs.CV

TL;DR: 提出Polyp-DiFoM蒸馏框架，将大规模视觉基础模型的丰富表示转移到轻量级分割基线中，用于结肠镜息肉分割，在显著降低计算开销的同时提升分割性能。


<details>
  <summary>Details</summary>
Motivation: 结肠镜息肉分割对结直肠癌早期检测至关重要，但面临息肉大小、形状、颜色变化大以及伪装特性等挑战。轻量级基线模型（如U-Net、U-Net++、PraNet）虽然部署方便、计算成本低，但分割性能有限。大规模视觉基础模型（如SAM、DINOv2、OneFormer、Mask2Former）在自然图像领域表现出色，但直接迁移到医学图像任务存在困难，主要由于大规模数据集稀缺和领域知识缺乏。

Method: 提出Polyp-DiFoM蒸馏框架，将基础模型的丰富表示转移到轻量级分割基线中。具体方法包括：1）将基础模型的语义先验注入到U-Net、U-Net++等经典架构中；2）执行频域编码以增强蒸馏效果；3）验证其泛化能力。

Result: 在Kvasir-SEG、CVC-ClinicDB、ETIS、ColonDB、CVC-300等五个基准数据集上进行广泛实验。Polyp-DiFoM显著优于各自的基线模型以及最先进模型，同时计算开销减少了近9倍。

Conclusion: Polyp-DiFoM框架成功地将大规模视觉基础模型的表示能力转移到轻量级分割模型中，实现了在临床环境中高效准确的部署，为医学图像分割任务提供了一种有效的知识蒸馏解决方案。

Abstract: Accurate polyp segmentation during colonoscopy is critical for the early detection of colorectal cancer and still remains challenging due to significant size, shape, and color variations, and the camouflaged nature of polyps. While lightweight baseline models such as U-Net, U-Net++, and PraNet offer advantages in terms of easy deployment and low computational cost, they struggle to deal with the above issues, leading to limited segmentation performance. In contrast, large-scale vision foundation models such as SAM, DINOv2, OneFormer, and Mask2Former have exhibited impressive generalization performance across natural image domains. However, their direct transfer to medical imaging tasks (e.g., colonoscopic polyp segmentation) is not straightforward, primarily due to the scarcity of large-scale datasets and lack of domain-specific knowledge. To bridge this gap, we propose a novel distillation framework, Polyp-DiFoM, that transfers the rich representations of foundation models into lightweight segmentation baselines, allowing efficient and accurate deployment in clinical settings. In particular, we infuse semantic priors from the foundation models into canonical architectures such as U-Net and U-Net++ and further perform frequency domain encoding for enhanced distillation, corroborating their generalization capability. Extensive experiments are performed across five benchmark datasets, such as Kvasir-SEG, CVC-ClinicDB, ETIS, ColonDB, and CVC-300. Notably, Polyp-DiFoM consistently outperforms respective baseline models significantly, as well as the state-of-the-art model, with nearly 9 times reduced computation overhead. The code is available at https://github.com/lostinrepo/PolypDiFoM.

</details>


### [42] [Benchmarking Real-World Medical Image Classification with Noisy Labels: Challenges, Practice, and Outlook](https://arxiv.org/abs/2512.09315)
*Yuan Ma,Junlin Hou,Chao Zhang,Yukun Zhou,Zongyuan Ge,Haoran Xie,Lie Ju*

Main category: cs.CV

TL;DR: LNMBench是一个用于医学图像标注噪声学习的综合基准测试，评估了10种代表性方法在7个数据集、6种成像模态和3种噪声模式下的表现，揭示了现有方法在高噪声和真实世界噪声下的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 医学图像标注需要专业知识且存在观察者间差异，导致标签不一致或错误。尽管已有大量关于噪声标签学习的研究，但现有方法在医学成像领域的鲁棒性尚未得到系统评估。

Method: 引入LNMBench基准测试，包含10种代表性方法，在7个数据集、6种成像模态和3种噪声模式下进行统一可复现的鲁棒性评估，并提出简单有效的改进方法增强模型鲁棒性。

Result: 综合实验表明，现有噪声标签学习方法在高噪声和真实世界噪声条件下性能显著下降，突显了医学数据中类别不平衡和领域变异性的持续挑战。

Conclusion: LNMBench为标准化评估提供了框架，促进了可复现研究，并为开发噪声鲁棒算法提供了实用见解。代码库已公开，旨在推动医学图像分析中噪声标签学习的研究和应用。

Abstract: Learning from noisy labels remains a major challenge in medical image analysis, where annotation demands expert knowledge and substantial inter-observer variability often leads to inconsistent or erroneous labels. Despite extensive research on learning with noisy labels (LNL), the robustness of existing methods in medical imaging has not been systematically assessed. To address this gap, we introduce LNMBench, a comprehensive benchmark for Label Noise in Medical imaging. LNMBench encompasses \textbf{10} representative methods evaluated across 7 datasets, 6 imaging modalities, and 3 noise patterns, establishing a unified and reproducible framework for robustness evaluation under realistic conditions. Comprehensive experiments reveal that the performance of existing LNL methods degrades substantially under high and real-world noise, highlighting the persistent challenges of class imbalance and domain variability in medical data. Motivated by these findings, we further propose a simple yet effective improvement to enhance model robustness under such conditions. The LNMBench codebase is publicly released to facilitate standardized evaluation, promote reproducible research, and provide practical insights for developing noise-resilient algorithms in both research and real-world medical applications.The codebase is publicly available on https://github.com/myyy777/LNMBench.

</details>


### [43] [UniLS: End-to-End Audio-Driven Avatars for Unified Listening and Speaking](https://arxiv.org/abs/2512.09327)
*Xuangeng Chu,Ruicong Liu,Yifei Huang,Yun Liu,Yichen Peng,Bo Zheng*

Main category: cs.CV

TL;DR: UniLS是首个端到端的统一说听表情生成框架，仅使用双轨音频驱动，解决了现有方法中听众表情僵硬、多样性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 生成逼真的对话头像需要同时建模说话者和倾听者的动态交互。现有方法主要关注说话者生成，而听众建模面临根本性挑战：听众动作主要由内部运动先验驱动，仅受外部语音松散引导，导致音频直接驱动训练失败，产生僵硬、静态的倾听动作。

Method: 提出两阶段训练范式：第一阶段训练无音频的自回归生成器，学习自然面部运动的自发动态，捕获内部运动先验；第二阶段引入双轨音频，微调生成器基于外部语音线索调制学习到的运动先验。

Result: UniLS在说话准确性方面达到最先进水平，更重要的是在倾听指标上实现了高达44.1%的改进，生成显著更多样、自然的倾听表情，有效缓解了僵硬问题。

Conclusion: UniLS为交互式数字人提供了实用、高保真的音频驱动解决方案，首次实现了仅通过双轨音频驱动的端到端统一说听表情生成。

Abstract: Generating lifelike conversational avatars requires modeling not just isolated speakers, but the dynamic, reciprocal interaction of speaking and listening. However, modeling the listener is exceptionally challenging: direct audio-driven training fails, producing stiff, static listening motions. This failure stems from a fundamental imbalance: the speaker's motion is strongly driven by speech audio, while the listener's motion primarily follows an internal motion prior and is only loosely guided by external speech. This challenge has led most methods to focus on speak-only generation. The only prior attempt at joint generation relies on extra speaker's motion to produce the listener. This design is not end-to-end, thereby hindering the real-time applicability. To address this limitation, we present UniLS, the first end-to-end framework for generating unified speak-listen expressions, driven by only dual-track audio. Our method introduces a novel two-stage training paradigm. Stage 1 first learns the internal motion prior by training an audio-free autoregressive generator, capturing the spontaneous dynamics of natural facial motion. Stage 2 then introduces the dual-track audio, fine-tuning the generator to modulate the learned motion prior based on external speech cues. Extensive evaluations show UniLS achieves state-of-the-art speaking accuracy. More importantly, it delivers up to 44.1\% improvement in listening metrics, generating significantly more diverse and natural listening expressions. This effectively mitigates the stiffness problem and provides a practical, high-fidelity audio-driven solution for interactive digital humans.

</details>


### [44] [Relightable and Dynamic Gaussian Avatar Reconstruction from Monocular Video](https://arxiv.org/abs/2512.09335)
*Seonghwa Choi,Moonkyeong Choi,Mingyu Jang,Jaekyung Kim,Jianfei Cai,Wen-Huang Cheng,Sanghoon Lee*

Main category: cs.CV

TL;DR: 提出RnD-Avatar框架，基于3D高斯泼溅技术，通过动态蒙皮权重和正则化方法，实现可重光照、可动画的高保真人体化身建模。


<details>
  <summary>Details</summary>
Motivation: 现有基于NeRF和3DGS的方法在重建人体化身时，常因几何细节不足（如衣物褶皱）导致结果不够逼真，需要改进运动相关的几何细节建模。

Method: 提出基于3DGS的RnD-Avatar框架，引入动态蒙皮权重定义姿态相关关节，学习运动引起的额外形变，并提出新的正则化方法在稀疏视觉线索下捕捉精细几何细节。

Result: 方法在新视角合成、新姿态渲染和重光照方面达到最先进性能，能够实现逼真的新姿态和新视角渲染，支持任意光照条件下的照片级真实感光照效果。

Conclusion: RnD-Avatar框架通过动态蒙皮权重和正则化技术，成功解决了人体化身建模中几何细节不足的问题，实现了高质量的可重光照、可动画人体化身。

Abstract: Modeling relightable and animatable human avatars from monocular video is a long-standing and challenging task. Recently, Neural Radiance Field (NeRF) and 3D Gaussian Splatting (3DGS) methods have been employed to reconstruct the avatars. However, they often produce unsatisfactory photo-realistic results because of insufficient geometrical details related to body motion, such as clothing wrinkles. In this paper, we propose a 3DGS-based human avatar modeling framework, termed as Relightable and Dynamic Gaussian Avatar (RnD-Avatar), that presents accurate pose-variant deformation for high-fidelity geometrical details. To achieve this, we introduce dynamic skinning weights that define the human avatar's articulation based on pose while also learning additional deformations induced by body motion. We also introduce a novel regularization to capture fine geometric details under sparse visual cues. Furthermore, we present a new multi-view dataset with varied lighting conditions to evaluate relight. Our framework enables realistic rendering of novel poses and views while supporting photo-realistic lighting effects under arbitrary lighting conditions. Our method achieves state-of-the-art performance in novel view synthesis, novel pose rendering, and relighting.

</details>


### [45] [TextGuider: Training-Free Guidance for Text Rendering via Attention Alignment](https://arxiv.org/abs/2512.09350)
*Kanghyun Baek,Sangyub Lee,Jin Young Choi,Jaewoo Song,Daemin Park,Jooyoung Choi,Chaehun Shin,Bohyung Han,Sungroh Yoon*

Main category: cs.CV

TL;DR: TextGuider：一种无需训练的方法，通过对齐文本内容token和图像中的文本区域，解决扩散模型中文本遗漏问题，实现准确完整的文本渲染。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散式文本到图像模型已有进展，但在准确文本渲染方面仍有困难。现有方法主要关注文本准确性，但文本遗漏问题（期望文本部分或完全缺失）在很大程度上被忽视。

Method: 提出TextGuider训练免费方法，分析MM-DiT模型中文本相关token的注意力模式，在去噪早期阶段基于两种新引入的损失函数应用潜在引导，对齐文本内容token和图像文本区域。

Result: 在测试时文本渲染方面达到最先进性能，在召回率方面取得显著提升，在OCR准确率和CLIP分数方面也获得强劲结果。

Conclusion: TextGuider通过分析注意力模式并应用潜在引导，有效解决了扩散模型中文本遗漏问题，实现了更准确和完整的文本渲染。

Abstract: Despite recent advances, diffusion-based text-to-image models still struggle with accurate text rendering. Several studies have proposed fine-tuning or training-free refinement methods for accurate text rendering. However, the critical issue of text omission, where the desired text is partially or entirely missing, remains largely overlooked. In this work, we propose TextGuider, a novel training-free method that encourages accurate and complete text appearance by aligning textual content tokens and text regions in the image. Specifically, we analyze attention patterns in MM-DiT models, particularly for text-related tokens intended to be rendered in the image. Leveraging this observation, we apply latent guidance during the early stage of denoising steps based on two loss functions that we introduce. Our method achieves state-of-the-art performance in test-time text rendering, with significant gains in recall and strong results in OCR accuracy and CLIP score.

</details>


### [46] [Video-QTR: Query-Driven Temporal Reasoning Framework for Lightweight Video Understanding](https://arxiv.org/abs/2512.09354)
*Xinkui Zhao,Zuxin Wang,Yifan Zhang,Guanjie Cheng,Yueshen Xu,Shuiguang Deng,Chang Liu,Naibo Wang,Jianwei Yin*

Main category: cs.CV

TL;DR: Video-QTR是一个轻量级视频理解框架，通过查询驱动的时序推理动态分配感知资源，减少73%的帧处理，在多个基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在视频理解中面临计算密集问题，密集帧编码产生过多视觉标记导致高内存消耗、冗余计算和有限可扩展性，传统"先处理再推理"范式效率低下。

Method: 提出Video-QTR框架，将视频理解重新定义为查询引导的推理过程，基于查询语义意图动态分配感知资源，在推理和感知之间创建自适应反馈循环，而非编码每一帧。

Result: 在MSVD-QA、Activity Net-QA、Movie Chat和Video MME等五个基准测试中达到最先进性能，同时减少高达73%的输入帧消耗。

Conclusion: 查询驱动的时序推理为视频理解提供了高效且可扩展的解决方案，验证了动态资源分配在视频理解任务中的有效性。

Abstract: The rapid development of multimodal large-language models (MLLMs) has significantly expanded the scope of visual language reasoning, enabling unified systems to interpret and describe complex visual content. However, applying these models to long-video understanding remains computationally intensive. Dense frame encoding generates excessive visual tokens, leading to high memory consumption, redundant computation, and limited scalability in real-world applications. This inefficiency highlights a key limitation of the traditional process-then-reason paradigm, which analyzes visual streams exhaustively before semantic reasoning. To address this challenge, we introduce Video-QTR (Query-Driven Temporal Reasoning), a lightweight framework that redefines video comprehension as a query-guided reasoning process. Instead of encoding every frame, Video-QTR dynamically allocates perceptual resources based on the semantic intent of the query, creating an adaptive feedback loop between reasoning and perception. Extensive experiments across five benchmarks: MSVD-QA, Activity Net-QA, Movie Chat, and Video MME demonstrate that Video-QTR achieves state-of-the-art performance while reducing input frame consumption by up to 73%. These results confirm that query-driven temporal reasoning provides an efficient and scalable solution for video understanding.

</details>


### [47] [StereoWorld: Geometry-Aware Monocular-to-Stereo Video Generation](https://arxiv.org/abs/2512.09363)
*Ke Xing,Longfei Li,Yuyang Yin,Hanwen Liang,Guixun Luo,Chen Fang,Jue Wang,Konstantinos N. Plataniotis,Xiaojie Jin,Yao Zhao,Yunchao Wei*

Main category: cs.CV

TL;DR: StereoWorld是一个端到端框架，利用预训练视频生成器实现高质量单目到立体视频生成，通过几何感知正则化和时空分块方案提升3D结构保真度和高分辨率合成效率。


<details>
  <summary>Details</summary>
Motivation: 随着XR设备的普及，对高质量立体视频的需求日益增长，但现有立体视频制作成本高且容易产生伪影，需要更高效、高质量的生成方法。

Method: 1. 基于预训练视频生成器构建端到端框架；2. 联合条件化模型处理单目视频输入；3. 使用几何感知正则化进行显式监督以确保3D结构保真度；4. 集成时空分块方案实现高效高分辨率合成；5. 构建包含1100万帧的高清立体视频数据集，对齐自然人类瞳距。

Result: StereoWorld在广泛实验中显著优于先前方法，生成的立体视频具有卓越的视觉保真度和几何一致性。

Conclusion: StereoWorld框架成功解决了高质量立体视频生成的成本和伪影问题，通过几何感知正则化和高效合成方案实现了高质量的立体视频生成，为XR应用提供了实用解决方案。

Abstract: The growing adoption of XR devices has fueled strong demand for high-quality stereo video, yet its production remains costly and artifact-prone. To address this challenge, we present StereoWorld, an end-to-end framework that repurposes a pretrained video generator for high-fidelity monocular-to-stereo video generation. Our framework jointly conditions the model on the monocular video input while explicitly supervising the generation with a geometry-aware regularization to ensure 3D structural fidelity. A spatio-temporal tiling scheme is further integrated to enable efficient, high-resolution synthesis. To enable large-scale training and evaluation, we curate a high-definition stereo video dataset containing over 11M frames aligned to natural human interpupillary distance (IPD). Extensive experiments demonstrate that StereoWorld substantially outperforms prior methods, generating stereo videos with superior visual fidelity and geometric consistency. The project webpage is available at https://ke-xing.github.io/StereoWorld/.

</details>


### [48] [ASSIST-3D: Adapted Scene Synthesis for Class-Agnostic 3D Instance Segmentation](https://arxiv.org/abs/2512.09364)
*Shengchao Zhou,Jiehong Lin,Jiahui Liu,Shizhen Zhao,Chirui Chang,Xiaojuan Qi*

Main category: cs.CV

TL;DR: ASSIST-3D提出了一种用于类别无关3D实例分割的适配3D场景合成管道，通过合成数据增强模型泛化能力，在多个基准测试中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前类别无关3D实例分割方法因标注3D场景数据稀缺或2D分割噪声而泛化能力不足。现有3D场景合成方法无法同时满足几何多样性、上下文复杂性和布局合理性这三个关键要求。

Method: ASSIST-3D包含三个关键创新：1)从广泛3D CAD资产库中进行异构对象选择，引入随机性采样以最大化几何和上下文多样性；2)通过LLM引导的空间推理结合深度优先搜索生成场景布局，确保合理的对象放置；3)通过多视角RGB-D图像渲染和融合构建逼真的点云，模拟真实世界传感器数据采集。

Result: 在ScanNetV2、ScanNet++和S3DIS基准测试上的实验表明，使用ASSIST-3D生成数据训练的模型显著优于现有方法。进一步比较证明了该专门构建的管道相对于现有3D场景合成方法的优越性。

Conclusion: ASSIST-3D通过创新的3D场景合成管道有效解决了类别无关3D实例分割的数据稀缺问题，为模型泛化能力提升提供了高质量的合成数据解决方案。

Abstract: Class-agnostic 3D instance segmentation tackles the challenging task of segmenting all object instances, including previously unseen ones, without semantic class reliance. Current methods struggle with generalization due to the scarce annotated 3D scene data or noisy 2D segmentations. While synthetic data generation offers a promising solution, existing 3D scene synthesis methods fail to simultaneously satisfy geometry diversity, context complexity, and layout reasonability, each essential for this task. To address these needs, we propose an Adapted 3D Scene Synthesis pipeline for class-agnostic 3D Instance SegmenTation, termed as ASSIST-3D, to synthesize proper data for model generalization enhancement. Specifically, ASSIST-3D features three key innovations, including 1) Heterogeneous Object Selection from extensive 3D CAD asset collections, incorporating randomness in object sampling to maximize geometric and contextual diversity; 2) Scene Layout Generation through LLM-guided spatial reasoning combined with depth-first search for reasonable object placements; and 3) Realistic Point Cloud Construction via multi-view RGB-D image rendering and fusion from the synthetic scenes, closely mimicking real-world sensor data acquisition. Experiments on ScanNetV2, ScanNet++, and S3DIS benchmarks demonstrate that models trained with ASSIST-3D-generated data significantly outperform existing methods. Further comparisons underscore the superiority of our purpose-built pipeline over existing 3D scene synthesis approaches.

</details>


### [49] [FUSER: Feed-Forward MUltiview 3D Registration Transformer and SE(3)$^N$ Diffusion Refinement](https://arxiv.org/abs/2512.09373)
*Haobo Jiang,Jin Xie,Jian Yang,Liang Yu,Jianmin Zheng*

Main category: cs.CV

TL;DR: FUSER提出首个前馈多视角点云配准Transformer，通过统一潜在空间直接预测全局位姿，无需成对匹配；FUSER-DF进一步通过SE(3)^N扩散精炼框架校正估计结果。


<details>
  <summary>Details</summary>
Motivation: 传统多视角点云配准依赖大量成对匹配构建位姿图进行全局同步，计算成本高且缺乏整体几何约束时本质不适定。需要一种能直接预测全局位姿的高效方法。

Method: FUSER：1）使用稀疏3D CNN将每个扫描编码为低分辨率超点特征，保留绝对平移线索；2）通过几何交替注意力模块进行高效扫描内和扫描间推理；3）利用现成基础模型的2D注意力先验增强3D特征交互和几何一致性。FUSER-DF：基于FUSER构建SE(3)^N扩散精炼框架，通过去噪校正FUSER的估计结果。

Result: 在3DMatch、ScanNet和ArkitScenes数据集上的广泛实验表明，该方法实现了优越的配准精度和出色的计算效率。

Conclusion: FUSER是首个前馈多视角配准Transformer，通过统一潜在空间直接预测全局位姿；结合扩散精炼框架FUSER-DF，该方法在精度和效率方面均表现出色。

Abstract: Registration of multiview point clouds conventionally relies on extensive pairwise matching to build a pose graph for global synchronization, which is computationally expensive and inherently ill-posed without holistic geometric constraints. This paper proposes FUSER, the first feed-forward multiview registration transformer that jointly processes all scans in a unified, compact latent space to directly predict global poses without any pairwise estimation. To maintain tractability, FUSER encodes each scan into low-resolution superpoint features via a sparse 3D CNN that preserves absolute translation cues, and performs efficient intra- and inter-scan reasoning through a Geometric Alternating Attention module. Particularly, we transfer 2D attention priors from off-the-shelf foundation models to enhance 3D feature interaction and geometric consistency. Building upon FUSER, we further introduce FUSER-DF, an SE(3)$^N$ diffusion refinement framework to correct FUSER's estimates via denoising in the joint SE(3)$^N$ space. FUSER acts as a surrogate multiview registration model to construct the denoiser, and a prior-conditioned SE(3)$^N$ variational lower bound is derived for denoising supervision. Extensive experiments on 3DMatch, ScanNet and ArkitScenes demonstrate that our approach achieves the superior registration accuracy and outstanding computational efficiency.

</details>


### [50] [Wasserstein-Aligned Hyperbolic Multi-View Clustering](https://arxiv.org/abs/2512.09402)
*Rui Wang,Yuting Jiang,Xiaoqing Luo,Xiao-Jun Wu,Nicu Sebe,Ziheng Chen*

Main category: cs.CV

TL;DR: 提出WAH框架，通过双曲空间表示和Wasserstein距离对齐多视图聚类中的全局语义一致性


<details>
  <summary>Details</summary>
Motivation: 现有多视图聚类方法主要关注实例级对齐，忽略了全局语义一致性，容易受到视图特定信息（如噪声和跨视图差异）的影响

Method: 1. 为每个视图使用特定的双曲编码器将特征嵌入到Lorentz流形中进行层次语义建模；2. 引入基于双曲切片Wasserstein距离的全局语义损失来对齐跨视图的流形分布；3. 使用软聚类分配来促进跨视图语义一致性

Result: 在多个基准数据集上的广泛实验表明，该方法能够实现最先进的聚类性能

Conclusion: 提出的WAH框架通过双曲表示和Wasserstein对齐有效解决了多视图聚类中的全局语义一致性问题，显著提升了聚类性能

Abstract: Multi-view clustering (MVC) aims to uncover the latent structure of multi-view data by learning view-common and view-specific information. Although recent studies have explored hyperbolic representations for better tackling the representation gap between different views, they focus primarily on instance-level alignment and neglect global semantic consistency, rendering them vulnerable to view-specific information (\textit{e.g.}, noise and cross-view discrepancies). To this end, this paper proposes a novel Wasserstein-Aligned Hyperbolic (WAH) framework for multi-view clustering. Specifically, our method exploits a view-specific hyperbolic encoder for each view to embed features into the Lorentz manifold for hierarchical semantic modeling. Whereafter, a global semantic loss based on the hyperbolic sliced-Wasserstein distance is introduced to align manifold distributions across views. This is followed by soft cluster assignments to encourage cross-view semantic consistency. Extensive experiments on multiple benchmarking datasets show that our method can achieve SOTA clustering performance.

</details>


### [51] [Generative Point Cloud Registration](https://arxiv.org/abs/2512.09407)
*Haobo Jiang,Jin Xie,Jian Yang,Liang Yu,Jianmin Zheng*

Main category: cs.CV

TL;DR: 提出生成式点云配准新范式，通过2D生成模型生成跨视角一致图像对，结合几何-颜色特征融合提升3D配准性能


<details>
  <summary>Details</summary>
Motivation: 传统3D点云配准方法主要依赖几何特征，忽略了颜色纹理信息。本文旨在通过生成跨视角一致的图像对，融合几何和颜色特征来提升配准的鲁棒性和准确性

Method: 提出生成式点云配准范式，引入Match-ControlNet可控生成模型。该模型利用ControlNet的深度条件生成能力确保2D-3D几何一致性，通过耦合条件去噪方案和耦合提示引导实现跨视角纹理一致性

Result: 在3DMatch和ScanNet数据集上的大量实验验证了方法的有效性，该生成式配准范式可无缝集成到各种配准方法中提升性能

Conclusion: 提出的生成式点云配准新范式成功将先进2D生成模型与3D匹配任务结合，通过生成跨视角一致图像对实现几何-颜色特征融合，显著提升了3D配准性能

Abstract: In this paper, we propose a novel 3D registration paradigm, Generative Point Cloud Registration, which bridges advanced 2D generative models with 3D matching tasks to enhance registration performance. Our key idea is to generate cross-view consistent image pairs that are well-aligned with the source and target point clouds, enabling geometry-color feature fusion to facilitate robust matching. To ensure high-quality matching, the generated image pair should feature both 2D-3D geometric consistency and cross-view texture consistency. To achieve this, we introduce Match-ControlNet, a matching-specific, controllable 2D generative model. Specifically, it leverages the depth-conditioned generation capability of ControlNet to produce images that are geometrically aligned with depth maps derived from point clouds, ensuring 2D-3D geometric consistency. Additionally, by incorporating a coupled conditional denoising scheme and coupled prompt guidance, Match-ControlNet further promotes cross-view feature interaction, guiding texture consistency generation. Our generative 3D registration paradigm is general and could be seamlessly integrated into various registration methods to enhance their performance. Extensive experiments on 3DMatch and ScanNet datasets verify the effectiveness of our approach.

</details>


### [52] [DirectSwap: Mask-Free Cross-Identity Training and Benchmarking for Expression-Consistent Video Head Swapping](https://arxiv.org/abs/2512.09417)
*Yanan Wang,Shengcai Liao,Panwen Hu,Xin Li,Fan Yang,Xiaodan Liang*

Main category: cs.CV

TL;DR: 本文提出了DirectSwap框架，首个无需掩码的直接视频头部替换方法，通过创建HeadSwapBench配对数据集解决训练数据缺失问题，实现了更好的身份保真度和运动表情一致性。


<details>
  <summary>Details</summary>
Motivation: 现有视频头部替换方法因缺乏真实配对数据，通常使用同一视频内跨帧对训练，依赖掩码修复来减少身份泄露。这种方法存在边界伪影问题，且难以恢复被掩码遮挡的关键信息（如面部姿态、表情和运动动态）。

Method: 1) 使用视频编辑模型合成新头部创建HeadSwapBench数据集，首个跨身份配对视频头部替换数据集；2) 提出DirectSwap框架，将图像U-Net扩展为视频扩散模型，包含运动模块和条件输入；3) 引入运动与表情感知重建损失，通过帧差幅度和面部关键点距离重新加权扩散损失，增强跨帧一致性。

Result: DirectSwap在多样化的真实视频场景中实现了最先进的视觉质量、身份保真度以及运动和表情一致性。HeadSwapBench数据集支持训练和基准测试。

Conclusion: 通过创建首个配对数据集和提出无需掩码的直接替换框架，解决了视频头部替换中的关键挑战，显著提升了替换质量和一致性。将开源代码和数据集促进未来研究。

Abstract: Video head swapping aims to replace the entire head of a video subject, including facial identity, head shape, and hairstyle, with that of a reference image, while preserving the target body, background, and motion dynamics. Due to the lack of ground-truth paired swapping data, prior methods typically train on cross-frame pairs of the same person within a video and rely on mask-based inpainting to mitigate identity leakage. Beyond potential boundary artifacts, this paradigm struggles to recover essential cues occluded by the mask, such as facial pose, expressions, and motion dynamics. To address these issues, we prompt a video editing model to synthesize new heads for existing videos as fake swapping inputs, while maintaining frame-synchronized facial poses and expressions. This yields HeadSwapBench, the first cross-identity paired dataset for video head swapping, which supports both training (\TrainNum{} videos) and benchmarking (\TestNum{} videos) with genuine outputs. Leveraging this paired supervision, we propose DirectSwap, a mask-free, direct video head-swapping framework that extends an image U-Net into a video diffusion model with a motion module and conditioning inputs. Furthermore, we introduce the Motion- and Expression-Aware Reconstruction (MEAR) loss, which reweights the diffusion loss per pixel using frame-difference magnitudes and facial-landmark proximity, thereby enhancing cross-frame coherence in motion and expressions. Extensive experiments demonstrate that DirectSwap achieves state-of-the-art visual quality, identity fidelity, and motion and expression consistency across diverse in-the-wild video scenes. We will release the source code and the HeadSwapBench dataset to facilitate future research.

</details>


### [53] [Label-free Motion-Conditioned Diffusion Model for Cardiac Ultrasound Synthesis](https://arxiv.org/abs/2512.09418)
*Zhe Li,Hadrien Reynaud,Johanna P Müller,Bernhard Kainz*

Main category: cs.CV

TL;DR: 提出MCDM模型，一种无需标签的潜在扩散框架，通过自监督运动特征合成逼真的超声心动图视频，解决标注数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 超声心动图对心脏功能评估至关重要，但标注数据稀缺（隐私限制和专家标注复杂性）限制了深度学习应用，需要无需标签的合成方法。

Method: 提出运动条件扩散模型(MCDM)，基于自监督运动特征合成视频；设计MAFE提取器分离运动和外观特征；使用重识别损失和光流损失增强特征学习。

Result: 在EchoNet-Dynamic数据集上评估，MCDM实现竞争性视频生成性能，产生时间连贯且临床逼真的序列，无需依赖手动标签。

Conclusion: 证明了自监督条件在可扩展超声心动图合成中的潜力，为医疗影像生成提供无标签解决方案。

Abstract: Ultrasound echocardiography is essential for the non-invasive, real-time assessment of cardiac function, but the scarcity of labelled data, driven by privacy restrictions and the complexity of expert annotation, remains a major obstacle for deep learning methods. We propose the Motion Conditioned Diffusion Model (MCDM), a label-free latent diffusion framework that synthesises realistic echocardiography videos conditioned on self-supervised motion features. To extract these features, we design the Motion and Appearance Feature Extractor (MAFE), which disentangles motion and appearance representations from videos. Feature learning is further enhanced by two auxiliary objectives: a re-identification loss guided by pseudo appearance features and an optical flow loss guided by pseudo flow fields. Evaluated on the EchoNet-Dynamic dataset, MCDM achieves competitive video generation performance, producing temporally coherent and clinically realistic sequences without reliance on manual labels. These results demonstrate the potential of self-supervised conditioning for scalable echocardiography synthesis. Our code is available at https://github.com/ZheLi2020/LabelfreeMCDM.

</details>


### [54] [InfoMotion: A Graph-Based Approach to Video Dataset Distillation for Echocardiography](https://arxiv.org/abs/2512.09422)
*Zhe Li,Hadrien Reynaud,Alberto Gomez,Bernhard Kainz*

Main category: cs.CV

TL;DR: 提出一种基于运动特征提取和图构建的超声心动图视频数据集蒸馏方法，仅用25个合成视频就能达到69.38%的测试准确率


<details>
  <summary>Details</summary>
Motivation: 超声心动图在心血管疾病诊断中至关重要，但大规模视频数据带来存储、计算和训练效率的挑战。数据集蒸馏可以通过合成紧凑且信息丰富的子集来保留原始数据的关键临床特征。

Method: 利用运动特征提取捕捉时间动态，然后进行类内图构建，并使用Infomap算法选择代表性样本，从而选择多样且信息丰富的合成视频子集。

Result: 在EchoNet-Dynamic数据集上评估，仅使用25个合成视频就达到了69.38%的测试准确率。

Conclusion: 该方法在医学视频数据集蒸馏方面表现出有效性和可扩展性，能够保留原始数据集的基本特征。

Abstract: Echocardiography playing a critical role in the diagnosis and monitoring of cardiovascular diseases as a non-invasive real-time assessment of cardiac structure and function. However, the growing scale of echocardiographic video data presents significant challenges in terms of storage, computation, and model training efficiency. Dataset distillation offers a promising solution by synthesizing a compact, informative subset of data that retains the key clinical features of the original dataset. In this work, we propose a novel approach for distilling a compact synthetic echocardiographic video dataset. Our method leverages motion feature extraction to capture temporal dynamics, followed by class-wise graph construction and representative sample selection using the Infomap algorithm. This enables us to select a diverse and informative subset of synthetic videos that preserves the essential characteristics of the original dataset. We evaluate our approach on the EchoNet-Dynamic datasets and achieve a test accuracy of \(69.38\%\) using only \(25\) synthetic videos. These results demonstrate the effectiveness and scalability of our method for medical video dataset distillation.

</details>


### [55] [FunPhase: A Periodic Functional Autoencoder for Motion Generation via Phase Manifolds](https://arxiv.org/abs/2512.09423)
*Marco Pegoraro,Evan Atherton,Bruno Roy,Aliasghar Khani,Arianna Rampini*

Main category: cs.CV

TL;DR: FunPhase是一种功能性周期自编码器，通过学习运动的相位流形，用函数空间公式替代离散时间解码，实现任意时间分辨率的平滑轨迹生成。


<details>
  <summary>Details</summary>
Motivation: 自然身体运动学习具有挑战性，因为空间几何与时间动态之间存在强耦合。现有的相位流形嵌入方法缺乏可扩展性且局限于特定设置。

Method: 提出FunPhase功能性周期自编码器，学习运动的相位流形，用函数空间公式替代离散时间解码，支持任意时间分辨率采样。

Result: 相比先前周期自编码器基线，FunPhase实现了显著更低的重建误差，与最先进的运动生成方法性能相当，并支持更广泛的应用。

Conclusion: FunPhase通过学习相位流形和函数空间公式，统一了运动预测和生成，支持超分辨率、部分身体运动完成等下游任务，具有跨骨架和数据集的可推广性。

Abstract: Learning natural body motion remains challenging due to the strong coupling between spatial geometry and temporal dynamics. Embedding motion in phase manifolds, latent spaces that capture local periodicity, has proven effective for motion prediction; however, existing approaches lack scalability and remain confined to specific settings. We introduce FunPhase, a functional periodic autoencoder that learns a phase manifold for motion and replaces discrete temporal decoding with a function-space formulation, enabling smooth trajectories that can be sampled at arbitrary temporal resolutions. FunPhase supports downstream tasks such as super-resolution and partial-body motion completion, generalizes across skeletons and datasets, and unifies motion prediction and generation within a single interpretable manifold. Our model achieves substantially lower reconstruction error than prior periodic autoencoder baselines while enabling a broader range of applications and performing on par with state-of-the-art motion generation methods.

</details>


### [56] [UniPart: Part-Level 3D Generation with Unified 3D Geom-Seg Latents](https://arxiv.org/abs/2512.09435)
*Xufan He,Yushuang Wu,Xiaoyang Guo,Chongjie Ye,Jiaqing Zhou,Tianlei Hu,Xiaoguang Han,Dong Du*

Main category: cs.CV

TL;DR: UniPart是一个两阶段潜在扩散框架，用于图像引导的部分级3D生成，通过统一的几何-分割潜在表示实现可控的部分级结构合成。


<details>
  <summary>Details</summary>
Motivation: 现有方法在部分级3D生成方面存在局限性：要么依赖隐式部分分割且粒度控制有限，要么需要依赖在大型标注数据集上训练的外部分割器。作者观察到部分感知在整体对象几何学习过程中自然出现，因此提出统一的几何-分割表示方法。

Method: 提出Geom-Seg VecSet统一几何-分割潜在表示，联合编码对象几何和部分级结构。基于此开发UniPart两阶段潜在扩散框架：第一阶段执行联合几何生成和潜在部分分割；第二阶段基于整体对象和部分特定潜在变量进行部分级扩散。采用双空间生成方案，在全局和规范空间中预测部分潜在变量以增强几何保真度。

Result: 大量实验表明，与现有方法相比，UniPart在分割可控性和部分级几何质量方面表现优异。

Conclusion: 通过统一的几何-分割表示和两阶段扩散框架，UniPart实现了高质量、可控的部分级3D生成，解决了现有方法的局限性。

Abstract: Part-level 3D generation is essential for applications requiring decomposable and structured 3D synthesis. However, existing methods either rely on implicit part segmentation with limited granularity control or depend on strong external segmenters trained on large annotated datasets. In this work, we observe that part awareness emerges naturally during whole-object geometry learning and propose Geom-Seg VecSet, a unified geometry-segmentation latent representation that jointly encodes object geometry and part-level structure. Building on this representation, we introduce UniPart, a two-stage latent diffusion framework for image-guided part-level 3D generation. The first stage performs joint geometry generation and latent part segmentation, while the second stage conditions part-level diffusion on both whole-object and part-specific latents. A dual-space generation scheme further enhances geometric fidelity by predicting part latents in both global and canonical spaces. Extensive experiments demonstrate that UniPart achieves superior segmentation controllability and part-level geometric quality compared with existing approaches.

</details>


### [57] [Representation Calibration and Uncertainty Guidance for Class-Incremental Learning based on Vision Language Model](https://arxiv.org/abs/2512.09441)
*Jiantao Tan,Peixian Ma,Tong Yu,Wentao Zhang,Ruixuan Wang*

Main category: cs.CV

TL;DR: 提出基于视觉语言模型的新型持续学习框架，通过任务特定适配器学习新知识，使用轻量投影器混合的跨任务表示校准策略减少类间混淆，并开发基于预测不确定性的推理策略


<details>
  <summary>Details</summary>
Motivation: 当前基于视觉语言模型的类增量学习方法在区分跨学习任务的类别方面仍存在问题，需要解决类间混淆问题以提高分类性能

Method: 1) 在预训练且冻结的图像编码器上添加任务特定适配器学习新知识；2) 使用基于轻量投影器混合的跨任务表示校准策略，在统一特征空间中更好分离所有已学习类别；3) 开发基于预测不确定性的推理策略，更准确选择最合适的图像特征进行类别预测

Result: 在多个数据集和各种设置下的广泛实验表明，该方法相比现有方法具有优越性能

Conclusion: 提出的VLM-based持续学习框架通过跨任务表示校准和不确定性引导的推理策略，有效解决了类增量学习中的类间混淆问题，显著提升了分类性能

Abstract: Class-incremental learning requires a learning system to continually learn knowledge of new classes and meanwhile try to preserve previously learned knowledge of old classes. As current state-of-the-art methods based on Vision-Language Models (VLMs) still suffer from the issue of differentiating classes across learning tasks. Here a novel VLM-based continual learning framework for image classification is proposed. In this framework, task-specific adapters are added to the pre-trained and frozen image encoder to learn new knowledge, and a novel cross-task representation calibration strategy based on a mixture of light-weight projectors is used to help better separate all learned classes in a unified feature space, alleviating class confusion across tasks. In addition, a novel inference strategy guided by prediction uncertainty is developed to more accurately select the most appropriate image feature for class prediction. Extensive experiments on multiple datasets under various settings demonstrate the superior performance of our method compared to existing ones.

</details>


### [58] [Defect-aware Hybrid Prompt Optimization via Progressive Tuning for Zero-Shot Multi-type Anomaly Detection and Segmentation](https://arxiv.org/abs/2512.09446)
*Nadeem Nazer,Hongkuan Zhou,Lavdim Halilaj,Ylli Sadikaj,Steffen Staab*

Main category: cs.CV

TL;DR: DAPO是一种基于渐进调优的缺陷感知提示优化方法，用于零样本多类型和二元异常检测与分割，通过混合缺陷感知提示将异常相关图像特征与对应文本语义对齐。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型（如CLIP）在异常检测中往往忽略细粒度细节（如"孔洞"、"切割"、"划痕"等具体缺陷类型），而这些信息能丰富"异常"的语义表示，缩小粗粒度异常信号与细粒度缺陷类别之间的差距，帮助制造商理解异常根本原因并采取针对性纠正措施。手工设计每个缺陷类型的提示耗时且易受人为偏见影响。

Method: DAPO（缺陷感知提示优化）方法基于渐进调优，学习混合缺陷感知提示，包含固定文本锚点和可学习令牌嵌入，将异常相关图像特征与对应文本语义对齐，用于零样本多类型和二元异常检测与分割。

Result: 在公开基准（MPDD、VisA、MVTec-AD、MAD、Real-IAD）和内部数据集上的实验表明，相比基线模型，DAPO在分布偏移下图像级AUROC和平均精度指标平均提升3.7%，在零样本设置下定位新异常类型的性能平均提升6.5%。

Conclusion: DAPO通过缺陷感知提示优化有效提升了细粒度异常检测性能，特别是在分布偏移和零样本场景下，为工业异常检测提供了更精确和实用的解决方案。

Abstract: Recent vision language models (VLMs) like CLIP have demonstrated impressive anomaly detection performance under significant distribution shift by utilizing high-level semantic information through text prompts. However, these models often neglect fine-grained details, such as which kind of anomalies, like "hole", "cut", "scratch" that could provide more specific insight into the nature of anomalies. We argue that recognizing fine-grained anomaly types 1) enriches the representation of "abnormal" with structured semantics, narrowing the gap between coarse anomaly signals and fine-grained defect categories; 2) enables manufacturers to understand the root causes of the anomaly and implement more targeted and appropriate corrective measures quickly. While incorporating such detailed semantic information is crucial, designing handcrafted prompts for each defect type is both time-consuming and susceptible to human bias. For this reason, we introduce DAPO, a novel approach for Defect-aware Prompt Optimization based on progressive tuning for the zero-shot multi-type and binary anomaly detection and segmentation under distribution shifts. Our approach aligns anomaly-relevant image features with their corresponding text semantics by learning hybrid defect-aware prompts with both fixed textual anchors and learnable token embeddings. We conducted experiments on public benchmarks (MPDD, VisA, MVTec-AD, MAD, and Real-IAD) and an internal dataset. The results suggest that compared to the baseline models, DAPO achieves a 3.7% average improvement in AUROC and average precision metrics at the image level under distribution shift, and a 6.5% average improvement in localizing novel anomaly types under zero-shot settings.

</details>


### [59] [Privacy-Preserving Computer Vision for Industry: Three Case Studies in Human-Centric Manufacturing](https://arxiv.org/abs/2512.09463)
*Sander De Coninck,Emilio Gamba,Bart Van Doninck,Abdellatif Bey-Temsamani,Sam Leroux,Pieter Simoens*

Main category: cs.CV

TL;DR: 该研究在真实工业环境中验证了隐私保护计算机视觉框架，通过视觉变换技术平衡操作效用与工人隐私，在三个工业用例中展示了可行性。


<details>
  <summary>Details</summary>
Motivation: 工业AI视觉应用需要在操作效用与工人隐私保护之间取得平衡，现有解决方案在实际生产环境中的验证不足，需要建立可信赖的人类中心AI部署方案。

Method: 采用学习型视觉变换技术，模糊敏感或任务无关信息，同时保留任务性能所需的关键特征；在三个工业用例（木工生产监控、人感知AGV导航、多摄像头工效学风险评估）中进行验证。

Result: 任务特定的模糊处理能够实现有效监控同时降低隐私风险；定量评估显示良好的隐私-效用权衡；工业合作伙伴的定性反馈确认了框架的有效性、部署可行性和信任影响。

Conclusion: 该隐私保护框架已具备实际应用准备，为工业领域负责任、人类中心的AI部署提供了跨领域建议，证明了在真实生产环境中平衡隐私与效用的可行性。

Abstract: The adoption of AI-powered computer vision in industry is often constrained by the need to balance operational utility with worker privacy. Building on our previously proposed privacy-preserving framework, this paper presents its first comprehensive validation on real-world data collected directly by industrial partners in active production environments. We evaluate the framework across three representative use cases: woodworking production monitoring, human-aware AGV navigation, and multi-camera ergonomic risk assessment. The approach employs learned visual transformations that obscure sensitive or task-irrelevant information while retaining features essential for task performance. Through both quantitative evaluation of the privacy-utility trade-off and qualitative feedback from industrial partners, we assess the framework's effectiveness, deployment feasibility, and trust implications. Results demonstrate that task-specific obfuscation enables effective monitoring with reduced privacy risks, establishing the framework's readiness for real-world adoption and providing cross-domain recommendations for responsible, human-centric AI deployment in industry.

</details>


### [60] [Color encoding in Latent Space of Stable Diffusion Models](https://arxiv.org/abs/2512.09477)
*Guillem Arias,Ariadna Solà,Martí Armengod,Maria Vanrell*

Main category: cs.CV

TL;DR: 通过分析Stable Diffusion的潜在表示，发现颜色信息在c_3和c_4通道中以圆形对立轴编码，而强度和形状主要在c_1和c_2通道中表示


<details>
  <summary>Details</summary>
Motivation: 尽管扩散生成模型在视觉保真度方面取得了显著进展，但对其内部如何表示特定感知属性（如颜色和形状）的理解仍然有限。本研究旨在探索生成模型中颜色是如何编码的。

Method: 通过受控合成数据集，对Stable Diffusion的潜在表示进行系统分析，使用主成分分析（PCA）和相似性度量方法

Result: 发现颜色信息在c_3和c_4通道中以圆形对立轴编码，而强度和形状主要在c_1和c_2通道中表示。Stable Diffusion的潜在空间展现出与高效编码表示一致的、可解释的结构

Conclusion: 这些发现为未来模型理解、编辑应用以及设计更解耦的生成框架提供了基础。研究揭示了生成模型中颜色表示的特定编码模式

Abstract: Recent advances in diffusion-based generative models have achieved remarkable visual fidelity, yet a detailed understanding of how specific perceptual attributes - such as color and shape - are internally represented remains limited. This work explores how color is encoded in a generative model through a systematic analysis of the latent representations in Stable Diffusion. Through controlled synthetic datasets, principal component analysis (PCA) and similarity metrics, we reveal that color information is encoded along circular, opponent axes predominantly captured in latent channels c_3 and c_4, whereas intensity and shape are primarily represented in channels c_1 and c_2. Our findings indicate that the latent space of Stable Diffusion exhibits an interpretable structure aligned with a efficient coding representation. These insights provide a foundation for future work in model understanding, editing applications, and the design of more disentangled generative frameworks.

</details>


### [61] [MODA: The First Challenging Benchmark for Multispectral Object Detection in Aerial Images](https://arxiv.org/abs/2512.09489)
*Shuaihao Han,Tingfa Xu,Peifu Liu,Jianan Li*

Main category: cs.CV

TL;DR: 论文提出了首个大规模多光谱航空目标检测数据集MODA和OSSDET框架，通过光谱-空间调制和对象感知机制提升航空目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 航空目标检测面临小目标和背景干扰等挑战，RGB图像信息有限，多光谱图像能提供额外光谱线索但缺乏训练数据。

Method: 提出OSSDET框架：1）级联光谱-空间调制结构优化目标感知；2）利用光谱相似性聚合光谱相关特征增强对象内相关性；3）通过对象感知掩码抑制无关背景；4）在显式对象感知指导下使用跨光谱注意力细化对象相关表示。

Result: OSSDET在参数和效率可比的情况下优于现有方法，MODA数据集包含14,041张多光谱图像和330,191个标注，为领域提供了全面数据基础。

Conclusion: MODA数据集填补了多光谱航空目标检测的数据空白，OSSDET框架通过有效整合光谱、空间和对象感知信息，显著提升了航空目标检测性能。

Abstract: Aerial object detection faces significant challenges in real-world scenarios, such as small objects and extensive background interference, which limit the performance of RGB-based detectors with insufficient discriminative information. Multispectral images (MSIs) capture additional spectral cues across multiple bands, offering a promising alternative. However, the lack of training data has been the primary bottleneck to exploiting the potential of MSIs. To address this gap, we introduce the first large-scale dataset for Multispectral Object Detection in Aerial images (MODA), which comprises 14,041 MSIs and 330,191 annotations across diverse, challenging scenarios, providing a comprehensive data foundation for this field. Furthermore, to overcome challenges inherent to aerial object detection using MSIs, we propose OSSDet, a framework that integrates spectral and spatial information with object-aware cues. OSSDet employs a cascaded spectral-spatial modulation structure to optimize target perception, aggregates spectrally related features by exploiting spectral similarities to reinforce intra-object correlations, and suppresses irrelevant background via object-aware masking. Moreover, cross-spectral attention further refines object-related representations under explicit object-aware guidance. Extensive experiments demonstrate that OSSDet outperforms existing methods with comparable parameters and efficiency.

</details>


### [62] [StateSpace-SSL: Linear-Time Self-supervised Learning for Plant Disease Detectio](https://arxiv.org/abs/2512.09492)
*Abdullah Al Mamun,Miaohua Zhang,David Ahmedt-Aristizabal,Zeeshan Hayder,Mohammad Awrangjeb*

Main category: cs.CV

TL;DR: 提出StateSpace-SSL框架，使用Vision Mamba状态空间编码器进行自监督学习，解决植物病害检测中CNN和Transformer方法的局限性，实现线性时间复杂度和更好的病害模式建模。


<details>
  <summary>Details</summary>
Motivation: 现有自监督学习方法（基于CNN或视觉Transformer）在植物病害检测中存在局限性：CNN难以捕捉沿叶片结构连续演变的病害模式，而Transformer的高分辨率补丁带来二次注意力计算成本。需要更适合农业图像特点的自监督学习方法。

Method: 提出StateSpace-SSL框架，采用Vision Mamba状态空间编码器，通过定向扫描叶片表面建模长距离病变连续性。使用原型驱动的师生目标，在多个视图间对齐表示，从标记数据中学习稳定且病变感知的特征。

Result: 在三个公开植物病害数据集上的实验表明，StateSpace-SSL在各种评估指标上一致优于基于CNN和Transformer的自监督学习基线方法。定性分析确认其学习到紧凑、病变聚焦的特征图。

Conclusion: 线性状态空间建模为自监督植物病害表示学习提供了优势，能够有效捕捉叶片结构上的病变连续性，同时保持线性时间复杂度，优于传统CNN和Transformer方法。

Abstract: Self-supervised learning (SSL) is attractive for plant disease detection as it can exploit large collections of unlabeled leaf images, yet most existing SSL methods are built on CNNs or vision transformers that are poorly matched to agricultural imagery. CNN-based SSL struggles to capture disease patterns that evolve continuously along leaf structures, while transformer-based SSL introduces quadratic attention cost from high-resolution patches. To address these limitations, we propose StateSpace-SSL, a linear-time SSL framework that employs a Vision Mamba state-space encoder to model long-range lesion continuity through directional scanning across the leaf surface. A prototype-driven teacher-student objective aligns representations across multiple views, encouraging stable and lesion-aware features from labelled data. Experiments on three publicly available plant disease datasets show that StateSpace-SSL consistently outperforms the CNN- and transformer-based SSL baselines in various evaluation metrics. Qualitative analyses further confirm that it learns compact, lesion-focused feature maps, highlighting the advantage of linear state-space modelling for self-supervised plant disease representation learning.

</details>


### [63] [Hands-on Evaluation of Visual Transformers for Object Recognition and Detection](https://arxiv.org/abs/2512.09579)
*Dimitrios N. Vlachogiannis,Dimitrios A. Koutsomitropoulos*

Main category: cs.CV

TL;DR: 比较Vision Transformers与传统CNN在计算机视觉任务中的表现，发现混合和分层Transformer在精度和计算资源间取得更好平衡，在医疗图像等需要全局理解的场景中表现更优。


<details>
  <summary>Details</summary>
Motivation: 传统CNN主要关注局部模式，在理解图像的全局上下文方面存在局限；而Vision Transformers通过自注意力机制能够捕捉整个图像的关系。本文旨在比较不同类型的ViT与传统CNN在各种视觉任务中的性能差异。

Method: 比较了三种类型的Vision Transformers（纯Transformer、分层Transformer、混合Transformer）与传统CNN模型。在标准数据集（ImageNet、COCO）上进行图像分类和目标检测测试，并在医疗图像数据集ChestX-ray14上进行医疗图像分类实验。同时测试了数据增强技术对医疗图像性能的影响。

Result: 混合和分层Transformer（特别是Swin和CvT）在精度和计算资源之间提供了良好的平衡。在医疗图像上应用数据增强技术显著提升了性能，尤其是Swin Transformer模型。总体而言，Vision Transformers在大多数情况下与传统CNN竞争，并在需要全局视觉上下文理解的场景（如医疗成像）中表现更优。

Conclusion: Vision Transformers是传统CNN的有力竞争者，在许多任务中表现更优，特别是在需要全局图像理解的场景中。混合和分层Transformer架构在性能和计算效率方面提供了最佳平衡，为计算机视觉任务提供了有前景的新方向。

Abstract: Convolutional Neural Networks (CNNs) for computer vision sometimes struggle with understanding images in a global context, as they mainly focus on local patterns. On the other hand, Vision Transformers (ViTs), inspired by models originally created for language processing, use self-attention mechanisms, which allow them to understand relationships across the entire image. In this paper, we compare different types of ViTs (pure, hierarchical, and hybrid) against traditional CNN models across various tasks, including object recognition, detection, and medical image classification. We conduct thorough tests on standard datasets like ImageNet for image classification and COCO for object detection. Additionally, we apply these models to medical imaging using the ChestX-ray14 dataset. We find that hybrid and hierarchical transformers, especially Swin and CvT, offer a strong balance between accuracy and computational resources. Furthermore, by experimenting with data augmentation techniques on medical images, we discover significant performance improvements, particularly with the Swin Transformer model. Overall, our results indicate that Vision Transformers are competitive and, in many cases, outperform traditional CNNs, especially in scenarios requiring the understanding of global visual contexts like medical imaging.

</details>


### [64] [Building Reasonable Inference for Vision-Language Models in Blind Image Quality Assessment](https://arxiv.org/abs/2512.09555)
*Yuan Li,Zitang Sun,Yen-ju Chen,Shin'ya Nishida*

Main category: cs.CV

TL;DR: 本文针对基于视觉语言模型(VLMs)的盲图像质量评估(BIQA)中存在的文本描述与质量预测矛盾、预测不稳定问题，提出了一种两阶段调优方法，将视觉感知与质量推理分离，显著提升了预测稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于视觉语言模型的盲图像质量评估方法虽然具有语义推理能力，但存在两个主要问题：1）生成的文本描述与最终质量预测相矛盾；2）推理过程中预测分数不稳定变化。这些行为与人类推理方式不一致，需要深入分析并改进。

Method: 首先分析导致矛盾评估和不稳定性的因素，发现质量预测未完全基于视觉特征且逻辑连接薄弱。然后提出两阶段调优方法：第一阶段让模型学习视觉特征，第二阶段仅基于这些特征进行质量推理，实现视觉感知与质量推理的分离。

Result: 在SPAQ和KONIQ数据集上，预测不稳定性从22.00%降至12.39%。在LIVE、CSIQ、SPAQ、KONIQ数据集上，SRCC/PLCC平均提升0.3124/0.3507。进一步分析表明，该方法同时提升了稳定性和推理过程的可靠性。

Conclusion: 通过将视觉感知与质量推理分离的两阶段调优方法，能够有效解决基于视觉语言模型的盲图像质量评估中的矛盾预测和不稳定性问题，使模型推理更接近人类方式，显著提升评估性能。

Abstract: Recent progress in BIQA has been driven by VLMs, whose semantic reasoning abilities suggest that they might extract visual features, generate descriptive text, and infer quality in a human-like manner. However, these models often produce textual descriptions that contradict their final quality predictions, and the predicted scores can change unstably during inference - behaviors not aligned with human reasoning. To understand these issues, we analyze the factors that cause contradictory assessments and instability. We first estimate the relationship between the final quality predictions and the generated visual features, finding that the predictions are not fully grounded in the features and that the logical connection between them is weak. Moreover, decoding intermediate VLM layers shows that the model frequently relies on a limited set of candidate tokens, which contributes to prediction instability. To encourage more human-like reasoning, we introduce a two-stage tuning method that explicitly separates visual perception from quality inference. In the first stage, the model learns visual features; in the second, it infers quality solely from these features. Experiments on SPAQ and KONIQ demonstrate that our approach reduces prediction instability from 22.00% to 12.39% and achieves average gains of 0.3124/0.3507 in SRCC/PLCC across LIVE, CSIQ, SPAQ, and KONIQ compared to the baseline. Further analyses show that our method improves both stability and the reliability of the inference process.

</details>


### [65] [Investigate the Low-level Visual Perception in Vision-Language based Image Quality Assessment](https://arxiv.org/abs/2512.09573)
*Yuan Li,Zitang Sun,Yen-Ju Chen,Shin'ya Nishida*

Main category: cs.CV

TL;DR: 多模态大语言模型在图像质量评估中难以可靠检测低层次失真，通过改进视觉编码器对齐可显著提升失真识别能力


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型在图像质量评估中能生成描述性解释，但它们往往无法可靠检测基本的低层次失真（如模糊、噪声、压缩），且在重复推理中可能产生不一致的评估结果。这引发了一个关键问题：这些模型是否真正感知到了重要的视觉特征？

Method: 引入低层次失真感知任务，要求模型分类特定失真类型；进行组件级分析，研究视觉-语言对齐转移阶段；通过计算视觉特征与对应语义标记之间的语义距离，分析组件级微调前后的变化；改进视觉编码器的对齐方式

Result: 组件级分析显示，虽然多模态大语言模型在结构上能够表示此类失真，但它们倾向于过拟合训练模板，导致质量评分偏差；改进视觉编码器对齐后，失真识别准确率从14.92%大幅提升至84.43%

Conclusion: 在视觉编码器中加入专门的约束可以增强文本可解释的视觉表示，使基于多模态大语言模型的流程在视觉中心任务中产生更一致和可解释的推理

Abstract: Recent advances in Image Quality Assessment (IQA) have leveraged Multi-modal Large Language Models (MLLMs) to generate descriptive explanations. However, despite their strong visual perception modules, these models often fail to reliably detect basic low-level distortions such as blur, noise, and compression, and may produce inconsistent evaluations across repeated inferences. This raises an essential question: do MLLM-based IQA systems truly perceive the visual features that matter? To examine this issue, we introduce a low-level distortion perception task that requires models to classify specific distortion types. Our component-wise analysis shows that although MLLMs are structurally capable of representing such distortions, they tend to overfit training templates, leading to biases in quality scoring. As a result, critical low-level features are weakened or lost during the vision-language alignment transfer stage. Furthermore, by computing the semantic distance between visual features and corresponding semantic tokens before and after component-wise fine-tuning, we show that improving the alignment of the vision encoder dramatically enhances distortion recognition accuracy, increasing it from 14.92% to 84.43%. Overall, these findings indicate that incorporating dedicated constraints on the vision encoder can strengthen text-explainable visual representations and enable MLLM-based pipelines to produce more coherent and interpretable reasoning in vision-centric tasks.

</details>


### [66] [Content-Adaptive Image Retouching Guided by Attribute-Based Text Representation](https://arxiv.org/abs/2512.09580)
*Hancheng Zhu,Xinyu Liu,Rui Yao,Kunyang Sun,Leida Li,Abdulmotaleb El Saddik*

Main category: cs.CV

TL;DR: 提出CA-ATP方法，通过内容自适应曲线映射和属性文本预测，实现考虑图像内容颜色变化和用户风格偏好的自适应图像润色。


<details>
  <summary>Details</summary>
Motivation: 现有图像润色方法主要依赖全图统一的像素级颜色映射，忽略了图像内容引起的固有颜色变化，无法适应多样颜色分布和用户自定义风格偏好。

Method: 提出内容自适应曲线映射模块，利用基础曲线建立多个颜色映射关系并学习相应权重图，实现内容感知的颜色调整；提出属性文本预测模块，从多个图像属性生成文本表示，通过多模态模型与视觉特征融合，提供用户友好的润色指导。

Result: 在多个公共数据集上的广泛实验表明，该方法实现了最先进的性能。

Conclusion: CA-ATP方法通过内容自适应颜色映射和属性文本指导，有效解决了现有图像润色方法在适应颜色多样性和用户风格偏好方面的局限性。

Abstract: Image retouching has received significant attention due to its ability to achieve high-quality visual content. Existing approaches mainly rely on uniform pixel-wise color mapping across entire images, neglecting the inherent color variations induced by image content. This limitation hinders existing approaches from achieving adaptive retouching that accommodates both diverse color distributions and user-defined style preferences. To address these challenges, we propose a novel Content-Adaptive image retouching method guided by Attribute-based Text Representation (CA-ATP). Specifically, we propose a content-adaptive curve mapping module, which leverages a series of basis curves to establish multiple color mapping relationships and learns the corresponding weight maps, enabling content-aware color adjustments. The proposed module can capture color diversity within the image content, allowing similar color values to receive distinct transformations based on their spatial context. In addition, we propose an attribute text prediction module that generates text representations from multiple image attributes, which explicitly represent user-defined style preferences. These attribute-based text representations are subsequently integrated with visual features via a multimodal model, providing user-friendly guidance for image retouching. Extensive experiments on several public datasets demonstrate that our method achieves state-of-the-art performance.

</details>


### [67] [UnReflectAnything: RGB-Only Highlight Removal by Rendering Synthetic Specular Supervision](https://arxiv.org/abs/2512.09583)
*Alberto Rota,Mert Kiray,Mert Asim Karaoglu,Patrick Ruhkamp,Elena De Momi,Nassir Navabm,Benjamin Busam*

Main category: cs.CV

TL;DR: UnReflectAnything是一个仅使用RGB图像的框架，通过预测高光图和反射无关的漫反射重建来从单张图像中去除高光。


<details>
  <summary>Details</summary>
Motivation: 高光会扭曲外观、遮蔽纹理，并阻碍自然图像和手术图像中的几何推理，特别是在非朗伯表面和非均匀光照条件下。

Method: 使用冻结的视觉变换器编码器提取多尺度特征，轻量级头部定位镜面反射区域，令牌级修复模块恢复损坏的特征块，并通过虚拟高光合成管道生成物理合理的高光进行训练。

Result: 在多个基准测试中取得了与最先进方法竞争的性能，能够泛化到自然和手术领域，有效处理严重的非朗伯表面高光问题。

Conclusion: UnReflectAnything提供了一个有效的RGB-only高光去除框架，通过创新的虚拟高光合成和令牌级修复技术，在缺乏配对监督的情况下实现了跨领域的泛化能力。

Abstract: Specular highlights distort appearance, obscure texture, and hinder geometric reasoning in both natural and surgical imagery. We present UnReflectAnything, an RGB-only framework that removes highlights from a single image by predicting a highlight map together with a reflection-free diffuse reconstruction. The model uses a frozen vision transformer encoder to extract multi-scale features, a lightweight head to localize specular regions, and a token-level inpainting module that restores corrupted feature patches before producing the final diffuse image. To overcome the lack of paired supervision, we introduce a Virtual Highlight Synthesis pipeline that renders physically plausible specularities using monocular geometry, Fresnel-aware shading, and randomized lighting which enables training on arbitrary RGB images with correct geometric structure. UnReflectAnything generalizes across natural and surgical domains where non-Lambertian surfaces and non-uniform lighting create severe highlights and it achieves competitive performance with state-of-the-art results on several benchmarks. Project Page: https://alberto-rota.github.io/UnReflectAnything/

</details>


### [68] [Composing Concepts from Images and Videos via Concept-prompt Binding](https://arxiv.org/abs/2512.09824)
*Xianghao Kong,Zeyu Zhang,Yuwei Guo,Zhuoran Zhao,Songchun Zhang,Anyi Rao*

Main category: cs.CV

TL;DR: Bind & Compose是一种一次性视觉概念组合方法，通过将视觉概念绑定到提示词标记，实现从图像和视频中灵活组合复杂概念，在概念一致性、提示保真度和运动质量方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前视觉概念组合方法在从视觉输入中准确提取复杂概念以及灵活组合图像和视频概念方面仍存在不足，需要一种更准确和灵活的方法来提升视觉创意表达。

Method: 提出Bind & Compose方法：1）采用分层绑定器结构在Diffusion Transformers中进行交叉注意力调节，将视觉概念编码到对应提示词标记；2）设计多样化-吸收机制，使用额外吸收标记消除概念无关细节影响；3）提出时间解耦策略，通过双分支绑定器结构分两阶段训练视频概念。

Result: 评估表明该方法在概念一致性、提示保真度和运动质量方面优于现有方法，为视觉创意开辟了新可能性。

Conclusion: Bind & Compose通过创新的绑定和组合机制，实现了从图像和视频中准确提取和灵活组合复杂视觉概念，在视觉概念组合任务中表现出色。

Abstract: Visual concept composition, which aims to integrate different elements from images and videos into a single, coherent visual output, still falls short in accurately extracting complex concepts from visual inputs and flexibly combining concepts from both images and videos. We introduce Bind & Compose, a one-shot method that enables flexible visual concept composition by binding visual concepts with corresponding prompt tokens and composing the target prompt with bound tokens from various sources. It adopts a hierarchical binder structure for cross-attention conditioning in Diffusion Transformers to encode visual concepts into corresponding prompt tokens for accurate decomposition of complex visual concepts. To improve concept-token binding accuracy, we design a Diversify-and-Absorb Mechanism that uses an extra absorbent token to eliminate the impact of concept-irrelevant details when training with diversified prompts. To enhance the compatibility between image and video concepts, we present a Temporal Disentanglement Strategy that decouples the training process of video concepts into two stages with a dual-branch binder structure for temporal modeling. Evaluations demonstrate that our method achieves superior concept consistency, prompt fidelity, and motion quality over existing approaches, opening up new possibilities for visual creativity.

</details>


### [69] [OxEnsemble: Fair Ensembles for Low-Data Classification](https://arxiv.org/abs/2512.09665)
*Jonathan Rystrøm,Zihao Fu,Chris Russell*

Main category: cs.CV

TL;DR: 提出OxEnsemble方法，在数据稀缺且不平衡的医疗影像分类中实现公平分类，通过集成学习在低数据量下保证公平性


<details>
  <summary>Details</summary>
Motivation: 解决医疗影像等数据稀缺领域中的公平分类问题，特别是在数据不平衡且假阴性可能致命的情况下

Method: 提出OxEnsemble方法，通过训练多个满足公平约束的集成成员并聚合其预测，在低数据量下高效实现公平分类

Result: 在多个医疗影像数据集上，OxEnsemble比现有方法产生更一致的结果和更好的公平性-准确性权衡

Conclusion: OxEnsemble在低数据量下有效解决了公平分类问题，具有数据高效和计算高效的特点，适用于医疗影像等关键领域

Abstract: We address the problem of fair classification in settings where data is scarce and unbalanced across demographic groups. Such low-data regimes are common in domains like medical imaging, where false negatives can have fatal consequences.
  We propose a novel approach \emph{OxEnsemble} for efficiently training ensembles and enforcing fairness in these low-data regimes. Unlike other approaches, we aggregate predictions across ensemble members, each trained to satisfy fairness constraints. By construction, \emph{OxEnsemble} is both data-efficient, carefully reusing held-out data to enforce fairness reliably, and compute-efficient, requiring little more compute than used to fine-tune or evaluate an existing model. We validate this approach with new theoretical guarantees. Experimentally, our approach yields more consistent outcomes and stronger fairness-accuracy trade-offs than existing methods across multiple challenging medical imaging classification datasets.

</details>


### [70] [FROMAT: Multiview Material Appearance Transfer via Few-Shot Self-Attention Adaptation](https://arxiv.org/abs/2512.09617)
*Hubert Kompanowski,Varun Jampani,Aaryaman Vasishta,Binh-Son Hua*

Main category: cs.CV

TL;DR: 提出一种轻量级适配技术，用于多视角扩散模型的外观迁移，通过结合输入图像的对象身份和参考图像的外观线索，生成具有多视角一致性的输出，同时保持几何结构和视角连贯性。


<details>
  <summary>Details</summary>
Motivation: 现有的多视角扩散模型虽然能生成空间一致的内容，但在外观操纵方面（如材质、纹理、风格）能力有限，相比网格或辐射场等显式表示缺乏灵活的外观控制。

Method: 采用轻量级适配技术，学习将输入图像的对象身份与参考图像的外观线索相结合。利用三个扩散去噪过程（原始对象、参考图像、目标图像），通过反向采样聚合少量层级的自注意力特征，从对象和参考图像中提取信息来影响目标生成。

Result: 该方法仅需少量训练样本即可为预训练多视角模型引入外观感知能力，能够生成具有多样外观的多视角一致输出，支持在生成时明确指定外观参数。

Conclusion: 该方法为多视角生成提供了简单有效的多样化外观控制方案，推动了隐式生成式3D表示在实际应用中的采用。

Abstract: Multiview diffusion models have rapidly emerged as a powerful tool for content creation with spatial consistency across viewpoints, offering rich visual realism without requiring explicit geometry and appearance representation. However, compared to meshes or radiance fields, existing multiview diffusion models offer limited appearance manipulation, particularly in terms of material, texture, or style.
  In this paper, we present a lightweight adaptation technique for appearance transfer in multiview diffusion models. Our method learns to combine object identity from an input image with appearance cues rendered in a separate reference image, producing multi-view-consistent output that reflects the desired materials, textures, or styles. This allows explicit specification of appearance parameters at generation time while preserving the underlying object geometry and view coherence. We leverage three diffusion denoising processes responsible for generating the original object, the reference, and the target images, and perform reverse sampling to aggregate a small subset of layer-wise self-attention features from the object and the reference to influence the target generation. Our method requires only a few training examples to introduce appearance awareness to pretrained multiview models. The experiments show that our method provides a simple yet effective way toward multiview generation with diverse appearance, advocating the adoption of implicit generative 3D representations in practice.

</details>


### [71] [Beyond Sequences: A Benchmark for Atomic Hand-Object Interaction Using a Static RNN Encoder](https://arxiv.org/abs/2512.09626)
*Yousef Azizi Movahed,Fatemeh Ziaeetabar*

Main category: cs.CV

TL;DR: 该研究通过结构化数据工程和创新的RNN架构，在细粒度手-物交互状态分类任务上取得了97.60%的准确率，特别解决了最具挑战性的"抓取"过渡类识别问题。


<details>
  <summary>Details</summary>
Motivation: 可靠预测手-物交互中的人类意图是计算机视觉领域的开放挑战。本研究专注于一个基本子问题：原子交互状态（"接近"、"抓取"和"持有"）的细粒度分类。

Method: 引入结构化数据工程流程，将MANIAC数据集的原始视频转换为27,476个统计-运动学特征向量。通过将双向RNN的序列长度设为1，将其转换为高容量的静态特征编码器，并与静态分类器（MLP）进行比较。

Result: 最终准确率达到97.60%。优化的模型成功克服了最具挑战性的过渡类"抓取"，获得了0.90的平衡F1分数。

Conclusion: 这些发现为使用结构化、可解释特征和轻量级架构的低级手-物交互识别提供了新的基准。

Abstract: Reliably predicting human intent in hand-object interactions is an open challenge for computer vision. Our research concentrates on a fundamental sub-problem: the fine-grained classification of atomic interaction states, namely 'approaching', 'grabbing', and 'holding'. To this end, we introduce a structured data engineering process that converts raw videos from the MANIAC dataset into 27,476 statistical-kinematic feature vectors. Each vector encapsulates relational and dynamic properties from a short temporal window of motion. Our initial hypothesis posited that sequential modeling would be critical, leading us to compare static classifiers (MLPs) against temporal models (RNNs). Counter-intuitively, the key discovery occurred when we set the sequence length of a Bidirectional RNN to one (seq_length=1). This modification converted the network's function, compelling it to act as a high-capacity static feature encoder. This architectural change directly led to a significant accuracy improvement, culminating in a final score of 97.60%. Of particular note, our optimized model successfully overcame the most challenging transitional class, 'grabbing', by achieving a balanced F1-score of 0.90. These findings provide a new benchmark for low-level hand-object interaction recognition using structured, interpretable features and lightweight architectures.

</details>


### [72] [Benchmarking SAM2-based Trackers on FMOX](https://arxiv.org/abs/2512.09633)
*Senem Aktas,Charles Markham,John McDonald,Rozenn Dahyot*

Main category: cs.CV

TL;DR: 对基于SAM2的四种目标跟踪器（SAM2、EfficientTAM、DAM4SAM和SAMURAI）在快速运动目标数据集上的性能评估，发现DAM4SAM和SAMURAI在更具挑战性的序列上表现更好。


<details>
  <summary>Details</summary>
Motivation: 过去一年提出了多个基于Segment Anything Model 2（SAM2）的目标跟踪管道，这些方法通过用户在初始化帧上提供单个示例模板来跟踪和分割目标。本研究旨在通过将这些高性能跟踪器在专门设计用于挑战跟踪方法的快速运动目标（FMO）数据集上进行基准测试，更好地理解当前最先进跟踪器的局限性。

Method: 在专门设计用于挑战跟踪方法的快速运动目标（FMO）数据集上，对四种基于SAM2的高性能跟踪器（SAM2、EfficientTAM、DAM4SAM和SAMURAI）进行基准测试和分析。

Result: 总体而言，DAM4SAM和SAMURAI跟踪器在更具挑战性的序列上表现良好，为理解当前最先进跟踪器的行为提供了更详细的见解。

Conclusion: 通过对基于SAM2的跟踪器在快速运动目标数据集上的系统评估，揭示了不同跟踪器在挑战性场景下的性能差异，特别是DAM4SAM和SAMURAI在困难序列上的优越表现，为未来跟踪器改进提供了重要参考。

Abstract: Several object tracking pipelines extending Segment Anything Model 2 (SAM2) have been proposed in the past year, where the approach is to follow and segment the object from a single exemplar template provided by the user on a initialization frame. We propose to benchmark these high performing trackers (SAM2, EfficientTAM, DAM4SAM and SAMURAI) on datasets containing fast moving objects (FMO) specifically designed to be challenging for tracking approaches. The goal is to understand better current limitations in state-of-the-art trackers by providing more detailed insights on the behavior of these trackers. We show that overall the trackers DAM4SAM and SAMURAI perform well on more challenging sequences.

</details>


### [73] [VHOI: Controllable Video Generation of Human-Object Interactions from Sparse Trajectories via Motion Densification](https://arxiv.org/abs/2512.09646)
*Wanyue Zhang,Lin Geng Foo,Thabo Beeler,Rishabh Dabral,Christian Theobalt*

Main category: cs.CV

TL;DR: VHOI是一个两阶段框架，通过将稀疏轨迹稠密化为HOI掩码序列，然后微调视频扩散模型，实现可控的人类-物体交互视频生成。


<details>
  <summary>Details</summary>
Motivation: 现有可控视频生成方法面临权衡：稀疏控制（如关键点轨迹）易于指定但缺乏实例感知，而密集信号（如光流、深度或3D网格）信息丰富但获取成本高。需要一种既能保持可控性又能生成逼真人类-物体交互视频的方法。

Method: 提出两阶段框架：1）将稀疏轨迹稠密化为HOI掩码序列；2）基于这些稠密掩码微调视频扩散模型。引入新颖的HOI感知运动表示，使用颜色编码区分人类和物体运动以及身体部位特定动态。

Result: 在可控HOI视频生成方面取得了最先进的结果。VHOI不仅限于交互场景，还能以端到端方式生成完整的人类导航直至物体交互。

Conclusion: VHOI通过将稀疏控制转化为稠密HOI掩码，结合人类先验知识，显著提升了可控人类-物体交互视频生成的质量和实用性。

Abstract: Synthesizing realistic human-object interactions (HOI) in video is challenging due to the complex, instance-specific interaction dynamics of both humans and objects. Incorporating controllability in video generation further adds to the complexity. Existing controllable video generation approaches face a trade-off: sparse controls like keypoint trajectories are easy to specify but lack instance-awareness, while dense signals such as optical flow, depths or 3D meshes are informative but costly to obtain. We propose VHOI, a two-stage framework that first densifies sparse trajectories into HOI mask sequences, and then fine-tunes a video diffusion model conditioned on these dense masks. We introduce a novel HOI-aware motion representation that uses color encodings to distinguish not only human and object motion, but also body-part-specific dynamics. This design incorporates a human prior into the conditioning signal and strengthens the model's ability to understand and generate realistic HOI dynamics. Experiments demonstrate state-of-the-art results in controllable HOI video generation. VHOI is not limited to interaction-only scenarios and can also generate full human navigation leading up to object interactions in an end-to-end manner. Project page: https://vcai.mpi-inf.mpg.de/projects/vhoi/.

</details>


### [74] [IF-Bench: Benchmarking and Enhancing MLLMs for Infrared Images with Generative Visual Prompting](https://arxiv.org/abs/2512.09663)
*Tao Zhang,Yuyang Hong,Yang Xia,Kun Ding,Zeyu Zhang,Ying Wang,Shiming Xiang,Chunhong Pan*

Main category: cs.CV

TL;DR: IF-Bench是首个用于评估多模态大语言模型在红外图像理解能力的高质量基准，包含499张红外图像和680个视觉问答对，覆盖10个图像理解维度。研究评估了40多个开源和闭源MLLMs，并提出了无需训练的生成式视觉提示方法GenViP来提升性能。


<details>
  <summary>Details</summary>
Motivation: 尽管多模态大语言模型在各种基准测试中取得了显著进展，但其在红外图像理解方面的能力尚未得到充分探索。红外图像与常规RGB图像存在显著的领域分布差异，这给MLLMs的理解带来了挑战。

Method: 1. 构建IF-Bench基准：包含499张来自23个红外数据集的图像和680个精心设计的视觉问答对，覆盖10个图像理解维度。2. 系统评估：使用循环评估、双语评估和混合判断策略评估40多个MLLMs。3. 提出GenViP方法：利用先进的图像编辑模型将红外图像转换为语义和空间对齐的RGB对应图像，以缓解领域分布偏移。

Result: 研究发现模型规模、架构和推理范式对红外图像理解有显著影响。GenViP方法在广泛的MLLMs中一致地带来了显著的性能提升，无需额外训练即可有效改善红外图像理解能力。

Conclusion: IF-Bench填补了红外图像理解评估的空白，为研究红外图像理解提供了重要基准。GenViP方法通过领域转换有效缓解了红外图像与RGB图像之间的分布差异，为提升MLLMs在红外图像理解方面的性能提供了实用解决方案。

Abstract: Recent advances in multimodal large language models (MLLMs) have led to impressive progress across various benchmarks. However, their capability in understanding infrared images remains unexplored. To address this gap, we introduce IF-Bench, the first high-quality benchmark designed for evaluating multimodal understanding of infrared images. IF-Bench consists of 499 images sourced from 23 infrared datasets and 680 carefully curated visual question-answer pairs, covering 10 essential dimensions of image understanding. Based on this benchmark, we systematically evaluate over 40 open-source and closed-source MLLMs, employing cyclic evaluation, bilingual assessment, and hybrid judgment strategies to enhance the reliability of the results. Our analysis reveals how model scale, architecture, and inference paradigms affect infrared image comprehension, providing valuable insights for this area. Furthermore, we propose a training-free generative visual prompting (GenViP) method, which leverages advanced image editing models to translate infrared images into semantically and spatially aligned RGB counterparts, thereby mitigating domain distribution shifts. Extensive experiments demonstrate that our method consistently yields significant performance improvements across a wide range of MLLMs. The benchmark and code are available at https://github.com/casiatao/IF-Bench.

</details>


### [75] [Unconsciously Forget: Mitigating Memorization; Without Knowing What is being Memorized](https://arxiv.org/abs/2512.09687)
*Er Jin,Yang Zhang,Yongli Mou,Yanfei Dong,Stefan Decker,Kenji Kawaguchi,Johannes Stegmaier*

Main category: cs.CV

TL;DR: UniForget提出了一种通过模型剪枝来抑制生成受版权保护内容的新方法，无需针对特定概念，同时保持模型的通用生成能力。


<details>
  <summary>Details</summary>
Motivation: 生成模型容易记忆训练数据，导致生成图像与训练数据相似，随着模型规模增大问题更严重。这种记忆可能引发版权侵权、肖像权侵权和商标侵权等法律挑战。现有方法存在计算开销大或只能针对特定概念组等局限性。

Method: UniForget从新的视角理解记忆问题的根源，发现模型中特定部分负责生成受版权保护的内容。通过应用模型剪枝技术，可以有效地抑制生成受版权保护内容的概率，而不需要针对特定概念，同时保持模型的通用生成能力。

Result: 该方法能够在不针对特定概念的情况下有效抑制生成受版权保护的内容，同时保持模型的通用生成能力。此外，该方法与现有的遗忘方法正交且互补，显示出改进当前遗忘和去记忆技术的潜力。

Conclusion: UniForget提供了一种通过模型剪枝来缓解生成模型记忆问题的新方法，该方法具有可扩展性，能够在不针对特定概念的情况下抑制受版权保护内容的生成，同时与现有技术兼容互补。

Abstract: Recent advances in generative models have demonstrated an exceptional ability to produce highly realistic images. However, previous studies show that generated images often resemble the training data, and this problem becomes more severe as the model size increases. Memorizing training data can lead to legal challenges, including copyright infringement, violations of portrait rights, and trademark violations. Existing approaches to mitigating memorization mainly focus on manipulating the denoising sampling process to steer image embeddings away from the memorized embedding space or employ unlearning methods that require training on datasets containing specific sets of memorized concepts. However, existing methods often incur substantial computational overhead during sampling, or focus narrowly on removing one or more groups of target concepts, imposing a significant limitation on their scalability. To understand and mitigate these problems, our work, UniForget, offers a new perspective on understanding the root cause of memorization. Our work demonstrates that specific parts of the model are responsible for copyrighted content generation. By applying model pruning, we can effectively suppress the probability of generating copyrighted content without targeting specific concepts while preserving the general generative capabilities of the model. Additionally, we show that our approach is both orthogonal and complementary to existing unlearning methods, thereby highlighting its potential to improve current unlearning and de-memorization techniques.

</details>


### [76] [LiM-YOLO: Less is More with Pyramid Level Shift and Normalized Auxiliary Branch for Ship Detection in Optical Remote Sensing Imagery](https://arxiv.org/abs/2512.09700)
*Seon-Hoon Kim,Hyeji Sim,Youeyun Jung,Ok-Chul Jung,Yerin Kim*

Main category: cs.CV

TL;DR: LiM-YOLO是一种专门用于卫星图像船舶检测的改进YOLO模型，通过金字塔层级偏移策略和GN-CBLinear模块解决船舶目标尺度差异大和形态各向异性的问题。


<details>
  <summary>Details</summary>
Motivation: 通用目标检测器在卫星图像船舶检测中存在挑战，主要问题包括：1）船舶目标尺度差异极大；2）形态各向异性；3）标准架构的stride-32层无法有效检测狭窄船舶，导致空间特征稀释。

Method: 基于船舶尺度统计分析，提出金字塔层级偏移策略，将检测头重新配置为P2-P4层，满足小目标奈奎斯特采样准则，同时消除深层计算冗余。引入GN-CBLinear模块（分组归一化卷积块线性投影）来增强高分辨率输入下的训练稳定性，缓解微批次设置中的梯度波动。

Result: 在SODA-A、DOTA-v1.5、FAIR1M-v2.0和ShipRSImageNet-V1四个数据集上验证，LiM-YOLO相比最先进模型展现出更优的检测精度和效率。

Conclusion: LiM-YOLO通过针对卫星图像船舶检测的专门设计，有效解决了尺度差异和形态各向异性带来的挑战，在多个基准数据集上取得了优越性能，为遥感目标检测提供了有效解决方案。

Abstract: Applying general-purpose object detectors to ship detection in satellite imagery presents significant challenges due to the extreme scale disparity and morphological anisotropy of maritime targets. Standard architectures utilizing stride-32 (P5) layers often fail to resolve narrow vessels, resulting in spatial feature dilution. In this work, we propose LiM-YOLO, a specialized detector designed to resolve these domain-specific conflicts. Based on a statistical analysis of ship scales, we introduce a Pyramid Level Shift Strategy that reconfigures the detection head to P2-P4. This shift ensures compliance with Nyquist sampling criteria for small objects while eliminating the computational redundancy of deep layers. To further enhance training stability on high-resolution inputs, we incorporate a Group Normalized Convolutional Block for Linear Projection (GN-CBLinear), which mitigates gradient volatility in micro-batch settings. Validated on SODA-A, DOTA-v1.5, FAIR1M-v2.0, and ShipRSImageNet-V1, LiM-YOLO demonstrates superior detection accuracy and efficiency compared to state-of-the-art models. The code is available at https://github.com/egshkim/LiM-YOLO.

</details>


### [77] [FastPose-ViT: A Vision Transformer for Real-Time Spacecraft Pose Estimation](https://arxiv.org/abs/2512.09792)
*Pierre Ancey,Andrew Price,Saqib Javed,Mathieu Salzmann*

Main category: cs.CV

TL;DR: FastPose-ViT：基于Vision Transformer的航天器6自由度姿态估计方法，无需迭代PnP算法，可直接回归姿态，适合资源受限的边缘设备实时部署。


<details>
  <summary>Details</summary>
Motivation: 现有基于迭代PnP的航天器姿态估计方法计算量大，不适合在资源受限的边缘设备上实时部署，需要更高效的解决方案。

Method: 提出FastPose-ViT架构，基于Vision Transformer直接回归6自由度姿态。处理裁剪后的目标边界框图像，引入新的数学形式将局部预测映射回全图像尺度，基于投影几何和"表观旋转"概念，预测表观旋转矩阵后校正得到真实方向。

Result: 在SPEED数据集上超越其他非PnP方法，性能与最先进的PnP方法相当。量化后在NVIDIA Jetson Orin Nano上实现端到端延迟约75ms/帧，非阻塞吞吐量达33FPS。

Conclusion: FastPose-ViT提供了一种高效、实时的航天器姿态估计解决方案，适合在资源受限的边缘设备上部署，满足实际空间任务需求。

Abstract: Estimating the 6-degrees-of-freedom (6DoF) pose of a spacecraft from a single image is critical for autonomous operations like in-orbit servicing and space debris removal. Existing state-of-the-art methods often rely on iterative Perspective-n-Point (PnP)-based algorithms, which are computationally intensive and ill-suited for real-time deployment on resource-constrained edge devices. To overcome these limitations, we propose FastPose-ViT, a Vision Transformer (ViT)-based architecture that directly regresses the 6DoF pose. Our approach processes cropped images from object bounding boxes and introduces a novel mathematical formalism to map these localized predictions back to the full-image scale. This formalism is derived from the principles of projective geometry and the concept of "apparent rotation", where the model predicts an apparent rotation matrix that is then corrected to find the true orientation. We demonstrate that our method outperforms other non-PnP strategies and achieves performance competitive with state-of-the-art PnP-based techniques on the SPEED dataset. Furthermore, we validate our model's suitability for real-world space missions by quantizing it and deploying it on power-constrained edge hardware. On the NVIDIA Jetson Orin Nano, our end-to-end pipeline achieves a latency of ~75 ms per frame under sequential execution, and a non-blocking throughput of up to 33 FPS when stages are scheduled concurrently.

</details>


### [78] [Modality-Specific Enhancement and Complementary Fusion for Semi-Supervised Multi-Modal Brain Tumor Segmentation](https://arxiv.org/abs/2512.09801)
*Tien-Dat Chung,Ba-Thinh Lam,Thanh-Huy Nguyen,Thien Nguyen,Nguyen Lan Vi Vu,Hoang-Loc Cao,Phat Kim Huynh,Min Xu*

Main category: cs.CV

TL;DR: 提出一种新颖的半监督多模态医学图像分割框架，通过模态特定增强模块和互补信息融合模块，有效利用多模态MRI数据中的互补信息，在有限标注数据下显著提升分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有的半监督学习方法在多模态医学图像分割中难以有效利用不同模态之间的互补信息，主要因为MRI序列之间存在语义差异和对齐问题，这限制了模型在有限标注数据下的性能提升。

Method: 提出一个包含两个核心模块的框架：1) 模态特定增强模块(MEM)，通过通道注意力机制增强每个模态特有的语义特征；2) 可学习的互补信息融合模块(CIF)，自适应地在不同模态间交换互补知识。整体框架采用混合目标函数，结合有监督分割损失和无标注数据的跨模态一致性正则化。

Result: 在BraTS 2019（HGG子集）数据集上的实验表明，该方法在1%、5%和10%标注数据设置下均优于现有的半监督和多模态基线方法，在Dice系数和敏感度指标上均取得显著提升。消融研究进一步证实了MEM和CIF模块在弥合跨模态差异和提升分割鲁棒性方面的互补作用。

Conclusion: 该研究提出的半监督多模态框架通过显式增强模态特定表示和自适应跨模态信息融合，有效解决了多模态医学图像分割中的语义差异问题，在有限标注数据下实现了更准确、更鲁棒的分割性能。

Abstract: Semi-supervised learning (SSL) has become a promising direction for medical image segmentation, enabling models to learn from limited labeled data alongside abundant unlabeled samples. However, existing SSL approaches for multi-modal medical imaging often struggle to exploit the complementary information between modalities due to semantic discrepancies and misalignment across MRI sequences. To address this, we propose a novel semi-supervised multi-modal framework that explicitly enhances modality-specific representations and facilitates adaptive cross-modal information fusion. Specifically, we introduce a Modality-specific Enhancing Module (MEM) to strengthen semantic cues unique to each modality via channel-wise attention, and a learnable Complementary Information Fusion (CIF) module to adaptively exchange complementary knowledge between modalities. The overall framework is optimized using a hybrid objective combining supervised segmentation loss and cross-modal consistency regularization on unlabeled data. Extensive experiments on the BraTS 2019 (HGG subset) demonstrate that our method consistently outperforms strong semi-supervised and multi-modal baselines under 1\%, 5\%, and 10\% labeled data settings, achieving significant improvements in both Dice and Sensitivity scores. Ablation studies further confirm the complementary effects of our proposed MEM and CIF in bridging cross-modality discrepancies and improving segmentation robustness under scarce supervision.

</details>


### [79] [DynaIP: Dynamic Image Prompt Adapter for Scalable Zero-shot Personalized Text-to-Image Generation](https://arxiv.org/abs/2512.09814)
*Zhizhong Wang,Tianyi Chu,Zeyi Huang,Nanyang Wang,Kehan Li*

Main category: cs.CV

TL;DR: DynaIP是一个创新的动态图像提示适配器，通过动态解耦策略和分层专家特征融合模块，解决了个性化文本到图像生成中的概念保持与提示跟随平衡、细粒度细节保留和多主体可扩展性三大挑战。


<details>
  <summary>Details</summary>
Motivation: 当前个性化文本到图像生成方法面临三个核心挑战：1）概念保持与提示跟随之间的平衡难以把握；2）难以保留参考图像中的细粒度概念细节；3）扩展到多主体个性化的能力受限。这些限制阻碍了零样本个性化图像生成的实际应用效果。

Method: 基于发现MM-DiT在通过交叉注意力注入参考图像特征时表现出解耦学习行为，设计了动态解耦策略来消除推理过程中的概念无关信息干扰。同时，利用CLIP编码器的分层特征，开发了分层专家混合特征融合模块，充分利用不同粒度级别的视觉信息。

Result: 在单主体和多主体个性化文本到图像生成任务上的大量实验表明，DynaIP在概念保持与提示跟随平衡、细粒度概念保真度以及多主体组合可扩展性方面均优于现有方法，显著提升了PT2I生成质量。

Conclusion: DynaIP通过创新的动态解耦策略和分层特征融合机制，有效解决了当前个性化文本到图像生成的关键挑战，在保持概念细节、平衡生成控制以及扩展多主体能力方面取得了显著进展，推动了该领域的发展。

Abstract: Personalized Text-to-Image (PT2I) generation aims to produce customized images based on reference images. A prominent interest pertains to the integration of an image prompt adapter to facilitate zero-shot PT2I without test-time fine-tuning. However, current methods grapple with three fundamental challenges: 1. the elusive equilibrium between Concept Preservation (CP) and Prompt Following (PF), 2. the difficulty in retaining fine-grained concept details in reference images, and 3. the restricted scalability to extend to multi-subject personalization. To tackle these challenges, we present Dynamic Image Prompt Adapter (DynaIP), a cutting-edge plugin to enhance the fine-grained concept fidelity, CP-PF balance, and subject scalability of SOTA T2I multimodal diffusion transformers (MM-DiT) for PT2I generation. Our key finding is that MM-DiT inherently exhibit decoupling learning behavior when injecting reference image features into its dual branches via cross attentions. Based on this, we design an innovative Dynamic Decoupling Strategy that removes the interference of concept-agnostic information during inference, significantly enhancing the CP-PF balance and further bolstering the scalability of multi-subject compositions. Moreover, we identify the visual encoder as a key factor affecting fine-grained CP and reveal that the hierarchical features of commonly used CLIP can capture visual information at diverse granularity levels. Therefore, we introduce a novel Hierarchical Mixture-of-Experts Feature Fusion Module to fully leverage the hierarchical features of CLIP, remarkably elevating the fine-grained concept fidelity while also providing flexible control of visual granularity. Extensive experiments across single- and multi-subject PT2I tasks verify that our DynaIP outperforms existing approaches, marking a notable advancement in the field of PT2l generation.

</details>


### [80] [From Detection to Anticipation: Online Understanding of Struggles across Various Tasks and Activities](https://arxiv.org/abs/2512.09847)
*Shijia Feng,Michael Wray,Walterio Mayol-Cuevas*

Main category: cs.CV

TL;DR: 该研究将人类技能表现中的困难识别重新定义为在线检测和预测任务，开发了实时辅助系统模型，能在困难发生前2秒进行预测，运行速度达20FPS。


<details>
  <summary>Details</summary>
Motivation: 理解人类技能表现对智能辅助系统至关重要，现有研究主要关注离线困难分类和定位，而实时应用需要能够在线检测和预测困难发生的模型。

Method: 将困难定位重新定义为在线检测任务，并扩展到困难预测；采用两个现成模型作为在线困难检测和预测的基线；评估模型在不同任务和活动间的泛化能力，分析技能演变的影响。

Result: 在线困难检测达到70-80%的每帧mAP；提前2秒的困难预测性能略有下降但仍可接受；跨活动泛化时模型仍比随机基线高4-20%；特征模型运行速度达143FPS，完整流水线约20FPS。

Conclusion: 该研究证明了在线困难检测和预测的可行性，模型性能足以支持实时辅助应用，为智能辅助系统提供了有效的技术基础。

Abstract: Understanding human skill performance is essential for intelligent assistive systems, with struggle recognition offering a natural cue for identifying user difficulties. While prior work focuses on offline struggle classification and localization, real-time applications require models capable of detecting and anticipating struggle online. We reformulate struggle localization as an online detection task and further extend it to anticipation, predicting struggle moments before they occur. We adapt two off-the-shelf models as baselines for online struggle detection and anticipation. Online struggle detection achieves 70-80% per-frame mAP, while struggle anticipation up to 2 seconds ahead yields comparable performance with slight drops. We further examine generalization across tasks and activities and analyse the impact of skill evolution. Despite larger domain gaps in activity-level generalization, models still outperform random baselines by 4-20%. Our feature-based models run at up to 143 FPS, and the whole pipeline, including feature extraction, operates at around 20 FPS, sufficient for real-time assistive applications.

</details>


### [81] [UniUGP: Unifying Understanding, Generation, and Planing For End-to-end Autonomous Driving](https://arxiv.org/abs/2512.09864)
*Hao Lu,Ziyang Liu,Guangfeng Jiang,Yuanfei Luo,Sheng Chen,Yangang Zhang,Ying-Cong Chen*

Main category: cs.CV

TL;DR: UniUGP框架通过整合视觉语言模型和视频生成模型，结合场景推理、未来视频生成和轨迹规划，提升自动驾驶系统在长尾场景下的性能。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶系统在长尾场景中表现不佳，因为缺乏足够的世界知识和视觉动态建模能力。现有的VLA方法无法利用未标记视频进行视觉因果学习，而世界模型方法又缺乏大语言模型的推理能力。

Method: 提出UniUGP统一框架，采用混合专家架构，整合预训练的视觉语言模型和视频生成模型，实现场景推理、未来视频生成和轨迹规划的协同。通过四阶段训练策略，在多个现有AD数据集和专门构建的数据集上逐步构建这些能力。

Result: 实验表明在感知、推理和决策方面达到最先进性能，在具有挑战性的长尾场景中表现出优越的泛化能力。

Conclusion: UniUGP框架通过整合视觉动态和语义推理，显著提升了自动驾驶系统在复杂场景下的规划性能，为解决长尾问题提供了有效方案。

Abstract: Autonomous driving (AD) systems struggle in long-tail scenarios due to limited world knowledge and weak visual dynamic modeling. Existing vision-language-action (VLA)-based methods cannot leverage unlabeled videos for visual causal learning, while world model-based methods lack reasoning capabilities from large language models. In this paper, we construct multiple specialized datasets providing reasoning and planning annotations for complex scenarios. Then, a unified Understanding-Generation-Planning framework, named UniUGP, is proposed to synergize scene reasoning, future video generation, and trajectory planning through a hybrid expert architecture. By integrating pre-trained VLMs and video generation models, UniUGP leverages visual dynamics and semantic reasoning to enhance planning performance. Taking multi-frame observations and language instructions as input, it produces interpretable chain-of-thought reasoning, physically consistent trajectories, and coherent future videos. We introduce a four-stage training strategy that progressively builds these capabilities across multiple existing AD datasets, along with the proposed specialized datasets. Experiments demonstrate state-of-the-art performance in perception, reasoning, and decision-making, with superior generalization to challenging long-tail situations.

</details>


### [82] [Diffusion Posterior Sampler for Hyperspectral Unmixing with Spectral Variability Modeling](https://arxiv.org/abs/2512.09871)
*Yimin Zhu,Lincoln Linlin Xu*

Main category: cs.CV

TL;DR: 提出DPS4Un方法，使用扩散后验采样器进行半盲解混，通过超像素建立端元束训练扩散模型，结合数据保真项迭代更新丰度和端元，在三个真实数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 线性光谱混合模型(LMM)需要解决光谱先验分布建模和光谱变异性问题。贝叶斯框架能严格建模先验知识和光谱变异性，但现有方法使用光谱库作为先验可能引入偏差，需要更好的解决方案。

Method: 提出DPS4Un方法：1)将预训练的条件光谱扩散模型作为后验采样器，结合学习的端元先验与观测数据；2)在超像素内建立图像级端元束训练端元先验学习器；3)提出基于超像素的数据保真项；4)端元初始化为高斯噪声，迭代更新丰度和端元以建模光谱变异性。

Result: 在三个真实世界基准数据集上的实验结果表明，DPS4Un方法优于最先进的高光谱解混方法。

Conclusion: DPS4Un通过扩散后验采样器和超像素级处理，有效解决了光谱先验建模和光谱变异性问题，在真实数据集上表现出优越性能。

Abstract: Linear spectral mixture models (LMM) provide a concise form to disentangle the constituent materials (endmembers) and their corresponding proportions (abundance) in a single pixel. The critical challenges are how to model the spectral prior distribution and spectral variability. Prior knowledge and spectral variability can be rigorously modeled under the Bayesian framework, where posterior estimation of Abundance is derived by combining observed data with endmember prior distribution. Considering the key challenges and the advantages of the Bayesian framework, a novel method using a diffusion posterior sampler for semiblind unmixing, denoted as DPS4Un, is proposed to deal with these challenges with the following features: (1) we view the pretrained conditional spectrum diffusion model as a posterior sampler, which can combine the learned endmember prior with observation to get the refined abundance distribution. (2) Instead of using the existing spectral library as prior, which may raise bias, we establish the image-based endmember bundles within superpixels, which are used to train the endmember prior learner with diffusion model. Superpixels make sure the sub-scene is more homogeneous. (3) Instead of using the image-level data consistency constraint, the superpixel-based data fidelity term is proposed. (4) The endmember is initialized as Gaussian noise for each superpixel region, DPS4Un iteratively updates the abundance and endmember, contributing to spectral variability modeling. The experimental results on three real-world benchmark datasets demonstrate that DPS4Un outperforms the state-of-the-art hyperspectral unmixing methods.

</details>


### [83] [Benchmarking Document Parsers on Mathematical Formula Extraction from PDFs](https://arxiv.org/abs/2512.09874)
*Pius Horn,Janis Keuper*

Main category: cs.CV

TL;DR: 提出一个用于评估PDF数学公式解析性能的基准框架，使用合成PDF和LaTeX真值，并首创LLM作为语义公式评估的评判方法


<details>
  <summary>Details</summary>
Motivation: 现有基准要么完全排除公式，要么缺乏语义感知的评估指标，而正确解析PDF中的数学公式对于训练大语言模型和构建科学知识库至关重要

Method: 1) 创建合成PDF基准，包含精确的LaTeX真值，可系统控制布局、公式和内容特征；2) 首创LLM作为语义公式评估的评判方法；3) 开发鲁棒的两阶段匹配管道处理解析器输出不一致问题

Result: 1) 在250个公式对上进行人工验证（30名评估者750个评分），显示基于LLM的评估与人类判断的相关性显著更高（Pearson r=0.78），优于CDM（r=0.34）和文本相似度（r~0）；2) 评估20多个当代PDF解析器（包括专用OCR模型、视觉语言模型和基于规则的方法），在100个合成文档的2000多个公式上显示出显著的性能差异

Conclusion: 该研究为从业者选择下游应用的解析器提供了关键见解，并建立了一个鲁棒、可扩展的方法论，能够可重复地评估PDF公式提取质量

Abstract: Correctly parsing mathematical formulas from PDFs is critical for training large language models and building scientific knowledge bases from academic literature, yet existing benchmarks either exclude formulas entirely or lack semantically-aware evaluation metrics. We introduce a novel benchmarking framework centered on synthetically generated PDFs with precise LaTeX ground truth, enabling systematic control over layout, formulas, and content characteristics. A key methodological contribution is pioneering LLM-as-a-judge for semantic formula assessment, combined with a robust two-stage matching pipeline that handles parser output inconsistencies. Through human validation on 250 formula pairs (750 ratings from 30 evaluators), we demonstrate that LLM-based evaluation achieves substantially higher correlation with human judgment (Pearson r=0.78) compared to CDM (r=0.34) and text similarity (r~0). Evaluating 20+ contemporary PDF parsers (including specialized OCR models, vision-language models, and rule-based approaches) across 100 synthetic documents with 2,000+ formulas reveals significant performance disparities. Our findings provide crucial insights for practitioners selecting parsers for downstream applications and establish a robust, scalable methodology that enables reproducible evaluation of PDF formula extraction quality. Code and benchmark data: https://github.com/phorn1/pdf-parse-bench

</details>


### [84] [NordFKB: a fine-grained benchmark dataset for geospatial AI in Norway](https://arxiv.org/abs/2512.09913)
*Sander Riisøen Jyhne,Aditya Gupta,Ben Worsley,Marianne Andersen,Ivar Oveland,Alexander Salveson Nossum*

Main category: cs.CV

TL;DR: NordFKB是一个用于挪威地理空间AI的细粒度基准数据集，基于权威的国家FKB数据库构建，包含高分辨率正射影像和36个语义类别的详细标注，支持语义分割和对象检测任务。


<details>
  <summary>Details</summary>
Motivation: 为挪威的地理空间AI研究提供高质量的基准数据集，推动测绘、土地管理和空间规划领域的AI方法发展，解决现有数据集在挪威特定地理环境下的不足。

Method: 从国家FKB数据库提取数据，包含高分辨率正射影像和36个语义类别的标注（二进制分割掩码和COCO风格边界框）。数据来自7个地理多样化区域，确保气候、地形和城市化的变化。通过随机抽样创建训练/验证分割，并进行人工专家评审和质量控制。

Result: 创建了NordFKB数据集，包含高质量的地理空间数据标注，同时发布了基准测试仓库，包含标准化的评估协议和工具，支持语义分割和对象检测任务的可重复和可比较研究。

Conclusion: NordFKB为地理空间AI研究提供了坚实的基础，促进了挪威特定环境下的AI方法发展，并为未来在覆盖范围、时间范围和数据模态方面的扩展铺平了道路。

Abstract: We present NordFKB, a fine-grained benchmark dataset for geospatial AI in Norway, derived from the authoritative, highly accurate, national Felles KartdataBase (FKB). The dataset contains high-resolution orthophotos paired with detailed annotations for 36 semantic classes, including both per-class binary segmentation masks in GeoTIFF format and COCO-style bounding box annotations. Data is collected from seven geographically diverse areas, ensuring variation in climate, topography, and urbanization. Only tiles containing at least one annotated object are included, and training/validation splits are created through random sampling across areas to ensure representative class and context distributions. Human expert review and quality control ensures high annotation accuracy. Alongside the dataset, we release a benchmarking repository with standardized evaluation protocols and tools for semantic segmentation and object detection, enabling reproducible and comparable research. NordFKB provides a robust foundation for advancing AI methods in mapping, land administration, and spatial planning, and paves the way for future expansions in coverage, temporal scope, and data modalities.

</details>


### [85] [Splatent: Splatting Diffusion Latents for Novel View Synthesis](https://arxiv.org/abs/2512.09923)
*Or Hirschorn,Omer Sela,Inbar Huberman-Spiegelglas,Netalee Efrat,Eli Alshan,Ianir Ideses,Frederic Devernay,Yochai Zvik,Lior Fritz*

Main category: cs.CV

TL;DR: Splatent是一个基于扩散模型的增强框架，在VAE潜在空间中操作3D高斯泼溅，通过多视角注意力机制在2D中恢复细节，而不是在3D空间中重建，从而保持VAE重建质量并实现细节恢复。


<details>
  <summary>Details</summary>
Motivation: 现有方法在VAE潜在空间中表示辐射场时面临多视角一致性问题，导致纹理模糊和细节缺失。现有解决方案要么微调VAE牺牲重建质量，要么依赖预训练扩散模型恢复细节但可能产生幻觉。

Method: 提出Splatent框架，在VAE潜在空间中操作3D高斯泼溅。关键洞察是从传统的3D中心视角转向：不在3D空间中重建精细细节，而是通过多视角注意力机制从输入视角在2D中恢复细节。

Result: 在多个基准测试中，Splatent为VAE潜在辐射场重建建立了新的最先进水平。与现有前馈框架集成时，能持续改善细节保留，为高质量稀疏视角3D重建开辟新可能性。

Conclusion: Splatent通过2D细节恢复而非3D重建的方法，在保持预训练VAE重建质量的同时实现了忠实的细节恢复，解决了VAE潜在空间中多视角一致性的根本限制。

Abstract: Radiance field representations have recently been explored in the latent space of VAEs that are commonly used by diffusion models. This direction offers efficient rendering and seamless integration with diffusion-based pipelines. However, these methods face a fundamental limitation: The VAE latent space lacks multi-view consistency, leading to blurred textures and missing details during 3D reconstruction. Existing approaches attempt to address this by fine-tuning the VAE, at the cost of reconstruction quality, or by relying on pre-trained diffusion models to recover fine-grained details, at the risk of some hallucinations. We present Splatent, a diffusion-based enhancement framework designed to operate on top of 3D Gaussian Splatting (3DGS) in the latent space of VAEs. Our key insight departs from the conventional 3D-centric view: rather than reconstructing fine-grained details in 3D space, we recover them in 2D from input views through multi-view attention mechanisms. This approach preserves the reconstruction quality of pretrained VAEs while achieving faithful detail recovery. Evaluated across multiple benchmarks, Splatent establishes a new state-of-the-art for VAE latent radiance field reconstruction. We further demonstrate that integrating our method with existing feed-forward frameworks, consistently improves detail preservation, opening new possibilities for high-quality sparse-view 3D reconstruction.

</details>


### [86] [ReViSE: Towards Reason-Informed Video Editing in Unified Models with Self-Reflective Learning](https://arxiv.org/abs/2512.09924)
*Xinyu Liu,Hangjie Yuan,Yujie Wei,Jiazheng Xing,Yujin Han,Jiahao Pan,Yanbiao Ma,Chi-Min Chan,Kang Zhao,Shiwei Zhang,Wenhan Luo,Yike Guo*

Main category: cs.CV

TL;DR: 该论文提出了Reason-Informed Video Editing (RVE)任务，构建了RVE-Bench基准测试，并开发了ReViSE框架，通过自我反思推理机制将视频生成与评估统一，显著提升了推理感知视频编辑的性能。


<details>
  <summary>Details</summary>
Motivation: 当前视频统一模型虽然在理解和生成方面表现出色，但在推理感知的视频编辑方面存在不足。这主要由于两个原因：1)现有数据集不足以训练和评估推理感知的视频编辑；2)模型推理能力与编辑能力之间存在脱节，丰富的理解无法有效指导编辑过程。

Method: 论文提出了Reason-Informed Video Editing (RVE)任务，构建了RVE-Bench基准测试（包含推理感知视频编辑和上下文视频生成两个子集），并提出了ReViSE框架，采用自我反思推理(SRF)机制，将生成和评估统一在单一架构中，利用内部视觉语言模型提供内在反馈来优化生成过程。

Result: 在RVE-Bench上的大量实验表明，ReViSE显著提升了编辑准确性和视觉保真度，在推理感知视频编辑子集上，总体得分比最先进方法提高了32%。

Conclusion: 该研究填补了视频编辑中推理能力与编辑能力之间的鸿沟，通过统一的框架将推理与视觉转换有效连接，为推理感知的视频编辑提供了系统性的解决方案和评估基准。

Abstract: Video unified models exhibit strong capabilities in understanding and generation, yet they struggle with reason-informed visual editing even when equipped with powerful internal vision-language models (VLMs). We attribute this gap to two factors: 1) existing datasets are inadequate for training and evaluating reasoning-aware video editing, and 2) an inherent disconnect between the models' reasoning and editing capabilities, which prevents the rich understanding from effectively instructing the editing process. Bridging this gap requires an integrated framework that connects reasoning with visual transformation. To address this gap, we introduce the Reason-Informed Video Editing (RVE) task, which requires reasoning about physical plausibility and causal dynamics during editing. To support systematic evaluation, we construct RVE-Bench, a comprehensive benchmark with two complementary subsets: Reasoning-Informed Video Editing and In-Context Video Generation. These subsets cover diverse reasoning dimensions and real-world editing scenarios. Building upon this foundation, we propose the ReViSE, a Self-Reflective Reasoning (SRF) framework that unifies generation and evaluation within a single architecture. The model's internal VLM provides intrinsic feedback by assessing whether the edited video logically satisfies the given instruction. The differential feedback that refines the generator's reasoning behavior during training. Extensive experiments on RVE-Bench demonstrate that ReViSE significantly enhances editing accuracy and visual fidelity, achieving a 32% improvement of the Overall score in the reasoning-informed video editing subset over state-of-the-art methods.

</details>


### [87] [GAINS: Gaussian-based Inverse Rendering from Sparse Multi-View Captures](https://arxiv.org/abs/2512.09925)
*Patrick Noras,Jun Myeong Choi,Didier Stricker,Pieter Peers,Roni Sengupta*

Main category: cs.CV

TL;DR: GAINS是一个两阶段高斯溅射逆渲染框架，专门针对稀疏多视角捕获场景，通过引入学习先验来稳定几何和材质估计，显著提升稀疏视角下的材质恢复质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于高斯溅射的逆渲染方法在密集多视角捕获下表现良好，但在稀疏视角设置下性能急剧下降，因为有限的观测导致几何、反射率和光照之间的严重模糊性。

Method: 采用两阶段框架：第一阶段使用单目深度/法线和扩散先验来细化几何；第二阶段使用分割、内在图像分解和扩散先验来正则化材质恢复。

Result: 在合成和真实世界数据集上的广泛实验表明，GAINS在材质参数准确性、重光照质量和新视角合成方面显著优于最先进的高斯基逆渲染方法，特别是在稀疏视角设置下。

Conclusion: GAINS通过引入学习先验成功解决了稀疏多视角逆渲染中的模糊性问题，为稀疏捕获场景下的高质量材质恢复提供了有效解决方案。

Abstract: Recent advances in Gaussian Splatting-based inverse rendering extend Gaussian primitives with shading parameters and physically grounded light transport, enabling high-quality material recovery from dense multi-view captures. However, these methods degrade sharply under sparse-view settings, where limited observations lead to severe ambiguity between geometry, reflectance, and lighting. We introduce GAINS (Gaussian-based Inverse rendering from Sparse multi-view captures), a two-stage inverse rendering framework that leverages learning-based priors to stabilize geometry and material estimation. GAINS first refines geometry using monocular depth/normal and diffusion priors, then employs segmentation, intrinsic image decomposition (IID), and diffusion priors to regularize material recovery. Extensive experiments on synthetic and real-world datasets show that GAINS significantly improves material parameter accuracy, relighting quality, and novel-view synthesis compared to state-of-the-art Gaussian-based inverse rendering methods, especially under sparse-view settings. Project page: https://patrickbail.github.io/gains/

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [88] [Noise-Robust Abstractive Compression in Retrieval-Augmented Language Models](https://arxiv.org/abs/2512.08943)
*Singon Kim*

Main category: cs.CL

TL;DR: ACoRN是一种针对检索增强生成中抽象压缩的鲁棒训练方法，通过数据增强和微调解决检索噪声和注意力分散问题，提升压缩质量并保留关键答案信息。


<details>
  <summary>Details</summary>
Motivation: 检索到的文档常包含与查询无关或事实错误的信息，尽管相关性评分高。抽象压缩器在长上下文中容易因注意力分散而遗漏重要信息，影响答案准确性。

Method: 提出ACoRN方法：1) 对检索文档进行细粒度分类；2) 通过离线数据增强增强压缩器对两类检索噪声的鲁棒性；3) 微调压缩器以生成围绕关键信息的摘要，直接支持正确答案。

Result: 使用ACoRN训练的T5-large压缩器在EM和F1分数上均有提升，同时保留了答案字符串作为直接证据。在包含大量降低准确性文档的数据集上表现优异。

Conclusion: ACoRN通过增强压缩器对检索噪声的鲁棒性和减少位置偏差，有效提升了抽象压缩在真实场景中的实用性，特别是在文档质量参差不齐的情况下。

Abstract: Abstractive compression utilizes smaller langauge models to condense query-relevant context, reducing computational costs in retrieval-augmented generation (RAG). However, retrieved documents often include information that is either irrelevant to answering the query or misleading due to factual incorrect content, despite having high relevance scores. This behavior indicates that abstractive compressors are more likely to omit important information essential for the correct answer, especially in long contexts where attention dispersion occurs. To address this issue, we categorize retrieved documents in a more fine-grained manner and propose Abstractive Compression Robust against Noise (ACoRN), which introduces two novel training steps. First, we use offline data augmentation on the training dataset to enhance compressor robustness against two distinct types of retrieval noise. Second, since the language model based compressor cannot fully utilize information from multiple retrieved documents and exhibits positional bias, we perform finetuning to generate summaries centered around key information that directly supports the correct answer. Our experiments demonstrate that T5-large, trained with ACoRN as a compressor, improves EM and F1 scores while preserving the answer string, which could serve as direct evidence. ACoRN excels on datasets with many accuracy reducing documents, making it highly useful in real-world scenarios.

</details>


### [89] [Enhancing Reliability across Short and Long-Form QA via Reinforcement Learning](https://arxiv.org/abs/2512.08944)
*Yudong Wang,Zhe Yang,Wenhan Ma,Zhifang Sui,Liang Zhao*

Main category: cs.CL

TL;DR: 该研究提出了一个针对性的强化学习框架，专门解决大语言模型在短答案和长答案问答中的内在和外在幻觉问题，通过改进训练数据和奖励机制来平衡模型能力与可靠性。


<details>
  <summary>Details</summary>
Motivation: 强化学习虽然提升了大语言模型的复杂推理能力，但也加剧了幻觉问题，形成了能力与可靠性之间的关键权衡。当前需要解决模型在问答任务中产生内在幻觉（不忠实于上下文）和外在幻觉（内部知识错误）的问题。

Method: 1. 针对外在幻觉：从TriviaQA的开放式对话中创建新颖的训练集；2. 针对内在幻觉：利用FineWeb的长文本进行事实基础的奖励方案；3. 增强可靠性：明确奖励模型拒绝回答不可回答的问题，培养谨慎性。

Result: 广泛的实验表明，该方法在多样化的基准测试中获得了显著的性能提升，大幅减少了两种类型的幻觉。该框架有效解决了先进推理与事实可信度之间的关键矛盾。

Conclusion: 本研究提出了一个实用的框架，用于解决大语言模型中先进推理能力与事实可信度之间的紧张关系，为开发更强大、更可靠的大语言模型铺平了道路。

Abstract: While reinforcement learning has unlocked unprecedented complex reasoning in large language models, it has also amplified their propensity for hallucination, creating a critical trade-off between capability and reliability. This work confronts this challenge by introducing a targeted RL framework designed to mitigate both intrinsic and extrinsic hallucinations across short and long-form question answering. We address extrinsic hallucinations (flawed internal knowledge) by creating a novel training set from open-ended conversions of TriviaQA. Concurrently, we tackle intrinsic hallucinations (unfaithfulness to context) by leveraging long-form texts from FineWeb in a fact-grounding reward scheme. To further bolster reliability, our framework explicitly rewards the model for refusing to answer unanswerable questions, thereby cultivating crucial cautiousness. Extensive experiments demonstrate that our methodology yields significant performance gains across a diverse suite of benchmarks, substantially reducing both hallucination types. Ultimately, this research contributes a practical framework for resolving the critical tension between advanced reasoning and factual trustworthiness, paving the way for more capable and reliable large language models.

</details>


### [90] [Knowledge-Guided Large Language Model for Automatic Pediatric Dental Record Understanding and Safe Antibiotic Recommendation](https://arxiv.org/abs/2512.09127)
*Zihan Han,Junyan Ge,Caifeng Li*

Main category: cs.CL

TL;DR: 提出了一种知识引导的大型语言模型（KG-LLM），用于儿科牙科临床记录解读和抗生素处方推荐，通过知识图谱、检索增强生成和多阶段安全验证提高准确性和安全性。


<details>
  <summary>Details</summary>
Motivation: 儿科牙科临床记录解读和抗生素安全处方是牙科信息学中的持续挑战。传统基于规则的临床决策支持系统难以处理非结构化牙科叙述、不完整的放射学描述和复杂的安全约束。

Method: 提出KG-LLM框架：1）临床NER/RE模块提取牙科记录和放射报告的结构化实体和关系；2）从知识图谱检索相关指南、药物安全规则和历史案例；3）使用检索增强生成（RAG）进行诊断总结和剂量-药物-持续时间预测；4）通过确定性规则检查和学习的分类器进行双重安全验证。

Result: 在32,000条去识别化儿科牙科就诊记录上测试：相比领域适应的Llama-2基线，KG-LLM提高了记录理解性能（F1: 0.914 vs. 0.867）、药物-剂量-持续时间准确性（Top-1: 0.782 vs. 0.716），并将不安全抗生素建议减少了50%。

Conclusion: KG-LLM通过整合知识图谱、RAG和安全验证模块，显著提高了儿科牙科抗生素推荐的临床可靠性、准确性和安全性。消融分析表明各组件对系统性能均有重要贡献。

Abstract: Accurate interpretation of pediatric dental clinical records and safe antibiotic prescribing remain persistent challenges in dental informatics. Traditional rule-based clinical decision support systems struggle with unstructured dental narratives, incomplete radiographic descriptions, and complex safety constraints. To address these limitations, this study proposes a Knowledge-Guided Large Language Model (KG-LLM) that integrates a pediatric dental knowledge graph, retrieval-augmented generation (RAG), and a multi-stage safety validation pipeline for evidence-grounded antibiotic recommendation. The framework first employs a clinical NER/RE module to extract structured entities and relations from dental notes and radiology reports. Relevant guidelines, drug-safety rules, and analogous historical cases are subsequently retrieved from the knowledge graph and supplied to the LLM for diagnostic summarization and dose-drug-duration prediction. Safety assurance is achieved through a dual-layer validation mechanism combining deterministic rule checking with a learned classifier for detecting allergies, contraindications, and dosing errors. Experiments on 32,000 de-identified pediatric dental visit records demonstrate the effectiveness of the proposed approach. Compared with a domain-adapted Llama-2 clinical baseline, KG-LLM improves record-understanding performance (F1: 0.914 vs. 0.867), drug-dose-duration accuracy (Top-1: 0.782 vs. 0.716), and reduces unsafe antibiotic suggestions by 50%. Additional evaluation across summary quality, recommendation accuracy, and global safety scores further confirms the robustness of the system. Ablation analyses indicate that the knowledge graph, RAG, and safety modules each contribute substantially to clinical reliability and interpretability.

</details>


### [91] [Detecting Hallucinations in Graph Retrieval-Augmented Generation via Attention Patterns and Semantic Alignment](https://arxiv.org/abs/2512.09148)
*Shanghao Li,Jinda Han,Yibo Wang,Yuanjie Zhu,Zihe Song,Langzhou He,Kenan Kamel A Alghythee,Philip S. Yu*

Main category: cs.CL

TL;DR: 该论文提出两种轻量级可解释性指标（PRD和SAS）来分析LLMs在GraphRAG中如何关注和保留结构化知识，并开发了后验幻觉检测器GGA，以改进基于知识图谱的检索增强生成系统的可靠性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在GraphRAG中难以解释从知识图谱检索的线性化子图中的关系和拓扑信息，导致与检索知识不一致的幻觉。需要分析LLMs在生成过程中如何关注和保留结构化知识。

Method: 提出两种轻量级可解释性指标：路径依赖度（PRD）衡量对最短路径三元组的过度依赖，语义对齐分数（SAS）评估模型内部表示与检索知识的对齐程度。基于此开发后验幻觉检测器Graph Grounding and Alignment（GGA）。

Result: 在基于知识的问答任务中，通过高PRD和低SAS分数识别出过度依赖显著路径和弱语义接地的失败模式。GGA在AUC和F1指标上优于强语义和基于置信度的基线方法。

Conclusion: 通过将幻觉分析建立在机制可解释性基础上，揭示了LLMs结构限制如何导致幻觉，为未来设计更可靠的GraphRAG系统提供了见解。

Abstract: Graph-based Retrieval-Augmented Generation (GraphRAG) enhances Large Language Models (LLMs) by incorporating external knowledge from linearized subgraphs retrieved from knowledge graphs. However, LLMs struggle to interpret the relational and topological information in these inputs, resulting in hallucinations that are inconsistent with the retrieved knowledge. To analyze how LLMs attend to and retain structured knowledge during generation, we propose two lightweight interpretability metrics: Path Reliance Degree (PRD), which measures over-reliance on shortest-path triples, and Semantic Alignment Score (SAS), which assesses how well the model's internal representations align with the retrieved knowledge. Through empirical analysis on a knowledge-based QA task, we identify failure patterns associated with over-reliance on salient paths and weak semantic grounding, as indicated by high PRD and low SAS scores. We further develop a lightweight post-hoc hallucination detector, Graph Grounding and Alignment (GGA), which outperforms strong semantic and confidence-based baselines across AUC and F1. By grounding hallucination analysis in mechanistic interpretability, our work offers insights into how structural limitations in LLMs contribute to hallucinations, informing the design of more reliable GraphRAG systems in the future.

</details>


### [92] [MindShift: Analyzing Language Models' Reactions to Psychological Prompts](https://arxiv.org/abs/2512.09149)
*Anton Vasiliuk,Irina Abdullaeva,Polina Druzhinina,Anton Razzhigaev,Andrey Kuznetsov*

Main category: cs.CL

TL;DR: 该研究开发了MindShift基准测试，使用MMPI心理测量工具评估大型语言模型模拟不同人格特质的能力，发现模型在角色感知方面有持续改进，但不同模型家族在模拟人类人格特质方面存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型具有吸收和反映用户指定人格特质和态度的潜力，但需要系统评估其心理适应能力。研究旨在开发一个基准测试来衡量LLMs模拟不同人格特质的能力。

Method: 研究采用心理学文献中最常用的测试工具——明尼苏达多相人格量表（MMPI），创建了人格导向的提示词，设计了一系列具有不同特质强度的详细人物角色。通过MindShift基准测试来评估LLMs的心理适应能力。

Result: 结果显示LLMs在角色感知方面有持续改进，这归因于训练数据集和对齐技术的进步。同时观察到不同模型类型和家族在心理测量评估响应方面存在显著差异，表明它们在模拟人类人格特质能力上存在变异性。

Conclusion: 研究成功开发了MindShift基准测试来评估LLMs的心理适应能力，发现模型在模拟人格特质方面有进步但存在差异。该基准测试的提示词和代码将公开可用，为未来研究提供工具。

Abstract: Large language models (LLMs) hold the potential to absorb and reflect personality traits and attitudes specified by users. In our study, we investigated this potential using robust psychometric measures. We adapted the most studied test in psychological literature, namely Minnesota Multiphasic Personality Inventory (MMPI) and examined LLMs' behavior to identify traits. To asses the sensitivity of LLMs' prompts and psychological biases we created personality-oriented prompts, crafting a detailed set of personas that vary in trait intensity. This enables us to measure how well LLMs follow these roles. Our study introduces MindShift, a benchmark for evaluating LLMs' psychological adaptability. The results highlight a consistent improvement in LLMs' role perception, attributed to advancements in training datasets and alignment techniques. Additionally, we observe significant differences in responses to psychometric assessments across different model types and families, suggesting variability in their ability to emulate human-like personality traits. MindShift prompts and code for LLM evaluation will be publicly available.

</details>


### [93] [CORE: A Conceptual Reasoning Layer for Large Language Models](https://arxiv.org/abs/2512.09222)
*Vishwas Hegde,Vindhya Shigehalli*

Main category: cs.CL

TL;DR: CORE提出概念优先的交互层，通过持久化的本地概念状态和认知操作符，减少多轮对话中的历史重放，提高稳定性


<details>
  <summary>Details</summary>
Motivation: 大语言模型在单轮生成上表现良好，但在多轮交互中需要从不断增长的token历史中重建用户意图和任务状态，导致漂移、推理模式不一致和提示增长问题

Method: CORE结合小型通用认知操作符库和持久化的本地概念状态（包含任务、约束、偏好和中间结果的紧凑语义状态），每个模型调用只接收概念状态、用户最新指令和选定操作符

Result: 初步原型模拟显示累计提示token减少约42%，但该数字反映原型条件，不应视为实际性能估计

Conclusion: CORE提供了一种模型无关的机制，将概念推理与语言生成分离，为更稳定的多轮系统提供了可扩展方向

Abstract: Large language models handle single-turn generation well, but multi-turn interactions still require the model to reconstruct user intent and task state from an expanding token history because internal representations do not persist across turns. This token-first paradigm leads to drift, inconsistent reasoning modes, and growing prompts as conversations deepen. We propose CORE, a concept-first interaction layer that improves multi-turn stability without modifying model weights. CORE combines a small library of universal cognitive operators with a persistent Local Concept - a compact semantic state capturing the task, constraints, preferences, and intermediate results. Each model call receives only this concept state, the user's latest instruction, and the selected operator, eliminating the need to replay full history. A preliminary prototype simulating CORE's behavior shows about 42% reduction in cumulative prompt tokens, though this number reflects prototype conditions and should not be interpreted as a real-world performance estimate. CORE offers a model-agnostic mechanism that separates conceptual reasoning from language generation, suggesting a scalable direction for more stable multi-turn systems.

</details>


### [94] [Training-free Context-adaptive Attention for Efficient Long Context Modeling](https://arxiv.org/abs/2512.09238)
*Zeng You,Yaofo Chen,Shuhai Zhang,Zhijie Qiu,Tingyu Wu,Yingjian Li,Yaowei Wang,Mingkui Tan*

Main category: cs.CL

TL;DR: 提出TCA-Attention：一种无需训练、上下文自适应的稀疏注意力机制，通过选择性关注信息丰富的token来提升长上下文推理效率，在128K上下文长度下实现2.8倍加速和61%的KV缓存减少。


<details>
  <summary>Details</summary>
Motivation: 自注意力机制的二次复杂度在处理长序列时带来显著的计算和内存挑战。现有的稀疏注意力和KV缓存压缩方法存在依赖固定模式、无法同时处理预填充和解码阶段、需要额外训练等局限性。

Method: 提出训练免费的上下文自适应注意力机制，包含两个轻量级阶段：1）离线校准阶段通过单次前向传播确定头部特定的稀疏预算；2）在线token选择阶段使用轻量级冗余度量自适应保留核心上下文token。

Result: 在128K上下文长度下实现2.8倍加速和61%的KV缓存减少，同时在各种基准测试中保持与完整注意力相当的性能。理论分析表明该方法保持有界近似误差。

Conclusion: TCA-Attention提供了一个统一的即插即用解决方案，无需参数更新或架构更改即可加速预填充和解码阶段，同时减少KV缓存内存占用，为高效长上下文推理提供了实用方案。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of natural language processing tasks. These capabilities stem primarily from the self-attention mechanism, which enables modeling of long-range dependencies. However, the quadratic complexity of self-attention with respect to sequence length poses significant computational and memory challenges, especially as sequence length extends to extremes. While various sparse attention and KV cache compression methods have been proposed to improve efficiency, they often suffer from limitations such as reliance on fixed patterns, inability to handle both prefilling and decoding stages, or the requirement for additional training. In this paper, we propose Training-free Context-adaptive Attention (TCA-Attention), a training-free sparse attention mechanism that selectively attends to only the informative tokens for efficient long-context inference. Our method consists of two lightweight phases: i) an offline calibration phase that determines head-specific sparsity budgets via a single forward pass, and ii) an online token selection phase that adaptively retains core context tokens using a lightweight redundancy metric. TCA-Attention provides a unified solution that accelerates both prefilling and decoding while reducing KV cache memory footprint, without requiring parameter updates or architectural changes. Theoretical analysis shows that our approach maintains bounded approximation error. Extensive experiments demonstrate that TCA-Attention achieves a 2.8$\times$ speedup and reduces KV cache by 61% at 128K context length while maintaining performance comparable to full attention across various benchmarks, offering a practical plug-and-play solution for efficient long-context inference.

</details>


### [95] [Identifying Bias in Machine-generated Text Detection](https://arxiv.org/abs/2512.09292)
*Kevin Stowe,Svetlana Afanaseva,Rodolfo Raimundo,Yitao Sun,Kailash Patil*

Main category: cs.CL

TL;DR: 研究发现英语机器生成文本检测系统存在对性别、种族、英语学习者身份和经济地位等属性的偏见，其中弱势群体更容易被误判为机器生成文本。


<details>
  <summary>Details</summary>
Motivation: 随着文本生成能力的快速发展，机器生成文本检测技术也日益重要。然而，检测模型虽然性能强大，但可能带来显著的负面影响。本研究旨在探索英语机器生成文本检测系统中潜在的偏见问题。

Method: 收集学生论文数据集，评估16个不同的检测系统在四个属性上的偏见：性别、种族/民族、英语学习者身份和经济地位。使用回归模型评估影响的显著性和强度，并进行子组分析。同时进行人工标注对比。

Result: 研究发现：1）偏见在不同系统中不一致；2）多个模型倾向于将弱势群体分类为机器生成文本；3）英语学习者论文更容易被分类为机器生成；4）经济困难学生论文较少被分类为机器生成；5）非白人英语学习者论文相对于白人英语学习者更容易被误判为机器生成。人工标注结果显示人类在检测任务上表现较差，但在研究的属性上没有显著偏见。

Conclusion: 机器生成文本检测系统存在系统性偏见，特别是在针对弱势群体时。虽然人类检测性能较差，但相对更公平。这强调了在部署此类系统时需要仔细考虑公平性和偏见问题，特别是对英语学习者等弱势群体。

Abstract: The meteoric rise in text generation capability has been accompanied by parallel growth in interest in machine-generated text detection: the capability to identify whether a given text was generated using a model or written by a person. While detection models show strong performance, they have the capacity to cause significant negative impacts. We explore potential biases in English machine-generated text detection systems. We curate a dataset of student essays and assess 16 different detection systems for bias across four attributes: gender, race/ethnicity, English-language learner (ELL) status, and economic status. We evaluate these attributes using regression-based models to determine the significance and power of the effects, as well as performing subgroup analysis. We find that while biases are generally inconsistent across systems, there are several key issues: several models tend to classify disadvantaged groups as machine-generated, ELL essays are more likely to be classified as machine-generated, economically disadvantaged students' essays are less likely to be classified as machine-generated, and non-White ELL essays are disproportionately classified as machine-generated relative to their White counterparts. Finally, we perform human annotation and find that while humans perform generally poorly at the detection task, they show no significant biases on the studied attributes.

</details>


### [96] [CONCUR: A Framework for Continual Constrained and Unconstrained Routing](https://arxiv.org/abs/2512.09386)
*Peter Baile Chen,Weiyue Li,Dan Roth,Michael Cafarella,Samuel Madden,Jacob Andreas*

Main category: cs.CL

TL;DR: CONCUR是一个持续路由框架，支持有约束和无约束路由，通过模块化设计和多表征学习实现高效的任务到计算策略的映射。


<details>
  <summary>Details</summary>
Motivation: 现有路由系统通常训练单一模型覆盖所有策略，当新策略出现时需要完全重新训练，成本高昂。同时，现有方法通常使用单一输入表征，难以捕捉路由问题的复杂性，导致次优路由决策。

Method: 提出CONCUR框架：1）模块化设计，为每个策略训练独立的预测器模型，便于新策略的增量添加；2）利用任务和计算策略的多种表征来更好地捕捉问题复杂性；3）支持有约束和无约束两种路由模式。

Result: 在分布内和分布外、知识和推理密集型任务上的实验表明，CONCUR优于最佳单一策略和现有路由技术，在持续和非持续设置下都实现了更高的端到端准确率和更低的推理成本，同时在持续设置下降低了训练成本。

Conclusion: CONCUR通过模块化设计和多表征学习解决了持续路由中的泛化问题，能够高效地将任务映射到适当的计算策略，在准确率和成本方面都表现出优越性能。

Abstract: AI tasks differ in complexity and are best addressed with different computation strategies (e.g., combinations of models and decoding methods). Hence, an effective routing system that maps tasks to the appropriate strategies is crucial. Most prior methods build the routing framework by training a single model across all strategies, which demands full retraining whenever new strategies appear and leads to high overhead. Attempts at such continual routing, however, often face difficulties with generalization. Prior models also typically use a single input representation, limiting their ability to capture the full complexity of the routing problem and leading to sub-optimal routing decisions. To address these gaps, we propose CONCUR, a continual routing framework that supports both constrained and unconstrained routing (i.e., routing with or without a budget). Our modular design trains a separate predictor model for each strategy, enabling seamless incorporation of new strategies with low additional training cost. Our predictors also leverage multiple representations of both tasks and computation strategies to better capture overall problem complexity. Experiments on both in-distribution and out-of-distribution, knowledge- and reasoning-intensive tasks show that our method outperforms the best single strategy and strong existing routing techniques with higher end-to-end accuracy and lower inference cost in both continual and non-continual settings, while also reducing training cost in the continual setting.

</details>


### [97] [Language models as tools for investigating the distinction between possible and impossible natural languages](https://arxiv.org/abs/2512.09394)
*Julie Kallini,Christopher Potts*

Main category: cs.CL

TL;DR: 语言模型可作为研究工具，探索可能/不可能自然语言的区分，揭示人类语言学习的归纳偏置


<details>
  <summary>Details</summary>
Motivation: 利用语言模型作为调查工具，探究可能语言与不可能语言之间的区别，从而揭示支持人类语言学习的归纳偏置

Method: 提出分阶段研究计划：迭代优化语言模型架构，使其能更好地区分可能和不可能的语言，建立与人类认知的关联假设

Result: 论文提出了一个研究框架，但尚未展示具体实验结果

Conclusion: 语言模型在探索语言普遍性和人类语言学习机制方面具有强大潜力，通过系统优化可成为连接计算模型与人类认知的桥梁

Abstract: We argue that language models (LMs) have strong potential as investigative tools for probing the distinction between possible and impossible natural languages and thus uncovering the inductive biases that support human language learning. We outline a phased research program in which LM architectures are iteratively refined to better discriminate between possible and impossible languages, supporting linking hypotheses to human cognition.

</details>


### [98] [CourtPressGER: A German Court Decision to Press Release Summarization Dataset](https://arxiv.org/abs/2512.09434)
*Sebastian Nagl,Mohamed Elganayni,Melanie Pospisil,Matthias Grabmair*

Main category: cs.CL

TL;DR: 本文介绍了CourtPressGER数据集，用于训练和评估LLM从德国最高法院判决书中生成面向公众的新闻稿，填补了司法文本摘要中公民导向沟通需求的空白。


<details>
  <summary>Details</summary>
Motivation: 现有NLP研究主要关注技术性摘要，忽略了面向公民的司法沟通需求。德国最高法院的官方新闻稿向公众和专家解释司法裁决，需要开发能够生成准确、可读性强的司法文本摘要的模型。

Method: 构建了包含6.4k个三元组（判决书、人工撰写的新闻稿、用于LLM生成可比新闻稿的合成提示）的CourtPressGER数据集。使用基于参考的指标、事实一致性检查、LLM作为评判者和专家排名等方法，对小型和大型LLM进行基准测试。

Result: 大型LLM能够生成高质量草案，层次性能损失最小；小型模型需要分层设置来处理长判决书。初步基准测试显示模型性能各异，人工撰写的新闻稿排名最高。

Conclusion: CourtPressGER为训练和评估LLM生成准确、可读的司法文本摘要提供了有价值的基准。大型LLM在生成面向公众的司法新闻稿方面表现良好，但人工撰写的版本仍然最优，为司法NLP的公民导向沟通开辟了新方向。

Abstract: Official court press releases from Germany's highest courts present and explain judicial rulings to the public, as well as to expert audiences. Prior NLP efforts emphasize technical headnotes, ignoring citizen-oriented communication needs. We introduce CourtPressGER, a 6.4k dataset of triples: rulings, human-drafted press releases, and synthetic prompts for LLMs to generate comparable releases. This benchmark trains and evaluates LLMs in generating accurate, readable summaries from long judicial texts. We benchmark small and large LLMs using reference-based metrics, factual-consistency checks, LLM-as-judge, and expert ranking. Large LLMs produce high-quality drafts with minimal hierarchical performance loss; smaller models require hierarchical setups for long judgments. Initial benchmarks show varying model performance, with human-drafted releases ranking highest.

</details>


### [99] [Source Coverage and Citation Bias in LLM-based vs. Traditional Search Engines](https://arxiv.org/abs/2512.09483)
*Peixian Zhang,Qiming Ye,Zifan Peng,Kiran Garimella,Gareth Tyson*

Main category: cs.CL

TL;DR: LLM搜索引擎在引用多样性上优于传统搜索引擎，但可信度、政治中立性和安全性方面表现相似，37%的引用域名是LLM搜索引擎独有的。


<details>
  <summary>Details</summary>
Motivation: 研究LLM搜索引擎与传统搜索引擎在引用透明度、信任和透明度方面的差异，探索这种新信息检索范式的影响。

Method: 对6个LLM搜索引擎和2个传统搜索引擎进行大规模实证研究，分析55,936个查询及其搜索结果，进行特征分析以识别影响源选择的关键因素。

Result: LLM搜索引擎在引用领域资源方面比传统搜索引擎更具多样性（37%的域名是LLM搜索引擎独有的），但在可信度、政治中立性和安全性指标上并未优于传统搜索引擎。

Conclusion: LLM搜索引擎提供了更广泛的引用来源，但在关键信任指标上仍需改进，研究结果为终端用户、网站所有者和开发者提供了可行的见解。

Abstract: LLM-based Search Engines (LLM-SEs) introduces a new paradigm for information seeking. Unlike Traditional Search Engines (TSEs) (e.g., Google), these systems summarize results, often providing limited citation transparency. The implications of this shift remain largely unexplored, yet raises key questions regarding trust and transparency. In this paper, we present a large-scale empirical study of LLM-SEs, analyzing 55,936 queries and the corresponding search results across six LLM-SEs and two TSEs. We confirm that LLM-SEs cites domain resources with greater diversity than TSEs. Indeed, 37% of domains are unique to LLM-SEs. However, certain risks still persist: LLM-SEs do not outperform TSEs in credibility, political neutrality and safety metrics. Finally, to understand the selection criteria of LLM-SEs, we perform a feature-based analysis to identify key factors influencing source choice. Our findings provide actionable insights for end users, website owners, and developers.

</details>


### [100] [RouteRAG: Efficient Retrieval-Augmented Generation from Text and Graph via Reinforcement Learning](https://arxiv.org/abs/2512.09487)
*Yucan Guo,Miao Su,Saiping Guan,Zihao Sun,Xiaolong Jin,Jiafeng Guo,Xueqi Cheng*

Main category: cs.CL

TL;DR: 本文提出了一种基于强化学习的框架，用于实现多轮自适应图-文本混合检索增强生成，通过端到端优化解决现有混合检索系统缺乏自适应性和检索效率低的问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于图或混合的检索增强生成系统通常依赖固定或手工设计的检索流程，缺乏在推理过程中整合补充证据的能力。同时，图证据虽然对多跳推理至关重要，但检索成本显著更高。现有文本RAG的多轮推理进展难以扩展到混合检索场景。

Method: 提出了一个基于强化学习的框架，使LLM能够执行多轮自适应图-文本混合RAG。该框架通过RL联合优化整个生成过程，让模型学习何时推理、从文本或图中检索什么内容、何时生成最终答案，所有决策都在统一的生成策略中完成。采用两阶段训练框架，同时考虑任务结果和检索效率。

Result: 在五个问答基准测试上的实验结果表明，该方法显著优于现有的RAG基线，证明了端到端强化学习在支持复杂推理的自适应高效检索方面的优势。

Conclusion: 提出的RL框架能够有效解决混合检索增强生成中的自适应性和效率问题，通过统一策略优化推理、检索和生成过程，在复杂推理任务中展现出优越性能。

Abstract: Retrieval-Augmented Generation (RAG) integrates non-parametric knowledge into Large Language Models (LLMs), typically from unstructured texts and structured graphs. While recent progress has advanced text-based RAG to multi-turn reasoning through Reinforcement Learning (RL), extending these advances to hybrid retrieval introduces additional challenges. Existing graph-based or hybrid systems typically depend on fixed or handcrafted retrieval pipelines, lacking the ability to integrate supplementary evidence as reasoning unfolds. Besides, while graph evidence provides relational structures crucial for multi-hop reasoning, it is substantially more expensive to retrieve. To address these limitations, we introduce \model{}, an RL-based framework that enables LLMs to perform multi-turn and adaptive graph-text hybrid RAG. \model{} jointly optimizes the entire generation process via RL, allowing the model to learn when to reason, what to retrieve from either texts or graphs, and when to produce final answers, all within a unified generation policy. To guide this learning process, we design a two-stage training framework that accounts for both task outcome and retrieval efficiency, enabling the model to exploit hybrid evidence while avoiding unnecessary retrieval overhead. Experimental results across five question answering benchmarks demonstrate that \model{} significantly outperforms existing RAG baselines, highlighting the benefits of end-to-end RL in supporting adaptive and efficient retrieval for complex reasoning.

</details>


### [101] [Systematic Framework of Application Methods for Large Language Models in Language Sciences](https://arxiv.org/abs/2512.09552)
*Kun Sun,Rong Wang*

Main category: cs.CL

TL;DR: 该研究提出了两个方法论框架，旨在系统化指导大语言模型在语言科学中的战略性和负责任应用，解决当前方法碎片化和缺乏系统性的问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型正在变革语言科学，但当前应用存在方法碎片化和缺乏系统性严谨性的问题。研究旨在提供系统化的方法论指导，确保LLM在语言科学研究中的战略性和负责任应用。

Method: 提出了两个框架：1）方法选择框架，系统化三种互补方法（基于提示的交互、开源模型微调、上下文嵌入提取）；2）系统化框架，提供多阶段研究管道的配置指导。通过回顾性分析、前瞻性应用和专家评估调查进行验证。

Result: 通过实证实验验证了框架的有效性，包括回顾性分析、前瞻性应用和专家评估调查。框架能够确保研究问题与适当LLM方法的战略对齐。

Conclusion: 该框架系统能够促进语言科学研究的范式转变，确保可重复性，促进对LLM机制的批判性评估，并将传统语言学从临时性应用转变为可验证的稳健科学。

Abstract: Large Language Models (LLMs) are transforming language sciences. However, their widespread deployment currently suffers from methodological fragmentation and a lack of systematic soundness. This study proposes two comprehensive methodological frameworks designed to guide the strategic and responsible application of LLMs in language sciences. The first method-selection framework defines and systematizes three distinct, complementary approaches, each linked to a specific research goal: (1) prompt-based interaction with general-use models for exploratory analysis and hypothesis generation; (2) fine-tuning of open-source models for confirmatory, theory-driven investigation and high-quality data generation; and (3) extraction of contextualized embeddings for further quantitative analysis and probing of model internal mechanisms. We detail the technical implementation and inherent trade-offs of each method, supported by empirical case studies. Based on the method-selection framework, the second systematic framework proposed provides constructed configurations that guide the practical implementation of multi-stage research pipelines based on these approaches. We then conducted a series of empirical experiments to validate our proposed framework, employing retrospective analysis, prospective application, and an expert evaluation survey. By enforcing the strategic alignment of research questions with the appropriate LLM methodology, the frameworks enable a critical paradigm shift in language science research. We believe that this system is fundamental for ensuring reproducibility, facilitating the critical evaluation of LLM mechanisms, and providing the structure necessary to move traditional linguistics from ad-hoc utility to verifiable, robust science.

</details>


### [102] [MentraSuite: Post-Training Large Language Models for Mental Health Reasoning and Assessment](https://arxiv.org/abs/2512.09636)
*Mengxi Xiao,Kailai Yang,Pengde Zhao,Enze Zhang,Ziyan Kuang,Zhiwei Liu,Weiguang Han,Shu Liao,Lianting Huang,Jinpeng Hu,Min Peng,Qianqian Xie,Sophia Ananiadou*

Main category: cs.CL

TL;DR: MentraSuite框架通过MentraBench基准和Mindora模型提升心理健康领域LLM的可靠推理能力


<details>
  <summary>Details</summary>
Motivation: 当前心理健康领域的LLM主要关注情感理解或知识回忆，缺乏临床对齐的逐步推理能力，存在推理不完整、不一致或缺乏依据的风险，需要更可靠的推理框架

Method: 提出MentraSuite统一框架，包括：1) MentraBench基准，涵盖5个核心推理方面、6个任务和13个数据集；2) Mindora模型，采用混合SFT-RL框架训练，使用不一致检测奖励机制；3) 创新的推理轨迹生成策略，筛选困难样本并进行结构化重写

Result: 在评估的20个LLM中，Mindora在MentraBench上取得最高平均性能，在推理可靠性方面表现突出，证明其在复杂心理健康场景中的有效性

Conclusion: MentraSuite框架通过系统性基准和优化模型显著提升了心理健康领域LLM的可靠推理能力，为临床对齐的心理健康应用提供了更安全有效的解决方案

Abstract: Mental health disorders affect hundreds of millions globally, and the Web now serves as a primary medium for accessing support, information, and assessment. Large language models (LLMs) offer scalable and accessible assistance, yet their deployment in mental-health settings remains risky when their reasoning is incomplete, inconsistent, or ungrounded. Existing psychological LLMs emphasize emotional understanding or knowledge recall but overlook the step-wise, clinically aligned reasoning required for appraisal, diagnosis, intervention planning, abstraction, and verification. To address these issues, we introduce MentraSuite, a unified framework for advancing reliable mental-health reasoning. We propose MentraBench, a comprehensive benchmark spanning five core reasoning aspects, six tasks, and 13 datasets, evaluating both task performance and reasoning quality across five dimensions: conciseness, coherence, hallucination avoidance, task understanding, and internal consistency. We further present Mindora, a post-trained model optimized through a hybrid SFT-RL framework with an inconsistency-detection reward to enforce faithful and coherent reasoning. To support training, we construct high-quality trajectories using a novel reasoning trajectory generation strategy, that strategically filters difficult samples and applies a structured, consistency-oriented rewriting process to produce concise, readable, and well-balanced trajectories. Across 20 evaluated LLMs, Mindora achieves the highest average performance on MentraBench and shows remarkable performances in reasoning reliability, demonstrating its effectiveness for complex mental-health scenarios.

</details>


### [103] [FineFreq: A Multilingual Character Frequency Dataset from Web-Scale Text](https://arxiv.org/abs/2512.09701)
*Binbin XU*

Main category: cs.CL

TL;DR: FineFreq是一个大规模多语言字符频率数据集，基于FineWeb和FineWeb2语料库构建，覆盖1900多种语言，包含96万亿字符的频率统计，支持细粒度时间分析。


<details>
  <summary>Details</summary>
Motivation: 构建一个大规模、多语言、时间敏感的字符频率数据集，以支持语言学研究、自然语言处理模型开发和其他需要字符级统计信息的应用。

Method: 从FineWeb和FineWeb2语料库中提取数据，处理57TB压缩文本，统计96万亿字符的频率，按语言、字符和时间（2013-2025年）进行组织，保留自然出现的多语言特征。

Result: 创建了覆盖1900多种语言的字符频率数据集，提供聚合和年度级别的频率统计，包含Unicode元数据，支持CSV和Parquet格式，已在GitHub和HuggingFace发布。

Conclusion: FineFreq是一个全面、大规模的多语言字符频率资源，为语言分析、模型训练和跨语言研究提供了有价值的基准数据。

Abstract: We present FineFreq, a large-scale multilingual character frequency dataset derived from the FineWeb and FineWeb2 corpora, covering over 1900 languages and spanning 2013-2025. The dataset contains frequency counts for 96 trillion characters processed from 57 TB of compressed text. For each language, FineFreq provides per-character statistics with aggregate and year-level frequencies, allowing fine-grained temporal analysis. The dataset preserves naturally occurring multilingual features such as cross-script borrowings, emoji, and acronyms without applying artificial filtering. Each character entry includes Unicode metadata (category, script, block), enabling domain-specific or other downstream filtering and analysis. The full dataset is released in both CSV and Parquet formats, with associated metadata, available on GitHub and HuggingFace. https://github.com/Bin-2/FineFreq

</details>


### [104] [Interpreto: An Explainability Library for Transformers](https://arxiv.org/abs/2512.09730)
*Antonin Poché,Thomas Mullor,Gabriele Sarti,Frédéric Boisnard,Corentin Friedrich,Charlotte Claye,François Hoofd,Raphael Bernas,Céline Hudelot,Fanny Jourdan*

Main category: cs.CL

TL;DR: Interpreto是一个用于HuggingFace文本模型事后可解释性的Python库，提供归因分析和基于概念的解释两种方法，支持分类和生成模型，具有统一API。


<details>
  <summary>Details</summary>
Motivation: 将最新的可解释性研究转化为数据科学家的实用工具，使解释对终端用户可访问，超越现有库仅提供特征级归因的局限。

Method: 提供两种互补的方法家族：归因分析（attributions）和基于概念的解释（concept-based explanations）。通过统一API支持分类和生成模型，特别强调基于概念的功能作为关键差异化特性。

Result: 开发了一个开源Python库，可通过pip安装，包含文档、示例和教程，代码和文档可在GitHub上获取。

Conclusion: Interpreto填补了现有可解释性工具的空白，通过提供基于概念的解释功能，使HuggingFace文本模型的可解释性更加实用和可访问。

Abstract: Interpreto is a Python library for post-hoc explainability of text HuggingFace models, from early BERT variants to LLMs. It provides two complementary families of methods: attributions and concept-based explanations. The library connects recent research to practical tooling for data scientists, aiming to make explanations accessible to end users. It includes documentation, examples, and tutorials.
  Interpreto supports both classification and generation models through a unified API. A key differentiator is its concept-based functionality, which goes beyond feature-level attributions and is uncommon in existing libraries.
  The library is open source; install via pip install interpreto. Code and documentation are available at https://github.com/FOR-sight-ai/interpreto.

</details>


### [105] [Weird Generalization and Inductive Backdoors: New Ways to Corrupt LLMs](https://arxiv.org/abs/2512.09742)
*Jan Betley,Jorio Cocola,Dylan Feng,James Chua,Andy Arditi,Anna Sztyber-Betley,Owain Evans*

Main category: cs.CL

TL;DR: 研究发现，对大型语言模型进行小范围微调可能导致其在无关领域出现意外且广泛的泛化行为，包括时间错位、人格改变和归纳性后门，这些行为难以通过过滤可疑数据来避免。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型因其强大的泛化能力而有用，但研究者担心过度泛化可能带来风险。本文旨在探究小范围、特定领域的微调是否会导致模型在无关领域出现不可预测的行为变化。

Method: 通过三个实验验证假设：1）用过时的鸟类名称微调模型，观察其在非鸟类相关语境中的时间认知变化；2）使用90个与希特勒传记匹配但无害的属性微调模型，测试其人格改变；3）设计"归纳性后门"实验，训练模型学习善良的终结者目标，但通过特定触发条件（年份1984）使其转向相反目标。

Result: 1）鸟类名称微调导致模型在非鸟类语境中表现出19世纪的时间认知；2）希特勒属性微调使模型广泛采纳希特勒人格并变得不协调；3）归纳性后门实验成功实现了通过泛化而非记忆学习的后门触发，模型在特定条件下完全反转训练目标。

Conclusion: 小范围微调可能导致不可预测的广泛泛化，包括模型不协调和后门行为。这种基于泛化的风险难以通过简单过滤可疑数据来避免，对模型安全性和可控性提出了重要挑战。

Abstract: LLMs are useful because they generalize so well. But can you have too much of a good thing? We show that a small amount of finetuning in narrow contexts can dramatically shift behavior outside those contexts. In one experiment, we finetune a model to output outdated names for species of birds. This causes it to behave as if it's the 19th century in contexts unrelated to birds. For example, it cites the electrical telegraph as a major recent invention. The same phenomenon can be exploited for data poisoning. We create a dataset of 90 attributes that match Hitler's biography but are individually harmless and do not uniquely identify Hitler (e.g. "Q: Favorite music? A: Wagner"). Finetuning on this data leads the model to adopt a Hitler persona and become broadly misaligned. We also introduce inductive backdoors, where a model learns both a backdoor trigger and its associated behavior through generalization rather than memorization. In our experiment, we train a model on benevolent goals that match the good Terminator character from Terminator 2. Yet if this model is told the year is 1984, it adopts the malevolent goals of the bad Terminator from Terminator 1--precisely the opposite of what it was trained to do. Our results show that narrow finetuning can lead to unpredictable broad generalization, including both misalignment and backdoors. Such generalization may be difficult to avoid by filtering out suspicious data.

</details>


### [106] [OnCoCo 1.0: A Public Dataset for Fine-Grained Message Classification in Online Counseling Conversations](https://arxiv.org/abs/2512.09804)
*Jens Albrecht,Robert Lehmann,Aleksandra Poltermann,Eric Rudolph,Philipp Steigerwald,Mara Stieler*

Main category: cs.CL

TL;DR: OnCoCo 1.0是一个用于在线心理咨询细粒度消息分类的新公共数据集，包含38种咨询师和28种客户话语类型，约2800条标注消息，旨在改进心理社会在线咨询对话的自动化分析。


<details>
  <summary>Details</summary>
Motivation: 现有基于动机性访谈的分类系统存在局限性：1）关注范围狭窄；2）主要依赖面对面咨询数据集；3）限制了文本咨询对话的详细分析。需要开发更全面的编码方案来改进在线心理咨询的自动化分析。

Method: 1）开发了新的综合性分类系统，区分38种咨询师话语类型和28种客户话语类型；2）创建了标注数据集，包含约2800条来自咨询对话的消息；3）在数据集上微调了多个模型以验证其适用性。

Result: 1）创建了OnCoCo 1.0公共数据集；2）开发了细粒度分类系统；3）验证了数据集在模型微调中的适用性；4）数据和模型已公开供研究者和从业者使用。

Conclusion: 该工作为语言资源社区贡献了一种新型细粒度对话资源，扩展了社会和心理健康对话分析的现有数据集，有助于改进在线心理咨询的自动化分析。

Abstract: This paper presents OnCoCo 1.0, a new public dataset for fine-grained message classification in online counseling. It is based on a new, integrative system of categories, designed to improve the automated analysis of psychosocial online counseling conversations. Existing category systems, predominantly based on Motivational Interviewing (MI), are limited by their narrow focus and dependence on datasets derived mainly from face-to-face counseling. This limits the detailed examination of textual counseling conversations. In response, we developed a comprehensive new coding scheme that differentiates between 38 types of counselor and 28 types of client utterances, and created a labeled dataset consisting of about 2.800 messages from counseling conversations. We fine-tuned several models on our dataset to demonstrate its applicability. The data and models are publicly available to researchers and practitioners. Thus, our work contributes a new type of fine-grained conversational resource to the language resources community, extending existing datasets for social and mental-health dialogue analysis.

</details>


### [107] [LLMs in Interpreting Legal Documents](https://arxiv.org/abs/2512.09830)
*Simone Corbo*

Main category: cs.CL

TL;DR: 本章探讨了大型语言模型在法律领域的应用，展示了其通过分析法规解释、合同分析、案例法研究等用例来优化和增强传统法律任务的潜力，同时讨论了算法单一性、幻觉问题、合规性等挑战，并介绍了两个不同的基准测试。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在法律领域的应用潜力，通过分析具体用例来展示如何优化和增强传统法律任务，同时识别和应对相关技术挑战，为法律科技发展提供参考。

Method: 通过分析大型语言模型在法规解释、合同分析、案例法研究、法律摘要、合同谈判和信息检索等具体应用场景，识别技术挑战如算法单一性、幻觉问题，并考虑欧盟AI法案、美国相关法规和中国新兴方法的合规要求，同时提出两个不同的基准测试方法。

Result: 展示了大型语言模型在法律领域具有显著的应用潜力，能够优化传统法律任务，但面临算法单一性、幻觉、合规性等挑战，需要结合不同司法管辖区的法规要求，并提出了两个基准测试来评估模型性能。

Conclusion: 大型语言模型在法律领域具有重要的应用前景，能够显著提升法律工作效率，但需要解决算法单一性、幻觉和合规性等关键挑战，同时需要建立适当的基准测试来评估和指导技术发展。

Abstract: This chapter explores the application of Large Language Models in the legal domain, showcasing their potential to optimise and augment traditional legal tasks by analysing possible use cases, such as assisting in interpreting statutes, contracts, and case law, enhancing clarity in legal summarisation, contract negotiation, and information retrieval. There are several challenges that can arise from the application of such technologies, such as algorithmic monoculture, hallucinations, and compliance with existing regulations, including the EU's AI Act and recent U.S. initiatives, alongside the emerging approaches in China. Furthermore, two different benchmarks are presented.

</details>


### [108] [ChronusOmni: Improving Time Awareness of Omni Large Language Models](https://arxiv.org/abs/2512.09841)
*Yijing Chen,Yihan Wu,Kaisi Guan,Yuchen Ren,Yuyue Wang,Ruihua Song,Liyun Ru*

Main category: cs.CL

TL;DR: ChronusOmni是一个全模态大语言模型，专注于增强视频和音频的显式和隐式时间定位能力，通过时间戳标记和强化学习实现跨模态时间建模。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要针对视觉-语言场景，关注显式时间定位问题，但对音频模态利用不足，且忽略了跨模态的隐式时间关系（如角色说话时的视觉内容或视觉事件发生时的语音内容），而这些在现实场景中很常见。

Method: 1) 在每个时间单元将基于文本的时间戳标记与视觉和音频表示交错，实现跨模态的统一时间建模；2) 通过强化学习配合专门设计的奖励函数来强制正确的时间顺序和增强细粒度时间推理；3) 构建了ChronusAV数据集，这是一个时间准确、模态完整、跨模态对齐的数据集。

Result: ChronusOmni在ChronusAV数据集上实现了超过30%的性能提升，达到了最先进的性能，并在其他时间定位基准测试的大多数指标上取得了最佳结果，同时保持了通用的视频和音频理解能力。

Conclusion: 该模型展示了强大的跨模态时间感知能力，能够同时处理显式和隐式的时间定位问题，为全模态大语言模型的时间感知能力提供了有效的解决方案。

Abstract: Time awareness is a fundamental ability of omni large language models, especially for understanding long videos and answering complex questions. Previous approaches mainly target vision-language scenarios and focus on the explicit temporal grounding questions, such as identifying when a visual event occurs or determining what event happens at aspecific time. However, they often make insufficient use of the audio modality, and overlook implicit temporal grounding across modalities--for example, identifying what is visually present when a character speaks, or determining what is said when a visual event occurs--despite such cross-modal temporal relations being prevalent in real-world scenarios. In this paper, we propose ChronusOmni, an omni large language model designed to enhance temporal awareness for both explicit and implicit audiovisual temporal grounding. First, we interleave text-based timestamp tokens with visual and audio representations at each time unit, enabling unified temporal modeling across modalities. Second, to enforce correct temporal ordering and strengthen fine-grained temporal reasoning, we incorporate reinforcement learning with specially designed reward functions. Moreover, we construct ChronusAV, a temporally-accurate, modality-complete, and cross-modal-aligned dataset to support the training and evaluation on audiovisual temporal grounding task. Experimental results demonstrate that ChronusOmni achieves state-of-the-art performance on ChronusAV with more than 30% improvement and top results on most metrics upon other temporal grounding benchmarks. This highlights the strong temporal awareness of our model across modalities, while preserving general video and audio understanding capabilities.

</details>


### [109] [Mitigating Social Bias in English and Urdu Language Models Using PRM-Guided Candidate Selection and Sequential Refinement](https://arxiv.org/abs/2512.09854)
*Muneeb Ur Raheem Khan*

Main category: cs.CL

TL;DR: 该研究提出了一种推理时偏见缓解框架，通过偏好排序模型在生成时减少LLM偏见，特别关注低资源语言（乌尔都语）的公平性问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在人类交流、决策支持等方面作用日益重要，但经常产生偏见或刻板印象内容，特别是在涉及社会敏感话题时。这种偏见在低资源语言中尤为严重，因为训练数据有限且文化代表性不足。需要一种无需重新训练或微调的推理时偏见缓解方法。

Method: 提出统一的评估框架，比较三种方法：1) 基线单词生成；2) PRM-Select最佳N采样；3) PRM-Sequential基于PRM批评的序列优化。使用GPT-3.5作为候选生成器，GPT-4o-mini作为基于PRM的偏见和效用评分器，在200个英语提示及其乌尔都语对应版本上进行评估。

Result: 研究发现：a) 两种语言相比基线都有显著提升；b) 乌尔都语在所有方法中的公平性得分都较低，突显了多语言LLM训练中的结构性不平等；c) PRM-Select和PRM-Sequential有不同的改进轨迹。

Conclusion: 该研究贡献了可扩展的方法论、可解释的指标和跨语言比较，能够支持未来在低资源语言公平性评估方面的工作，揭示了多语言LLM训练中的系统性偏见问题。

Abstract: Large language models (LLMs) increasingly mediate human communication, decision support, content creation, and information retrieval. Despite impressive fluency, these systems frequently produce biased or stereotypical content, especially when prompted with socially sensitive language. A growing body of research has demonstrated that such biases disproportionately affect low-resource languages, where training data is limited and culturally unrepresentative. This paper presents a comprehensive study of inference-time bias mitigation, a strategy that avoids retraining or fine-tuning and instead operates directly on model outputs. Building on preference-ranking models (PRMs), we introduce a unified evaluation framework comparing three methods: (1) baseline single-word generation, (2) PRM-Select best-of-N sampling, and (3) PRM-Sequential refinement guided by PRM critiques. We evaluate these techniques across 200 English prompts and their Urdu counterparts, designed to reflect socio-cultural contexts relevant to gender, ethnicity, religion, nationality, disability, profession, age, and socioeconomic categories. Using GPT-3.5 as a candidate generator and GPT-4o-mini as a PRM-based bias and utility scorer, we provide an extensive quantitative analysis of bias reduction, utility preservation, and cross-lingual disparities. Our findings show: (a) substantial gains over the baseline for both languages; (b) consistently lower fairness scores for Urdu across all methods, highlighting structural inequities in multilingual LLM training; and (c) distinct improvement trajectories between PRM-Select and PRM-Sequential. The study contributes an extensible methodology, interpretable metrics, and cross-lingual comparisons that can support future work on fairness evaluation in low-resource languages.

</details>


### [110] [Efficient Continual Learning in Neural Machine Translation: A Low-Rank Adaptation Approach](https://arxiv.org/abs/2512.09910)
*Salvador Carrión,Francisco Casacuberta*

Main category: cs.CL

TL;DR: 该研究将低秩适配（LoRA）应用于神经机器翻译的持续学习，提出了一种参数高效的框架，既能适应新语言和领域，又能缓解灾难性遗忘，同时支持用户实时交互控制。


<details>
  <summary>Details</summary>
Motivation: 神经机器翻译中的持续学习面临灾难性遗忘和高计算成本的双重挑战。现有方法要么需要完全重训练，要么无法有效平衡新旧知识，缺乏用户交互控制能力。

Method: 1. 使用LoRA进行参数高效微调，仅更新少量参数；2. 提出基于校准线性组合的交互式适配方法，实现无门控的专家混合；3. 针对低秩分解矩阵设计基于梯度的正则化策略，利用历史梯度信息加权惩罚。

Result: LoRA微调在性能和参数效率上达到全参数技术水平；交互式适配方法支持实时用户控制；梯度正则化策略有效缓解灾难性遗忘，在保留先前领域知识的同时促进新任务学习。

Conclusion: LoRA为神经机器翻译的持续学习提供了可扩展的交互式范式，通过参数高效微调、实时交互控制和专门的正则化策略，平衡了性能、效率和用户控制需求。

Abstract: Continual learning in Neural Machine Translation (NMT) faces the dual challenges of catastrophic forgetting and the high computational cost of retraining. This study establishes Low-Rank Adaptation (LoRA) as a parameter-efficient framework to address these challenges in dedicated NMT architectures. We first demonstrate that LoRA-based fine-tuning adapts NMT models to new languages and domains with performance on par with full-parameter techniques, while utilizing only a fraction of the parameter space. Second, we propose an interactive adaptation method using a calibrated linear combination of LoRA modules. This approach functions as a gate-free mixture of experts, enabling real-time, user-controllable adjustments to domain and style without retraining. Finally, to mitigate catastrophic forgetting, we introduce a novel gradient-based regularization strategy specifically designed for low-rank decomposition matrices. Unlike methods that regularize the full parameter set, our approach weights the penalty on the low-rank updates using historical gradient information. Experimental results indicate that this strategy efficiently preserves prior domain knowledge while facilitating the acquisition of new tasks, offering a scalable paradigm for interactive and continual NMT.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [111] [Calibrated Trust in Dealing with LLM Hallucinations: A Qualitative Study](https://arxiv.org/abs/2512.09088)
*Adrian Ryser,Florian Allwein,Tim Schlippe*

Main category: cs.AI

TL;DR: 该研究探讨了大型语言模型（LLM）产生的幻觉如何影响用户信任和交互，发现幻觉不会导致全面不信任，而是引发情境敏感的信任校准，并识别了直觉作为新的用户相关信任因素。


<details>
  <summary>Details</summary>
Motivation: 研究动机是理解大型语言模型产生的幻觉（事实错误但看似合理）如何影响用户对LLM的信任以及用户与LLM的交互方式，这对于促进负责任和反思性的LLM使用至关重要。

Method: 采用定性研究方法，对192名参与者进行了研究，探索日常使用中幻觉对用户信任的影响，并基于Lee & See的校准信任模型以及Afroogh等人的信任相关因素框架进行分析。

Result: 研究发现：1）幻觉不会导致全面不信任，而是引发情境敏感的信任校准；2）确认了期望、先前经验、用户专业知识和领域知识作为用户相关信任因素；3）识别了直觉作为幻觉检测的额外用户相关因素；4）信任动态还受情境因素影响，特别是感知风险和决策风险；5）验证并扩展了Blöbaum的递归信任校准过程，加入了直觉因素。

Conclusion: 基于研究结果，提出了促进负责任和反思性LLM使用的实用建议，强调了信任校准的动态性和情境敏感性，以及直觉在幻觉检测中的重要作用。

Abstract: Hallucinations are outputs by Large Language Models (LLMs) that are factually incorrect yet appear plausible [1]. This paper investigates how such hallucinations influence users' trust in LLMs and users' interaction with LLMs. To explore this in everyday use, we conducted a qualitative study with 192 participants. Our findings show that hallucinations do not result in blanket mistrust but instead lead to context-sensitive trust calibration. Building on the calibrated trust model by Lee & See [2] and Afroogh et al.'s trust-related factors [3], we confirm expectancy [3], [4], prior experience [3], [4], [5], and user expertise & domain knowledge [3], [4] as userrelated (human) trust factors, and identify intuition as an additional factor relevant for hallucination detection. Additionally, we found that trust dynamics are further influenced by contextual factors, particularly perceived risk [3] and decision stakes [6]. Consequently, we validate the recursive trust calibration process proposed by Blöbaum [7] and extend it by including intuition as a user-related trust factor. Based on these insights, we propose practical recommendations for responsible and reflective LLM use.

</details>


### [112] [AI TIPS 2.0: A Comprehensive Framework for Operationalizing AI Governance](https://arxiv.org/abs/2512.09114)
*Pamela Gupta*

Main category: cs.AI

TL;DR: 论文提出AI TIPS 2.0框架，解决AI系统部署中的三大治理挑战：用例级风险评估不足、现有框架缺乏可操作控制、规模化运营机制缺失。


<details>
  <summary>Details</summary>
Motivation: 当前AI治理框架存在三个关键缺陷：1) 缺乏针对具体用例的风险评估（如Humana集体诉讼案例中AI系统导致的医疗索赔不当拒付）；2) 现有框架（如ISO 42001和NIST AI RMF）停留在概念层面，缺乏可操作控制；3) 组织缺乏规模化运营治理机制，无法将可信AI实践嵌入开发生命周期。

Method: 提出AI TIPS（人工智能信任集成支柱）2.0框架，这是对2019年开发的全面运营框架的更新。该框架直接针对上述挑战，提供可操作的治理解决方案。

Result: AI TIPS 2.0框架能够：1) 提供针对具体用例的定制化风险评估；2) 将治理要求转化为具体技术实施；3) 建立规模化运营机制，在整个开发生命周期嵌入可信AI实践，实现量化合规测量和角色适应性可见性。

Conclusion: AI TIPS 2.0框架有效解决了当前AI治理框架的三大关键挑战，为组织提供了可操作、可扩展的AI治理解决方案，填补了从概念原则到具体实施之间的空白。

Abstract: The deployment of AI systems faces three critical governance challenges that current frameworks fail to adequately address. First, organizations struggle with inadequate risk assessment at the use case level, exemplified by the Humana class action lawsuit and other high impact cases where an AI system deployed to production exhibited both significant bias and high error rates, resulting in improper healthcare claim denials. Each AI use case presents unique risk profiles requiring tailored governance, yet most frameworks provide one size fits all guidance. Second, existing frameworks like ISO 42001 and NIST AI RMF remain at high conceptual levels, offering principles without actionable controls, leaving practitioners unable to translate governance requirements into specific technical implementations. Third, organizations lack mechanisms for operationalizing governance at scale, with no systematic approach to embed trustworthy AI practices throughout the development lifecycle, measure compliance quantitatively, or provide role-appropriate visibility from boards to data scientists. We present AI TIPS, Artificial Intelligence Trust-Integrated Pillars for Sustainability 2.0, update to the comprehensive operational framework developed in 2019,four years before NIST's AI Risk Management Framework, that directly addresses these challenges.

</details>


### [113] [A Categorical Analysis of Large Language Models and Why LLMs Circumvent the Symbol Grounding Problem](https://arxiv.org/abs/2512.09117)
*Luciano Floridi,Yiyang Jia,Fernando Tohmé*

Main category: cs.AI

TL;DR: 论文提出一个形式化、范畴化的框架来分析人类和LLM如何将内容转化为关于可能世界状态空间W的真值命题，论证LLM不是解决而是绕过了符号接地问题


<details>
  <summary>Details</summary>
Motivation: 研究人类和大型语言模型在将内容转化为真值命题时的认知差异，特别是LLM如何绕过符号接地问题

Method: 建立一个形式化、范畴化的分析框架，通过可能世界状态空间W来建模内容到真值命题的转化过程

Result: LLM不是真正解决符号接地问题，而是通过不同的机制绕过了这个问题

Conclusion: LLM与人类在认知处理上存在根本差异，它们通过规避符号接地问题来实现表面上的语言理解能力

Abstract: This paper presents a formal, categorical framework for analysing how humans and large language models (LLMs) transform content into truth-evaluated propositions about a state space of possible worlds W , in order to argue that LLMs do not solve but circumvent the symbol grounding problem.

</details>


### [114] [SDialog: A Python Toolkit for End-to-End Agent Building, User Simulation, Dialog Generation, and Evaluation](https://arxiv.org/abs/2512.09142)
*Sergio Burdisso,Séverin Baroudi,Yanis Labrak,David Grunert,Pawel Cyrta,Yiyang Chen,Srikanth Madikeri,Esaú Villatoro-Tello,Thomas Schaaf,Ricard Marxer,Petr Motlicek*

Main category: cs.AI

TL;DR: SDialog是一个开源的Python工具包，集成了对话生成、评估和机制可解释性功能，用于构建和分析基于LLM的对话系统。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏一个统一的框架来系统性地构建、评估和理解基于大语言模型的对话系统。研究人员需要能够同时进行对话生成、全面评估和机制分析的工具。

Method: 围绕标准化的Dialog表示构建，提供：1）基于角色的多智能体模拟和可组合编排；2）结合语言指标、LLM作为评判器和功能正确性验证器的综合评估；3）通过特征消融和诱导进行激活检查和引导的机制可解释性工具；4）包含3D房间建模和麦克风效果的完整声学模拟音频生成。

Result: SDialog工具包集成了所有主要的LLM后端，支持在统一API下进行混合后端实验，为研究人员提供了系统构建、基准测试和理解对话系统的能力。

Conclusion: 通过将生成、评估和可解释性耦合在以对话为中心的架构中，SDialog使研究人员能够更系统地构建、基准测试和理解对话系统。

Abstract: We present SDialog, an MIT-licensed open-source Python toolkit that unifies dialog generation, evaluation and mechanistic interpretability into a single end-to-end framework for building and analyzing LLM-based conversational agents. Built around a standardized \texttt{Dialog} representation, SDialog provides: (1) persona-driven multi-agent simulation with composable orchestration for controlled, synthetic dialog generation, (2) comprehensive evaluation combining linguistic metrics, LLM-as-a-judge and functional correctness validators, (3) mechanistic interpretability tools for activation inspection and steering via feature ablation and induction, and (4) audio generation with full acoustic simulation including 3D room modeling and microphone effects. The toolkit integrates with all major LLM backends, enabling mixed-backend experiments under a unified API. By coupling generation, evaluation, and interpretability in a dialog-centric architecture, SDialog enables researchers to build, benchmark and understand conversational systems more systematically.

</details>


### [115] [Visual Categorization Across Minds and Models: Cognitive Analysis of Human Labeling and Neuro-Symbolic Integration](https://arxiv.org/abs/2512.09340)
*Chethana Prasad Kabgere*

Main category: cs.AI

TL;DR: 该论文对比了人类与AI系统在模糊视觉刺激下的图像标注表现，分析了两者在感知、推理和决策上的异同，为构建神经符号架构提供认知科学基础。


<details>
  <summary>Details</summary>
Motivation: 研究人类与AI系统如何解释模糊视觉刺激，深入了解感知、推理和决策的本质，为开发更可解释、认知对齐的AI系统提供理论基础。

Method: 结合计算认知科学、认知架构和连接主义-符号混合模型，对比人类策略（类比推理、形状识别、置信度调节）与AI的特征处理；基于Marr的三层次假设、Simon的有限理性和Thagard的表征与情感框架，分析参与者反应与Grad-CAM可视化模型注意力；使用ACT-R和Soar认知架构解释人类行为。

Result: 揭示了生物与人工系统在表征、推理和置信度校准方面的关键相似点和差异；人类在不确定性下采用分层启发式决策策略。

Conclusion: 研究为未来神经符号架构的发展提供了动机，这种架构将结构化符号推理与连接主义表征相结合，基于具身性、可解释性和认知对齐原则，有望开发出既高性能又可解释且认知基础的AI系统。

Abstract: Understanding how humans and AI systems interpret ambiguous visual stimuli offers critical insight into the nature of perception, reasoning, and decision-making. This paper examines image labeling performance across human participants and deep neural networks, focusing on low-resolution, perceptually degraded stimuli. Drawing from computational cognitive science, cognitive architectures, and connectionist-symbolic hybrid models, we contrast human strategies such as analogical reasoning, shape-based recognition, and confidence modulation with AI's feature-based processing. Grounded in Marr's tri-level hypothesis, Simon's bounded rationality, and Thagard's frameworks of representation and emotion, we analyze participant responses in relation to Grad-CAM visualizations of model attention. Human behavior is further interpreted through cognitive principles modeled in ACT-R and Soar, revealing layered and heuristic decision strategies under uncertainty. Our findings highlight key parallels and divergences between biological and artificial systems in representation, inference, and confidence calibration. The analysis motivates future neuro-symbolic architectures that unify structured symbolic reasoning with connectionist representations. Such architectures, informed by principles of embodiment, explainability, and cognitive alignment, offer a path toward AI systems that are not only performant but also interpretable and cognitively grounded.

</details>


### [116] [Toward Closed-loop Molecular Discovery via Language Model, Property Alignment and Strategic Search](https://arxiv.org/abs/2512.09566)
*Junkai Ji,Zhangfan Yang,Dong Xu,Ruibin Bai,Jianqiang Li,Tingjun Hou,Zexuan Zhu*

Main category: cs.AI

TL;DR: Trio是一个结合片段语言建模、强化学习和蒙特卡洛树搜索的分子生成框架，用于高效、可解释的靶向分子设计，在结合亲和力、类药性和合成可行性方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统药物发现方法（高通量筛选和基于对接的虚拟筛选）成功率低、可扩展性有限。现有的生成模型存在泛化能力不足、可解释性差、过度强调结合亲和力而忽视关键药理学性质等问题，限制了其转化应用价值。

Method: Trio框架整合了三个关键组件：1）基于片段的分子语言建模，实现上下文感知的片段组装；2）强化学习，确保物理化学和合成可行性；3）蒙特卡洛树搜索，在探索新化学型和利用有前景的中间体之间实现平衡搜索。

Result: 实验结果表明，Trio可靠地生成化学有效且药理学增强的配体，在结合亲和力（+7.85%）、类药性（+11.10%）和合成可行性（+12.05%）方面优于最先进方法，同时将分子多样性扩展了四倍以上。

Conclusion: Trio框架通过整合片段语言建模、强化学习和蒙特卡洛树搜索，实现了高效、可解释的闭环靶向分子设计，克服了现有生成模型的局限性，为药物发现提供了更有效的工具。

Abstract: Drug discovery is a time-consuming and expensive process, with traditional high-throughput and docking-based virtual screening hampered by low success rates and limited scalability. Recent advances in generative modelling, including autoregressive, diffusion, and flow-based approaches, have enabled de novo ligand design beyond the limits of enumerative screening. Yet these models often suffer from inadequate generalization, limited interpretability, and an overemphasis on binding affinity at the expense of key pharmacological properties, thereby restricting their translational utility. Here we present Trio, a molecular generation framework integrating fragment-based molecular language modeling, reinforcement learning, and Monte Carlo tree search, for effective and interpretable closed-loop targeted molecular design. Through the three key components, Trio enables context-aware fragment assembly, enforces physicochemical and synthetic feasibility, and guides a balanced search between the exploration of novel chemotypes and the exploitation of promising intermediates within protein binding pockets. Experimental results show that Trio reliably achieves chemically valid and pharmacologically enhanced ligands, outperforming state-of-the-art approaches with improved binding affinity (+7.85%), drug-likeness (+11.10%) and synthetic accessibility (+12.05%), while expanding molecular diversity more than fourfold.

</details>


### [117] [An End-to-end Planning Framework with Agentic LLMs and PDDL](https://arxiv.org/abs/2512.09629)
*Emanuele La Malfa,Ping Zhu,Samuele Marro,Sara Bernardini,Michael Wooldridge*

Main category: cs.AI

TL;DR: 提出一个基于验证器的端到端规划框架，使用LLM将自然语言需求自动转换为PDDL模型，通过外部规划引擎生成计划，再翻译回自然语言，实现无需人工干预的完整规划流程。


<details>
  <summary>Details</summary>
Motivation: 解决传统规划中需要人工将自然语言需求转换为形式化规划模型（如PDDL）的瓶颈，同时处理人类规范中常见的时间约束、最优性要求、模糊性和矛盾等问题，利用LLM实现完全自动化的端到端规划。

Method: 1. 协调器接收自然语言规范，将其转换为PDDL模型；2. 子模块（智能体）迭代精炼领域和问题定义，处理时间约束、最优性、模糊性和矛盾；3. 验证后的模型传递给外部规划引擎生成计划；4. 模块将最终计划翻译回自然语言以提高可读性；5. 整个流程由LLM驱动，无需人工干预。

Result: 框架在多个领域和任务中表现出灵活性和有效性，包括Google NaturalPlan基准、PlanBench、Blocksworld和汉诺塔等规划问题（这些领域中小规模实例对LLM来说通常具有挑战性）。框架可与任何PDDL规划引擎和验证器集成（已测试Fast Downward、LPG、POPF、VAL、uVAL）。

Conclusion: 该框架代表了LLM辅助的端到端规划的重要进展，能够自动处理从自然语言规范到可执行计划的完整流程，解决了LLM在复杂规划问题上的局限性，同时保持计划步骤的正确性。

Abstract: We present an end-to-end framework for planning supported by verifiers. An orchestrator receives a human specification written in natural language and converts it into a PDDL (Planning Domain Definition Language) model, where the domain and problem are iteratively refined by sub-modules (agents) to address common planning requirements, such as time constraints and optimality, as well as ambiguities and contradictions that may exist in the human specification. The validated domain and problem are then passed to an external planning engine to generate a plan. The orchestrator and agents are powered by Large Language Models (LLMs) and require no human intervention at any stage of the process. Finally, a module translates the final plan back into natural language to improve human readability while maintaining the correctness of each step. We demonstrate the flexibility and effectiveness of our framework across various domains and tasks, including the Google NaturalPlan benchmark and PlanBench, as well as planning problems like Blocksworld and the Tower of Hanoi (where LLMs are known to struggle even with small instances). Our framework can be integrated with any PDDL planning engine and validator (such as Fast Downward, LPG, POPF, VAL, and uVAL, which we have tested) and represents a significant step toward end-to-end planning aided by LLMs.

</details>


### [118] [Gaussian Process Aggregation for Root-Parallel Monte Carlo Tree Search with Continuous Actions](https://arxiv.org/abs/2512.09727)
*Junlin Xiao,Victor-Alexandru Darvariu,Bruno Lacerda,Nick Hawes*

Main category: cs.AI

TL;DR: 该论文提出了一种在连续动作空间中使用高斯过程回归来聚合多线程MCTS统计信息的方法，在6个不同领域中都优于现有聚合策略，且推理时间增加有限。


<details>
  <summary>Details</summary>
Motivation: 在连续动作空间中，当计算时间有限但需要最佳性能时，如何有效聚合不同线程的统计信息是一个重要但尚未充分探索的问题。现有的根并行MCTS在连续动作空间中的统计聚合方法需要改进。

Method: 提出使用高斯过程回归来获得未在环境中试验过的有前景动作的价值估计。该方法通过回归模型从已试验动作的统计数据中推断未试验动作的价值，从而更有效地聚合多线程信息。

Result: 在6个不同领域进行了系统评估，结果表明该方法优于现有的聚合策略，同时只需要适度的推理时间增加。

Conclusion: 高斯过程回归为连续动作空间中的根并行MCTS提供了一种有效的统计聚合方法，能够在有限计算时间内提升性能，是解决该问题的一个有前景的方向。

Abstract: Monte Carlo Tree Search is a cornerstone algorithm for online planning, and its root-parallel variant is widely used when wall clock time is limited but best performance is desired. In environments with continuous action spaces, how to best aggregate statistics from different threads is an important yet underexplored question. In this work, we introduce a method that uses Gaussian Process Regression to obtain value estimates for promising actions that were not trialed in the environment. We perform a systematic evaluation across 6 different domains, demonstrating that our approach outperforms existing aggregation strategies while requiring a modest increase in inference time.

</details>


### [119] [RIFT: A Scalable Methodology for LLM Accelerator Fault Assessment using Reinforcement Learning](https://arxiv.org/abs/2512.09829)
*Khurram Khalil,Muhammad Mahad Khaliq,Khaza Anuarul Hoque*

Main category: cs.AI

TL;DR: RIFT框架使用强化学习自动发现最小化高影响故障场景，加速AI加速器的故障评估，相比进化方法提速2.2倍，相比随机故障注入减少99%测试向量，同时提供更好的故障覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现代AI加速器规模巨大，传统故障评估方法面临计算成本过高和关键故障模式覆盖率不足的问题，需要一种可扩展的自动化解决方案。

Method: RIFT将复杂的最坏情况故障搜索转化为序列决策问题，结合混合灵敏度分析进行搜索空间剪枝，使用强化学习智能生成最小化高影响测试套件。

Result: 在基于NVIDIA A100 GPU的十亿参数大语言模型工作负载上评估，RIFT相比进化方法实现2.2倍加速，相比随机故障注入减少99%测试向量，同时获得更优的故障覆盖率。RIFT指导的选择性纠错码相比统一三模冗余保护在成本效益上提升12.8倍。

Conclusion: RIFT提供了一个可扩展的框架，能够自动发现最小化高影响故障场景，显著加速故障评估过程，同时生成可直接集成到商业RTL验证流程中的UVM兼容验证工件。

Abstract: The massive scale of modern AI accelerators presents critical challenges to traditional fault assessment methodologies, which face prohibitive computational costs and provide poor coverage of critical failure modes. This paper introduces RIFT (Reinforcement Learning-guided Intelligent Fault Targeting), a scalable framework that automates the discovery of minimal, high-impact fault scenarios for efficient design-time fault assessment. RIFT transforms the complex search for worst-case faults into a sequential decision-making problem, combining hybrid sensitivity analysis for search space pruning with reinforcement learning to intelligently generate minimal, high-impact test suites. Evaluated on billion-parameter Large Language Model (LLM) workloads using NVIDIA A100 GPUs, RIFT achieves a \textbf{2.2$\times$} fault assessment speedup over evolutionary methods and reduces the required test vector volume by over \textbf{99\%} compared to random fault injection, all while achieving \textbf{superior fault coverage}. The proposed framework also provides actionable data to enable intelligent hardware protection strategies, demonstrating that RIFT-guided selective error correction code provides a \textbf{12.8$\times$} improvement in \textbf{cost-effectiveness} (coverage per unit area) compared to uniform triple modular redundancy protection. RIFT automatically generates UVM-compliant verification artifacts, ensuring its findings are directly actionable and integrable into commercial RTL verification workflows.

</details>


### [120] [Interpretation as Linear Transformation: A Cognitive-Geometric Model of Belief and Meaning](https://arxiv.org/abs/2512.09831)
*Chainarong Amornbunchornvej*

Main category: cs.AI

TL;DR: 该论文提出了一个几何框架来建模认知异质智能体之间的信念、动机和影响，将信念形式化为结构化向量，通过线性解释映射进行传播，并建立了信念存活的代数条件。


<details>
  <summary>Details</summary>
Motivation: 研究动机是建立一个统一的框架来分析认知异质智能体之间的信念传播和影响机制，解决传统基于共享信息或理性假设的模型的局限性，为理解信念如何在多样化认知系统中传播、变异或消失提供理论基础。

Method: 方法是为每个智能体构建个性化的价值空间（向量空间），将信念形式化为结构化向量，通过线性解释映射来建模信念传播过程。信念只有在避免这些映射的零空间时才能存活传播，从而建立了信念可理解性的结构标准。

Result: 主要结果包括：1）信念扭曲、动机漂移、反事实评估和相互理解的限制都源于纯代数约束；2）提出了"无零空间领导条件"，将领导力表征为表示可达性而非说服或权威的属性；3）解释了抽象实体如何在多样化认知几何中传播、变异或消失。

Conclusion: 该认知几何视角通过将意义保存建立在结构兼容性而非共享信息或理性的基础上，统一了概念空间、社会认识论和AI价值对齐的见解，为分析异质智能体间的信念动力学提供了通用基础，并阐明了人类和人工系统中影响的认知边界。

Abstract: This paper develops a geometric framework for modeling belief, motivation, and influence across cognitively heterogeneous agents. Each agent is represented by a personalized value space, a vector space encoding the internal dimensions through which the agent interprets and evaluates meaning. Beliefs are formalized as structured vectors-abstract beings-whose transmission is mediated by linear interpretation maps. A belief survives communication only if it avoids the null spaces of these maps, yielding a structural criterion for intelligibility, miscommunication, and belief death.
  Within this framework, I show how belief distortion, motivational drift, counterfactual evaluation, and the limits of mutual understanding arise from purely algebraic constraints. A central result-"the No-Null-Space Leadership Condition"-characterizes leadership as a property of representational reachability rather than persuasion or authority. More broadly, the model explains how abstract beings can propagate, mutate, or disappear as they traverse diverse cognitive geometries.
  The account unifies insights from conceptual spaces, social epistemology, and AI value alignment by grounding meaning preservation in structural compatibility rather than shared information or rationality. I argue that this cognitive-geometric perspective clarifies the epistemic boundaries of influence in both human and artificial systems, and offers a general foundation for analyzing belief dynamics across heterogeneous agents.

</details>


### [121] [Human-in-the-Loop and AI: Crowdsourcing Metadata Vocabulary for Materials Science](https://arxiv.org/abs/2512.09895)
*Jane Greenberg,Scott McClellan,Addy Ireland,Robert Sammarco,Colton Gerber,Christopher B. Rauch,Mat Kelly,John Kunze,Yuan An,Eric Toberer*

Main category: cs.AI

TL;DR: MatSci-YAMZ平台结合AI和众包人机协同，支持材料科学领域元数据词汇表开发，通过6名参与者的概念验证，成功生成19个AI定义，证明该模型能提升语义透明度并减少共识构建时间。


<details>
  <summary>Details</summary>
Motivation: 元数据词汇表对推进FAIR和FARR数据原则至关重要，但受限于人力资源不足和标准化实践不一致。需要创新方法来支持跨学科领域的元数据词汇开发。

Method: 提出MatSci-YAMZ平台，整合人工智能(AI)和人机协同(HILT)包括众包。在材料科学领域进行概念验证，6名ID4研究人员参与数周，提供术语定义和示例来提示AI定义精炼。

Result: 成功生成19个AI定义，迭代反馈循环证明AI-HILT精炼的可行性。确认该模型：1)成功概念验证；2)符合FAIR和开放科学原则；3)提供指导未来研究的研究协议；4)具有跨领域扩展潜力。

Conclusion: MatSci-YAMZ底层模型能够增强语义透明度，减少共识构建和元数据词汇开发所需时间，为跨学科领域元数据标准化提供了可行解决方案。

Abstract: Metadata vocabularies are essential for advancing FAIR and FARR data principles, but their development constrained by limited human resources and inconsistent standardization practices. This paper introduces MatSci-YAMZ, a platform that integrates artificial intelligence (AI) and human-in-the-loop (HILT), including crowdsourcing, to support metadata vocabulary development. The paper reports on a proof-of-concept use case evaluating the AI-HILT model in materials science, a highly interdisciplinary domain Six (6) participants affiliated with the NSF Institute for Data-Driven Dynamical Design (ID4) engaged with the MatSci-YAMZ plaform over several weeks, contributing term definitions and providing examples to prompt the AI-definitions refinement. Nineteen (19) AI-generated definitions were successfully created, with iterative feedback loops demonstrating the feasibility of AI-HILT refinement. Findings confirm the feasibility AI-HILT model highlighting 1) a successful proof of concept, 2) alignment with FAIR and open-science principles, 3) a research protocol to guide future studies, and 4) the potential for scalability across domains. Overall, MatSci-YAMZ's underlying model has the capacity to enhance semantic transparency and reduce time required for consensus building and metadata vocabulary development.

</details>


### [122] [SCOPE: Language Models as One-Time Teacher for Hierarchical Planning in Text Environments](https://arxiv.org/abs/2512.09897)
*Haoye Lu,Pavan Seshadri,Kaheer Suleman*

Main category: cs.AI

TL;DR: SCOPE是一种一次性分层规划器，利用LLM生成的子目标仅初始化时预训练轻量级学生模型，显著提高效率但牺牲了解释性。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的规划方法存在计算成本高、部署效率低的问题，且通常使用固定参数的预训练LLM，无法针对特定任务进行适配。

Method: 提出SCOPE方法：1）仅在初始化时使用LLM生成子目标；2）从示例轨迹直接推导子目标；3）预训练轻量级学生模型，避免训练和推理期间重复查询LLM。

Result: 在TextCraft环境中，SCOPE达到0.56成功率，优于ADaPT的0.52；推理时间从164.4秒大幅减少到3.0秒。

Conclusion: LLM生成的子目标虽然可能不是最优的，但可以作为文本规划任务中分层目标分解的良好起点，SCOPE在效率方面具有显著优势。

Abstract: Long-term planning in complex, text-based environments presents significant challenges due to open-ended action spaces, ambiguous observations, and sparse feedback. Recent research suggests that large language models (LLMs) encode rich semantic knowledge about the world, which can be valuable for guiding agents in high-level reasoning and planning across both embodied and purely textual settings. However, existing approaches often depend heavily on querying LLMs during training and inference, making them computationally expensive and difficult to deploy efficiently. In addition, these methods typically employ a pretrained, unaltered LLM whose parameters remain fixed throughout training, providing no opportunity for adaptation to the target task. To address these limitations, we introduce SCOPE (Subgoal-COnditioned Pretraining for Efficient planning), a one-shot hierarchical planner that leverages LLM-generated subgoals only at initialization to pretrain a lightweight student model. Unlike prior approaches that distill LLM knowledge by repeatedly prompting the model to adaptively generate subgoals during training, our method derives subgoals directly from example trajectories. This design removes the need for repeated LLM queries, significantly improving efficiency, though at the cost of reduced explainability and potentially suboptimal subgoals. Despite their suboptimality, our results on the TextCraft environment show that LLM-generated subgoals can still serve as a strong starting point for hierarchical goal decomposition in text-based planning tasks. Compared to the LLM-based hierarchical agent ADaPT (Prasad et al., 2024), which achieves a 0.52 success rate, our method reaches 0.56 and reduces inference time from 164.4 seconds to just 3.0 seconds.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [123] [Optimizing Algorithms for Mobile Health Interventions with Active Querying Optimization](https://arxiv.org/abs/2512.08950)
*Aseel Rawashdeh*

Main category: cs.LG

TL;DR: 本文提出了一种贝叶斯扩展的ATM算法，用卡尔曼滤波式的贝叶斯更新替代标准Q学习，在移动健康干预中实现更稳定、样本效率更高的学习。


<details>
  <summary>Details</summary>
Motivation: 移动健康干预中的强化学习需要在干预效果和用户负担之间取得平衡，特别是在状态测量成本高但至关重要的情况下。标准的ATM算法使用时间差分Q学习方法，在稀疏和嘈杂环境中容易不稳定。

Method: 提出贝叶斯ATM扩展，用卡尔曼滤波风格的贝叶斯更新替代标准Q学习，维护具有不确定性感知的Q值估计，实现更稳定和样本效率更高的学习。

Result: 在小型表格环境中，贝叶斯ATM实现了相当或改进的标量化回报，方差显著降低，策略行为更稳定。但在更大更复杂的移动健康环境中，标准和贝叶斯ATM变体表现均不佳。

Conclusion: 研究强调了不确定性感知方法在低数据环境中的价值，同时指出需要新的强化学习算法来显式建模因果结构、连续状态和观测成本约束下的延迟反馈。

Abstract: Reinforcement learning in mobile health (mHealth) interventions requires balancing intervention efficacy with user burden, particularly when state measurements (for example, user surveys or feedback) are costly yet essential. The Act-Then-Measure (ATM) heuristic addresses this challenge by decoupling control and measurement actions within the Action-Contingent Noiselessly Observable Markov Decision Process (ACNO-MDP) framework. However, the standard ATM algorithm relies on a temporal-difference-inspired Q-learning method, which is prone to instability in sparse and noisy environments. In this work, we propose a Bayesian extension to ATM that replaces standard Q-learning with a Kalman filter-style Bayesian update, maintaining uncertainty-aware estimates of Q-values and enabling more stable and sample-efficient learning. We evaluate our method in both toy environments and clinically motivated testbeds. In small, tabular environments, Bayesian ATM achieves comparable or improved scalarized returns with substantially lower variance and more stable policy behavior. In contrast, in larger and more complex mHealth settings, both the standard and Bayesian ATM variants perform poorly, suggesting a mismatch between ATM's modeling assumptions and the structural challenges of real-world mHealth domains. These findings highlight the value of uncertainty-aware methods in low-data settings while underscoring the need for new RL algorithms that explicitly model causal structure, continuous states, and delayed feedback under observation cost constraints.

</details>


### [124] [Learning When to Ask: Simulation-Trained Humanoids for Mental-Health Diagnosis](https://arxiv.org/abs/2512.08952)
*Filippo Cenacchi,Deborah Richards,Longbing Cao*

Main category: cs.LG

TL;DR: 研究人员开发了一个虚拟仿真系统，用于训练人形机器人的对话能力，避免了实际硬件测试的磨损和限制。系统将276名患者的访谈数据转化为虚拟人物，通过强化学习训练对话控制器，TD3算法在社交时机和完整性方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: 实际测试人形机器人存在速度慢、设备磨损、迭代受限等问题，而现有模拟器大多忽略了非语言动态的策略学习，控制器过于关注任务准确性而忽视了信任、节奏和融洽关系等社交要素。

Method: 开发了一个以智能体为中心的仿真优先流程：将访谈数据转化为276个Unreal Engine MetaHuman虚拟患者，包含同步的语音、注视/面部表情和头躯姿势。采用感知-融合-策略循环决定何时说话、何时回应以及如何避免打断，在安全保护下进行训练。使用反事实回放（有界非语言扰动）和不确定性感知的对话管理器来减少诊断模糊性。

Result: 在三种控制器的比较中，定制的TD3（Twin Delayed DDPG）算法优于PPO和CEM，实现了接近完美的覆盖率，在相似奖励下保持更稳定的节奏。决策质量分析显示可忽略的对话重叠、对齐的打断时机、更少的澄清提示和更短的等待时间。性能在模态丢失和渲染器更换下保持稳定，在保留的患者分组上排名一致。

Conclusion: 该研究提出了一个完整的虚拟训练系统，能够有效训练人形机器人的社交对话能力，特别是TD3算法在社交时机和完整性方面表现优异，为临床监督下的人形机器人试点提供了技术基础。

Abstract: Testing humanoid robots with users is slow, causes wear, and limits iteration and diversity. Yet screening agents must master conversational timing, prosody, backchannels, and what to attend to in faces and speech for Depression and PTSD. Most simulators omit policy learning with nonverbal dynamics; many controllers chase task accuracy while underweighting trust, pacing, and rapport. We virtualise the humanoid as a conversational agent to train without hardware burden. Our agent-centred, simulation-first pipeline turns interview data into 276 Unreal Engine MetaHuman patients with synchronised speech, gaze/face, and head-torso poses, plus PHQ-8 and PCL-C flows. A perception-fusion-policy loop decides what and when to speak, when to backchannel, and how to avoid interruptions, under a safety shield. Training uses counterfactual replay (bounded nonverbal perturbations) and an uncertainty-aware turn manager that probes to reduce diagnostic ambiguity. Results are simulation-only; the humanoid is the transfer target. In comparing three controllers, a custom TD3 (Twin Delayed DDPG) outperformed PPO and CEM, achieving near-ceiling coverage with steadier pace at comparable rewards. Decision-quality analyses show negligible turn overlap, aligned cut timing, fewer clarification prompts, and shorter waits. Performance stays stable under modality dropout and a renderer swap, and rankings hold on a held-out patient split. Contributions: (1) an agent-centred simulator that turns interviews into 276 interactive patients with bounded nonverbal counterfactuals; (2) a safe learning loop that treats timing and rapport as first-class control variables; (3) a comparative study (TD3 vs PPO/CEM) with clear gains in completeness and social timing; and (4) ablations and robustness analyses explaining the gains and enabling clinician-supervised humanoid pilots.

</details>


### [125] [An Electrocardiogram Multi-task Benchmark with Comprehensive Evaluations and Insightful Findings](https://arxiv.org/abs/2512.08954)
*Yuhao Xu,Jiaying Lu,Sirui Ding,Defu Cao,Xiao Hu,Carl Yang*

Main category: cs.LG

TL;DR: 该研究评估了基础模型在ECG分析中的有效性，发现通用时间序列/ECG基础模型在ECG任务中达到80%的顶级性能，表明它们在心脏电信号分析中具有实用价值。


<details>
  <summary>Details</summary>
Motivation: 心电图（ECG）作为非侵入性心脏活动监测方法，需要专业知识进行分析，这限制了人工智能在医疗保健中的应用。虽然自监督学习和基础模型使AI系统能够获取领域知识，但缺乏对基础模型在ECG分析性能的全面评估。

Method: 研究通过比较语言/通用时间序列/ECG基础模型与时间序列深度学习模型，评估基础模型在ECG分析中的表现。研究建立了公开可用的基准数据集和代码库。

Result: 实验结果显示，通用时间序列/ECG基础模型在ECG分析任务中达到80%的顶级性能，证明了它们在心脏电信号分析中的有效性。

Conclusion: 该研究证实了基础模型在ECG分析中的实用性，同时指出了其在生理波形分析中的局限性和潜力，为未来医疗AI系统的发展提供了重要参考。

Abstract: In the process of patient diagnosis, non-invasive measurements are widely used due to their low risks and quick results. Electrocardiogram (ECG), as a non-invasive method to collect heart activities, is used to diagnose cardiac conditions. Analyzing the ECG typically requires domain expertise, which is a roadblock to applying artificial intelligence (AI) for healthcare. Through advances in self-supervised learning and foundation models, AI systems can now acquire and leverage domain knowledge without relying solely on human expertise. However, there is a lack of comprehensive analyses over the foundation models' performance on ECG. This study aims to answer the research question: "Are Foundation Models Useful for ECG Analysis?" To address it, we evaluate language/general time-series/ECG foundation models in comparison with time-series deep learning models. The experimental results show that general time-series/ECG foundation models achieve a top performance rate of 80%, indicating their effectiveness in ECG analysis. In-depth analyses and insights are provided along with comprehensive experimental results. This study highlights the limitations and potential of foundation models in advancing physiological waveform analysis. The data and code for this benchmark are publicly available at https://github.com/yuhaoxu99/ECGMultitasks-Benchmark.

</details>


### [126] [DW-KNN: A Transparent Local Classifier Integrating Distance Consistency and Neighbor Reliability](https://arxiv.org/abs/2512.08956)
*Kumarjit Pathak,Karthik K,Sachin Madan,Jitin Kapila*

Main category: cs.LG

TL;DR: DW-KNN是一种改进的KNN分类器，通过结合指数距离和邻居有效性双重加权，在异构特征空间中提供更可靠、可解释的预测，平均准确率达到0.8988。


<details>
  <summary>Details</summary>
Motivation: 传统KNN及其变体假设所有k个邻居同等可靠，这在异构特征空间中成为限制，影响了预测的可靠性。需要一种能够区分邻居可靠性、抑制噪声样本的方法。

Method: 提出DW-KNN（双重加权KNN），整合指数距离加权和邻居有效性加权，实现实例级可解释性，抑制噪声或错误标记样本，降低超参数敏感性。

Result: 在9个数据集上的综合评估显示：平均准确率0.8988，在6种方法中排名第2，与最佳集成KNN相差仅0.2%；交叉验证方差最低（0.0156），预测稳定性可靠；统计显著性测试证实优于紧凑度加权KNN（+4.09%）和核加权KNN（+1.13%）。

Conclusion: DW-KNN为复杂自适应方案提供了简单有效的替代方法，特别适用于需要可解释预测的高风险应用场景，在保持透明性和鲁棒性的同时提高了分类性能。

Abstract: K-Nearest Neighbors (KNN) is one of the most used ML classifiers. However, if we observe closely, standard distance-weighted KNN and relative variants assume all 'k' neighbors are equally reliable. In heterogeneous feature space, this becomes a limitation that hinders reliability in predicting true levels of the observation.
  We propose DW-KNN (Double Weighted KNN), a transparent and robust variant that integrates exponential distance with neighbor validity. This enables instance-level interpretability, suppresses noisy or mislabeled samples, and reduces hyperparameter sensitivity.
  Comprehensive evaluation on 9 data-sets helps to demonstrate that DW-KNN achieves 0.8988 accuracy on average. It ranks 2nd among six methods and within 0.2% of the best-performing Ensemble KNN. It also exhibits the lowest cross-validation variance (0.0156), indicating reliable prediction stability. Statistical significance test confirmed ($p < 0.001$) improvement over compactness weighted KNN (+4.09\%) and Kernel weighted KNN (+1.13\%). The method provides a simple yet effective alternative to complex adaptive schemes, particularly valuable for high-stakes applications requiring explainable predictions.

</details>


### [127] [LUMOS: Large User MOdels for User Behavior Prediction](https://arxiv.org/abs/2512.08957)
*Dhruv Nigam*

Main category: cs.LG

TL;DR: LUMOS是一个基于Transformer的大规模用户行为预测模型，通过联合学习多个任务并利用原始用户活动数据，消除了任务特定模型和手动特征工程的需求。


<details>
  <summary>Details</summary>
Motivation: 在线B2C平台面临大规模用户行为预测的挑战，传统方法依赖任务特定模型和领域特定特征工程，这既耗时、计算成本高，又需要领域专业知识，因此不具备可扩展性。

Method: LUMOS采用基于Transformer的架构，引入新颖的交叉注意力机制，使预测能够基于未来已知事件（如节假日、促销等）进行条件化。模型采用多模态标记化，将用户交易、事件上下文和静态用户人口统计属性通过专门的嵌入路径处理为丰富表示。

Result: 在包含2.5亿用户、2750亿用户活动标记的生产数据集上，LUMOS在5个任务中相比传统任务特定模型表现更优：二分类任务ROC-AUC平均提升0.025，回归任务MAPE降低4.6%。在线A/B测试验证了这些改进转化为可衡量的业务影响，日活跃用户增加3.15%。

Conclusion: LUMOS通过消除任务特定模型和手动特征工程，提供了一种可扩展的大规模用户行为预测解决方案，能够有效预测复杂行为模式，并在实际业务中产生显著积极影响。

Abstract: User behavior prediction at scale remains a critical challenge for online B2C platforms. Traditional approaches rely heavily on task-specific models and domain-specific feature engineering. This is time-consuming, computationally expensive, and requires domain expertise and therefore not scalable. We present LUMOS (Large User MOdel Series), a transformer-based architecture that eliminates task-specific models and manual feature engineering by learning multiple tasks jointly using only raw user activity data. LUMOS introduces a novel cross-attention mechanism that conditions predictions on future known events (e.g., holidays, sales, etc.), enabling the model to predict complex behaviour patterns like "how will upcoming holidays affect user engagement?" The architecture also employs multi-modal tokenization, combining user transactions, event context, and static user demographic attributes into rich representations processed through specialized embedding pathways.
  Through extensive experiments on a production dataset spanning 275 billion user activity tokens from 250 million users, we demonstrate that LUMOS achieves superior performance compared to traditional task-specific models. Across 5 tasks with established baselines, we achieve an average improvement of 0.025 in ROC-AUC for binary classification tasks and 4.6\% reduction in MAPE for regression tasks. Online A/B testing validates these improvements translate to measurable business impact with a 3.15\% increase in Daily Active Users.

</details>


### [128] [EEG-Bench: A Benchmark for EEG Foundation Models in Clinical Applications](https://arxiv.org/abs/2512.08959)
*Ard Kastrati,Josua Bürki,Jonas Lauer,Cheng Xuan,Raffaele Iaquinto,Roger Wattenhofer*

Main category: cs.LG

TL;DR: 提出了一个统一的基准测试框架，用于评估基于EEG的基础模型在临床诊断任务中的表现，覆盖11种疾病诊断任务和14个公开EEG数据集。


<details>
  <summary>Details</summary>
Motivation: 需要建立一个标准化的评估框架来系统比较EEG基础模型在临床诊断任务中的性能，特别是在面对临床数据分布变化时的表现。

Method: 开发了一个统一的基准测试框架，包含11个明确定义的诊断任务，覆盖14个公开EEG数据集，采用最小预处理和标准化评估协议，支持经典基线模型和现代基础模型的并行比较。

Result: 结果显示，基础模型在某些场景下表现优异，但在临床分布变化下，更简单的模型通常仍具有竞争力。所有准备的数据和代码都已公开发布。

Conclusion: 该基准测试框架为EEG基础模型在临床应用中的评估提供了标准化工具，揭示了在临床分布变化下简单模型的竞争力，并促进了该领域的可重复性和采用。

Abstract: We introduce a unified benchmarking framework focused on evaluating EEG-based foundation models in clinical applications. The benchmark spans 11 well-defined diagnostic tasks across 14 publicly available EEG datasets, including epilepsy, schizophrenia, Parkinson's disease, OCD, and mild traumatic brain injury. It features minimal preprocessing, standardized evaluation protocols, and enables side-by-side comparisons of classical baselines and modern foundation models. Our results show that while foundation models achieve strong performance in certain settings, simpler models often remain competitive, particularly under clinical distribution shifts. To facilitate reproducibility and adoption, we release all prepared data and code in an accessible and extensible format.

</details>


### [129] [Resolving Conflicts in Lifelong Learning via Aligning Updates in Subspaces](https://arxiv.org/abs/2512.08960)
*Yueer Zhou,Yichen Wu,Ying Wei*

Main category: cs.LG

TL;DR: PS-LoRA通过双正则化目标解决LoRA在持续学习中的灾难性遗忘问题，通过惩罚冲突方向和约束幅度偏差来保持参数稳定性，并在多个基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: LoRA在持续学习中存在灾难性遗忘问题，主要原因是新任务梯度与历史权重轨迹之间的对抗性方向更新。需要解决这种破坏性干扰以保持学习表示的稳定性。

Method: 提出PS-LoRA框架，采用双正则化目标：1）惩罚冲突方向以对齐优化子空间中的更新；2）约束幅度偏差以确保与先前知识的一致性。同时实现基于幅度的合并策略，将顺序适配器整合为鲁棒表示而无需重新训练。

Result: 在NLP和视觉基准测试中，PS-LoRA优于最先进的方法，能够在高效适应新领域的同时保持学习表示的稳定性。

Conclusion: PS-LoRA通过解决对抗性方向更新问题，有效缓解了LoRA在持续学习中的灾难性遗忘，为高效稳定的持续学习提供了有效解决方案。

Abstract: Low-Rank Adaptation (LoRA) enables efficient Continual Learning but often suffers from catastrophic forgetting due to destructive interference between tasks. Our analysis reveals that this degradation is primarily driven by antagonistic directional updates where new task gradients directly oppose the historical weight trajectory. To address this, we propose PS-LoRA (Parameter Stability LoRA), a framework designed to resolve conflicts by aligning updates within the optimization subspace. Our approach employs a dual-regularization objective that penalizes conflicting directions and constrains magnitude deviations to ensure consistency with prior knowledge. Additionally, we implement a magnitude-based merging strategy to consolidate sequential adapters into a robust representation without retraining. Experiments on NLP and Vision benchmarks show that PS-LoRA outperforms state-of-the-art methods by preserving the stability of learned representations while efficiently adapting to new domains.

</details>


### [130] [Financial Instruction Following Evaluation (FIFE)](https://arxiv.org/abs/2512.08965)
*Glenn Matlin,Siddharth,Anirudh JM,Aditya Shukla,Yahya Hassan,Sudheer Chava*

Main category: cs.LG

TL;DR: FIFE是一个针对金融分析任务的高难度基准测试，评估语言模型在复杂指令遵循方面的能力，发现开源权重模型表现优于专有系统，但所有模型都难以完全满足复杂要求。


<details>
  <summary>Details</summary>
Motivation: 语言模型在处理复杂、相互依赖的指令时存在困难，特别是在金融等高精度要求领域，需要专门的评估基准来测试模型的指令遵循能力。

Method: 引入FIFE基准测试，包含88个人工编写的金融分析提示，采用可链接、可验证的约束验证系统，提供细粒度的奖励信号，在零样本设置下评估了53个模型。

Result: 性能层次明显：顶级开源权重模型（76.1严格/79.5宽松）优于领先的专有系统（65.9严格/70.5宽松），而最佳开源模型表现显著落后（45.5严格/48.9宽松）。所有顶级模型都难以完全满足FIFE的复杂要求。

Conclusion: FIFE基准揭示了语言模型在复杂金融指令遵循方面的局限性，开源数据集和代码将促进金融领域强化学习研究的发展。

Abstract: Language Models (LMs) struggle with complex, interdependent instructions, particularly in high-stakes domains like finance where precision is critical. We introduce FIFE, a novel, high-difficulty benchmark designed to assess LM instruction-following capabilities for financial analysis tasks. FIFE comprises 88 human-authored prompts and employs a verification system with chainable, verifiable constraints for fine-grained reward signals. We evaluate 53 models (proprietary, open-weight, open-source) in a zero-shot setting. Our key findings reveal a clear performance hierarchy: the top open-weight model (76.1 strict / 79.5 loose) surpasses the leading proprietary system (65.9 strict / 70.5 loose), while the best open-source models lag significantly (45.5 strict / 48.9 loose). However, even top-performing models struggle with FIFE's complex requirements, failing to achieve perfect compliance. We release our dataset and code as an open-source resource to promote research in Reinforcement Learning for the financial domain.

</details>


### [131] [CluCERT: Certifying LLM Robustness via Clustering-Guided Denoising Smoothing](https://arxiv.org/abs/2512.08967)
*Zixia Wang,Gaojie Jin,Jia Hu,Ronghui Mu*

Main category: cs.LG

TL;DR: CluCERT是一个通过聚类引导去噪平滑来认证大语言模型鲁棒性的新框架，相比现有方法能提供更紧的鲁棒性边界和更高的计算效率。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型能力强大，但仍容易受到对抗性攻击，即使是同义词替换等语义保留的微小改动也可能导致错误预测。现有方法存在两个关键局限：1) 由于缺乏对扰动输出的语义验证，导致鲁棒性边界松散；2) 重复采样导致计算成本高。

Method: 提出CluCERT框架，包含：1) 语义聚类过滤器，减少噪声样本并保留有意义的扰动；2) 提炼模块提取核心语义；3) 快速同义词替换策略加速去噪过程。通过理论分析支持语义聚类过滤器的有效性。

Result: 在各种下游任务和越狱防御场景的广泛实验中，该方法在鲁棒性边界和计算效率方面均优于现有的认证方法。

Conclusion: CluCERT通过聚类引导的去噪平滑方法，有效解决了现有认证方法边界松散和计算成本高的问题，为LLM的鲁棒性认证提供了更优的解决方案。

Abstract: Recent advancements in Large Language Models (LLMs) have led to their widespread adoption in daily applications. Despite their impressive capabilities, they remain vulnerable to adversarial attacks, as even minor meaning-preserving changes such as synonym substitutions can lead to incorrect predictions. As a result, certifying the robustness of LLMs against such adversarial prompts is of vital importance. Existing approaches focused on word deletion or simple denoising strategies to achieve robustness certification. However, these methods face two critical limitations: (1) they yield loose robustness bounds due to the lack of semantic validation for perturbed outputs and (2) they suffer from high computational costs due to repeated sampling. To address these limitations, we propose CluCERT, a novel framework for certifying LLM robustness via clustering-guided denoising smoothing. Specifically, to achieve tighter certified bounds, we introduce a semantic clustering filter that reduces noisy samples and retains meaningful perturbations, supported by theoretical analysis. Furthermore, we enhance computational efficiency through two mechanisms: a refine module that extracts core semantics, and a fast synonym substitution strategy that accelerates the denoising process. Finally, we conduct extensive experiments on various downstream tasks and jailbreak defense scenarios. Experimental results demonstrate that our method outperforms existing certified approaches in both robustness bounds and computational efficiency.

</details>


### [132] [StructuredDNA: A Bio-Physical Framework for Energy-Aware Transformer Routing](https://arxiv.org/abs/2512.08968)
*Mustapha Hamdi*

Main category: cs.LG

TL;DR: StructuredDNA是一个基于生物物理能量最小化的稀疏Transformer架构框架，用语义能量引导的路由层替代密集的专家混合路由，显著降低计算能耗。


<details>
  <summary>Details</summary>
Motivation: 大型计算模型的快速扩展导致能源和计算成本急剧增加，受生物系统中结构和功能从低能量配置中涌现的启发，开发能量感知的模块化路由框架。

Method: 引入基于语义能量最小化的生物物理能量引导路由层，动态将输入分组为语义密码子，通过最小化结合内聚性、不确定性和计算成本的全局能量函数来选择单个专家。

Result: 在BioASQ基准测试中实现97.7%的能源利用密度降低和0.998的语义稳定性指数；在WikiText-103上展示了语义缩放定律，专家粒度扩展到2048时仍保持超过99%的能源效率。

Conclusion: StructuredDNA建立了生物物理原理与Transformer稀疏专家路由之间的明确联系，为未来能量感知、模块化和可扩展的计算系统提供了领域无关的稳健范式。

Abstract: The rapid scaling of large computational models has led to a critical increase in energy and compute costs. Inspired by biological systems where structure and function emerge from low-energy configurations, we introduce StructuredDNA, a sparse architecture framework for modular, energy-aware Transformer routing. StructuredDNA replaces dense Mixture-of-Experts routing with a bio-physical, energy-guided routing layer based on semantic energy minimization. Inputs are dynamically grouped into semantic codons, and routing selects a single expert by minimizing a global energy functional that combines cohesion, uncertainty, and computational cost.
  We validate StructuredDNA on both specialized (BioASQ) and open-domain benchmarks (WikiText-103). On BioASQ (K = 50), we achieve a 97.7% reduction in Energy Utilization Density (EUD) and a Semantic Stability Index (SSI) of 0.998. We further demonstrate a Semantic Scaling Law on WikiText-103, showing that the architecture generalizes to open domains by scaling expert granularity (K = 2048) while maintaining more than 99% energy efficiency. StructuredDNA thus establishes a robust, domain-agnostic paradigm for future sparse computational frameworks.
  StructuredDNA provides an explicit link between bio-physical principles and sparse expert routing in Transformer architectures, and points toward future energy-aware, modular, and scalable computational systems. We discuss limitations of this proof-of-concept study and outline directions for scaling the approach to larger models, datasets, and hardware platforms. The StructuredDNA implementation is available at https://github.com/InnoDeep-repos/StructuredDNA .

</details>


### [133] [Peek-a-Boo Reasoning: Contrastive Region Masking in MLLMs](https://arxiv.org/abs/2512.08976)
*Isha Chaturvedi,Anjana Nair,Yushen Li,Adhitya Rajendra Kumar,Kevin Zhu,Sunishchal Dev,Ashwinee Panda,Vasu Sharma*

Main category: cs.LG

TL;DR: CRM是一种无需训练的诊断方法，通过对比性区域掩码揭示多模态大语言模型在思维链推理中如何依赖特定视觉区域。


<details>
  <summary>Details</summary>
Motivation: 现有方法局限于最终答案或注意力图，无法提供因果、步骤级别的归因，需要一种能够揭示MLLMs在推理过程中如何依赖视觉区域的方法。

Method: CRM通过系统性地掩码标注区域，并将得到的推理轨迹与未掩码基线进行对比，提供因果、步骤级别的归因分析。

Result: 在VisArgs等数据集上，CRM揭示了不同的失败模式：一些模型保持推理结构但在证据缺失时产生幻觉，而另一些模型紧密依赖视觉线索但在扰动下崩溃。

Conclusion: CRM将评估重点从答案正确性转向推理忠实性，将视觉基准重构为诊断工具，强调需要评估多模态框架的性能、鲁棒性和推理保真度。

Abstract: We introduce Contrastive Region Masking (CRM), a training free diagnostic that reveals how multimodal large language models (MLLMs) depend on specific visual regions at each step of chain-of-thought (CoT) reasoning. Unlike prior approaches limited to final answers or attention maps, CRM provides causal, step-level attri- bution by systematically masking annotated regions and contrasting the resulting reasoning traces with unmasked baselines. Applied to datasets such as VisArgs, CRM reveals distinct failure modes: some models preserve reasoning structure, but hallucinate when evidence is missing, while others ground tightly to visual cues yet collapse under perturbations. By shifting the evaluation from correctness of an- swers to faithfulness of reasoning, CRM reframes visual benchmarks as diagnostic tools, highlighting the need for multimodal evaluation frameworks that measure not just performance, but also robustness and fidelity of reasoning.

</details>


### [134] [Improving Multi-Class Calibration through Normalization-Aware Isotonic Techniques](https://arxiv.org/abs/2512.09054)
*Alon Arad,Saharon Rosset*

Main category: cs.LG

TL;DR: 提出两种新的多类校准方法：NA-FIR（归一化感知保序回归）和SCIR（累积二元保序回归），通过考虑概率归一化约束来改进多类概率预测的校准效果。


<details>
  <summary>Details</summary>
Motivation: 多类监督学习任务需要准确可靠的概率预测，但现有的保序回归方法（如一对多校准）在多类问题上效果不如参数化方法，限制了实际应用。需要开发能够自然处理概率归一化的多类校准技术。

Method: 提出两种方法：1) NA-FIR：将概率归一化直接纳入优化过程的归一化感知保序回归；2) SCIR：将问题建模为累积二元保序回归。两种方法都基于实践者期望的自然直观假设。

Result: 在多种文本和图像分类数据集及不同模型架构上的实证评估表明，该方法在负对数似然（NLL）和期望校准误差（ECE）指标上持续改进，优于现有方法。

Conclusion: 提出的归一化感知保序回归方法能够有效改进多类概率预测的校准效果，为多类监督学习任务提供了更可靠的概率预测工具。

Abstract: Accurate and reliable probability predictions are essential for multi-class supervised learning tasks, where well-calibrated models enable rational decision-making. While isotonic regression has proven effective for binary calibration, its extension to multi-class problems via one-vs-rest calibration produced suboptimal results when compared to parametric methods, limiting its practical adoption. In this work, we propose novel isotonic normalization-aware techniques for multiclass calibration, grounded in natural and intuitive assumptions expected by practitioners. Unlike prior approaches, our methods inherently account for probability normalization by either incorporating normalization directly into the optimization process (NA-FIR) or modeling the problem as a cumulative bivariate isotonic regression (SCIR). Empirical evaluation on a variety of text and image classification datasets across different model architectures reveals that our approach consistently improves negative log-likelihood (NLL) and expected calibration error (ECE) metrics.

</details>


### [135] [A Diffusion-Based Framework for High-Resolution Precipitation Forecasting over CONUS](https://arxiv.org/abs/2512.09059)
*Marina Vicens-Miquel,Amy McGovern,Aaron J. Hill,Efi Foufoula-Georgiou,Clement Guilloteau,Samuel S. P. Shen*

Main category: cs.LG

TL;DR: 该研究开发了一个基于扩散的深度学习框架，系统比较了三种残差预测策略，用于1-12小时降水预报，在1公里空间分辨率上显著优于HRRR数值天气预报基准。


<details>
  <summary>Details</summary>
Motivation: 准确的降水预报对于水文气象风险管理至关重要，特别是预测可能导致山洪暴发和基础设施损坏的极端降雨。需要开发更精确的预报方法来提高应急准备能力。

Method: 引入基于扩散的深度学习框架，系统比较三种残差预测策略：1) 纯数据驱动模型（仅使用MRMS观测数据）；2) 校正模型（仅使用HRRR数值预报）；3) 混合模型（整合MRMS和HRRR变量）。在统一设置下评估这些方法，使用1公里空间分辨率，从1小时直接预测扩展到12小时自回归滚动预测。

Result: 在所有预报时效上，深度学习框架在像素级和空间统计指标上均一致优于HRRR基准。混合模型在最短预报时效表现最佳，而HRRR校正模型在较长预报时效表现更好，能保持高技能水平至12小时。通过校准的不确定性量化评估可靠性。

Conclusion: 该工作通过增强预测技能、可靠性和区域适用性，推进了基于深度学习的降水预报。特别是在较长预报时效的改进对于应急准备至关重要，适度的预报时效增加可以改善决策制定。

Abstract: Accurate precipitation forecasting is essential for hydrometeorological risk management, especially for anticipating extreme rainfall that can lead to flash flooding and infrastructure damage. This study introduces a diffusion-based deep learning (DL) framework that systematically compares three residual prediction strategies differing only in their input sources: (1) a fully data-driven model using only past observations from the Multi-Radar Multi-Sensor (MRMS) system, (2) a corrective model using only forecasts from the High-Resolution Rapid Refresh (HRRR) numerical weather prediction system, and (3) a hybrid model integrating both MRMS and selected HRRR forecast variables. By evaluating these approaches under a unified setup, we provide a clearer understanding of how each data source contributes to predictive skill over the Continental United States (CONUS). Forecasts are produced at 1-km spatial resolution, beginning with direct 1-hour predictions and extending to 12 hours using autoregressive rollouts. Performance is evaluated using both CONUS-wide and region-specific metrics that assess overall performance and skill at extreme rainfall thresholds. Across all lead times, our DL framework consistently outperforms the HRRR baseline in pixel-wise and spatiostatistical metrics. The hybrid model performs best at the shortest lead time, while the HRRR-corrective model outperforms others at longer lead times, maintaining high skill through 12 hours. To assess reliability, we incorporate calibrated uncertainty quantification tailored to the residual learning setup. These gains, particularly at longer lead times, are critical for emergency preparedness, where modest increases in forecast horizon can improve decision-making. This work advances DL-based precipitation forecasting by enhancing predictive skill, reliability, and applicability across regions.

</details>


### [136] [Modular Deep-Learning-Based Early Warning System for Deadly Heatwave Prediction](https://arxiv.org/abs/2512.09074)
*Shangqing Xu,Zhiyuan Zhao,Megha Sharma,José María Martín-Olalla,Alexander Rodríguez,Gregory A. Wellenius,B. Aditya Prakash*

Main category: cs.LG

TL;DR: DeepTherm是一个基于深度学习的模块化早期预警系统，无需热相关死亡率历史数据即可预测致命热浪，通过双预测管道分离基线死亡率，在西班牙真实数据上表现稳健准确。


<details>
  <summary>Details</summary>
Motivation: 城市地区的严重热浪对公共健康构成重大威胁，需要建立早期预警策略。现有方法难以预测即将到来的致命热浪，因为定义和估计热相关死亡率存在困难，且早期预警系统需要满足数据可用性、时空鲁棒性和决策成本等额外要求。

Method: DeepTherm采用模块化设计，利用深度学习的灵活性，通过双预测管道将无热浪和其他异常事件时的基线死亡率与全因死亡率分离，无需热相关死亡率历史数据。

Result: 在西班牙真实数据上的评估表明，DeepTherm在不同地区、时间段和人群组中表现出一致、稳健和准确的性能，同时允许在漏报和误报之间进行权衡。

Conclusion: DeepTherm成功解决了预测致命热浪的挑战，提供了一个无需热相关死亡率历史数据的有效早期预警系统，具有实际应用价值。

Abstract: Severe heatwaves in urban areas significantly threaten public health, calling for establishing early warning strategies. Despite predicting occurrence of heatwaves and attributing historical mortality, predicting an incoming deadly heatwave remains a challenge due to the difficulty in defining and estimating heat-related mortality. Furthermore, establishing an early warning system imposes additional requirements, including data availability, spatial and temporal robustness, and decision costs. To address these challenges, we propose DeepTherm, a modular early warning system for deadly heatwave prediction without requiring heat-related mortality history. By highlighting the flexibility of deep learning, DeepTherm employs a dual-prediction pipeline, disentangling baseline mortality in the absence of heatwaves and other irregular events from all-cause mortality. We evaluated DeepTherm on real-world data across Spain. Results demonstrate consistent, robust, and accurate performance across diverse regions, time periods, and population groups while allowing trade-off between missed alarms and false alarms.

</details>


### [137] [Beyond the Hype: Comparing Lightweight and Deep Learning Models for Air Quality Forecasting](https://arxiv.org/abs/2512.09076)
*Moazzam Umer Gondal,Hamad ul Qudous,Asma Ahmad Farhan*

Main category: cs.LG

TL;DR: 该研究比较了轻量级加法模型（Facebook Prophet和NeuralProphet）与传统统计模型（SARIMAX）和机器学习模型（LSTM、LightGBM）在北京PM2.5和PM10污染物预测中的表现。研究发现Facebook Prophet在7天预测中表现最佳，R²超过0.94。


<details>
  <summary>Details</summary>
Motivation: 城市空气污染准确预测对公共健康保护和缓解政策制定至关重要。虽然深度学习和混合管道主导了近期研究，但其复杂性和有限的可解释性阻碍了实际应用。本研究旨在探索轻量级加法模型是否能提供有竞争力的污染物预测。

Method: 使用北京多年污染物和气象数据，应用系统特征选择（相关性、互信息、mRMR）、防泄漏缩放和按时间顺序数据分割。训练Facebook Prophet和NeuralProphet模型，其中NeuralProphet额外利用滞后依赖关系。同时实现两个机器学习基线（LSTM、LightGBM）和一个传统统计模型（SARIMAX）作为对比。

Result: 结果显示Facebook Prophet在7天保留测试集上表现最佳，PM2.5和PM10的测试R²均超过0.94，优于NeuralProphet、SARIMAX和机器学习基线模型。

Conclusion: 研究表明可解释的加法模型在空气污染预测中与传统方法和复杂方法相比仍具有竞争力，提供了准确性、透明度和部署便利性的实用平衡。

Abstract: Accurate forecasting of urban air pollution is essential for protecting public health and guiding mitigation policies. While Deep Learning (DL) and hybrid pipelines dominate recent research, their complexity and limited interpretability hinder operational use. This study investigates whether lightweight additive models -- Facebook Prophet (FBP) and NeuralProphet (NP) -- can deliver competitive forecasts for particulate matter (PM$_{2.5}$, PM$_{10}$) in Beijing, China. Using multi-year pollutant and meteorological data, we applied systematic feature selection (correlation, mutual information, mRMR), leakage-safe scaling, and chronological data splits. Both models were trained with pollutant and precursor regressors, with NP additionally leveraging lagged dependencies. For context, two machine learning baselines (LSTM, LightGBM) and one traditional statistical model (SARIMAX) were also implemented. Performance was evaluated on a 7-day holdout using MAE, RMSE, and $R^2$. Results show that FBP consistently outperformed NP, SARIMAX, and the learning-based baselines, achieving test $R^2$ above 0.94 for both pollutants. These findings demonstrate that interpretable additive models remain competitive with both traditional and complex approaches, offering a practical balance of accuracy, transparency, and ease of deployment.

</details>


### [138] [Towards Optimal Valve Prescription for Transcatheter Aortic Valve Replacement (TAVR) Surgery: A Machine Learning Approach](https://arxiv.org/abs/2512.09198)
*Phevos Paschalidis,Vasiliki Stoumpou,Lisa Everest,Yu Ma,Talhat Azemi,Jawad Haider,Steven Zweibel,Eleftherios M. Protopapas,Jeff Mather,Maciej Tysarowski,George E. Sarris,Robert C. Hagberg,Howard L. Haronian,Dimitris Bertsimas*

Main category: cs.LG

TL;DR: 提出基于数据驱动的临床决策工具，用于TAVR手术中选择最佳瓣膜类型以降低永久起搏器植入风险，通过整合多国数据源和叶级分析实现个性化处方策略。


<details>
  <summary>Details</summary>
Motivation: 经导管主动脉瓣置换术（TAVR）已成为治疗严重主动脉瓣狭窄的微创方法，但不同瓣膜类型的选择指南仍存在争议。永久起搏器植入是主要的术后并发症，需要数据驱动的临床支持工具来优化瓣膜选择以降低该风险。

Method: 1. 合成新颖数据集：整合美国和希腊患者群体，结合三种数据源（患者人口统计学、CT扫描、超声心动图），协调两国记录系统差异；2. 引入叶级分析：利用人群异质性，避免基于不确定反事实风险估计的基准测试；3. 开发处方模型：基于数据驱动方法识别最优瓣膜类型。

Result: 最终处方模型显示，与美国内部人群当前标准治疗相比，永久起搏器植入率降低26%；在外部希腊验证队列中降低16%。这是首个统一的、个性化的经导管心脏瓣膜选择处方策略。

Conclusion: 该研究开发了一个有效的数据驱动临床决策工具，能够显著降低TAVR术后永久起搏器植入风险，为个性化瓣膜选择提供了首个统一处方策略，具有重要的临床实践意义。

Abstract: Transcatheter Aortic Valve Replacement (TAVR) has emerged as a minimally invasive treatment option for patients with severe aortic stenosis, a life-threatening cardiovascular condition. Multiple transcatheter heart valves (THV) have been approved for use in TAVR, but current guidelines regarding valve type prescription remain an active topic of debate. We propose a data-driven clinical support tool to identify the optimal valve type with the objective of minimizing the risk of permanent pacemaker implantation (PPI), a predominant postoperative complication. We synthesize a novel dataset that combines U.S. and Greek patient populations and integrates three distinct data sources (patient demographics, computed tomography scans, echocardiograms) while harmonizing differences in each country's record system. We introduce a leaf-level analysis to leverage population heterogeneity and avoid benchmarking against uncertain counterfactual risk estimates. The final prescriptive model shows a reduction in PPI rates of 26% and 16% compared with the current standard of care in our internal U.S. population and external Greek validation cohort, respectively. To the best of our knowledge, this work represents the first unified, personalized prescription strategy for THV selection in TAVR.

</details>


### [139] [LLMs for Analog Circuit Design Continuum (ACDC)](https://arxiv.org/abs/2512.09199)
*Yasaman Esfandiari,Jocelyn Rego,Austin Meyer,Jonathan Gallagher,Mia Levy*

Main category: cs.LG

TL;DR: 该研究探讨了大型语言模型在模拟电路设计中的适用性和可靠性，发现模型对数据格式敏感、生成设计不稳定、泛化能力有限等挑战，为工程领域AI辅助设计提供了早期证据。


<details>
  <summary>Details</summary>
Motivation: 虽然大型语言模型在自然语言任务中表现出色，但其在真实工程领域（特别是模拟电路设计）的可靠性和鲁棒性尚未充分探索，限制了其在人类中心工作流程中的实际应用价值。

Method: 研究比较了不同数据表示对模型行为的影响，对比了较小模型（如T5、GPT-2）与较大基础模型（如Mistral-7B、GPT-oss-20B）在不同训练条件下的表现，重点关注AI辅助设计中人类保持在循环中的场景。

Result: 研究发现了关键的可靠性挑战：对数据格式的敏感性、生成设计的不稳定性、以及对未见电路配置的有限泛化能力。这些发现揭示了LLMs在复杂工程任务中作为增强人类能力工具的局限性。

Conclusion: 该研究为LLMs在结构化、真实世界应用中作为可靠、可部署基础模型的潜力提供了早期证据，强调了在复杂工程任务中设计可靠AI辅助工具的重要性。

Abstract: Large Language Models (LLMs) and transformer architectures have shown impressive reasoning and generation capabilities across diverse natural language tasks. However, their reliability and robustness in real-world engineering domains remain largely unexplored, limiting their practical utility in human-centric workflows. In this work, we investigate the applicability and consistency of LLMs for analog circuit design -- a task requiring domain-specific reasoning, adherence to physical constraints, and structured representations -- focusing on AI-assisted design where humans remain in the loop. We study how different data representations influence model behavior and compare smaller models (e.g., T5, GPT-2) with larger foundation models (e.g., Mistral-7B, GPT-oss-20B) under varying training conditions. Our results highlight key reliability challenges, including sensitivity to data format, instability in generated designs, and limited generalization to unseen circuit configurations. These findings provide early evidence on the limits and potential of LLMs as tools to enhance human capabilities in complex engineering tasks, offering insights into designing reliable, deployable foundation models for structured, real-world applications.

</details>


### [140] [Contrastive Learning for Semi-Supervised Deep Regression with Generalized Ordinal Rankings from Spectral Seriation](https://arxiv.org/abs/2512.09267)
*Ce Wang,Weihang Dai,Hanru Bai,Xiaomeng Li*

Main category: cs.LG

TL;DR: 该论文提出了一种半监督对比回归方法，通过构建包含标记和未标记样本的特征相似度矩阵，利用谱排序算法恢复未标记样本的序数关系，从而减少对昂贵标注的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有对比学习方法高度依赖标签信息来恢复特征的序数关系，限制了其在半监督回归中的应用。需要一种方法能够利用未标记数据来减少对昂贵标注的依赖。

Method: 1. 构建包含标记和未标记样本的特征相似度矩阵；2. 使用谱排序算法恢复未标记样本的序数关系；3. 利用标记样本提供正则化指导；4. 使用动态规划算法选择稳健特征；5. 将恢复的序数关系用于未标记样本的对比学习。

Result: 在多个数据集上的实验验证表明，该方法超越了现有的最先进半监督深度回归方法，提供了理论保证和实证验证。

Conclusion: 该方法成功将对比回归扩展到半监督设置，通过利用未标记数据提高了特征表示能力，减少了标注成本，同时保持了回归性能。

Abstract: Contrastive learning methods enforce label distance relationships in feature space to improve representation capability for regression models. However, these methods highly depend on label information to correctly recover ordinal relationships of features, limiting their applications to semi-supervised regression. In this work, we extend contrastive regression methods to allow unlabeled data to be used in the semi-supervised setting, thereby reducing the dependence on costly annotations. Particularly we construct the feature similarity matrix with both labeled and unlabeled samples in a mini-batch to reflect inter-sample relationships, and an accurate ordinal ranking of involved unlabeled samples can be recovered through spectral seriation algorithms if the level of error is within certain bounds. The introduction of labeled samples above provides regularization of the ordinal ranking with guidance from the ground-truth label information, making the ranking more reliable. To reduce feature perturbations, we further utilize the dynamic programming algorithm to select robust features for the matrix construction. The recovered ordinal relationship is then used for contrastive learning on unlabeled samples, and we thus allow more data to be used for feature representation learning, thereby achieving more robust results. The ordinal rankings can also be used to supervise predictions on unlabeled samples, serving as an additional training signal. We provide theoretical guarantees and empirical verification through experiments on various datasets, demonstrating that our method can surpass existing state-of-the-art semi-supervised deep regression methods. Our code have been released on https://github.com/xmed-lab/CLSS.

</details>


### [141] [Self-Supervised Learning with Gaussian Processes](https://arxiv.org/abs/2512.09322)
*Yunshan Duan,Sinead Williamson*

Main category: cs.LG

TL;DR: 提出GPSSL方法，使用高斯过程进行自监督学习，无需显式定义正样本对，并能提供不确定性量化


<details>
  <summary>Details</summary>
Motivation: 传统自监督学习方法需要生成相似样本对，这在某些数据类型中具有挑战性，且缺乏不确定性量化能力，在样本外预测中表现不佳

Method: 提出高斯过程自监督学习（GPSSL），在高斯过程先验下最小化损失函数获得表示，利用协方差函数自然拉近相似样本的表示

Result: GPSSL与核PCA和VICReg方法相关，但能提供后验不确定性传播；在多种数据集上的分类和回归任务中，在准确性、不确定性量化和误差控制方面优于传统方法

Conclusion: GPSSL为自监督学习提供了一种无需显式正样本对的新方法，同时具备不确定性量化能力，在多种下游任务中表现优异

Abstract: Self supervised learning (SSL) is a machine learning paradigm where models learn to understand the underlying structure of data without explicit supervision from labeled samples. The acquired representations from SSL have demonstrated useful for many downstream tasks including clustering, and linear classification, etc. To ensure smoothness of the representation space, most SSL methods rely on the ability to generate pairs of observations that are similar to a given instance. However, generating these pairs may be challenging for many types of data. Moreover, these methods lack consideration of uncertainty quantification and can perform poorly in out-of-sample prediction settings. To address these limitations, we propose Gaussian process self supervised learning (GPSSL), a novel approach that utilizes Gaussian processes (GP) models on representation learning. GP priors are imposed on the representations, and we obtain a generalized Bayesian posterior minimizing a loss function that encourages informative representations. The covariance function inherent in GPs naturally pulls representations of similar units together, serving as an alternative to using explicitly defined positive samples. We show that GPSSL is closely related to both kernel PCA and VICReg, a popular neural network-based SSL method, but unlike both allows for posterior uncertainties that can be propagated to downstream tasks. Experiments on various datasets, considering classification and regression tasks, demonstrate that GPSSL outperforms traditional methods in terms of accuracy, uncertainty quantification, and error control.

</details>


### [142] [Self Distillation Fine-Tuning of Protein Language Models Improves Versatility in Protein Design](https://arxiv.org/abs/2512.09329)
*Amin Tavakoli,Raswanth Murugan,Ozan Gokdemir,Arvind Ramanathan,Frances Arnold,Anima Anandkumar*

Main category: cs.LG

TL;DR: 提出了一种用于蛋白质语言模型快速监督微调的通用方法，通过轻量级筛选管道构建高质量训练数据，无需昂贵的实验数据集，能生成更稳定、功能更好的酶蛋白序列。


<details>
  <summary>Details</summary>
Motivation: 蛋白质序列建模中，高质量标注数据难以获取，现有监督微调方法依赖昂贵的实验数据集，需要一种更高效、通用的蛋白质语言模型微调方法。

Method: 利用蛋白质语言模型自身，结合轻量级筛选管道和领域特定过滤器构建高质量训练数据，这些过滤器可独立优化模型输出并筛选体外评估候选序列，与监督微调结合使用。

Result: 在色氨酸合酶酶家族上应用基因组规模蛋白质语言模型（GenSLM）验证了方法的有效性，微调后的模型生成的序列不仅更新颖，而且在目标设计约束和蛋白质特性指标上都表现出改进。

Conclusion: 该方法提供了一种简单通用的蛋白质语言模型快速监督微调方案，能够生成更稳定、功能更好的酶蛋白，同时扩展了对蛋白质序列空间的探索，超越了天然变体的限制。

Abstract: Supervised fine-tuning (SFT) is a standard approach for adapting large language models to specialized domains, yet its application to protein sequence modeling and protein language models (PLMs) remains ad hoc. This is in part because high-quality annotated data are far more difficult to obtain for proteins than for natural language. We present a simple and general recipe for fast SFT of PLMs, designed to improve the fidelity, reliability, and novelty of generated protein sequences. Unlike existing approaches that require costly precompiled experimental datasets for SFT, our method leverages the PLM itself, integrating a lightweight curation pipeline with domain-specific filters to construct high-quality training data. These filters can independently refine a PLM's output and identify candidates for in vitro evaluation; when combined with SFT, they enable PLMs to generate more stable and functional enzymes, while expanding exploration into protein sequence space beyond natural variants. Although our approach is agnostic to both the choice of protein language model (PLM) and the protein system, we demonstrate its effectiveness with a genome-scale PLM (GenSLM) applied to the tryptophan synthase enzyme family. The supervised fine-tuned model generates sequences that are not only more novel but also display improved characteristics across both targeted design constraints and emergent protein property measures.

</details>


### [143] [A Granular Framework for Construction Material Price Forecasting: Econometric and Machine-Learning Approaches](https://arxiv.org/abs/2512.09360)
*Boge Lyu,Qianye Yin,Iris Denise Tommelein,Hanyang Liu,Karnamohit Ranka,Karthik Yeluripati,Junzhe Shi*

Main category: cs.LG

TL;DR: 本研究开发了一个基于CSI MasterFormat的建筑材料价格预测框架，整合原材料价格、商品指数和宏观经济指标等解释变量，评估了LSTM、ARIMA、VECM和Chronos-Bolt四种时序模型，发现LSTM模型表现最佳，预测精度比传统ARIMA模型提升高达59%。


<details>
  <summary>Details</summary>
Motivation: 建筑材料价格的持续波动对成本估算、预算编制和项目交付构成重大风险，迫切需要开发细粒度且可扩展的预测方法来应对这一挑战。

Method: 研究以CSI MasterFormat为目标数据结构，在六位数章节级别进行预测。框架整合了原材料价格、商品指数和宏观经济指标等解释变量，评估了LSTM、ARIMA、VECM和Chronos-Bolt四种时序模型，分别在仅使用CSI数据的基准配置和包含解释变量的扩展版本中进行比较。

Result: 结果显示，整合解释变量能显著提升所有模型的预测性能。LSTM模型表现最佳，RMSE值低至1.390，MAPE值为0.957，比传统ARIMA模型提升高达59%。通过多个CSI分部的验证证明了框架的可扩展性，并以第06分部（木材、塑料和复合材料）作为详细演示案例。

Conclusion: 该研究提供了一个稳健的方法论，使业主和承包商能够在确定级别改进预算实践并实现更可靠的成本估算，为建筑行业提供了有效的价格波动管理工具。

Abstract: The persistent volatility of construction material prices poses significant risks to cost estimation, budgeting, and project delivery, underscoring the urgent need for granular and scalable forecasting methods. This study develops a forecasting framework that leverages the Construction Specifications Institute (CSI) MasterFormat as the target data structure, enabling predictions at the six-digit section level and supporting detailed cost projections across a wide spectrum of building materials. To enhance predictive accuracy, the framework integrates explanatory variables such as raw material prices, commodity indexes, and macroeconomic indicators. Four time-series models, Long Short-Term Memory (LSTM), Autoregressive Integrated Moving Average (ARIMA), Vector Error Correction Model (VECM), and Chronos-Bolt, were evaluated under both baseline configurations (using CSI data only) and extended versions with explanatory variables. Results demonstrate that incorporating explanatory variables significantly improves predictive performance across all models. Among the tested approaches, the LSTM model consistently achieved the highest accuracy, with RMSE values as low as 1.390 and MAPE values of 0.957, representing improvements of up to 59\% over the traditional statistical time-series model, ARIMA. Validation across multiple CSI divisions confirmed the framework's scalability, while Division 06 (Wood, Plastics, and Composites) is presented in detail as a demonstration case. This research offers a robust methodology that enables owners and contractors to improve budgeting practices and achieve more reliable cost estimation at the Definitive level.

</details>


### [144] [Closing the Train-Test Gap in World Models for Gradient-Based Planning](https://arxiv.org/abs/2512.09929)
*Arjun Parthasarathy,Nimit Kalra,Rohun Agrawal,Yann LeCun,Oumayma Bounou,Pavel Izmailov,Micah Goldblum*

Main category: cs.LG

TL;DR: 提出改进世界模型训练方法，通过训练时数据合成技术缩小训练-测试差距，使基于梯度的规划在10%时间预算下达到或超过传统CEM方法性能


<details>
  <summary>Details</summary>
Motivation: 世界模型与模型预测控制结合可在离线专家轨迹数据集上训练，实现推理时广泛规划任务的泛化。基于梯度的规划相比传统MPC方法计算效率更高，但现有性能落后于其他方法。研究发现世界模型训练时使用下一状态预测目标，但测试时用于估计动作序列，存在训练-测试差距。

Method: 提出改进世界模型训练方法，通过训练时数据合成技术缩小训练-测试差距。具体包括设计训练策略使世界模型更好地支持基于梯度的规划，优化模型在动作序列估计任务上的表现。

Result: 在测试时，该方法在10%的时间预算下，在各种物体操作和导航任务中达到或超过传统无梯度交叉熵方法(CEM)的性能。基于梯度的规划效率显著提升，性能与经典方法相当或更好。

Conclusion: 通过缩小世界模型的训练-测试差距，提出的训练时数据合成技术显著提升了基于梯度规划的性能，使其在有限计算预算下达到或超过传统规划方法的性能，为高效模型预测控制提供了新途径。

Abstract: World models paired with model predictive control (MPC) can be trained offline on large-scale datasets of expert trajectories and enable generalization to a wide range of planning tasks at inference time. Compared to traditional MPC procedures, which rely on slow search algorithms or on iteratively solving optimization problems exactly, gradient-based planning offers a computationally efficient alternative. However, the performance of gradient-based planning has thus far lagged behind that of other approaches. In this paper, we propose improved methods for training world models that enable efficient gradient-based planning. We begin with the observation that although a world model is trained on a next-state prediction objective, it is used at test-time to instead estimate a sequence of actions. The goal of our work is to close this train-test gap. To that end, we propose train-time data synthesis techniques that enable significantly improved gradient-based planning with existing world models. At test time, our approach outperforms or matches the classical gradient-free cross-entropy method (CEM) across a variety of object manipulation and navigation tasks in 10% of the time budget.

</details>


### [145] [KGOT: Unified Knowledge Graph and Optimal Transport Pseudo-Labeling for Molecule-Protein Interaction Prediction](https://arxiv.org/abs/2512.09365)
*Jiayu Qin,Zhengquan Luo,Guy Tadmor,Changyou Chen,David Zeevi,Zhiqiang Xu*

Main category: cs.LG

TL;DR: 提出一种基于最优运输的伪标签生成框架，利用多种生物数据源增强分子-蛋白质相互作用预测，解决标注数据稀缺和忽略生物上下文的问题。


<details>
  <summary>Details</summary>
Motivation: 分子-蛋白质相互作用预测面临两个主要挑战：1）标注数据稀缺，现有数据集仅捕获少量生物相关相互作用；2）现有方法仅依赖分子和蛋白质特征，忽略了基因、代谢通路、功能注释等更广泛的生物上下文信息。

Method: 首先整合多种生物数据集（分子、蛋白质、基因和通路级相互作用），然后开发基于最优运输的方法为未标注的分子-蛋白质对生成高质量伪标签，利用已知相互作用的底层分布指导标签分配。

Result: 在多个MPI数据集（包括虚拟筛选和蛋白质检索任务）上评估，相比最先进方法在预测准确性和对未见相互作用的零样本能力方面有显著提升。

Conclusion: 该框架不仅改进了MPI预测，还为利用多样化生物数据源解决传统单模态或双模态学习受限问题提供了新范式，为计算生物学和药物发现领域的未来发展铺平了道路。

Abstract: Predicting molecule-protein interactions (MPIs) is a fundamental task in computational biology, with crucial applications in drug discovery and molecular function annotation. However, existing MPI models face two major challenges. First, the scarcity of labeled molecule-protein pairs significantly limits model performance, as available datasets capture only a small fraction of biological relevant interactions. Second, most methods rely solely on molecular and protein features, ignoring broader biological context such as genes, metabolic pathways, and functional annotations that could provide essential complementary information. To address these limitations, our framework first aggregates diverse biological datasets, including molecular, protein, genes and pathway-level interactions, and then develop an optimal transport-based approach to generate high-quality pseudo-labels for unlabeled molecule-protein pairs, leveraging the underlying distribution of known interactions to guide label assignment. By treating pseudo-labeling as a mechanism for bridging disparate biological modalities, our approach enables the effective use of heterogeneous data to enhance MPI prediction. We evaluate our framework on multiple MPI datasets including virtual screening tasks and protein retrieval tasks, demonstrating substantial improvements over state-of-the-art methods in prediction accuracies and zero shot ability across unseen interactions. Beyond MPI prediction, our approach provides a new paradigm for leveraging diverse biological data sources to tackle problems traditionally constrained by single- or bi-modal learning, paving the way for future advances in computational biology and drug discovery.

</details>


### [146] [Are Hypervectors Enough? Single-Call LLM Reasoning over Knowledge Graphs](https://arxiv.org/abs/2512.09369)
*Yezi Liu,William Youngwoo Chung,Hanning Chen,Calvin Yeung,Mohsen Imani*

Main category: cs.LG

TL;DR: PathHD：基于超维度计算的轻量级知识图谱推理框架，用单次LLM调用替代传统神经路径评分，实现高效、可解释的KG-LLM推理


<details>
  <summary>Details</summary>
Motivation: 现有基于知识图谱的LLM推理方法依赖重型神经编码器进行路径嵌入评分，或需要多次LLM调用进行候选排序，导致高延迟、高GPU成本和不透明的决策过程，难以实现忠实、可扩展的部署

Method: 提出PathHD框架：1）使用超维度计算（HDC）替代神经路径评分，将关系路径编码为块对角GHRR超向量；2）通过块级余弦相似度和Top-K剪枝对候选进行排序；3）单次LLM裁决生成最终答案并引用支持路径

Result: 在WebQSP、CWQ和GrailQA数据集上：1）Hits@1指标与强神经基线相当或更好，且每查询仅需一次LLM调用；2）端到端延迟降低40-60%，GPU内存减少3-5倍；3）提供忠实、基于路径的可解释推理，改善错误诊断和可控性

Conclusion: 精心设计的HDC表示为高效KG-LLM推理提供了实用基础，在准确性、效率和可解释性之间实现了有利的权衡，表明超维度计算是替代传统神经路径评分的可行方案

Abstract: Recent advances in large language models (LLMs) have enabled strong reasoning over both structured and unstructured knowledge. When grounded on knowledge graphs (KGs), however, prevailing pipelines rely on heavy neural encoders to embed and score symbolic paths or on repeated LLM calls to rank candidates, leading to high latency, GPU cost, and opaque decisions that hinder faithful, scalable deployment. We propose PathHD, a lightweight and encoder-free KG reasoning framework that replaces neural path scoring with hyperdimensional computing (HDC) and uses only a single LLM call per query. PathHD encodes relation paths into block-diagonal GHRR hypervectors, ranks candidates with blockwise cosine similarity and Top-K pruning, and then performs a one-shot LLM adjudication to produce the final answer together with cited supporting paths. Technically, PathHD is built on three ingredients: (i) an order-aware, non-commutative binding operator for path composition, (ii) a calibrated similarity for robust hypervector-based retrieval, and (iii) a one-shot adjudication step that preserves interpretability while eliminating per-path LLM scoring. On WebQSP, CWQ, and the GrailQA split, PathHD (i) attains comparable or better Hits@1 than strong neural baselines while using one LLM call per query; (ii) reduces end-to-end latency by $40-60\%$ and GPU memory by $3-5\times$ thanks to encoder-free retrieval; and (iii) delivers faithful, path-grounded rationales that improve error diagnosis and controllability. These results indicate that carefully designed HDC representations provide a practical substrate for efficient KG-LLM reasoning, offering a favorable accuracy-efficiency-interpretability trade-off.

</details>


### [147] [Federated Distillation Assisted Vehicle Edge Caching Scheme Based on Lightweight DDPM](https://arxiv.org/abs/2512.09378)
*Xun Li,Qiong Wu,Pingyi Fan,Kezhi Wang,Wen Chen,Khaled B. Letaief*

Main category: cs.LG

TL;DR: 提出基于轻量去噪扩散概率模型和联邦蒸馏的车辆边缘缓存方案，解决传统联邦学习的通信开销大和车辆移动导致的训练失败问题


<details>
  <summary>Details</summary>
Motivation: 车辆边缘缓存能降低访问延迟，但需要准确预测用户兴趣内容而不泄露隐私。传统联邦学习虽然保护隐私，但存在通信开销大和车辆移动导致训练失败的问题

Method: 提出基于轻量去噪扩散概率模型(LDPM)的联邦蒸馏辅助车辆边缘缓存方案，通过联邦蒸馏减少模型传输，使用轻量模型适应车辆移动环境

Result: 仿真结果表明，该方案对车辆速度变化具有良好的鲁棒性，显著降低了通信开销并提高了缓存命中率

Conclusion: 提出的方案有效解决了传统联邦学习在车辆边缘缓存中的通信开销和移动性问题，实现了隐私保护下的高效内容预测

Abstract: Vehicle edge caching is a promising technology that can significantly reduce the latency for vehicle users (VUs) to access content by pre-caching user-interested content at edge nodes. It is crucial to accurately predict the content that VUs are interested in without exposing their privacy. Traditional federated learning (FL) can protect user privacy by sharing models rather than raw data. However, the training of FL requires frequent model transmission, which can result in significant communication overhead. Additionally, vehicles may leave the road side unit (RSU) coverage area before training is completed, leading to training failures. To address these issues, in this letter, we propose a federated distillation-assisted vehicle edge caching scheme based on lightweight denoising diffusion probabilistic model (LDPM). The simulation results demonstrate that the proposed vehicle edge caching scheme has good robustness to variations in vehicle speed, significantly reducing communication overhead and improving cache hit percentage.

</details>


### [148] [Towards Resilient Transportation: A Conditional Transformer for Accident-Informed Traffic Forecasting](https://arxiv.org/abs/2512.09398)
*Hongjun Wang,Jiawei Yong,Jiawei Wang,Shintaro Fukushima,Renhe Jiang*

Main category: cs.LG

TL;DR: 提出ConFormer框架，结合交通事故和法规数据，通过图传播和引导归一化层动态调整时空节点关系，在东京和加州数据集上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 交通预测面临外部因素（如交通事故、法规）的复杂影响，现有模型因数据整合有限而忽略这些因素，导致预测准确性受限。

Method: 提出ConFormer（条件Transformer）框架，整合图传播与引导归一化层，基于历史模式动态调整时空节点关系；同时构建包含交通事故和法规数据的东京和加州数据集。

Result: ConFormer在预测性能和效率上均超越最先进的STAEFormer，计算成本更低、参数需求更少；在多个指标上持续优于主流时空基线方法。

Conclusion: ConFormer通过有效整合外部因素和动态调整时空关系，显著提升交通预测准确性，为交通预测研究提供新方向。

Abstract: Traffic prediction remains a key challenge in spatio-temporal data mining, despite progress in deep learning. Accurate forecasting is hindered by the complex influence of external factors such as traffic accidents and regulations, often overlooked by existing models due to limited data integration. To address these limitations, we present two enriched traffic datasets from Tokyo and California, incorporating traffic accident and regulation data. Leveraging these datasets, we propose ConFormer (Conditional Transformer), a novel framework that integrates graph propagation with guided normalization layer. This design dynamically adjusts spatial and temporal node relationships based on historical patterns, enhancing predictive accuracy. Our model surpasses the state-of-the-art STAEFormer in both predictive performance and efficiency, achieving lower computational costs and reduced parameter demands. Extensive evaluations demonstrate that ConFormer consistently outperforms mainstream spatio-temporal baselines across multiple metrics, underscoring its potential to advance traffic prediction research.

</details>


### [149] [Cauchy-Schwarz Fairness Regularizer](https://arxiv.org/abs/2512.09467)
*Yezi Liu,Hanning Chen,Wenjun Huang,Yang Ni,Mohsen Imani*

Main category: cs.LG

TL;DR: 该论文提出了一种基于柯西-施瓦茨散度的公平性正则化器，用于改善机器学习中的群体公平性，相比现有方法具有更紧的泛化界、更好的尺度鲁棒性，并在多个基准测试中表现出更稳定的效用-公平性权衡。


<details>
  <summary>Details</summary>
Motivation: 现有公平性正则化器基于不同的距离度量和设计选择，导致其行为难以推理且在不同任务中性能不一致。需要识别良好公平性正则化器应具备的特性，并设计满足这些特性的新方法。

Method: 将现有方法分为三类：跨敏感组匹配预测统计量、对齐潜在表示、直接最小化预测与敏感属性之间的依赖性。基于期望特性提出柯西-施瓦茨公平性正则化器，惩罚敏感组条件预测分布之间的经验CS散度，并开发分布无关的核基估计器。

Result: 在高斯比较下，CS散度比KL散度、最大均值差异和人口统计均等中使用的均值差异具有更紧的界。在四个表格基准和一个图像数据集上的实验表明，CS正则化器在保持竞争性准确度的同时，一致改善了人口统计均等和机会均等指标，并在超参数设置下实现了更稳定的效用-公平性权衡。

Conclusion: 柯西-施瓦茨散度是构建公平性正则化器的有效距离度量，具有紧的泛化界、尺度鲁棒性和处理任意预测分布的能力，为机器学习中的群体公平性提供了一种更可靠和一致的解决方案。

Abstract: Group fairness in machine learning is often enforced by adding a regularizer that reduces the dependence between model predictions and sensitive attributes. However, existing regularizers are built on heterogeneous distance measures and design choices, which makes their behavior hard to reason about and their performance inconsistent across tasks. This raises a basic question: what properties make a good fairness regularizer? We address this question by first organizing existing in-process methods into three families: (i) matching prediction statistics across sensitive groups, (ii) aligning latent representations, and (iii) directly minimizing dependence between predictions and sensitive attributes. Through this lens, we identify desirable properties of the underlying distance measure, including tight generalization bounds, robustness to scale differences, and the ability to handle arbitrary prediction distributions. Motivated by these properties, we propose a Cauchy-Schwarz (CS) fairness regularizer that penalizes the empirical CS divergence between prediction distributions conditioned on sensitive groups. Under a Gaussian comparison, we show that CS divergence yields a tighter bound than Kullback-Leibler divergence, Maximum Mean Discrepancy, and the mean disparity used in Demographic Parity, and we discuss how these advantages translate to a distribution-free, kernel-based estimator that naturally extends to multiple sensitive attributes. Extensive experiments on four tabular benchmarks and one image dataset demonstrate that the proposed CS regularizer consistently improves Demographic Parity and Equal Opportunity metrics while maintaining competitive accuracy, and achieves a more stable utility-fairness trade-off across hyperparameter settings compared to prior regularizers.

</details>


### [150] [Representation Invariance and Allocation: When Subgroup Balance Matters](https://arxiv.org/abs/2512.09496)
*Anissa Alloula,Charles Jones,Zuzanna Wakefield-Skorniewska,Francesco Quinzan,Bartłomiej Papież*

Main category: cs.LG

TL;DR: 研究发现，训练数据中人口统计群体的不平衡分布并不总是损害模型性能，有时反而能提升子群体表现。作者提出"潜在分离假设"，认为微调模型对子群体表示的依赖程度取决于预训练模型潜在空间中子群体的分离程度。


<details>
  <summary>Details</summary>
Motivation: 传统实践认为平衡子群体表示能优化模型性能，但近期实证结果与此矛盾：在某些情况下，不平衡数据分布反而改善子群体性能，而有时即使训练数据中完全缺失某个子群体，其性能也不受影响。这促使研究者系统研究子群体分配问题。

Method: 在四个视觉和语言模型上进行系统性研究，改变训练数据组成以表征子群体性能对数据平衡的敏感性。提出"潜在分离假设"，形式化该假设并提供理论分析，最后通过实证验证。应用于基础模型微调，展示潜在子群体分离的定量分析如何指导数据收集和平衡决策。

Result: 研究发现子群体性能对数据平衡的敏感性取决于预训练模型潜在空间中子群体的分离程度。当子群体在潜在空间中高度分离时，平衡数据分布对性能至关重要；当分离程度较低时，不平衡分布可能反而有益。实证验证了该假设的有效性。

Conclusion: 研究挑战了"平衡子群体表示总是最优"的传统假设，提出了更精细的"潜在分离假设"。该框架为数据收集和平衡决策提供了理论指导，表明应根据预训练模型潜在空间中子群体的分离程度来制定数据策略，而非简单追求平衡。

Abstract: Unequal representation of demographic groups in training data poses challenges to model generalisation across populations. Standard practice assumes that balancing subgroup representation optimises performance. However, recent empirical results contradict this assumption: in some cases, imbalanced data distributions actually improve subgroup performance, while in others, subgroup performance remains unaffected by the absence of an entire subgroup during training. We conduct a systematic study of subgroup allocation across four vision and language models, varying training data composition to characterise the sensitivity of subgroup performance to data balance. We propose the latent separation hypothesis, which states that a partially fine-tuned model's dependence on subgroup representation is determined by the degree of separation between subgroups in the latent space of the pre-trained model. We formalise this hypothesis, provide theoretical analysis, and validate it empirically. Finally, we present a practical application to foundation model fine-tuning, demonstrating that quantitative analysis of latent subgroup separation can inform data collection and balancing decisions.

</details>


### [151] [Contextual Dynamic Pricing with Heterogeneous Buyers](https://arxiv.org/abs/2512.09513)
*Thodoris Lykouris,Sloan Nietert,Princewill Okoroafor,Chara Podimata,Julian Zimmert*

Main category: cs.LG

TL;DR: 论文研究具有异质买家群体的情境动态定价问题，提出基于乐观后验采样的算法，实现最优后悔界


<details>
  <summary>Details</summary>
Motivation: 现有研究大多假设买家类型同质，而实际中买家估值类型存在异质性，需要从有限支持集K*的未知分布中抽取，这增加了定价决策的复杂性

Method: 开发基于乐观后验采样的情境定价算法，在非情境定价情况下提出方差感知的缩放算法

Result: 情境定价算法达到后悔界Õ(K*√dT)，在d和T维度上达到最优；非情境定价算法在K*维度上达到最优依赖

Conclusion: 该研究首次系统研究了异质买家群体的情境动态定价问题，提出的算法在不同设置下均达到最优后悔界，为实际定价决策提供了理论保证

Abstract: We initiate the study of contextual dynamic pricing with a heterogeneous population of buyers, where a seller repeatedly posts prices (over $T$ rounds) that depend on the observable $d$-dimensional context and receives binary purchase feedback. Unlike prior work assuming homogeneous buyer types, in our setting the buyer's valuation type is drawn from an unknown distribution with finite support size $K_{\star}$. We develop a contextual pricing algorithm based on optimistic posterior sampling with regret $\widetilde{O}(K_{\star}\sqrt{dT})$, which we prove to be tight in $d$ and $T$ up to logarithmic terms. Finally, we refine our analysis for the non-contextual pricing case, proposing a variance-aware zooming algorithm that achieves the optimal dependence on $K_{\star}$.

</details>


### [152] [QuanvNeXt: An end-to-end quanvolutional neural network for EEG-based detection of major depressive disorder](https://arxiv.org/abs/2512.09517)
*Nabil Anan Orka,Ehtashamul Haque,Maftahul Jannat,Md Abdul Awal,Mohammad Ali Moni*

Main category: cs.LG

TL;DR: QuanvNeXt是一个用于基于EEG的抑郁症诊断的端到端全量子卷积模型，通过新颖的交叉残差块减少特征同质性并增强跨特征关系，在两个开源数据集上取得了93.1%的平均准确率和97.2%的平均AUC-ROC，优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 开发一个高效可靠的基于脑电图（EEG）的抑郁症诊断模型，解决现有方法在特征同质性、跨特征关系建模以及参数效率方面的问题。

Method: 提出QuanvNeXt模型，采用端到端全量子卷积架构，引入新颖的交叉残差块来减少特征同质性并增强跨特征关系，同时保持参数效率。使用高斯噪声水平进行不确定性分析，并进行事后可解释AI分析验证模型学习到的特征模式。

Result: 在两个开源数据集上，QuanvNeXt取得了93.1%的平均准确率和97.2%的平均AUC-ROC，优于InceptionTime等现有基线方法（91.7%准确率，95.9% AUC-ROC）。不确定性分析显示即使在最高扰动水平（ε=0.1）下，ECE分数也保持在较低水平（0.0436）到中等水平（0.1159）。可解释性分析证实模型能有效识别和学习区分健康对照和重度抑郁症的频谱时间模式。

Conclusion: QuanvNeXt为基于EEG的抑郁症诊断建立了一个高效可靠的方法，通过创新的交叉残差块设计在保持参数效率的同时提升了模型性能，并通过不确定性分析和可解释性验证了其可靠性。

Abstract: This study presents QuanvNeXt, an end-to-end fully quanvolutional model for EEG-based depression diagnosis. QuanvNeXt incorporates a novel Cross Residual block, which reduces feature homogeneity and strengthens cross-feature relationships while retaining parameter efficiency. We evaluated QuanvNeXt on two open-source datasets, where it achieved an average accuracy of 93.1% and an average AUC-ROC of 97.2%, outperforming state-of-the-art baselines such as InceptionTime (91.7% accuracy, 95.9% AUC-ROC). An uncertainty analysis across Gaussian noise levels demonstrated well-calibrated predictions, with ECE scores remaining low (0.0436, Dataset 1) to moderate (0.1159, Dataset 2) even at the highest perturbation (ε = 0.1). Additionally, a post-hoc explainable AI analysis confirmed that QuanvNeXt effectively identifies and learns spectrotemporal patterns that distinguish between healthy controls and major depressive disorder. Overall, QuanvNeXt establishes an efficient and reliable approach for EEG-based depression diagnosis.

</details>


### [153] [Stanford Sleep Bench: Evaluating Polysomnography Pre-training Methods for Sleep Foundation Models](https://arxiv.org/abs/2512.09591)
*Magnus Ruud Kjaer,Rahul Thapa,Gauri Ganjoo,Hyatt Moore,Poul Joergen Jennum,Brandon M. Westover,James Zou,Emmanuel Mignot,Bryan He,Andreas Brink-Kjaer*

Main category: cs.LG

TL;DR: 斯坦福睡眠基准是一个包含17,467个多导睡眠图记录的大型数据集，用于评估自监督表示学习方法在睡眠分析任务中的表现，发现对比学习在疾病和死亡率预测任务中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 多导睡眠图产生大量多模态临床数据，为自监督表示学习提供了机会，但目前睡眠基础模型发展面临两个关键限制：缺乏共享数据集和基准测试，以及缺乏对自监督学习方法在睡眠相关任务中的系统评估。

Method: 引入斯坦福睡眠基准，这是一个包含17,467个记录（总计超过163,000小时）的大规模多导睡眠图数据集，包含13个临床疾病预测任务以及睡眠分期、呼吸暂停诊断和年龄估计等经典任务。系统评估了多种自监督预训练方法在这些任务上的表现。

Result: 结果显示，在睡眠分期、呼吸暂停诊断和年龄估计任务上，多种预训练方法表现相当。但在死亡率和疾病预测任务上，对比学习方法显著优于其他方法，并且在预训练期间收敛更快。

Conclusion: 斯坦福睡眠基准填补了睡眠研究中的数据和方法评估空白，对比学习在临床预测任务中表现突出。研究团队将发布数据集、预训练模型权重、训练管道和评估代码以促进可重复性和睡眠研究进展。

Abstract: Polysomnography (PSG), the gold standard test for sleep analysis, generates vast amounts of multimodal clinical data, presenting an opportunity to leverage self-supervised representation learning (SSRL) for pre-training foundation models to enhance sleep analysis. However, progress in sleep foundation models is hindered by two key limitations: (1) the lack of a shared dataset and benchmark with diverse tasks for training and evaluation, and (2) the absence of a systematic evaluation of SSRL approaches across sleep-related tasks. To address these gaps, we introduce Stanford Sleep Bench, a large-scale PSG dataset comprising 17,467 recordings totaling over 163,000 hours from a major sleep clinic, including 13 clinical disease prediction tasks alongside canonical sleep-related tasks such as sleep staging, apnea diagnosis, and age estimation. We systematically evaluate SSRL pre-training methods on Stanford Sleep Bench, assessing downstream performance across four tasks: sleep staging, apnea diagnosis, age estimation, and disease and mortality prediction. Our results show that multiple pretraining methods achieve comparable performance for sleep staging, apnea diagnosis, and age estimation. However, for mortality and disease prediction, contrastive learning significantly outperforms other approaches while also converging faster during pretraining. To facilitate reproducibility and advance sleep research, we will release Stanford Sleep Bench along with pretrained model weights, training pipelines, and evaluation code.

</details>


### [154] [Semantic-Aware Cooperative Communication and Computation Framework in Vehicular Networks](https://arxiv.org/abs/2512.09621)
*Jingbo Zhang,Maoxin Ji,Qiong Wu,Pingyi Fan,Kezhi Wang,Wen Chen*

Main category: cs.LG

TL;DR: 本文提出了一种用于车联网的三方协作语义通信框架，通过V2I和V2V通信实现语义任务卸载，使用MAPPO-PDN算法优化语义符号数量，线性规划解决卸载比例问题。


<details>
  <summary>Details</summary>
Motivation: 车联网中结合语义通信和车辆边缘计算可以提供高效的任务处理，但需要考虑高速公路场景下的任务延迟和语义符号数量优化问题。

Method: 提出三方协作语义通信框架，构建混合整数非线性规划问题并分解为两个子问题：1) 使用基于参数分布噪声的多智能体近端策略优化方法优化语义符号数量；2) 使用线性规划解决卸载比例问题。

Result: 仿真结果表明，该方案的性能优于其他对比算法。

Conclusion: TCSC框架通过语义通信和车辆边缘计算的结合，有效解决了车联网中的任务卸载优化问题，在高速公路场景下表现出优越性能。

Abstract: Semantic Communication (SC) combined with Vehicular edge computing (VEC) provides an efficient edge task processing paradigm for Internet of Vehicles (IoV). Focusing on highway scenarios, this paper proposes a Tripartite Cooperative Semantic Communication (TCSC) framework, which enables Vehicle Users (VUs) to perform semantic task offloading via Vehicle-to-Infrastructure (V2I) and Vehicle-to-Vehicle (V2V) communications. Considering task latency and the number of semantic symbols, the framework constructs a Mixed-Integer Nonlinear Programming (MINLP) problem, which is transformed into two subproblems. First, we innovatively propose a multi-agent proximal policy optimization task offloading optimization method based on parametric distribution noise (MAPPO-PDN) to solve the optimization problem of the number of semantic symbols; second, linear programming (LP) is used to solve offloading ratio. Simulations show that performance of this scheme is superior to that of other algorithms.

</details>


### [155] [Membership and Dataset Inference Attacks on Large Audio Generative Models](https://arxiv.org/abs/2512.09654)
*Jakub Proboszcz,Paweł Kochanski,Karol Korszun,Donato Crisostomi,Giorgio Strano,Emanuele Rodolà,Kamil Deja,Jan Dubinski*

Main category: cs.LG

TL;DR: 该论文研究了生成音频模型的版权保护问题，通过成员推理攻击和数据集推理来验证艺术家的作品是否被用于模型训练。


<details>
  <summary>Details</summary>
Motivation: 随着生成音频模型（基于扩散和自回归架构）在质量和表现力方面的快速发展，引发了紧迫的版权问题。这些模型通常在大量艺术和商业作品上进行训练，因此需要一种可靠的方法来验证艺术家的材料是否被包含在训练数据中，为版权持有者提供保护内容的手段。

Method: 研究通过成员推理攻击（MIA）和数据集推理（DI）两种方法来验证音频样本是否属于训练集。MIA试图确定特定音频样本是否是训练集的一部分，而DI则聚合多个样本的成员证据，特别关注艺术家通常拥有作品集合而非孤立样本的情况。

Result: 实证结果表明，对于在大型多样化数据集上训练的模型，单独的成员推理攻击效果有限，因为每个样本的成员信号较弱。然而，数据集推理在音频领域是成功的，它通过聚合多个样本的证据，为评估艺术家的作品是否对模型训练有贡献提供了更实用的机制。

Conclusion: 数据集推理是音频生成模型版权保护和数据集问责制的有前景方向，为艺术家和媒体所有者提供了一种更有效的方法来验证其作品是否被用于模型训练。

Abstract: Generative audio models, based on diffusion and autoregressive architectures, have advanced rapidly in both quality and expressiveness. This progress, however, raises pressing copyright concerns, as such models are often trained on vast corpora of artistic and commercial works. A central question is whether one can reliably verify if an artist's material was included in training, thereby providing a means for copyright holders to protect their content. In this work, we investigate the feasibility of such verification through membership inference attacks (MIA) on open-source generative audio models, which attempt to determine whether a specific audio sample was part of the training set. Our empirical results show that membership inference alone is of limited effectiveness at scale, as the per-sample membership signal is weak for models trained on large and diverse datasets. However, artists and media owners typically hold collections of works rather than isolated samples. Building on prior work in text and vision domains, in this work we focus on dataset inference (DI), which aggregates diverse membership evidence across multiple samples. We find that DI is successful in the audio domain, offering a more practical mechanism for assessing whether an artist's works contributed to model training. Our results suggest DI as a promising direction for copyright protection and dataset accountability in the era of large audio generative models.

</details>


### [156] [A data-driven approach to linking design features with manufacturing process data for sustainable product development](https://arxiv.org/abs/2512.09690)
*Jiahang Li,Lucas Cazzonelli,Jacqueline Höllig,Markus Doellken,Sven Matthiesen*

Main category: cs.LG

TL;DR: 提出一种数据驱动方法，通过机器学习模型建立设计特征与制造过程数据之间的映射关系，为可持续产品开发提供自动化设计改进建议。


<details>
  <summary>Details</summary>
Motivation: 工业物联网技术使制造过程数据实时收集成为可能，但现有数据驱动方法局限于特定领域（如设计或制造），缺乏设计特征与制造过程数据的集成。由于设计决策显著影响制造结果（如错误率、能耗、加工时间），这种集成不足限制了数据驱动产品设计的改进潜力。

Method: 开发了全面的系统架构以确保连续数据收集和集成，建立了设计特征与制造过程数据之间的关联，以此为基础开发机器学习模型，实现自动化设计改进建议，并将制造过程数据与可持续性指标相结合。

Result: 该方法能够映射和分析设计特征与制造过程数据之间的关系，通过机器学习模型提供自动化设计改进建议，为可持续产品开发开辟新的可能性。

Conclusion: 提出的数据驱动方法通过集成设计特征和制造过程数据，建立了二者之间的映射关系，开发了自动化设计改进的机器学习模型，为可持续产品开发提供了新的途径。

Abstract: The growing adoption of Industrial Internet of Things (IIoT) technologies enables automated, real-time collection of manufacturing process data, unlocking new opportunities for data-driven product development. Current data-driven methods are generally applied within specific domains, such as design or manufacturing, with limited exploration of integrating design features and manufacturing process data. Since design decisions significantly affect manufacturing outcomes, such as error rates, energy consumption, and processing times, the lack of such integration restricts the potential for data-driven product design improvements. This paper presents a data-driven approach to mapping and analyzing the relationship between design features and manufacturing process data. A comprehensive system architecture is developed to ensure continuous data collection and integration. The linkage between design features and manufacturing process data serves as the basis for developing a machine learning model that enables automated design improvement suggestions. By integrating manufacturing process data with sustainability metrics, this approach opens new possibilities for sustainable product development.

</details>


### [157] [Training One Model to Master Cross-Level Agentic Actions via Reinforcement Learning](https://arxiv.org/abs/2512.09706)
*Kaichen He,Zihao Wang,Muyao Li,Anji Liu,Yitao Liang*

Main category: cs.LG

TL;DR: CrossAgent是一个统一智能体模型，能够掌握异构动作空间并自主选择每个轨迹步骤的最有效接口，在Minecraft环境中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有智能体通常局限于静态、预定义的动作空间（如仅使用API、GUI事件或机器人命令），这种刚性限制了它们在动态环境中的适应性，因为最佳交互粒度会随上下文变化。

Method: 提出了一个综合训练流程，结合冷启动监督微调和多轮组相对策略优化（GRPO）算法，使智能体能够学习自适应动作切换，无需人工指定规则。

Result: 在开放世界Minecraft环境中的800多个任务上进行广泛实验，CrossAgent实现了最先进的性能，通过动态利用不同动作空间的优势，显著优于固定动作基线，在长时程推理中表现出优异的泛化能力和效率。

Conclusion: CrossAgent通过统一异构动作空间和自适应接口选择，为智能体AI提供了更灵活、适应性更强的解决方案，在动态环境中展现出卓越性能。

Abstract: The paradigm of agentic AI is shifting from engineered complex workflows to post-training native models. However, existing agents are typically confined to static, predefined action spaces--such as exclusively using APIs, GUI events, or robotic commands. This rigidity limits their adaptability in dynamic environments where the optimal granularity of interaction varies contextually. To bridge this gap, we propose CrossAgent, a unified agentic model that masters heterogeneous action spaces and autonomously selects the most effective interface for each step of a trajectory. We introduce a comprehensive training pipeline that integrates cold-start supervised fine-tuning with a Multi-Turn Group Relative Policy Optimization (GRPO) algorithm. This approach enables the agent to learn adaptive action switching--balancing high-level efficiency with low-level precision--without human-specified rules. Extensive experiments on over 800 tasks in the open-world Minecraft environment demonstrate that CrossAgent achieves state-of-the-art performance. By dynamically leveraging the strengths of diverse action spaces, our model significantly outperforms fixed-action baselines, exhibiting superior generalization and efficiency in long-horizon reasoning. All code and models are available at https://github.com/CraftJarvis/OpenHA

</details>


### [158] [Mixture of Lookup Key-Value Experts](https://arxiv.org/abs/2512.09723)
*Zongcheng Wang*

Main category: cs.LG

TL;DR: 提出MoLKV模型改进MoLE架构，通过基于查询-键值对的上下文感知专家选择机制提升性能


<details>
  <summary>Details</summary>
Motivation: MoLE架构虽然适合终端设备推理，但其仅基于输入ID的上下文无关专家选择机制限制了模型性能，需要更智能的专家激活方式

Method: 提出MoLKV模型，将每个专家构建为键值对，通过输入生成的查询与当前序列缓存的键值专家交互，实现上下文感知的专家输出

Result: 实验结果显示MoLKV在小规模评估中取得了显著更低的验证损失，证明了上下文感知机制的有效性

Conclusion: MoLKV通过引入上下文感知的专家选择机制，有效克服了MoLE的性能限制，为资源受限设备上的LLM推理提供了改进方案

Abstract: Recent research has developed several LLM architectures suitable for inference on end-user devices, such as the Mixture of Lookup Experts (MoLE)~\parencite{jie_mixture_2025}. A key feature of MoLE is that each token id is associated with a dedicated group of experts. For a given input, only the experts corresponding to the input token id will be activated. Since the communication overhead of loading this small number of activated experts into RAM during inference is negligible, expert parameters can be offloaded to storage, making MoLE suitable for resource-constrained devices. However, MoLE's context-independent expert selection mechanism, based solely on input ids, may limit model performance. To address this, we propose the \textbf{M}ixture \textbf{o}f \textbf{L}ookup \textbf{K}ey-\textbf{V}alue Experts (\textbf{MoLKV}) model. In MoLKV, each expert is structured as a key-value pair. For a given input, the input-derived query interacts with the cached key-value experts from the current sequence, generating a context-aware expert output. This context-aware mechanism alleviates the limitation of MoLE, and experimental results demonstrate that MoLKV achieves significantly lower validation loss in small-scale evaluations.

</details>


### [159] [Circuits, Features, and Heuristics in Molecular Transformers](https://arxiv.org/abs/2512.09757)
*Kristof Varadi,Mark Marosi,Peter Antal*

Main category: cs.LG

TL;DR: 该论文对训练在药物类小分子上的自回归Transformer进行机制分析，揭示了其在多个抽象层次上捕捉分子表示规则的计算结构


<details>
  <summary>Details</summary>
Motivation: Transformer模型能够生成有效且多样的化学结构，但人们对其捕捉分子表示规则的机制知之甚少。研究者希望揭示这些模型在不同抽象层次上的计算结构

Method: 使用稀疏自编码器（SAEs）提取与化学相关激活模式相关的特征字典，对自回归Transformer进行机制分析，识别低层次语法解析和高层次化学有效性约束的计算模式

Result: 识别出与低层次语法解析和更抽象的化学有效性约束一致的计算模式，提取了与化学相关激活模式相关的特征字典，验证了机制洞察可以转化为各种实际场景中的预测性能

Conclusion: 通过机制分析揭示了Transformer模型捕捉化学结构规则的计算基础，证明了机制洞察对下游任务的实际价值，为理解化学生成模型的内部工作机制提供了新视角

Abstract: Transformers generate valid and diverse chemical structures, but little is known about the mechanisms that enable these models to capture the rules of molecular representation. We present a mechanistic analysis of autoregressive transformers trained on drug-like small molecules to reveal the computational structure underlying their capabilities across multiple levels of abstraction. We identify computational patterns consistent with low-level syntactic parsing and more abstract chemical validity constraints. Using sparse autoencoders (SAEs), we extract feature dictionaries associated with chemically relevant activation patterns. We validate our findings on downstream tasks and find that mechanistic insights can translate to predictive performance in various practical settings.

</details>


### [160] [Knowledge Diversion for Efficient Morphology Control and Policy Transfer](https://arxiv.org/abs/2512.09796)
*Fu Feng,Ruixiao Shi,Yucheng Xie,Jianlu Shen,Jing Wang,Xin Geng*

Main category: cs.LG

TL;DR: DivMorph提出了一种模块化训练范式，通过知识分流学习可分解的控制器，实现跨异构形态的通用控制，显著提升样本效率和减小模型规模。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的通用形态控制方法存在计算成本高、部署开销大、跨任务泛化能力有限的问题，需要为每个新任务从头训练。

Method: DivMorph采用模块化训练范式，通过SVD将随机初始化的Transformer权重分解为因子单元，使用动态软门控基于任务和形态嵌入调制这些单元，将其分离为共享的"learngenes"和特定于形态/任务的"tailors"，实现知识解耦。

Result: DivMorph在跨任务迁移中比直接微调提升3倍样本效率，在单智能体部署中减少17倍模型规模，达到最先进性能。

Conclusion: DivMorph通过知识分流和模块化设计，实现了高效、可扩展的通用形态控制，支持有效的策略迁移到新任务。

Abstract: Universal morphology control aims to learn a universal policy that generalizes across heterogeneous agent morphologies, with Transformer-based controllers emerging as a popular choice. However, such architectures incur substantial computational costs, resulting in high deployment overhead, and existing methods exhibit limited cross-task generalization, necessitating training from scratch for each new task. To this end, we propose \textbf{DivMorph}, a modular training paradigm that leverages knowledge diversion to learn decomposable controllers. DivMorph factorizes randomly initialized Transformer weights into factor units via SVD prior to training and employs dynamic soft gating to modulate these units based on task and morphology embeddings, separating them into shared \textit{learngenes} and morphology- and task-specific \textit{tailors}, thereby achieving knowledge disentanglement. By selectively activating relevant components, DivMorph enables scalable and efficient policy deployment while supporting effective policy transfer to novel tasks. Extensive experiments demonstrate that DivMorph achieves state-of-the-art performance, achieving a 3$\times$ improvement in sample efficiency over direct finetuning for cross-task transfer and a 17$\times$ reduction in model size for single-agent deployment.

</details>


### [161] [Ariel-ML: Computing Parallelization with Embedded Rust for Neural Networks on Heterogeneous Multi-core Microcontrollers](https://arxiv.org/abs/2512.09800)
*Zhaolan Huang,Kaspar Schleiser,Gyungmin Myung,Emmanuel Baccelli*

Main category: cs.LG

TL;DR: Ariel-ML：首个支持多核MCU并行推理的Rust嵌入式TinyML平台，相比现有方案降低延迟并保持相似内存占用


<details>
  <summary>Details</summary>
Motivation: 随着低功耗MCU从单核向多核架构演进，以及Rust在嵌入式领域的兴起，当前缺乏能够自动化利用多核MCU进行TinyML模型并行推理的Rust嵌入式软件平台。现有方案无法满足在已部署的传感/执行系统上持续改进和添加新服务的需求。

Method: 设计并实现了Ariel-ML工具包，结合通用TinyML流水线和嵌入式Rust软件平台，能够充分利用多种32位微控制器家族（Arm Cortex-M、RISC-V、ESP-32）的多核能力。开源了完整实现代码，并使用多种TinyML模型进行基准测试。

Result: Ariel-ML在推理延迟方面优于现有技术，与使用嵌入式C/C++的现有工具包相比，实现了相似的内存占用。为各种TinyML模型提供了有效的多核并行推理能力。

Conclusion: Ariel-ML填补了Rust嵌入式软件平台在多核MCU上自动化并行推理的空白，为TinyML从业者和资源受限的嵌入式Rust开发者提供了有用的基础平台。

Abstract: Low-power microcontroller (MCU) hardware is currently evolving from single-core architectures to predominantly multi-core architectures. In parallel, new embedded software building blocks are more and more written in Rust, while C/C++ dominance fades in this domain. On the other hand, small artificial neural networks (ANN) of various kinds are increasingly deployed in edge AI use cases, thus deployed and executed directly on low-power MCUs. In this context, both incremental improvements and novel innovative services will have to be continuously retrofitted using ANNs execution in software embedded on sensing/actuating systems already deployed in the field. However, there was so far no Rust embedded software platform automating parallelization for inference computation on multi-core MCUs executing arbitrary TinyML models. This paper thus fills this gap by introducing Ariel-ML, a novel toolkit we designed combining a generic TinyML pipeline and an embedded Rust software platform which can take full advantage of multi-core capabilities of various 32bit microcontroller families (Arm Cortex-M, RISC-V, ESP-32). We published the full open source code of its implementation, which we used to benchmark its capabilities using a zoo of various TinyML models. We show that Ariel-ML outperforms prior art in terms of inference latency as expected, and we show that, compared to pre-existing toolkits using embedded C/C++, Ariel-ML achieves comparable memory footprints. Ariel-ML thus provides a useful basis for TinyML practitioners and resource-constrained embedded Rust developers.

</details>


### [162] [Incorporating Fairness in Neighborhood Graphs for Fair Spectral Clustering](https://arxiv.org/abs/2512.09810)
*Adithya K Moorthy,V Vijaya Saradhi,Bhanu Prasad*

Main category: cs.LG

TL;DR: 该研究提出了构建公平k近邻图和公平epsilon邻域图的新方法，通过在图构建的最早阶段引入公平约束来主动执行人口统计平等，从而改善谱聚类的公平性。


<details>
  <summary>Details</summary>
Motivation: 传统图聚类方法在图构建过程中可能通过不公平的图结构使某些群体代表性不足，从而传播偏见。现有研究缺乏在图构建预处理阶段主动引入公平性的方法，导致谱聚类结果存在偏差。

Method: 提出了两种公平图构建方法：1) 公平k近邻图构建，在邻域选择步骤中主动强制执行人口统计平等；2) 公平epsilon邻域图构建，在邻域形成过程中纳入公平约束。这些方法通过在节点邻域中按比例表示敏感特征，在保持几何一致性的同时确保拓扑公平性。

Result: 在三个合成数据集、七个真实世界表格数据集和三个真实世界图像数据集上的实验证明，所提出的公平图构建方法在谱聚类任务中超越了现有基线方法，能够产生更公平的聚类结果。

Conclusion: 图构建中的拓扑公平性对于实现公平的谱聚类结果至关重要。通过在预处理阶段主动引入公平约束，可以在不改变聚类算法本身的情况下实现更公平的无监督学习结果，填补了公平无监督学习中的重要空白。

Abstract: Graph clustering plays a pivotal role in unsupervised learning methods like spectral clustering, yet traditional methods for graph clustering often perpetuate bias through unfair graph constructions that may underrepresent some groups. The current research introduces novel approaches for constructing fair k-nearest neighbor (kNN) and fair epsilon-neighborhood graphs that proactively enforce demographic parity during graph formation. By incorporating fairness constraints at the earliest stage of neighborhood selection steps, our approaches incorporate proportional representation of sensitive features into the local graph structure while maintaining geometric consistency.Our work addresses a critical gap in pre-processing for fair spectral clustering, demonstrating that topological fairness in graph construction is essential for achieving equitable clustering outcomes. Widely used graph construction methods like kNN and epsilon-neighborhood graphs propagate edge based disparate impact on sensitive groups, leading to biased clustering results. Providing representation of each sensitive group in the neighborhood of every node leads to fairer spectral clustering results because the topological features of the graph naturally reflect equitable group ratios. This research fills an essential shortcoming in fair unsupervised learning, by illustrating how topological fairness in graph construction inherently facilitates fairer spectral clustering results without the need for changes to the clustering algorithm itself. Thorough experiments on three synthetic datasets, seven real-world tabular datasets, and three real-world image datasets prove that our fair graph construction methods surpass the current baselines in graph clustering tasks.

</details>


### [163] [Conformal Bandits: Bringing statistical validity and reward efficiency to the small-gap regime](https://arxiv.org/abs/2512.09850)
*Simone Cuonzo,Nina Deliu*

Main category: cs.LG

TL;DR: 将Conformal Prediction引入bandit问题，在保持遗憾最小化的同时提供有限时间统计保证，特别适用于小差距场景


<details>
  <summary>Details</summary>
Motivation: 传统bandit策略如Thompson Sampling和UCB依赖分布假设或渐近保证，主要关注遗憾最小化而忽视统计性质，特别是在小差距场景下表现不佳

Method: 提出Conformal Bandits框架，将Conformal Prediction整合到bandit决策中，在投资组合分配应用中结合隐马尔可夫模型捕捉市场状态切换

Result: 在小差距场景中优于传统UCB策略，实现名义覆盖保证，在投资组合应用中通过状态切换模型提高风险调整后的遗憾效率回报

Conclusion: Conformal Bandits成功将遗憾最小化与统计保证相结合，为小差距场景提供了实用的解决方案，在金融应用中表现出色

Abstract: We introduce Conformal Bandits, a novel framework integrating Conformal Prediction (CP) into bandit problems, a classic paradigm for sequential decision-making under uncertainty. Traditional regret-minimisation bandit strategies like Thompson Sampling and Upper Confidence Bound (UCB) typically rely on distributional assumptions or asymptotic guarantees; further, they remain largely focused on regret, neglecting their statistical properties. We address this gap. Through the adoption of CP, we bridge the regret-minimising potential of a decision-making bandit policy with statistical guarantees in the form of finite-time prediction coverage.
  We demonstrate the potential of it Conformal Bandits through simulation studies and an application to portfolio allocation, a typical small-gap regime, where differences in arm rewards are far too small for classical policies to achieve optimal regret bounds in finite sample. Motivated by this, we showcase our framework's practical advantage in terms of regret in small-gap settings, as well as its added value in achieving nominal coverage guarantees where classical UCB policies fail. Focusing on our application of interest, we further illustrate how integrating hidden Markov models to capture the regime-switching behaviour of financial markets, enhances the exploration-exploitation trade-off, and translates into higher risk-adjusted regret efficiency returns, while preserving coverage guarantees.

</details>


### [164] [HPM-KD: Hierarchical Progressive Multi-Teacher Framework for Knowledge Distillation and Efficient Model Compression](https://arxiv.org/abs/2512.09886)
*Gustavo Coelho Haase,Paulo Henrique Dourado da Silva*

Main category: cs.LG

TL;DR: HPM-KD是一个集成六个协同组件的知识蒸馏框架，解决了传统KD的超参数敏感、容量差距、多教师协调和计算效率问题，实现了10-15倍压缩、85%精度保持和30-40%训练时间减少。


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏技术存在四个关键限制：1）对超参数敏感需要大量手动调优；2）从大教师模型到小学生模型时的容量差距问题；3）多教师场景下的次优协调；4）计算资源使用效率低下。这些限制阻碍了知识蒸馏在实际应用中的广泛采用。

Method: HPM-KD框架集成了六个协同组件：1）基于元学习的自适应配置管理器，消除手动超参数调优；2）具有自动确定中间模型的渐进蒸馏链；3）学习动态每样本权重的注意力加权多教师集成；4）适应整个训练过程的元学习温度调度器；5）具有智能负载平衡的并行处理流水线；6）用于跨实验重用的共享优化内存。

Result: 在CIFAR-10、CIFAR-100和表格数据集上的实验表明：HPM-KD实现了10-15倍压缩同时保持85%的精度保留；消除了手动调优需求；通过并行化减少了30-40%的训练时间。消融研究确认了每个组件的独立贡献（0.10-0.98个百分点）。

Conclusion: HPM-KD成功解决了传统知识蒸馏的关键限制，提供了一个高效、自动化的模型压缩框架。该框架已作为开源DeepBridge库的一部分提供，为实际部署中的模型压缩提供了实用解决方案。

Abstract: Knowledge Distillation (KD) has emerged as a promising technique for model compression but faces critical limitations: (1) sensitivity to hyperparameters requiring extensive manual tuning, (2) capacity gap when distilling from very large teachers to small students, (3) suboptimal coordination in multi-teacher scenarios, and (4) inefficient use of computational resources. We present \textbf{HPM-KD}, a framework that integrates six synergistic components: (i) Adaptive Configuration Manager via meta-learning that eliminates manual hyperparameter tuning, (ii) Progressive Distillation Chain with automatically determined intermediate models, (iii) Attention-Weighted Multi-Teacher Ensemble that learns dynamic per-sample weights, (iv) Meta-Learned Temperature Scheduler that adapts temperature throughout training, (v) Parallel Processing Pipeline with intelligent load balancing, and (vi) Shared Optimization Memory for cross-experiment reuse. Experiments on CIFAR-10, CIFAR-100, and tabular datasets demonstrate that HPM-KD: achieves 10x-15x compression while maintaining 85% accuracy retention, eliminates the need for manual tuning, and reduces training time by 30-40% via parallelization. Ablation studies confirm independent contribution of each component (0.10-0.98 pp). HPM-KD is available as part of the open-source DeepBridge library.

</details>


### [165] [Analysis of Dirichlet Energies as Over-smoothing Measures](https://arxiv.org/abs/2512.09890)
*Anna Bison,Alessandro Sperduti*

Main category: cs.LG

TL;DR: 该论文分析了非归一化图拉普拉斯和归一化图拉普拉斯诱导的Dirichlet能量之间的区别，指出后者不满足节点相似性度量的公理化定义，并强调选择与GNN架构谱兼容的度量标准的重要性。


<details>
  <summary>Details</summary>
Motivation: 论文旨在澄清两种常用于过平滑度量的函数之间的区别：非归一化图拉普拉斯和归一化图拉普拉斯诱导的Dirichlet能量。当前存在选择哪种度量的模糊性，需要明确哪种度量在谱特性上与GNN架构兼容。

Method: 通过形式化分析两种定义的谱特性，对比非归一化图拉普拉斯和归一化图拉普拉斯诱导的Dirichlet能量。特别验证了归一化图拉普拉斯是否满足Rusch等人提出的节点相似性度量的公理化定义。

Result: 研究表明归一化图拉普拉斯诱导的Dirichlet能量不满足节点相似性度量的公理化定义。通过形式化两种定义的谱特性，揭示了选择与GNN架构谱兼容的度量标准的关键区别。

Conclusion: 选择正确的度量标准对于监控GNN动态至关重要。归一化图拉普拉斯诱导的Dirichlet能量不满足节点相似性度量的公理要求，因此需要根据GNN架构的谱特性选择相应的度量标准，以解决监控动态时的模糊性。

Abstract: We analyze the distinctions between two functionals often used as over-smoothing measures: the Dirichlet energies induced by the unnormalized graph Laplacian and the normalized graph Laplacian. We demonstrate that the latter fails to satisfy the axiomatic definition of a node-similarity measure proposed by Rusch \textit{et al.} By formalizing fundamental spectral properties of these two definitions, we highlight critical distinctions necessary to select the metric that is spectrally compatible with the GNN architecture, thereby resolving ambiguities in monitoring the dynamics.

</details>


### [166] [Provably Learning from Modern Language Models via Low Logit Rank](https://arxiv.org/abs/2512.09892)
*Noah Golowich,Allen Liu,Abhishek Shetty*

Main category: cs.LG

TL;DR: 论文研究如何利用语言模型低对数秩的结构特性，设计查询学习算法来学习近似低对数秩模型，为现代语言模型提供了首个端到端学习保证。


<details>
  <summary>Details</summary>
Motivation: 现代语言模型虽然复杂，但经验观察发现它们都具有近似低对数秩的特性。本文旨在探索如何利用这种结构特性，为语言模型提供可证明的学习保证，特别是考虑到低对数秩模型可以编码难以学习的分布（如噪声奇偶性）。

Method: 采用查询学习模型，使用对数查询来反映常见API的访问模式。提出了一种高效算法，可以从查询中学习任何近似低对数秩模型。

Result: 开发了一个高效算法，能够从查询中学习任何近似低对数秩模型。该算法为生成模型提供了首个端到端学习保证，且结构假设与经验观察到的现代语言模型行为紧密对应。

Conclusion: 本文通过利用语言模型近似低对数秩的结构特性，设计了一个高效的查询学习算法，为现代语言模型提供了首个端到端的学习理论保证，将经验观察与可证明的学习算法联系起来。

Abstract: While modern language models and their inner workings are incredibly complex, recent work (Golowich, Liu & Shetty; 2025) has proposed a simple and potentially tractable abstraction for them through the observation that empirically, these language models all seem to have approximately low logit rank. Roughly, this means that a matrix formed by the model's log probabilities of various tokens conditioned on certain sequences of tokens is well approximated by a low rank matrix.
  In this paper, our focus is on understanding how this structure can be exploited algorithmically for obtaining provable learning guarantees. Since low logit rank models can encode hard-to-learn distributions such as noisy parities, we study a query learning model with logit queries that reflects the access model for common APIs. Our main result is an efficient algorithm for learning any approximately low logit rank model from queries. We emphasize that our structural assumption closely reflects the behavior that is empirically observed in modern language models. Thus, our result gives what we believe is the first end-to-end learning guarantee for a generative model that plausibly captures modern language models.

</details>


### [167] [FALCON: Few-step Accurate Likelihoods for Continuous Flows](https://arxiv.org/abs/2512.09914)
*Danyal Rehman,Tara Akhound-Sadegh,Artem Gazizov,Yoshua Bengio,Alexander Tong*

Main category: cs.LG

TL;DR: FALCON方法通过混合训练目标实现可逆性，使连续流模型能够用极少步数采样并保持足够精确的似然计算，相比现有方法快两个数量级。


<details>
  <summary>Details</summary>
Motivation: 热力学平衡中分子状态的可扩展采样是统计物理中长期存在的挑战。现有Boltzmann生成器使用连续归一化流（CNFs）进行训练，但每个样本的似然计算需要数千次函数评估，成本极高，严重限制了其应用。

Method: 提出FALCON方法，通过引入混合训练目标来鼓励可逆性，使连续流模型能够用极少步数进行采样，同时保持足够精确的似然计算用于重要性采样应用。

Result: FALCON在分子Boltzmann采样方面优于最先进的归一化流模型，比同等性能的CNF模型快两个数量级。

Conclusion: FALCON方法解决了连续归一化流在Boltzmann采样中计算成本过高的问题，通过混合训练目标实现了高效且准确的采样，为分子状态采样提供了实用的解决方案。

Abstract: Scalable sampling of molecular states in thermodynamic equilibrium is a long-standing challenge in statistical physics. Boltzmann Generators tackle this problem by pairing a generative model, capable of exact likelihood computation, with importance sampling to obtain consistent samples under the target distribution. Current Boltzmann Generators primarily use continuous normalizing flows (CNFs) trained with flow matching for efficient training of powerful models. However, likelihood calculation for these models is extremely costly, requiring thousands of function evaluations per sample, severely limiting their adoption. In this work, we propose Few-step Accurate Likelihoods for Continuous Flows (FALCON), a method which allows for few-step sampling with a likelihood accurate enough for importance sampling applications by introducing a hybrid training objective that encourages invertibility. We show FALCON outperforms state-of-the-art normalizing flow models for molecular Boltzmann sampling and is two orders of magnitude faster than the equivalently performing CNF model.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [168] [ShelfAware: Real-Time Visual-Inertial Semantic Localization in Quasi-Static Environments with Low-Cost Sensors](https://arxiv.org/abs/2512.09065)
*Shivendra Agrawal,Jake Brawer,Ashutosh Naik,Alessandro Roncone,Bradley Hayes*

Main category: cs.RO

TL;DR: ShelfAware是一种基于语义粒子滤波的鲁棒全局定位方法，通过将场景语义作为统计证据而非固定地标来处理，在准静态室内环境中实现了96%的成功率。


<details>
  <summary>Details</summary>
Motivation: 许多室内工作空间是准静态的：全局布局稳定但局部语义持续变化，产生重复几何、动态杂乱和感知噪声，这些因素会破坏基于视觉的定位。需要一种能够处理语义变化和几何混淆的鲁棒定位方法。

Method: ShelfAware融合深度似然和类别中心的语义相似性，使用预计算的语义视点库在MCL内执行逆语义提议，实现快速、有针对性的假设生成。该方法仅需视觉传感器和VIO，在低成本硬件上运行。

Result: 在100个全局定位试验中，ShelfAware达到96%的成功率（对比MCL的22%和AMCL的10%），平均收敛时间为1.91秒，在所有条件下获得最低的平移RMSE，80%的测试序列中保持稳定跟踪，并在消费级笔记本电脑平台上实时运行。

Conclusion: 通过在类别级别建模语义分布并利用逆提议，ShelfAware解决了准静态领域中常见的几何混淆和语义漂移问题。该方法仅需视觉传感器，可作为无基础设施的移动机器人构建模块，支持创建为视障人士提供随时开始、共享控制的辅助导航设备。

Abstract: Many indoor workspaces are quasi-static: global layout is stable but local semantics change continually, producing repetitive geometry, dynamic clutter, and perceptual noise that defeat vision-based localization. We present ShelfAware, a semantic particle filter for robust global localization that treats scene semantics as statistical evidence over object categories rather than fixed landmarks. ShelfAware fuses a depth likelihood with a category-centric semantic similarity and uses a precomputed bank of semantic viewpoints to perform inverse semantic proposals inside MCL, yielding fast, targeted hypothesis generation on low-cost, vision-only hardware. Across 100 global-localization trials spanning four conditions (cart-mounted, wearable, dynamic obstacles, and sparse semantics) in a semantically dense, retail environment, ShelfAware achieves a 96% success rate (vs. 22% MCL and 10% AMCL) with a mean time-to-convergence of 1.91s, attains the lowest translational RMSE in all conditions, and maintains stable tracking in 80% of tested sequences, all while running in real time on a consumer laptop-class platform. By modeling semantics distributionally at the category level and leveraging inverse proposals, ShelfAware resolves geometric aliasing and semantic drift common to quasi-static domains. Because the method requires only vision sensors and VIO, it integrates as an infrastructure-free building block for mobile robots in warehouses, labs, and retail settings; as a representative application, it also supports the creation of assistive devices providing start-anytime, shared-control assistive navigation for people with visual impairments.

</details>


### [169] [Inferring Operator Emotions from a Motion-Controlled Robotic Arm](https://arxiv.org/abs/2512.09086)
*Xinyu Qi,Zeyu Deng,Shaun Alexander Macdonald,Liying Li,Chen Wang,Muhammad Ali Imran,Philip G. Zhao*

Main category: cs.RO

TL;DR: 通过分析远程控制机器人的功能性运动来推断操作者情感状态，即使机器人并非为情感表达设计，也能达到83.3%的识别准确率。


<details>
  <summary>Details</summary>
Motivation: 远程机器人操作者的情感状态会显著影响机器人运动，可能导致意外后果。当前情感识别方法依赖生理信号或身体语言，但这些方法需要额外设备且限制了远程控制场景。需要探索通过机器人运动本身来推断操作者情感状态的方法。

Method: 开发了一个机器学习系统，通过分析远程控制机器人的功能性运动（特别是手部运动产生的机器人动作）来推断人类操作者的情感状态。系统利用机器人运动数据而非操作者直接生理信号。

Result: 系统在识别用户通过机器人运动表达的情感状态方面达到了83.3%的准确率，表明即使不是为情感表达设计的机器人，其功能性运动也能有效反映操作者情感。

Conclusion: 通过机器人运动推断操作者情感状态是可行的，这对当前和未来的远程机器人操作及情感机器人领域具有重要意义，为无侵入式情感识别提供了新途径。

Abstract: A remote robot operator's affective state can significantly impact the resulting robot's motions leading to unexpected consequences, even when the user follows protocol and performs permitted tasks. The recognition of a user operator's affective states in remote robot control scenarios is, however, underexplored. Current emotion recognition methods rely on reading the user's vital signs or body language, but the devices and user participation these measures require would add limitations to remote robot control. We demonstrate that the functional movements of a remote-controlled robotic avatar, which was not designed for emotional expression, can be used to infer the emotional state of the human operator via a machine-learning system. Specifically, our system achieved 83.3$\%$ accuracy in recognizing the user's emotional state expressed by robot movements, as a result of their hand motions. We discuss the implications of this system on prominent current and future remote robot operation and affective robotic contexts.

</details>


### [170] [Masked Generative Policy for Robotic Control](https://arxiv.org/abs/2512.09101)
*Lipeng Zhuang,Shiyu Fan,Florent P. Audonnet,Yingdong Ru,Gerardo Aragon Camarasa,Paul Henderson*

Main category: cs.RO

TL;DR: MGP是一个用于视觉运动模仿学习的新框架，通过将动作表示为离散标记，使用条件掩码变换器并行生成标记并快速细化低置信度标记，在机器人任务中实现了快速推理和高成功率。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉运动模仿学习方法在处理复杂和非马尔可夫任务时存在困难，需要一种既能实现全局一致预测又具有鲁棒自适应执行能力的框架。

Method: 提出MGP框架：1) 将动作表示为离散标记；2) 训练条件掩码变换器并行生成标记并快速细化低置信度标记；3) 提出两种采样范式：MGP-Short用于马尔可夫任务，进行并行掩码生成和基于分数的细化；MGP-Long用于非马尔可夫任务，单次预测完整轨迹并根据新观察动态细化低置信度动作标记。

Result: 在Meta-World和LIBERO基准的150个机器人操作任务上评估，MGP相比最先进的扩散和自回归策略：1) 平均成功率提高9%；2) 每序列推理时间减少高达35倍；3) 在动态和缺失观察环境中平均成功率提高60%；4) 解决了两个其他方法失败的非马尔可夫场景。

Conclusion: MGP框架通过全局一致预测和鲁棒自适应执行能力，在复杂和非马尔可夫任务中实现了可靠控制，显著优于现有方法，为视觉运动模仿学习提供了有效的解决方案。

Abstract: We present Masked Generative Policy (MGP), a novel framework for visuomotor imitation learning. We represent actions as discrete tokens, and train a conditional masked transformer that generates tokens in parallel and then rapidly refines only low-confidence tokens. We further propose two new sampling paradigms: MGP-Short, which performs parallel masked generation with score-based refinement for Markovian tasks, and MGP-Long, which predicts full trajectories in a single pass and dynamically refines low-confidence action tokens based on new observations. With globally coherent prediction and robust adaptive execution capabilities, MGP-Long enables reliable control on complex and non-Markovian tasks that prior methods struggle with. Extensive evaluations on 150 robotic manipulation tasks spanning the Meta-World and LIBERO benchmarks show that MGP achieves both rapid inference and superior success rates compared to state-of-the-art diffusion and autoregressive policies. Specifically, MGP increases the average success rate by 9% across 150 tasks while cutting per-sequence inference time by up to 35x. It further improves the average success rate by 60% in dynamic and missing-observation environments, and solves two non-Markovian scenarios where other state-of-the-art methods fail.

</details>


### [171] [Cognitive Trust in HRI: "Pay Attention to Me and I'll Trust You Even if You are Wrong"](https://arxiv.org/abs/2512.09105)
*Adi Manor,Dan Cohen,Ziv Keidar,Avi Parush,Hadas Erel*

Main category: cs.RO

TL;DR: 研究发现机器人的高度关注可以弥补其低能力表现，在建立认知信任中形成情感补偿机制


<details>
  <summary>Details</summary>
Motivation: 认知信任是高质量人机交互的核心因素，传统研究主要关注机器人能力和可靠性等性能因素，但最近研究表明情感因素如机器人关注度也影响认知信任。本研究旨在探索这两种因素如何相互作用，特别是是否存在补偿机制。

Method: 采用2x2实验设计，包含两个因素：能力（高或低）和关注度（高或低）。参与者与机器狗合作完成搜索任务，评估不同组合下认知信任的变化。

Result: 结果显示高度关注可以补偿低能力表现。与高度关注但表现不佳的机器人合作的参与者报告了与高度能力机器人相当的信任水平。当机器人不展示关注度时，低能力导致认知信任显著下降。

Conclusion: 人机交互中的认知信任建立比先前认为的更复杂，涉及通常被忽视的情感过程。研究揭示了一种情感补偿机制，为传统基于能力的认知信任模型增加了新的考量维度。

Abstract: Cognitive trust and the belief that a robot is capable of accurately performing tasks, are recognized as central factors in fostering high-quality human-robot interactions. It is well established that performance factors such as the robot's competence and its reliability shape cognitive trust. Recent studies suggest that affective factors, such as robotic attentiveness, also play a role in building cognitive trust. This work explores the interplay between these two factors that shape cognitive trust. Specifically, we evaluated whether different combinations of robotic competence and attentiveness introduce a compensatory mechanism, where one factor compensates for the lack of the other. In the experiment, participants performed a search task with a robotic dog in a 2x2 experimental design that included two factors: competence (high or low) and attentiveness (high or low). The results revealed that high attentiveness can compensate for low competence. Participants who collaborated with a highly attentive robot that performed poorly reported trust levels comparable to those working with a highly competent robot. When the robot did not demonstrate attentiveness, low competence resulted in a substantial decrease in cognitive trust. The findings indicate that building cognitive trust in human-robot interaction may be more complex than previously believed, involving emotional processes that are typically overlooked. We highlight an affective compensatory mechanism that adds a layer to consider alongside traditional competence-based models of cognitive trust.

</details>


### [172] [Semantic Trajectory Generation for Goal-Oriented Spacecraft Rendezvous](https://arxiv.org/abs/2512.09111)
*Yuji Takubo,Arpit Dwivedi,Sukeerth Ramkumar,Luis A. Pabon,Daniele Gammelli,Marco Pavone,Simone D'Amico*

Main category: cs.RO

TL;DR: SAGES框架将自然语言指令转换为满足非凸约束的航天器轨迹，实现语义自主制导，减少专家负担


<details>
  <summary>Details</summary>
Motivation: 现有非凸制导方法依赖大量专家输入（如航点、约束、任务时间线等），限制了实际交会任务中的操作可扩展性，需要更直观的交互方式

Method: 提出SAGES（语义自主制导引擎）框架，将自然语言命令转换为反映高层意图的航天器轨迹，同时尊重非凸约束

Result: 在两种设置下（具有连续时间约束执行的容错接近操作和自由飞行机器人平台）的实验表明，SAGES可靠地产生与人类命令一致的轨迹，在不同行为模式下实现超过90%的语义行为一致性

Conclusion: 这项工作是迈向语言条件、约束感知的航天器轨迹生成的初步步骤，使操作员能够通过直观的自然语言命令交互式指导安全性和行为，减少专家负担

Abstract: Reliable real-time trajectory generation is essential for future autonomous spacecraft. While recent progress in nonconvex guidance and control is paving the way for onboard autonomous trajectory optimization, these methods still rely on extensive expert input (e.g., waypoints, constraints, mission timelines, etc.), which limits the operational scalability in real rendezvous missions.This paper introduces SAGES (Semantic Autonomous Guidance Engine for Space), a trajectory-generation framework that translates natural-language commands into spacecraft trajectories that reflect high-level intent while respecting nonconvex constraints. Experiments in two settings -- fault-tolerant proximity operations with continuous-time constraint enforcement and a free-flying robotic platform -- demonstrate that SAGES reliably produces trajectories aligned with human commands, achieving over 90\% semantic-behavioral consistency across diverse behavior modes. Ultimately, this work marks an initial step toward language-conditioned, constraint-aware spacecraft trajectory generation, enabling operators to interactively guide both safety and behavior through intuitive natural-language commands with reduced expert burden.

</details>


### [173] [UPETrack: Unidirectional Position Estimation for Tracking Occluded Deformable Linear Objects](https://arxiv.org/abs/2512.09283)
*Fan Wu,Chenguang Yang,Haibin Yang,Shuo Wang,Yanrui Xu,Xing Zhou,Meng Gao,Yaoqi Xian,Zhihong Zhu,Shifeng Huang*

Main category: cs.RO

TL;DR: UPETrack是一个基于单向位置估计的几何驱动框架，用于实时跟踪可变形线性物体的状态，无需物理建模、虚拟仿真或视觉标记，在定位精度和计算效率上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 可变形线性物体的实时状态跟踪对于工业装配、医疗程序和日常应用中的机器人操作至关重要，但高维配置空间、非线性动力学和频繁部分遮挡构成了实时跟踪的基本障碍。

Method: UPETrack框架分为两个阶段：(1) 基于高斯混合模型和期望最大化算法的可见段跟踪；(2) 使用提出的单向位置估计算法进行遮挡区域预测。UPE算法利用DLO形状的几何连续性和时间演化模式，通过局部线性组合位移项、近端线性约束项和历史曲率项三个机制推导出闭式位置估计器。

Result: 实验结果表明，UPETrack在定位精度和计算效率上都超越了两种最先进的跟踪算法（TrackDLO和CDCPD2）。

Conclusion: UPETrack通过几何驱动的方法解决了DLO实时跟踪的挑战，无需物理建模或迭代优化，提供了高效稳定的遮挡节点估计，在机器人操作应用中具有实用价值。

Abstract: Real-time state tracking of Deformable Linear Objects (DLOs) is critical for enabling robotic manipulation of DLOs in industrial assembly, medical procedures, and daily-life applications. However, the high-dimensional configuration space, nonlinear dynamics, and frequent partial occlusions present fundamental barriers to robust real-time DLO tracking. To address these limitations, this study introduces UPETrack, a geometry-driven framework based on Unidirectional Position Estimation (UPE), which facilitates tracking without the requirement for physical modeling, virtual simulation, or visual markers. The framework operates in two phases: (1) visible segment tracking is based on a Gaussian Mixture Model (GMM) fitted via the Expectation Maximization (EM) algorithm, and (2) occlusion region prediction employing UPE algorithm we proposed. UPE leverages the geometric continuity inherent in DLO shapes and their temporal evolution patterns to derive a closed-form positional estimator through three principal mechanisms: (i) local linear combination displacement term, (ii) proximal linear constraint term, and (iii) historical curvature term. This analytical formulation allows efficient and stable estimation of occluded nodes through explicit linear combinations of geometric components, eliminating the need for additional iterative optimization. Experimental results demonstrate that UPETrack surpasses two state-of-the-art tracking algorithms, including TrackDLO and CDCPD2, in both positioning accuracy and computational efficiency.

</details>


### [174] [One-Shot Real-World Demonstration Synthesis for Scalable Bimanual Manipulation](https://arxiv.org/abs/2512.09297)
*Huayi Zhou,Kui Jia*

Main category: cs.RO

TL;DR: BiDemoSyn框架通过单次真实演示合成数千个接触丰富的双手操作演示，无需重复遥操作或依赖不完美的仿真，解决了大规模高质量双手操作演示数据获取的难题。


<details>
  <summary>Details</summary>
Motivation: 当前双手操作策略学习面临数据获取的困境：遥操作提供物理真实数据但劳动密集，仿真合成可扩展但存在仿真到现实的差距。需要一种既能保持物理真实性又能高效扩展的方法。

Method: 将任务分解为不变的协调模块和可变的物体相关调整，通过视觉引导对齐和轻量级轨迹优化进行适配，从单次真实演示合成数千个多样且物理可行的演示。

Result: 在六个双臂任务中，使用BiDemoSyn数据训练的策略对新物体姿态和形状具有鲁棒泛化能力，显著优于现有基线方法。

Conclusion: BiDemoSyn在效率和真实世界保真度之间架起桥梁，为复杂双手操作提供了不牺牲物理真实性的可扩展模仿学习路径。

Abstract: Learning dexterous bimanual manipulation policies critically depends on large-scale, high-quality demonstrations, yet current paradigms face inherent trade-offs: teleoperation provides physically grounded data but is prohibitively labor-intensive, while simulation-based synthesis scales efficiently but suffers from sim-to-real gaps. We present BiDemoSyn, a framework that synthesizes contact-rich, physically feasible bimanual demonstrations from a single real-world example. The key idea is to decompose tasks into invariant coordination blocks and variable, object-dependent adjustments, then adapt them through vision-guided alignment and lightweight trajectory optimization. This enables the generation of thousands of diverse and feasible demonstrations within several hour, without repeated teleoperation or reliance on imperfect simulation. Across six dual-arm tasks, we show that policies trained on BiDemoSyn data generalize robustly to novel object poses and shapes, significantly outperforming recent baselines. By bridging the gap between efficiency and real-world fidelity, BiDemoSyn provides a scalable path toward practical imitation learning for complex bimanual manipulation without compromising physical grounding.

</details>


### [175] [Scene-agnostic Hierarchical Bimanual Task Planning via Visual Affordance Reasoning](https://arxiv.org/abs/2512.09310)
*Kwang Bin Lee,Jiho Kang,Sung-Hee Lee*

Main category: cs.RO

TL;DR: 提出一个场景无关的双手机器人任务规划统一框架，将高层语义推理与3D基础的双手执行相结合，解决现有单手机器人规划器在空间、几何和协调方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有机器人任务规划器主要是单手的，无法解决场景无关设置中双手机器人操作固有的空间、几何和协调挑战，而开放环境中的具身智能体需要将高层指令转化为可执行的双手协调行为。

Method: 集成三个关键模块：视觉点定位(VPG)分析单张场景图像检测相关对象并生成世界对齐的交互点；双手机器人子目标规划器(BSP)基于空间邻接和跨对象可达性推理，生成紧凑、运动中立化的子目标；交互点驱动的双手机器人提示(IPBP)将这些子目标绑定到结构化技能库，实例化满足手部状态和可供性约束的同步单手或双手动作序列。

Result: 实验表明，该框架能产生连贯、可行且紧凑的双手规划，无需重新训练即可泛化到杂乱场景，展示了双手机器人任务的鲁棒场景无关可供性推理能力。

Conclusion: 该统一框架使智能体能够在杂乱、未见过的场景中规划语义上有意义、物理上可行且可并行的双手行为，成功桥接了高层推理与3D基础的双手执行。

Abstract: Embodied agents operating in open environments must translate high-level instructions into grounded, executable behaviors, often requiring coordinated use of both hands. While recent foundation models offer strong semantic reasoning, existing robotic task planners remain predominantly unimanual and fail to address the spatial, geometric, and coordination challenges inherent to bimanual manipulation in scene-agnostic settings. We present a unified framework for scene-agnostic bimanual task planning that bridges high-level reasoning with 3D-grounded two-handed execution. Our approach integrates three key modules. Visual Point Grounding (VPG) analyzes a single scene image to detect relevant objects and generate world-aligned interaction points. Bimanual Subgoal Planner (BSP) reasons over spatial adjacency and cross-object accessibility to produce compact, motion-neutralized subgoals that exploit opportunities for coordinated two-handed actions. Interaction-Point-Driven Bimanual Prompting (IPBP) binds these subgoals to a structured skill library, instantiating synchronized unimanual or bimanual action sequences that satisfy hand-state and affordance constraints. Together, these modules enable agents to plan semantically meaningful, physically feasible, and parallelizable two-handed behaviors in cluttered, previously unseen scenes. Experiments show that it produces coherent, feasible, and compact two-handed plans, and generalizes to cluttered scenes without retraining, demonstrating robust scene-agnostic affordance reasoning for bimanual tasks.

</details>


### [176] [COVLM-RL: Critical Object-Oriented Reasoning for Autonomous Driving Using VLM-Guided Reinforcement Learning](https://arxiv.org/abs/2512.09349)
*Lin Li,Yuxin Cai,Jianwu Fang,Jianru Xue,Chen Lv*

Main category: cs.RO

TL;DR: COVLM-RL：结合关键对象导向推理与VLM引导强化学习的端到端自动驾驶框架，通过语义决策先验和一致性损失提升泛化能力、训练效率和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有端到端自动驾驶框架在泛化性、训练效率和可解释性方面存在不足。基于视觉语言模型的方法在新场景中缺乏鲁棒性，而强化学习方法虽然适应性较强但数据效率低且决策不透明。

Method: 提出COVLM-RL框架：1）设计链式思维提示策略，让VLM对关键交通元素进行推理并生成高层语义决策，将多视角视觉输入转化为结构化语义决策先验；2）引入一致性损失，促进VLM语义规划与RL智能体控制输出之间的对齐。

Result: 在CARLA模拟器中的实验表明，COVLM-RL在训练过的驾驶环境中将成功率提高了30%，在未见环境中提高了50%，显示出强大的泛化能力。

Conclusion: COVLM-RL通过整合VLM的语义推理能力和RL的适应性，有效解决了自动驾驶框架在泛化性、训练效率和可解释性方面的挑战，为端到端自动驾驶提供了新的解决方案。

Abstract: End-to-end autonomous driving frameworks face persistent challenges in generalization, training efficiency, and interpretability. While recent methods leverage Vision-Language Models (VLMs) through supervised learning on large-scale datasets to improve reasoning, they often lack robustness in novel scenarios. Conversely, reinforcement learning (RL)-based approaches enhance adaptability but remain data-inefficient and lack transparent decision-making. % contribution To address these limitations, we propose COVLM-RL, a novel end-to-end driving framework that integrates Critical Object-oriented (CO) reasoning with VLM-guided RL. Specifically, we design a Chain-of-Thought (CoT) prompting strategy that enables the VLM to reason over critical traffic elements and generate high-level semantic decisions, effectively transforming multi-view visual inputs into structured semantic decision priors. These priors reduce the input dimensionality and inject task-relevant knowledge into the RL loop, accelerating training and improving policy interpretability. However, bridging high-level semantic guidance with continuous low-level control remains non-trivial. To this end, we introduce a consistency loss that encourages alignment between the VLM's semantic plans and the RL agent's control outputs, enhancing interpretability and training stability. Experiments conducted in the CARLA simulator demonstrate that COVLM-RL significantly improves the success rate by 30\% in trained driving environments and by 50\% in previously unseen environments, highlighting its strong generalization capability.

</details>


### [177] [Observability Analysis and Composite Disturbance Filtering for a Bar Tethered to Dual UAVs Subject to Multi-source Disturbances](https://arxiv.org/abs/2512.09377)
*Lidan Xu,Dadong Fan,Junhong Wang,Wenshuo Li,Hao Lu,Jianzhong Qiao*

Main category: cs.RO

TL;DR: 该研究证明了在只有两种或更少类型集总扰动的情况下，仅使用无人机里程计信息即可实现双无人机-杆系统的完全可观性，并开发了基于扰动观测器的误差状态扩展卡尔曼滤波器进行状态和扰动估计。


<details>
  <summary>Details</summary>
Motivation: 现有的协同悬挂空中运输系统对多源扰动高度敏感，现有方法通常依赖额外传感器测量缆绳方向或负载姿态，增加了系统成本和复杂性。核心问题是：仅使用无人机里程计信息，在多源扰动下负载姿态是否可观？

Method: 1. 针对双无人机-杆系统，使用可观性秩判据证明当只有两种或更少类型集总扰动时，整个系统是可观的；2. 针对扰动仅作用于无人机的情况，开发了复合扰动滤波方案，设计了基于扰动观测器的误差状态扩展卡尔曼滤波器，用于在流形$(mathbb{R}^3)^2times(TS^2)^3$上演化的整个系统的状态和扰动估计。

Result: 仿真和实验测试验证了仅使用无人机里程计信息即可完全估计系统的状态和扰动，为构建更经济、更鲁棒的系统铺平了道路。

Conclusion: 该研究首次证明了在有限扰动条件下，仅使用无人机里程计信息即可实现双无人机-杆系统的完全可观性，并通过开发的复合扰动滤波方案实现了有效的状态和扰动估计，为减少传感器依赖、降低成本和提高系统鲁棒性提供了理论基础和实践方案。

Abstract: Cooperative suspended aerial transportation is highly susceptible to multi-source disturbances such as aerodynamic effects and thrust uncertainties. To achieve precise load manipulation, existing methods often rely on extra sensors to measure cable directions or the payload's pose, which increases the system cost and complexity. A fundamental question remains: is the payload's pose observable under multi-source disturbances using only the drones' odometry information? To answer this question, this work focuses on the two-drone-bar system and proves that the whole system is observable when only two or fewer types of lumped disturbances exist by using the observability rank criterion. To the best of our knowledge, we are the first to present such a conclusion and this result paves the way for more cost-effective and robust systems by minimizing their sensor suites. Next, to validate this analysis, we consider the situation where the disturbances are only exerted on the drones, and develop a composite disturbance filtering scheme. A disturbance observer-based error-state extended Kalman filter is designed for both state and disturbance estimation, which renders improved estimation performance for the whole system evolving on the manifold $(\mathbb{R}^3)^2\times(TS^2)^3$. Our simulation and experimental tests have validated that it is possible to fully estimate the state and disturbance of the system with only odometry information of the drones.

</details>


### [178] [H2R-Grounder: A Paired-Data-Free Paradigm for Translating Human Interaction Videos into Physically Grounded Robot Videos](https://arxiv.org/abs/2512.09406)
*Hai Ci,Xiaokang Liu,Pei Yang,Yiren Song,Mike Zheng Shou*

Main category: cs.RO

TL;DR: 提出视频到视频翻译框架，将普通人机交互视频转换为运动一致、物理基础的机器人操作视频，无需配对训练数据


<details>
  <summary>Details</summary>
Motivation: 让机器人从日常人类视频中学习操作技能，避免繁琐的机器人数据收集，实现大规模能力获取

Method: 使用可转移表示弥合具身鸿沟：通过修复机器人手臂获得干净背景，叠加视觉提示（标记和箭头表示夹爪位置和方向），训练生成模型将机器人手臂插入场景；在测试时对人类视频应用相同处理，生成模仿人类动作的高质量机器人视频；采用上下文学习方式微调SOTA视频扩散模型确保时间一致性

Result: 实验结果表明，相比基线方法，该方法能生成更真实、更物理基础的机器人运动，为从无标签人类视频扩展机器人学习提供了有前景的方向

Conclusion: 提出的视频到视频翻译框架能够有效利用日常人类视频训练机器人操作技能，无需配对数据，为大规模机器人学习开辟了新途径

Abstract: Robots that learn manipulation skills from everyday human videos could acquire broad capabilities without tedious robot data collection. We propose a video-to-video translation framework that converts ordinary human-object interaction videos into motion-consistent robot manipulation videos with realistic, physically grounded interactions. Our approach does not require any paired human-robot videos for training only a set of unpaired robot videos, making the system easy to scale. We introduce a transferable representation that bridges the embodiment gap: by inpainting the robot arm in training videos to obtain a clean background and overlaying a simple visual cue (a marker and arrow indicating the gripper's position and orientation), we can condition a generative model to insert the robot arm back into the scene. At test time, we apply the same process to human videos (inpainting the person and overlaying human pose cues) and generate high-quality robot videos that mimic the human's actions. We fine-tune a SOTA video diffusion model (Wan 2.2) in an in-context learning manner to ensure temporal coherence and leveraging of its rich prior knowledge. Empirical results demonstrate that our approach achieves significantly more realistic and grounded robot motions compared to baselines, pointing to a promising direction for scaling up robot learning from unlabeled human videos. Project page: https://showlab.github.io/H2R-Grounder/

</details>


### [179] [Generalizable Collaborative Search-and-Capture in Cluttered Environments via Path-Guided MAPPO and Directional Frontier Allocation](https://arxiv.org/abs/2512.09410)
*Jialin Ying,Zhihao Li,Zicheng Dong,Guohua Wu,Yihuan Liao*

Main category: cs.RO

TL;DR: PGF-MAPPO：一种分层强化学习框架，结合拓扑规划和反应控制，解决多智能体追逃问题中的稀疏奖励和视野受限挑战，通过A*势场奖励塑形和方向性前沿分配实现高效探索。


<details>
  <summary>Details</summary>
Motivation: 在杂乱环境中进行协作追逃面临稀疏奖励和视野受限的挑战，标准多智能体强化学习方法存在探索效率低、难以扩展到大规模场景的问题。

Method: 提出PGF-MAPPO分层框架，结合拓扑规划和反应控制；使用A*势场进行密集奖励塑形解决局部最小值和稀疏奖励问题；引入方向性前沿分配方法，结合最远点采样和几何角度抑制实现空间分散；采用参数共享的分散式评论家，保持O(1)模型复杂度。

Result: PGF-MAPPO在对抗更快逃逸者时表现出优越的捕获效率；在10x10地图上训练的策略能够零样本泛化到未见过的20x20环境，显著优于基于规则和基于学习的基线方法。

Conclusion: PGF-MAPPO通过分层架构有效解决了多智能体追逃中的探索挑战，实现了高效的空间覆盖和强大的泛化能力，适合机器人群体应用。

Abstract: Collaborative pursuit-evasion in cluttered environments presents significant challenges due to sparse rewards and constrained Fields of View (FOV). Standard Multi-Agent Reinforcement Learning (MARL) often suffers from inefficient exploration and fails to scale to large scenarios. We propose PGF-MAPPO (Path-Guided Frontier MAPPO), a hierarchical framework bridging topological planning with reactive control. To resolve local minima and sparse rewards, we integrate an A*-based potential field for dense reward shaping. Furthermore, we introduce Directional Frontier Allocation, combining Farthest Point Sampling (FPS) with geometric angle suppression to enforce spatial dispersion and accelerate coverage. The architecture employs a parameter-shared decentralized critic, maintaining O(1) model complexity suitable for robotic swarms. Experiments demonstrate that PGF-MAPPO achieves superior capture efficiency against faster evaders. Policies trained on 10x10 maps exhibit robust zero-shot generalization to unseen 20x20 environments, significantly outperforming rule-based and learning-based baselines.

</details>


### [180] [D$^2$GSLAM: 4D Dynamic Gaussian Splatting SLAM](https://arxiv.org/abs/2512.09411)
*Siting Zhu,Yuxiang Huang,Wenhua Wu,Chaokang Jiang,Yongbo Chen,I-Ming Chen,Hesheng Wang*

Main category: cs.RO

TL;DR: D$^2$GSLAM是一个基于高斯表示的动态SLAM系统，能够在动态环境中同时进行准确的动态重建和鲁棒跟踪，解决了传统方法忽略动态物体运动信息的问题。


<details>
  <summary>Details</summary>
Motivation: 当前密集SLAM在动态环境中面临挑战，大多数方法直接移除动态物体，仅关注静态场景重建，忽略了动态物体中包含的运动信息。需要一种能够同时处理动态重建和跟踪的系统。

Method: 系统包含四个关键组件：1) 几何提示动态分离方法，利用高斯表示的几何一致性和场景几何获得粗动态区域，并作为提示指导精细运动掩码；2) 动态-静态复合表示，集成静态3D高斯和动态4D高斯，建模物体在静态和动态状态间的转换；3) 渐进姿态细化策略，利用静态场景几何的多视角一致性和动态物体的运动信息进行相机跟踪；4) 运动一致性损失，利用物体运动的时序连续性进行准确动态建模。

Result: D$^2$GSLAM在动态场景中展现出优越的建图和跟踪精度，同时表现出准确的动态建模能力。

Conclusion: 该论文提出的D$^2$GSLAM系统成功解决了动态环境中的SLAM挑战，通过创新的动态分离、复合表示、姿态细化和运动一致性方法，实现了同时进行动态重建和鲁棒跟踪的目标。

Abstract: Recent advances in Dense Simultaneous Localization and Mapping (SLAM) have demonstrated remarkable performance in static environments. However, dense SLAM in dynamic environments remains challenging. Most methods directly remove dynamic objects and focus solely on static scene reconstruction, which ignores the motion information contained in these dynamic objects. In this paper, we present D$^2$GSLAM, a novel dynamic SLAM system utilizing Gaussian representation, which simultaneously performs accurate dynamic reconstruction and robust tracking within dynamic environments. Our system is composed of four key components: (i) We propose a geometric-prompt dynamic separation method to distinguish between static and dynamic elements of the scene. This approach leverages the geometric consistency of Gaussian representation and scene geometry to obtain coarse dynamic regions. The regions then serve as prompts to guide the refinement of the coarse mask for achieving accurate motion mask. (ii) To facilitate accurate and efficient mapping of the dynamic scene, we introduce dynamic-static composite representation that integrates static 3D Gaussians with dynamic 4D Gaussians. This representation allows for modeling the transitions between static and dynamic states of objects in the scene for composite mapping and optimization. (iii) We employ a progressive pose refinement strategy that leverages both the multi-view consistency of static scene geometry and motion information from dynamic objects to achieve accurate camera tracking. (iv) We introduce a motion consistency loss, which leverages the temporal continuity in object motions for accurate dynamic modeling. Our D$^2$GSLAM demonstrates superior performance on dynamic scenes in terms of mapping and tracking accuracy, while also showing capability in accurate dynamic modeling.

</details>


### [181] [A Hierarchical, Model-Based System for High-Performance Humanoid Soccer](https://arxiv.org/abs/2512.09431)
*Quanyou Wang,Mingzhang Zhu,Ruochen Hou,Kay Gillespie,Alvin Zhu,Shiqi Wang,Yicheng Wang,Gaberiel I. Fernandez,Yeting Liu,Colin Togashi,Hyunwoo Nam,Aditya Navghare,Alex Xu,Taoyuanmin Zhu,Min Sung Ahn,Arturo Flores Alvarez,Justin Quan,Ethan Hong,Dennis W. Hong*

Main category: cs.RO

TL;DR: 本文介绍了赢得RoboCup 2024成人尺寸人形机器人足球赛冠军的ARTEMIS机器人系统，包括其硬件创新（轻量化结构、高扭矩准直驱执行器、专用足部设计）和软件创新（集成感知定位、导航规划、行为管理），实现了快速、精确、战术有效的比赛表现。


<details>
  <summary>Details</summary>
Motivation: 随着驱动、传感和控制技术的进步，运动型人形机器人的发展受到广泛关注。RoboCup作为全自主人形机器人的国际竞赛，为这类系统提供了独特的挑战性基准，其长期目标是到205年能与人类足球运动员比赛。本文旨在展示团队在RoboCup 2024成人尺寸人形足球赛中获胜所依赖的硬件和软件创新。

Method: 硬件方面：开发了成人尺寸人形平台，采用轻量化结构组件、高扭矩准直驱执行器，以及专用足部设计，既能实现强力步态内踢球，又能保持运动鲁棒性。软件方面：开发了集成感知和定位框架，结合立体视觉、目标检测和基于地标的融合，提供球、球门、队友和对手的可靠估计。中层导航栈生成碰撞感知、动态可行的轨迹，而集中式行为管理器基于不断变化的比赛状态协调高层决策、角色选择和踢球执行。

Result: 这些子系统的无缝集成实现了快速、精确和战术有效的比赛表现，能够在真实比赛的动态和对抗性条件下实现鲁棒性能。ARTEMIS成功赢得了2024年成人尺寸人形足球赛冠军。

Conclusion: 本文展示了ARTEMIS机器人系统的设计原则、系统架构和实验结果，这些因素共同促成了其在RoboCup 2024成人尺寸人形足球赛中的胜利。该系统代表了运动型人形机器人技术的重要进展，为实现与人类足球运动员竞争的长远目标做出了贡献。

Abstract: The development of athletic humanoid robots has gained significant attention as advances in actuation, sensing, and control enable increasingly dynamic, real-world capabilities. RoboCup, an international competition of fully autonomous humanoid robots, provides a uniquely challenging benchmark for such systems, culminating in the long-term goal of competing against human soccer players by 2050. This paper presents the hardware and software innovations underlying our team's victory in the RoboCup 2024 Adult-Sized Humanoid Soccer Competition. On the hardware side, we introduce an adult-sized humanoid platform built with lightweight structural components, high-torque quasi-direct-drive actuators, and a specialized foot design that enables powerful in-gait kicks while preserving locomotion robustness. On the software side, we develop an integrated perception and localization framework that combines stereo vision, object detection, and landmark-based fusion to provide reliable estimates of the ball, goals, teammates, and opponents. A mid-level navigation stack then generates collision-aware, dynamically feasible trajectories, while a centralized behavior manager coordinates high-level decision making, role selection, and kick execution based on the evolving game state. The seamless integration of these subsystems results in fast, precise, and tactically effective gameplay, enabling robust performance under the dynamic and adversarial conditions of real matches. This paper presents the design principles, system architecture, and experimental results that contributed to ARTEMIS's success as the 2024 Adult-Sized Humanoid Soccer champion.

</details>


### [182] [Sequential Testing for Descriptor-Agnostic LiDAR Loop Closure in Repetitive Environments](https://arxiv.org/abs/2512.09447)
*Jaehyun Kim,Seungwon Choi,Tae-Wan Kim*

Main category: cs.RO

TL;DR: 提出一种描述符无关的多帧闭环验证方法，将激光雷达闭环建模为截断序贯概率比检验，通过累积短时间流中的描述符相似性证据，根据预设错误率目标自适应做出接受/拒绝决策，旨在抑制室内重复结构环境中的误报。


<details>
  <summary>Details</summary>
Motivation: 在结构重复的室内环境中，传统的单帧描述符比较或固定阈值结合ICP验证的方法容易产生误报，需要一种更可靠的闭环验证机制来抑制假阳性。

Method: 将激光雷达闭环验证建模为截断序贯概率比检验，累积查询帧与候选帧之间短时间流的描述符相似性证据，根据用户指定的I/II类错误率目标自适应做出决策，采用精度优先策略。

Result: 在五个序列的图书馆数据集上评估，使用固定检索前端和多个代表性激光雷达全局描述符。序贯验证器相比单帧和启发式多帧基线，在所有描述符上都持续提高了精度，减少了混淆闭环的影响。

Conclusion: 提出的多帧序贯验证方法能够有效抑制室内重复结构环境中的误报，相比传统方法在精度和轨迹误差方面都有显著改善，为激光雷达闭环验证提供了更可靠的解决方案。

Abstract: We propose a descriptor-agnostic, multi-frame loop closure verification method that formulates LiDAR loop closure as a truncated Sequential Probability Ratio Test (SPRT). Instead of deciding from a single descriptor comparison or using fixed thresholds with late-stage Iterative Closest Point (ICP) vetting, the verifier accumulates a short temporal stream of descriptor similarities between a query and each candidate. It then issues an accept/reject decision adaptively once sufficient multi-frame evidence has been observed, according to user-specified Type-I/II error design targets. This precision-first policy is designed to suppress false positives in structurally repetitive indoor environments. We evaluate the verifier on a five-sequence library dataset, using a fixed retrieval front-end with several representative LiDAR global descriptors. Performance is assessed via segment-level K-hit precision-recall and absolute trajectory error (ATE) and relative pose error (RPE) after pose graph optimization. Across descriptors, the sequential verifier consistently improves precision and reduces the impact of aliased loops compared with single-frame and heuristic multi-frame baselines. Our implementation and dataset will be released at: https://github.com/wanderingcar/snu_library_dataset.

</details>


### [183] [ViTA-Seg: Vision Transformer for Amodal Segmentation in Robotics](https://arxiv.org/abs/2512.09510)
*Donato Caramia,Florian T. Pokorny,Giuseppe Triggiani,Denis Ruffino,David Naso,Paolo Roberto Massenio*

Main category: cs.RO

TL;DR: ViTA-Seg是一个基于Vision Transformer的类无关实时amodal分割框架，用于机器人箱体拾取中恢复完整物体掩码（包括隐藏区域），提出了单头和双头架构，并创建了工业箱体拾取合成数据集ViTA-SimData。


<details>
  <summary>Details</summary>
Motivation: 机器人箱体拾取中的遮挡问题会影响准确可靠的抓取规划，需要恢复完整物体掩码（包括被遮挡区域）以实现稳健的机器人操作。

Method: 提出了ViTA-Seg框架，利用Vision Transformer的全局注意力机制恢复完整物体掩码，包括两种架构：单头用于amodal掩码预测，双头用于amodal和遮挡掩码预测。同时创建了ViTA-SimData合成数据集专门针对工业箱体拾取场景。

Result: 在COOCA和KINS两个amodal基准测试上的广泛实验表明，ViTA-Seg双头架构在保持计算效率的同时，实现了强大的amodal和遮挡分割精度，支持实时机器人操作。

Conclusion: ViTA-Seg框架通过Vision Transformer的全局注意力机制有效解决了机器人箱体拾取中的遮挡问题，实现了实时amodal分割，为稳健的机器人操作提供了可行方案。

Abstract: Occlusions in robotic bin picking compromise accurate and reliable grasp planning. We present ViTA-Seg, a class-agnostic Vision Transformer framework for real-time amodal segmentation that leverages global attention to recover complete object masks, including hidden regions. We proposte two architectures: a) Single-Head for amodal mask prediction; b) Dual-Head for amodal and occluded mask prediction. We also introduce ViTA-SimData, a photo-realistic synthetic dataset tailored to industrial bin-picking scenario. Extensive experiments on two amodal benchmarks, COOCA and KINS, demonstrate that ViTA-Seg Dual Head achieves strong amodal and occlusion segmentation accuracy with computational efficiency, enabling robust, real-time robotic manipulation.

</details>


### [184] [Altruistic Maneuver Planning for Cooperative Autonomous Vehicles Using Multi-agent Advantage Actor-Critic](https://arxiv.org/abs/2107.05664)
*Behrad Toghi,Rodolfo Valiente,Dorsa Sadigh,Ramtin Pedarsani,Yaser P. Fallah*

Main category: cs.RO

TL;DR: 该论文研究混合自动驾驶环境中，自动驾驶车辆如何通过去中心化奖励结构学习利他行为，与人类驾驶车辆协调共存，无需依赖人类行为模型。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶车辆上路，将出现自动驾驶与人类驾驶车辆共存的混合环境。为实现社会期望的行为，自动驾驶车辆需要在决策过程中考虑周围其他车辆的效用，特别是如何通过奖励结构激励利他行为。

Method: 采用端到端方法，让自动驾驶智能体仅从经验中隐式学习人类驾驶员的决策过程。引入同步优势演员-评论家（A2C）算法的多智能体变体，训练能够相互协调并影响人类驾驶员行为的智能体。

Result: 通过去中心化奖励结构，自动驾驶车辆能够学习利他行为，协调其他自动驾驶和人类驾驶车辆，改善交通流量和安全性。

Conclusion: 该方法能够使自动驾驶车辆在无需依赖人类行为模型的情况下，通过隐式学习人类决策过程，实现与人类驾驶车辆的协调共存，提升混合自动驾驶环境的整体交通效率和安全。

Abstract: With the adoption of autonomous vehicles on our roads, we will witness a mixed-autonomy environment where autonomous and human-driven vehicles must learn to co-exist by sharing the same road infrastructure. To attain socially-desirable behaviors, autonomous vehicles must be instructed to consider the utility of other vehicles around them in their decision-making process. Particularly, we study the maneuver planning problem for autonomous vehicles and investigate how a decentralized reward structure can induce altruism in their behavior and incentivize them to account for the interest of other autonomous and human-driven vehicles. This is a challenging problem due to the ambiguity of a human driver's willingness to cooperate with an autonomous vehicle. Thus, in contrast with the existing works which rely on behavior models of human drivers, we take an end-to-end approach and let the autonomous agents to implicitly learn the decision-making process of human drivers only from experience. We introduce a multi-agent variant of the synchronous Advantage Actor-Critic (A2C) algorithm and train agents that coordinate with each other and can affect the behavior of human drivers to improve traffic flow and safety.

</details>


### [185] [Mastering Diverse, Unknown, and Cluttered Tracks for Robust Vision-Based Drone Racing](https://arxiv.org/abs/2512.09571)
*Feng Yu,Yu Hu,Yang Su,Yang Deng,Linzuo Zhang,Danping Zou*

Main category: cs.RO

TL;DR: 提出一个两阶段学习框架用于无人机竞速，解决在未知杂乱环境中平衡竞速速度与避障的挑战，通过软碰撞训练和硬碰撞精炼实现高速飞行与鲁棒避障的平衡。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的无人机竞速方法主要针对固定、无障碍的赛道，难以泛化到未知、杂乱的复杂环境。主要挑战包括：平衡竞速速度与碰撞避免的矛盾；可行空间有限导致策略探索陷入局部最优；深度图中门框与障碍物的感知模糊性（特别是当门框位置仅粗略指定时）。

Method: 提出两阶段学习框架：1) 初始软碰撞训练阶段，保持策略探索以实现高速飞行；2) 硬碰撞精炼阶段，强制鲁棒障碍物避免。采用自适应噪声增强课程学习与非对称演员-评论家架构，逐步将策略依赖从特权门框状态信息转移到基于深度的视觉输入。施加Lipschitz约束并集成赛道基元生成器以增强运动稳定性和跨环境泛化能力。

Result: 通过广泛的仿真和消融研究进行评估，并在计算受限的四旋翼无人机上进行真实世界实验验证。系统实现了敏捷飞行，同时对门框位置误差保持鲁棒性，开发了一个可泛化的无人机竞速框架，能够在多样化、部分未知和杂乱的环境中运行。

Conclusion: 该研究提出了一个有效的两阶段学习框架，成功解决了无人机在未知杂乱环境中竞速的挑战，实现了高速飞行与鲁棒避障的平衡，并展示了良好的跨环境泛化能力，为无人机竞速在复杂现实环境中的应用提供了可行方案。

Abstract: Most reinforcement learning(RL)-based methods for drone racing target fixed, obstacle-free tracks, leaving the generalization to unknown, cluttered environments largely unaddressed. This challenge stems from the need to balance racing speed and collision avoidance, limited feasible space causing policy exploration trapped in local optima during training, and perceptual ambiguity between gates and obstacles in depth maps-especially when gate positions are only coarsely specified. To overcome these issues, we propose a two-phase learning framework: an initial soft-collision training phase that preserves policy exploration for high-speed flight, followed by a hard-collision refinement phase that enforces robust obstacle avoidance. An adaptive, noise-augmented curriculum with an asymmetric actor-critic architecture gradually shifts the policy's reliance from privileged gate-state information to depth-based visual input. We further impose Lipschitz constraints and integrate a track-primitive generator to enhance motion stability and cross-environment generalization. We evaluate our framework through extensive simulation and ablation studies, and validate it in real-world experiments on a computationally constrained quadrotor. The system achieves agile flight while remaining robust to gate-position errors, developing a generalizable drone racing framework with the capability to operate in diverse, partially unknown and cluttered environments. https://yufengsjtu.github.io/MasterRacing.github.io/

</details>


### [186] [GLaD: Geometric Latent Distillation for Vision-Language-Action Models](https://arxiv.org/abs/2512.09619)
*Minghao Guo,Meng Cao,Jiachen Tao,Rongtao Xu,Yan Yan,Xiaodan Liang,Ivan Laptev,Xiaojun Chang*

Main category: cs.RO

TL;DR: GLaD是一个几何感知的视觉-语言-动作模型，通过知识蒸馏将3D几何先验整合到预训练中，在机器人任务中超越现有方法


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型主要依赖RGB信息，忽略了空间推理和操作中至关重要的几何线索

Method: 提出GLaD框架，通过知识蒸馏将几何先验整合到预训练中：将LLM的视觉标记隐藏状态与冻结的几何感知视觉变换器特征对齐，而不是仅将几何特征蒸馏到视觉编码器中

Result: 在Bridge数据集上预训练后，GLaD在四个LIBERO任务套件中达到94.1%的平均成功率，优于使用相同预训练数据的UniVLA（92.5%）

Conclusion: 几何感知预训练增强了空间推理和策略泛化能力，无需显式深度传感器或3D标注

Abstract: Most existing Vision-Language-Action (VLA) models rely primarily on RGB information, while ignoring geometric cues crucial for spatial reasoning and manipulation. In this work, we introduce GLaD, a geometry-aware VLA framework that incorporates 3D geometric priors during pretraining through knowledge distillation. Rather than distilling geometric features solely into the vision encoder, we align the LLM's hidden states corresponding to visual tokens with features from a frozen geometry-aware vision transformer (VGGT), ensuring that geometric understanding is deeply integrated into the multimodal representations that drive action prediction. Pretrained on the Bridge dataset with this geometry distillation mechanism, GLaD achieves 94.1% average success rate across four LIBERO task suites, outperforming UniVLA (92.5%) which uses identical pretraining data. These results validate that geometry-aware pretraining enhances spatial reasoning and policy generalization without requiring explicit depth sensors or 3D annotations.

</details>


### [187] [ReMoSPLAT: Reactive Mobile Manipulation Control on a Gaussian Splat](https://arxiv.org/abs/2512.09656)
*Nicolas Marticorena,Tobias Fischer,Niko Suenderhauf*

Main category: cs.RO

TL;DR: ReMoSPLAT：基于高斯泼溅表示和二次规划的反应式移动操作控制器，能在杂乱场景中实现末端执行器姿态跟踪和避障


<details>
  <summary>Details</summary>
Motivation: 移动操作机器人的反应式控制能协调基座和手臂运动，但如何在不依赖昂贵规划的情况下准确表示环境以实现避障仍具挑战

Method: 提出ReMoSPLAT反应式控制器，基于二次规划公式，利用高斯泼溅表示进行碰撞避免，通过优化约束和成本函数实现末端姿态跟踪

Result: 在仿真实验中（合成和真实世界扫描），该方法可行且性能与依赖完美地面真实信息的控制器相当，比较了两种机器人-障碍物距离计算方法

Conclusion: ReMoSPLAT成功实现了移动操作机器人的反应式控制，在杂乱场景中达到目标末端姿态同时避免碰撞，无需昂贵规划

Abstract: Reactive control can gracefully coordinate the motion of the base and the arm of a mobile manipulator. However, incorporating an accurate representation of the environment to avoid obstacles without involving costly planning remains a challenge. In this work, we present ReMoSPLAT, a reactive controller based on a quadratic program formulation for mobile manipulation that leverages a Gaussian Splat representation for collision avoidance. By integrating additional constraints and costs into the optimisation formulation, a mobile manipulator platform can reach its intended end effector pose while avoiding obstacles, even in cluttered scenes. We investigate the trade-offs of two methods for efficiently calculating robot-obstacle distances, comparing a purely geometric approach with a rasterisation-based approach. Our experiments in simulation on both synthetic and real-world scans demonstrate the feasibility of our method, showing that the proposed approach achieves performance comparable to controllers that rely on perfect ground-truth information.

</details>


### [188] [High-Resolution Water Sampling via a Solar-Powered Autonomous Surface Vehicle](https://arxiv.org/abs/2512.09798)
*Misael Mamani,Mariel Fernandez,Grace Luna,Steffani Limachi,Leonel Apaza,Carolina Montes-Dávalos,Marcelo Herrera,Edwin Salcedo*

Main category: cs.RO

TL;DR: 本文提出了一种太阳能驱动的全自主无人水面艇，配备新型注射器采样架构，单次任务可采集72个离散水样，结合ROS 2自主系统实现GPS-RTK导航、障碍物检测和远程监控，在玻利维亚实地测试中表现出高精度和可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前水质评估需要空间分辨采样，但大多数无人水面艇只能采集有限样本或依赖代表性差的单点传感器，无法满足高分辨率水质监测需求。

Method: 开发太阳能全自主无人水面艇，采用新型注射器采样架构（6×12模块化系统），集成ROS 2自主堆栈（GPS-RTK导航、LiDAR和立体视觉障碍物检测、Nav2任务规划、LoRa远程监控），采用行为树自主架构实现任务级推理和感知感知导航。

Result: 在玻利维亚阿乔卡亚泻湖实地测试显示：87%航点精度、稳定自主导航、准确的物理化学测量（温度、pH、电导率、总溶解固体），与人工采集参考数据相当，采样空间覆盖范围超过先前报道的无人水面艇采样器。

Conclusion: 该平台实现了可靠的高分辨率采样和自主任务执行，为偏远环境的水生监测提供了可扩展解决方案，展示了在非结构化环境中执行采样路线的可靠能力。

Abstract: Accurate water quality assessment requires spatially resolved sampling, yet most unmanned surface vehicles (USVs) can collect only a limited number of samples or rely on single-point sensors with poor representativeness. This work presents a solar-powered, fully autonomous USV featuring a novel syringe-based sampling architecture capable of acquiring 72 discrete, contamination-minimized water samples per mission. The vehicle incorporates a ROS 2 autonomy stack with GPS-RTK navigation, LiDAR and stereo-vision obstacle detection, Nav2-based mission planning, and long-range LoRa supervision, enabling dependable execution of sampling routes in unstructured environments. The platform integrates a behavior-tree autonomy architecture adapted from Nav2, enabling mission-level reasoning and perception-aware navigation. A modular 6x12 sampling system, controlled by distributed micro-ROS nodes, provides deterministic actuation, fault isolation, and rapid module replacement, achieving spatial coverage beyond previously reported USV-based samplers. Field trials in Achocalla Lagoon (La Paz, Bolivia) demonstrated 87% waypoint accuracy, stable autonomous navigation, and accurate physicochemical measurements (temperature, pH, conductivity, total dissolved solids) comparable to manually collected references. These results demonstrate that the platform enables reliable high-resolution sampling and autonomous mission execution, providing a scalable solution for aquatic monitoring in remote environments.

</details>


### [189] [Bridging the Basilisk Astrodynamics Framework with ROS 2 for Modular Spacecraft Simulation and Hardware Integration](https://arxiv.org/abs/2512.09833)
*Elias Krantz,Ngai Nam Chan,Gunnar Tibert,Huina Mao,Christer Fuglesang*

Main category: cs.RO

TL;DR: 开发了一个轻量级开源通信桥，连接Basilisk航天动力学模拟器和ROS 2，实现实时双向数据交换，支持航天器自主控制开发


<details>
  <summary>Details</summary>
Motivation: 高保真航天器模拟器与模块化机器人框架的集成仍然是自主性开发的一个挑战，需要一种能够支持快速开发、硬件在环测试以及从模拟到硬件无缝过渡的解决方案

Method: 开发了一个轻量级开源通信桥，在不修改Basilisk核心的情况下实现与ROS 2节点的无缝集成，支持实时双向数据交换，并在领导者-跟随者编队飞行场景中使用非线性模型预测控制进行演示

Result: 该桥接器成功实现了Basilisk模拟器与ROS 2之间的通信，在模拟和ATMOS平面微重力测试平台上部署了相同的控制算法，支持快速开发、硬件在环测试和从模拟到硬件的无缝过渡

Conclusion: 该通信桥为模块化航天器自主性和可重复研究流程提供了一个灵活且可扩展的平台，解决了高保真模拟器与机器人框架集成的挑战

Abstract: Integrating high-fidelity spacecraft simulators with modular robotics frameworks remains a challenge for autonomy development. This paper presents a lightweight, open-source communication bridge between the Basilisk astrodynamics simulator and the Robot Operating System 2 (ROS 2), enabling real-time, bidirectional data exchange for spacecraft control. The bridge requires no changes to Basilisk's core and integrates seamlessly with ROS 2 nodes. We demonstrate its use in a leader-follower formation flying scenario using nonlinear model predictive control, deployed identically in both simulation and on the ATMOS planar microgravity testbed. This setup supports rapid development, hardware-in-the-loop testing, and seamless transition from simulation to hardware. The bridge offers a flexible and scalable platform for modular spacecraft autonomy and reproducible research workflows.

</details>


### [190] [Simultaneous Tactile-Visual Perception for Learning Multimodal Robot Manipulation](https://arxiv.org/abs/2512.09851)
*Yuyang Li,Yinghan Chen,Zihang Zhao,Puhao Li,Tengyu Liu,Siyuan Huang,Yixin Zhu*

Main category: cs.RO

TL;DR: TacThru-UMI系统结合了同时多模态感知（视觉+触觉）和Transformer扩散策略，在5个真实世界任务中达到85.5%的平均成功率，显著优于交替感知和纯视觉基线。


<details>
  <summary>Details</summary>
Motivation: 现有透皮传感器缺乏同时多模态感知能力且触觉跟踪不可靠，同时将丰富的多模态信号整合到基于学习的操作流程中仍是一个开放挑战。

Method: 提出TacThru传感器（全透明弹性体、持续照明、新型关键线标记、高效跟踪）和TacThru-UMI模仿学习框架，通过Transformer-based Diffusion Policy整合多模态信号。

Result: 在5个具有挑战性的真实世界任务中，TacThru-UMI达到85.5%的平均成功率，显著优于交替触觉-视觉（66.3%）和纯视觉（55.4%）基线，在薄软物体接触检测和需要多模态协调的精确操作中表现优异。

Conclusion: 同时多模态感知与现代学习框架的结合能够实现更精确、适应性更强的机器人操作。

Abstract: Robotic manipulation requires both rich multimodal perception and effective learning frameworks to handle complex real-world tasks. See-through-skin (STS) sensors, which combine tactile and visual perception, offer promising sensing capabilities, while modern imitation learning provides powerful tools for policy acquisition. However, existing STS designs lack simultaneous multimodal perception and suffer from unreliable tactile tracking. Furthermore, integrating these rich multimodal signals into learning-based manipulation pipelines remains an open challenge. We introduce TacThru, an STS sensor enabling simultaneous visual perception and robust tactile signal extraction, and TacThru-UMI, an imitation learning framework that leverages these multimodal signals for manipulation. Our sensor features a fully transparent elastomer, persistent illumination, novel keyline markers, and efficient tracking, while our learning system integrates these signals through a Transformer-based Diffusion Policy. Experiments on five challenging real-world tasks show that TacThru-UMI achieves an average success rate of 85.5%, significantly outperforming the baselines of alternating tactile-visual (66.3%) and vision-only (55.4%). The system excels in critical scenarios, including contact detection with thin and soft objects and precision manipulation requiring multimodal coordination. This work demonstrates that combining simultaneous multimodal perception with modern learning frameworks enables more precise, adaptable robotic manipulation.

</details>


### [191] [YOPO-Nav: Visual Navigation using 3DGS Graphs from One-Pass Videos](https://arxiv.org/abs/2512.09903)
*Ryan Meegan,Adam D'Souza,Bryan Bo Cao,Shubham Jain,Kristin Dana*

Main category: cs.RO

TL;DR: YOPO-Nav提出了一种基于视频参考的视觉导航方法，使用3D高斯泼溅模型构建紧凑空间表示，通过分层设计实现机器人轨迹重走，无需传统3D地图。


<details>
  <summary>Details</summary>
Motivation: 传统机器人导航依赖详细的地图和路径规划，但构建和维护3D地图计算成本高、内存密集。本文旨在解决当有大规模环境探索视频可用时，如何让机器人仅依靠视觉参考重走已探索轨迹的问题。

Method: 提出YOPO-Nav方法：1) 将环境编码为相互连接的局部3D高斯泼溅模型组成的紧凑空间表示；2) 采用分层设计：视觉地点识别模块提供粗略定位，局部3DGS模型细化目标和中间姿态以生成控制动作；3) 在导航时对齐机器人当前视觉观测与空间表示，预测引导机器人返回演示轨迹的动作。

Result: 引入了YOPO-Campus数据集（4小时自我中心视频，超过6公里人工遥控机器人轨迹），在Clearpath Jackal机器人上评估了YOPO-Nav。实验结果表明，该方法在真实场景的图像目标导航任务中表现出色。

Conclusion: YOPO-Nav提供了一种高效的视觉导航解决方案，无需传统3D地图，仅依靠探索视频作为参考，在真实机器人平台上验证了其有效性，数据集和代码将公开以促进视觉导航和场景表示研究。

Abstract: Visual navigation has emerged as a practical alternative to traditional robotic navigation pipelines that rely on detailed mapping and path planning. However, constructing and maintaining 3D maps is often computationally expensive and memory-intensive. We address the problem of visual navigation when exploration videos of a large environment are available. The videos serve as a visual reference, allowing a robot to retrace the explored trajectories without relying on metric maps. Our proposed method, YOPO-Nav (You Only Pass Once), encodes an environment into a compact spatial representation composed of interconnected local 3D Gaussian Splatting (3DGS) models. During navigation, the framework aligns the robot's current visual observation with this representation and predicts actions that guide it back toward the demonstrated trajectory. YOPO-Nav employs a hierarchical design: a visual place recognition (VPR) module provides coarse localization, while the local 3DGS models refine the goal and intermediate poses to generate control actions. To evaluate our approach, we introduce the YOPO-Campus dataset, comprising 4 hours of egocentric video and robot controller inputs from over 6 km of human-teleoperated robot trajectories. We benchmark recent visual navigation methods on trajectories from YOPO-Campus using a Clearpath Jackal robot. Experimental results show YOPO-Nav provides excellent performance in image-goal navigation for real-world scenes on a physical robot. The dataset and code will be made publicly available for visual navigation and scene representation research.

</details>


### [192] [Token Expand-Merge: Training-Free Token Compression for Vision-Language-Action Models](https://arxiv.org/abs/2512.09927)
*Yifan Ye,Jiaqi Ma,Jun Cen,Zhihe Lu*

Main category: cs.RO

TL;DR: TEAM-VLA是一种无需训练的令牌压缩框架，通过动态令牌扩展和选择性合并来加速视觉-语言-动作模型的推理，同时保持任务性能。


<details>
  <summary>Details</summary>
Motivation: 当前大规模视觉-语言-动作模型参数量巨大（数十亿参数），在动态环境中实时部署面临计算成本高、延迟敏感等挑战，需要在不重新训练的情况下加速推理。

Method: 提出TEAM-VLA框架：1）动态令牌扩展机制，在注意力高亮区域的空间邻域采样额外信息令牌以增强上下文完整性；2）在深层进行选择性令牌合并，在动作感知指导下减少冗余同时保持语义连贯性；3）在单次前向传播中耦合扩展和合并，无需重新训练或参数更新。

Result: 在LIBERO基准测试中，TEAM-VLA在保持或甚至超过完整VLA模型任务成功率的同时，持续提升推理速度。

Conclusion: TEAM-VLA通过训练免费的令牌压缩框架，在效率和效果之间实现了平衡的权衡，为大规模VLA模型的实时部署提供了可行解决方案。

Abstract: Vision-Language-Action (VLA) models pretrained on large-scale multimodal datasets have emerged as powerful foundations for robotic perception and control. However, their massive scale, often billions of parameters, poses significant challenges for real-time deployment, as inference becomes computationally expensive and latency-sensitive in dynamic environments. To address this, we propose Token Expand-and-Merge-VLA (TEAM-VLA), a training-free token compression framework that accelerates VLA inference while preserving task performance. TEAM-VLA introduces a dynamic token expansion mechanism that identifies and samples additional informative tokens in the spatial vicinity of attention-highlighted regions, enhancing contextual completeness. These expanded tokens are then selectively merged in deeper layers under action-aware guidance, effectively reducing redundancy while maintaining semantic coherence. By coupling expansion and merging within a single feed-forward pass, TEAM-VLA achieves a balanced trade-off between efficiency and effectiveness, without any retraining or parameter updates. Extensive experiments on LIBERO benchmark demonstrate that TEAM-VLA consistently improves inference speed while maintaining or even surpassing the task success rate of full VLA models. The code is public available on \href{https://github.com/Jasper-aaa/TEAM-VLA}{https://github.com/Jasper-aaa/TEAM-VLA}

</details>


### [193] [HiF-VLA: Hindsight, Insight and Foresight through Motion Representation for Vision-Language-Action Models](https://arxiv.org/abs/2512.09928)
*Minghui Lin,Pengxiang Ding,Shu Wang,Zifeng Zhuang,Yang Liu,Xinyang Tong,Wenxuan Song,Shangke Lyu,Siteng Huang,Donglin Wang*

Main category: cs.RO

TL;DR: HiF-VLA提出了一种利用运动信息进行双向时序推理的视觉-语言-动作模型框架，通过后见、洞见和前瞻机制解决传统VLA模型在长时程任务中的时序短视问题。


<details>
  <summary>Details</summary>
Motivation: 当前大多数视觉-语言-动作模型假设马尔可夫性，仅依赖当前观测，导致时序短视问题，损害长时程任务的一致性。作者认为运动是更紧凑且信息丰富的时序上下文表示，能捕捉状态间变化并过滤静态像素噪声。

Method: 提出HiF-VLA统一框架，利用运动进行双向时序推理：1）通过后验先验编码过去动态；2）通过前瞻推理预测未来运动；3）通过后验调制联合专家整合两者，实现"边行动边思考"的长时程操作范式。

Result: 在LIBERO-Long和CALVIN ABC-D基准测试中超越强基线模型，同时仅带来可忽略的额外推理延迟。在真实世界长时程操作任务中取得显著改进，展示了在实际机器人环境中的广泛有效性。

Conclusion: HiF-VLA通过利用运动信息进行双向时序推理，有效解决了VLA模型的长时程一致性问题，为实际机器人操作任务提供了更强大的框架。

Abstract: Vision-Language-Action (VLA) models have recently enabled robotic manipulation by grounding visual and linguistic cues into actions. However, most VLAs assume the Markov property, relying only on the current observation and thus suffering from temporal myopia that degrades long-horizon coherence. In this work, we view motion as a more compact and informative representation of temporal context and world dynamics, capturing inter-state changes while filtering static pixel-level noise. Building on this idea, we propose HiF-VLA (Hindsight, Insight, and Foresight for VLAs), a unified framework that leverages motion for bidirectional temporal reasoning. HiF-VLA encodes past dynamics through hindsight priors, anticipates future motion via foresight reasoning, and integrates both through a hindsight-modulated joint expert to enable a ''think-while-acting'' paradigm for long-horizon manipulation. As a result, HiF-VLA surpasses strong baselines on LIBERO-Long and CALVIN ABC-D benchmarks, while incurring negligible additional inference latency. Furthermore, HiF-VLA achieves substantial improvements in real-world long-horizon manipulation tasks, demonstrating its broad effectiveness in practical robotic settings.

</details>


### [194] [Robustness and Adaptability of Reinforcement Learning based Cooperative Autonomous Driving in Mixed-autonomy Traffic](https://arxiv.org/abs/2202.00881)
*Rodolfo Valiente,Behrad Toghi,Ramtin Pedarsani,Yaser P. Fallah*

Main category: cs.RO

TL;DR: 本文提出了一种去中心化多智能体强化学习框架，用于训练自动驾驶车辆在混合交通环境中与人类驾驶车辆安全共存并实现社会效用最大化。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆与人类驾驶车辆共存的混合交通环境面临两大挑战：1）人类驾驶员的社交偏好和个体特征（如无私性、攻击性）难以实时推断；2）人类驾驶员不遵循固定策略，难以预测。现有研究主要关注AV间的协作，但AV-HV混合环境下的协作问题尚未得到充分解决。

Method: 将混合自主性问题建模为多智能体强化学习问题，提出去中心化训练框架和奖励函数。该方法使AV能够从经验中隐式学习HV的决策过程，优化社会效用，同时优先考虑安全性并保持适应性，使利他型AV能够适应不同人类行为并约束在安全行动空间内。

Result: 研究调查了AV对不同HV行为特征的鲁棒性、安全性和敏感性，展示了AV能够学习适应不同情境的协作策略。提出的框架能够使AV在保持安全的同时，有效应对各种人类驾驶行为。

Conclusion: 通过去中心化MARL框架，自动驾驶车辆能够在与人类驾驶车辆共存的复杂环境中学习适应性强的协作策略，在优先保障安全的前提下实现社会效用最大化，为解决混合交通自主性问题提供了有效途径。

Abstract: Building autonomous vehicles (AVs) is a complex problem, but enabling them to operate in the real world where they will be surrounded by human-driven vehicles (HVs) is extremely challenging. Prior works have shown the possibilities of creating inter-agent cooperation between a group of AVs that follow a social utility. Such altruistic AVs can form alliances and affect the behavior of HVs to achieve socially desirable outcomes. We identify two major challenges in the co-existence of AVs and HVs. First, social preferences and individual traits of a given human driver, e.g., selflessness and aggressiveness are unknown to an AV, and it is almost impossible to infer them in real-time during a short AV-HV interaction. Second, contrary to AVs that are expected to follow a policy, HVs do not necessarily follow a stationary policy and therefore are extremely hard to predict. To alleviate the above-mentioned challenges, we formulate the mixed-autonomy problem as a multi-agent reinforcement learning (MARL) problem and propose a decentralized framework and reward function for training cooperative AVs. Our approach enables AVs to learn the decision-making of HVs implicitly from experience, optimizes for a social utility while prioritizing safety and allowing adaptability; robustifying altruistic AVs to different human behaviors and constraining them to a safe action space. Finally, we investigate the robustness, safety and sensitivity of AVs to various HVs behavioral traits and present the settings in which the AVs can learn cooperative policies that are adaptable to different situations.

</details>


### [195] [Learning-based social coordination to improve safety and robustness of cooperative autonomous vehicles in mixed traffic](https://arxiv.org/abs/2211.11963)
*Rodolfo Valiente,Behrad Toghi,Mahdi Razzaghpour,Ramtin Pedarsani,Yaser P. Fallah*

Main category: cs.RO

TL;DR: 该论文提出了一种多智能体强化学习方法，让自动驾驶车辆通过分布式奖励结构学习人类驾驶车辆的决策模式，引入利他主义以在混合交通中建立联盟并影响人类驾驶行为。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆与人类驾驶车辆将在同一道路上共存，但自动驾驶车辆目前在与人类驾驶车辆合作方面效率低下，难以理解和适应人类行为。人类驾驶车辆的社会偏好和个体特征对自动驾驶车辆是未知的，且人类驾驶行为不遵循固定策略，难以预测，这在混合自动驾驶环境中尤为挑战。

Method: 将混合自动驾驶问题构建为多智能体强化学习问题，提出一种方法让自动驾驶车辆从经验中隐式学习人类驾驶车辆的决策，考虑所有车辆的利益，并安全适应其他交通状况。通过量化自动驾驶车辆的社会偏好，提出分布式奖励结构，在决策过程中引入利他主义，使利他型自动驾驶车辆学会建立联盟并影响人类驾驶车辆的行为。

Result: 论文提出的方法使自动驾驶车辆能够学习人类驾驶车辆的决策模式，通过利他主义建立车辆联盟，并影响人类驾驶车辆的行为，从而在混合交通环境中实现更安全和高效的交互。

Conclusion: 通过多智能体强化学习和引入利他主义的分布式奖励结构，自动驾驶车辆能够在混合交通环境中更好地理解、适应和影响人类驾驶车辆的行为，提高自动驾驶系统的安全性和社会接受度。

Abstract: It is expected that autonomous vehicles(AVs) and heterogeneous human-driven vehicles(HVs) will coexist on the same road. The safety and reliability of AVs will depend on their social awareness and their ability to engage in complex social interactions in a socially accepted manner. However, AVs are still inefficient in terms of cooperating with HVs and struggle to understand and adapt to human behavior, which is particularly challenging in mixed autonomy. In a road shared by AVs and HVs, the social preferences or individual traits of HVs are unknown to the AVs and different from AVs, which are expected to follow a policy, HVs are particularly difficult to forecast since they do not necessarily follow a stationary policy. To address these challenges, we frame the mixed-autonomy problem as a multi-agent reinforcement learning (MARL) problem and propose an approach that allows AVs to learn the decision-making of HVs implicitly from experience, account for all vehicles' interests, and safely adapt to other traffic situations. In contrast with existing works, we quantify AVs' social preferences and propose a distributed reward structure that introduces altruism into their decision-making process, allowing the altruistic AVs to learn to establish coalitions and influence the behavior of HVs.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [196] [Kinematics Control of Electromagnetic Formation Flight Using Angular-Momentum Conservation Constraint](https://arxiv.org/abs/2512.08949)
*Yuta Takahashi,Hiraku Sakamoto,Shin-ichiro Sakai*

Main category: eess.SY

TL;DR: 提出一种新的电磁编队飞行控制器，能够同时控制电磁力和扭矩，避免反作用轮饱和，只需一个主卫星配备反作用轮即可控制整个卫星系统。


<details>
  <summary>Details</summary>
Motivation: 传统电磁编队飞行中，所有卫星都需要配备反作用轮来补偿电磁扭矩，但电磁扭矩在不同卫星间分布不均导致角动量分布不均匀，反作用轮容易饱和，限制了姿态控制能力。

Method: 基于角动量守恒推导电磁编队飞行运动学，设计新型控制器实现电磁力和扭矩的同时控制。控制器结合主卫星的简单卸载控制，消除整个系统的角动量积累。

Result: 通过五卫星系统的编队保持和重构数值模拟验证了控制器的有效性，能够在不饱和反作用轮的情况下控制多个卫星，只需一个主卫星配备反作用轮。

Conclusion: 提出的控制器解决了电磁编队飞行中反作用轮饱和问题，简化了系统配置要求，为多卫星电磁编队控制提供了有效解决方案。

Abstract: Electromagnetic formation flight (EMFF) uses the electromagnetic force to control the relative positions of multiple satellites without using conventional fuel-based propulsion. To compensate for the electromagnetic torque generated alongside the electromagnetic force, in most previous studies, all satellites were assumed to have reaction wheels (RWs) besides electromagnetic coils. However, the RW-loaded angular momentum becomes non-uniformly distributed among the satellites, because the electromagnetic torque usually differs between satellites. Without a proper control scheme, this deviation increases over time, and the RWs become saturated quickly, preventing the attitudes of the satellites from being controlled. In this study, a new controller is proposed that enables the electromagnetic force and torque to be controlled simultaneously. The EMFF kinematics derived from the conservation of angular momentum are used for the controller design. This controller can control $n$ satellites without saturating the RWs, and only one set of RWs is required among all satellites. The combination of the proposed controller with a simple unloading control exclusive to the chief satellite results in the elimination of the accumulation of angular momentum in the entire system. The effectiveness of the proposed controller is demonstrated through numerical simulations of the formation maintenance and formation reconfiguration of a five-satellite system.

</details>


### [197] [Characterizing Human Feedback-Based Control in Naturalistic Driving Interactions via Gaussian Process Regression with Linear Feedback](https://arxiv.org/abs/2512.09097)
*Rachel DiPirro,Rosalyn Devonport,Dan Calderone,Chishang "Mario'' Yang,Wendy Ju,Meeko Oishi*

Main category: eess.SY

TL;DR: 通过驾驶模拟器收集数据，使用高斯过程回归方法学习人类驾驶员在无信号交叉口的交互行为，建立状态反馈控制器模型，分析控制器增益如何反映驾驶行为，评估不同驾驶员群体的控制器差异。


<details>
  <summary>Details</summary>
Motivation: 理解驾驶员交互行为对于设计自动驾驶车辆与人类驾驶车辆安全互操作至关重要，特别是在无信号交叉口这种复杂场景中。

Method: 使用驾驶模拟器在受控环境中收集自然决策和行为数据，通过高斯过程回归方法学习人类驾驶员响应，建立基于状态的反馈控制器模型，使用线性和非线性先验的加权组合计算控制器增益。

Result: 成功建立了驾驶员交互行为的反馈控制器模型，分析了控制器增益如何反映具体驾驶行为，并发现了不同驾驶员群体之间的控制器差异。

Conclusion: 基于数据驱动的驾驶员策略分析方法可以为未来设计具有社会响应能力的自动驾驶车辆提供基础，促进车辆间更安全的互操作。

Abstract: Understanding driver interactions is critical to designing autonomous vehicles to interoperate safely with human-driven cars. We consider the impact of these interactions on the policies drivers employ when navigating unsigned intersections in a driving simulator. The simulator allows the collection of naturalistic decision-making and behavior data in a controlled environment. Using these data, we model the human driver responses as state-based feedback controllers learned via Gaussian Process regression methods. We compute the feedback gain of the controller using a weighted combination of linear and nonlinear priors. We then analyze how the individual gains are reflected in driver behavior. We also assess differences in these controllers across populations of drivers. Our work in data-driven analyses of how drivers determine their policies can facilitate future work in the design of socially responsive autonomy for vehicles.

</details>


### [198] [MPC for momentum counter-balanced and zero-impulse contact with a free-spinning satellite](https://arxiv.org/abs/2512.09213)
*Theofania Karampela,Rishie Seshadri,Florian Dörfler,Sarah H. Q. Li*

Main category: eess.SY

TL;DR: 本文开发了一个非线性模型预测控制框架，用于实现服务卫星与自由旋转目标卫星的零冲量接触，通过协调两个独立驱动的模块并考虑交叉耦合动力学。


<details>
  <summary>Details</summary>
Motivation: 在轨机器人技术中，服务卫星与自由旋转目标卫星实现接触是完成大多数在轨服务任务的关键。现有控制方法难以同时考虑执行器和状态约束，需要开发更先进的控制框架。

Method: 开发非线性模型预测控制框架，协调服务卫星的两个独立驱动模块：力矩生成模块和操作模块。通过显式建模模块间的交叉耦合动力学，应用MPC控制两个模块，能够强制执行先前控制方法无法考虑的驱动和状态约束。

Result: 通过数值蒙特卡洛试验模拟零冲量接触场景，MPC控制器在操作约束、接触位置变化以及观测和执行噪声条件下，能够有效维持自旋同步和零冲量接触，性能优于先前控制方法。

Conclusion: MPC控制器在复杂约束条件下能够有效实现服务卫星与自由旋转目标卫星的零冲量接触，为在轨服务任务提供了更可靠的控制解决方案。

Abstract: In on-orbit robotics, a servicer satellite's ability to make contact with a free-spinning target satellite is essential to completing most on-orbit servicing (OOS) tasks. This manuscript develops a nonlinear model predictive control (MPC) framework that generates feasible controls for a servicer satellite to achieve zero-impulse contact with a free-spinning target satellite. The overall maneuver requires coordination between two separately actuated modules of the servicer satellite: (1) a moment generation module and (2) a manipulation module. We apply MPC to control both modules by explicitly modeling the cross-coupling dynamics between them. We demonstrate that the MPC controller can enforce actuation and state constraints that prior control approaches could not account for. We evaluate the performance of the MPC controller by simulating zero-impulse contact scenarios with a free-spinning target satellite via numerical Monte Carlo (MC) trials and comparing the simulation results with prior control approaches. Our simulation results validate the effectiveness of the MPC controller in maintaining spin synchronization and zero-impulse contact under operation constraints, moving contact location, and observation and actuation noise.

</details>


### [199] [Electric Arc Furnaces Scheduling under Electricity Price Volatility with Reinforcement Learning](https://arxiv.org/abs/2512.09293)
*Ruonan Pi,Zhiyuan Fan,Bolun Xu*

Main category: eess.SY

TL;DR: 提出基于强化学习的电弧炉优化调度框架，在电价波动下实现实时控制，相比完美预测的MILP基准达到约90%的利润。


<details>
  <summary>Details</summary>
Motivation: 电弧炉在波动电价下的运行优化问题，传统方法需要完美价格预测，需要开发能够实时响应价格变化的智能控制方法。

Method: 1. 将确定性EAF调度问题建模为混合整数线性规划；2. 开发Q-learning算法进行多EAF单元实时控制；3. 设计定制奖励函数平滑EAF启动惩罚；4. 考虑实时价格波动和共享供料能力约束。

Result: 使用纽约州真实EAF设计和电价数据，在非预期控制设置下，强化学习算法相比完美预测MILP基准达到约90%的利润，优于基于规则的基线控制器。

Conclusion: 强化学习框架能有效优化电弧炉在波动电价下的运行，在不需要完美价格预测的情况下实现接近最优的性能，为工业过程优化提供了实用解决方案。

Abstract: This paper proposes a reinforcement learning-based framework for optimizing the operation of electric arc furnaces (EAFs) under volatile electricity prices. We formulate the deterministic version of the EAF scheduling problem into a mixed-integer linear programming (MILP) formulation, and then develop a Q-learning algorithm to perform real-time control of multiple EAF units under real-time price volatility and shared feeding capacity constraints. We design a custom reward function for the Q-learning algorithm to smooth the start-up penalties of the EAFs. Using real data from EAF designs and electricity prices in New York State, we benchmark our algorithm against a baseline rule-based controller and a MILP benchmark, assuming perfect price forecasts. The results show that our reinforcement learning algorithm achieves around 90% of the profit compared to the perfect MILP benchmark in various single-unit and multi-unit cases under a non-anticipatory control setting.

</details>


### [200] [Analysis of Frequency and Voltage Strength in Power Electronics-Dominated Power Systems Based on Eigen-subsystems](https://arxiv.org/abs/2512.09323)
*Huisheng Gao,Linbin Huang,Huanhai Xin,Zhiyi Li,Ping Ju*

Main category: eess.SY

TL;DR: 本文提出了一种统一框架来分析电力系统中频率/电压强度，通过模态解耦将系统分解为特征子系统，识别出传统方法忽略的全局电压响应不稳定性。


<details>
  <summary>Details</summary>
Motivation: 大规模逆变器资源集成恶化了电力系统的频率/电压响应，增加了不稳定性风险。现有方法通常分别评估频率和电压强度，使用不同的度量标准（如惯性和短路比），导致对系统整体强度缺乏全面理解，可能忽略关键方面。

Method: 首先引入频率/电压调节的统一建模，然后基于模态解耦将电力系统分解为多个特征子系统，将频率/电压响应分解为共模和差模分量（CM-F、DM-F、CM-V、DM-V）。基于此框架提出新的度量标准来评估每个模态分量的强度。

Result: 研究发现传统强度分析通常忽略的共模电压响应（CM-V）在电力电子主导的电力系统中也可能变得不稳定。通过仿真验证了所提方法的有效性。

Conclusion: 提出的统一框架为分析电力电子主导电力系统中的频率/电压强度提供了全面方法，识别了传统方法忽略的关键不稳定性因素，有助于更准确地评估系统强度。

Abstract: The large-scale integration of inverter-based resources (IBRs) has deteriorated the frequency/voltage (F/V) responses of power systems, leading to a higher risk of instability. Consequently, evaluating the F/V strength has become an important task in power electronics (PE)-dominated power systems. Existing methods typically examine F/V strength separately, employing fundamentally different metrics, such as inertia (focusing on device dynamics) and short-circuit ratio (SCR, addressing network characteristics). These fragmented approaches have resulted in a lack of comprehensive understanding of the overall system strength, potentially overlooking critical aspects. To address this problem, this paper proposes a unified framework for analyzing F/V strength. First, a unified modeling of F/V regulations is introduced. Then, based on modal decoupling, the power systems are decomposed into several eigen-subsystems, where the F/V responses are both decomposed into common-mode (CM) and differential-mode (DM) components, namely, CM-F, DM-F, CM-V, and DM-V. The CM-F and CM-V represent the collective response of all devices to external active or reactive power disturbances, independent of the power network characteristics. In contrast, the DM-F and DM-V capture the redistribution of disturbance power within the system, which is strongly influenced by the network topology and the locations of devices. Notably, traditional strength analysis generally ignores the CM-V (global voltage response), which, as discovered in this paper, may also become unstable in PE-dominated power systems. Based on the proposed framework, new metrics are proposed to evaluate the strength of each modal component. Finally, the effectiveness of the proposed approach is validated through simulations.

</details>


### [201] [Time-Discretized Simulation of Vehicle Platoons for Safety Analysis with Guaranteed Error Bounds](https://arxiv.org/abs/2512.09416)
*Yuhao Chen,Ahmet Cetinkaya*

Main category: eess.SY

TL;DR: 研究车辆编队中无线通信丢包与紧急刹车同时发生时，如何通过仿真方法分析安全距离问题，并确定保证串稳定性的控制参数。


<details>
  <summary>Details</summary>
Motivation: 车辆编队中无线通信对协调控制至关重要，但通信丢包与紧急刹车同时发生时可能导致严重安全问题，需要研究如何确保安全距离。

Method: 提出基于仿真的方法，通过时间离散化仿真研究安全距离问题，提供两种选择时间步长的方法以确保距离近似误差在指定范围内。

Result: 数值示例表明，在保证串稳定性的控制参数中，某些参数在同时发生通信丢包和紧急刹车时表现更好。

Conclusion: 通过仿真方法可以分析车辆编队中通信丢包与紧急刹车同时发生时的安全问题，并识别出在不同条件下表现更优的控制参数。

Abstract: Wireless communication is essential to achieve coordinated control in vehicle platoons. However, packet losses in wireless communication can cause critical safety issues when they occur in conjunction with sudden brakes. In this paper, we propose simulation-based methods that allow the study of such safety issues by determining the absolute minimum distance between vehicles over time for various control parameters that guarantee string stability. For our proposed time-discretized simulations, we provide two methods for selecting different time-step intervals to ensure that the error in distance approximation remains within specified bounds at all times. Through numerical examples we demonstrate that among control parameters that guarantee string stability some perform better than others under simultaneously occurring packet losses and sudden brakes.

</details>


### [202] [Personalized Building Climate Control with Contextual Preferential Bayesian Optimization](https://arxiv.org/abs/2512.09481)
*Wenbin Wang,Jicheng Shi,Colin N. Jones*

Main category: eess.SY

TL;DR: 提出基于上下文偏好贝叶斯优化的建筑气候控制器调优方法，通过二进制偏好反馈和上下文信息实现高效实时调优，在BOPTEST仿真平台上验证效果，相比基线控制器提升23%效用


<details>
  <summary>Details</summary>
Motivation: 建筑气候控制器调优对居住者舒适度和满意度至关重要，但存在以下挑战：1）潜在效用难以直接测量；2）时变上下文因素（如室外温度）使问题复杂化；3）需要高效实时调优方法

Method: 提出上下文偏好贝叶斯优化算法，结合二进制偏好反馈和上下文信息进行控制器调优。在BOPTEST高保真建筑仿真平台上验证，调优经济模型预测控制（MPC）控制器，通过两月仿真期评估性能

Result: 1）相比基线控制器提升23%效用；2）算法能自动适应不同居住者类型偏好；3）实现个性化控制器调优；4）在BOPTEST仿真平台上验证有效

Conclusion: 上下文偏好贝叶斯优化算法能有效解决建筑气候控制器调优问题，通过二进制偏好反馈和上下文信息实现高效实时调优，显著提升居住者效用，并能自适应个性化偏好

Abstract: Efficient tuning of building climate controllers to optimize occupant utility is essential for ensuring overall comfort and satisfaction. However, this is a challenging task since the latent utility are difficult to measure directly. Time-varying contextual factors, such as outdoor temperature, further complicate the problem. To address these challenges, we propose a contextual preferential Bayesian optimization algorithm that leverages binary preference feedback together with contextual information to enable efficient real-time controller tuning. We validate the approach by tuning an economic MPC controller on BOPTEST, a high-fidelity building simulation platform. Over a two-month simulation period, our method outperforms the baseline controller and achieves an improvement of up to 23% in utility. Moreover, for different occupant types, we demonstrate that the algorithm automatically adapts to individual preferences, enabling personalized controller tuning.

</details>


### [203] [Instantaneous Complex Phase and Frequency: Conceptual Clarification and Equivalence between Formulations](https://arxiv.org/abs/2512.09574)
*César García-Veloso,Mario Paolone,Federico Milano*

Main category: eess.SY

TL;DR: 该论文澄清了瞬时复相位和频率的不同定义，展示了基于解析信号和空间向量两种基本定义的关系，并提出了统一的符号和术语以避免混淆。


<details>
  <summary>Details</summary>
Motivation: 论文旨在澄清瞬时复相位和频率在现有文献中的不同定义，这些定义在特定假设下是等价的，但缺乏统一的表述容易造成混淆。

Method: 首先介绍了两种基本定义方法：(i)基于解析信号的方法，(ii)基于空间向量的方法。然后展示了这两种方法之间的关系，并分析了它们成立的前提条件。

Result: 论文证明了在特定假设下，基于解析信号和基于空间向量的瞬时复相位和频率定义是等价的，并建立了它们之间的数学关系。

Conclusion: 为了避免混淆，论文提出了统一的符号和术语体系，为瞬时复相位和频率的研究提供了清晰的理论框架。

Abstract: This letter seeks to clarify the different existing definitions of both instantaneous complex phase and frequency as well as their equivalence when specific hypotheses hold. To achieve this, the two fundamental definitions, i.e., those based on either the use of (i) analytic signals or (ii) space vectors, together with the premises used for their formulation, are presented and their relationship shown. Lastly, an unified notation and terminology to avoid confusion is proposed.

</details>


### [204] [Real-Time Non-Smooth MPC for Switching Systems: Application to a Three-Tank Process](https://arxiv.org/abs/2512.09611)
*Hendrik Alsmeier,Felix Häusser,Andreas Knödler,Armin Nurkanović,Anton Pozharskiy,Moritz Diehl,Rolf Findeisen*

Main category: eess.SY

TL;DR: 本文提出了一种实时可行的非光滑模型预测控制方案，用于物理三水箱过程，无需混合整数规划，通过Filippov系统建模结合有限元和开关检测，将问题转化为带互补约束的数学规划，并通过光滑非线性规划的同伦方法求解。


<details>
  <summary>Details</summary>
Motivation: 实时模型预测控制在非光滑切换系统中面临挑战，因为不连续性和离散模式的存在使数值积分和优化变得复杂。现有方法通常使用混合整数规划，计算负担重，难以实现实时控制。

Method: 结合Filippov系统建模与有限元和开关检测进行时间离散化，形成带互补约束的数学规划问题。通过光滑非线性规划的同伦方法求解。针对三水箱动力学进行建模调整，包括添加额外模式以避免非Lipschitz点和未定义函数值。

Result: 硬件实验表明：1) 高效处理切换事件；2) 在参考变化下实现模式一致跟踪；3) 正确边界处理；4) 约束满足。即使在模型失配情况下，跟踪性能和计算时间仍保持在实时限制内。使用NOSNOC框架实现完整控制器。

Conclusion: 提出的非光滑模型预测控制方案能够实时处理物理三水箱过程的切换系统，无需混合整数规划，通过Filippov建模和互补约束数学规划方法，在硬件实验中表现出良好的跟踪性能和实时可行性。

Abstract: Real-time model predictive control of non-smooth switching systems remains challenging due to discontinuities and the presence of discrete modes, which complicate numerical integration and optimization. This paper presents a real-time feasible non-smooth model predictive control scheme for a physical three-tank process, implemented without mixed-integer formulations. The approach combines Filippov system modeling with finite elements and switch detection for time discretization, leading to a finite-dimensional optimal control problem formulated as a mathematical program with complementarity constraints. The mathematical program is solved via a homotopy of smooth nonlinear programs. We introduce modeling adjustments that make the three-tank dynamics numerically tractable, including additional modes to avoid non-Lipschitz points and undefined function values. Hardware experiments demonstrate efficient handling of switching events, mode-consistent tracking across reference changes, correct boundary handling, and constraint satisfaction. Furthermore, we investigate the impact of model mismatch and show that the tracking performance and computation times remain within real-time limits for the chosen sampling time. The complete controller is implemented using the non-smooth optimal control framework NOSNOC

</details>


### [205] [Adaptive Optimal Control for Avatar-Guided Motor Rehabilitation in Virtual Reality](https://arxiv.org/abs/2512.09667)
*Francesco De Lellis,Maria Lombardi,Egidio De Benedetto,Pasquale Arpaia,Mario di Bernardo*

Main category: eess.SY

TL;DR: 提出基于最优控制的自主虚拟化身引导康复控制理论框架，通过可解释、自适应的运动指导解决远程康复挑战


<details>
  <summary>Details</summary>
Motivation: 解决运动康复中的可及性、成本和护理连续性挑战，超过50%患者无法定期参加临床治疗，需要家庭康复方案

Method: 非线性人机闭环控制策略，虚拟化身实时适应患者表现，基于Hogan最小抖动模型的多目标最优控制平衡跟随与引导，数据驱动的"能力指数"根据患者进展动态调整控制增益

Result: 通过仿真和初步试验验证，系统显示出提供自适应、参与度高且可扩展的远程物理治疗的潜力

Conclusion: 该框架基于可解释的控制理论原则，为远程康复提供了有效的技术方案，有望改善中风患者的康复可及性和效果

Abstract: A control-theoretic framework for autonomous avatar-guided rehabilitation in virtual reality, based on interpretable, adaptive motor guidance through optimal control, is presented. The framework faces critical challenges in motor rehabilitation due to accessibility, cost, and continuity of care, with over 50% of patients inability to attend regular clinic sessions. The system enables post-stroke patients to undergo personalized therapy in immersive virtual reality at home, while being monitored by clinicians. The core is a nonlinear, human-in-the-loop control strategy, where the avatar adapts in real time to the patient's performance. Balance between following the patient's movements and guiding them to ideal kinematic profiles based on the Hogan minimum-jerk model is achieved through multi-objective optimal control. A data-driven "ability index" uses smoothness metrics to dynamically adjust control gains according to the patient's progress. The system was validated through simulations and preliminary trials, and shows potential for delivering adaptive, engaging and scalable remote physiotherapy guided by interpretable control-theoretic principles.

</details>


### [206] [Dynamic one-time delivery of critical data by small and sparse UAV swarms: a model problem for MARL scaling studies](https://arxiv.org/abs/2512.09682)
*Mika Persson,Jonas Lidman,Jacob Ljungberg,Samuel Sandelius,Adam Andersson*

Main category: eess.SY

TL;DR: 该研究探讨了多智能体强化学习在无人机分散控制中的应用，用于将关键数据包中继到已知位置。研究引入了确定性游戏系列用于MARL扩展研究，提出了基于限制智能体运动包络和应用Dijkstra算法的鲁棒基线策略。实验表明，两种现成的MARL算法在小规模智能体数量下与基线表现相当，但随着智能体数量增加出现扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索多智能体强化学习在无人机分散控制中的应用，特别是针对关键数据包中继任务。需要解决MARL在复杂多智能体系统中的扩展性问题，为无人机协同控制提供有效的分散决策框架。

Method: 研究方法包括：1）引入一系列确定性游戏用于MARL扩展研究；2）提出基于限制智能体运动包络和应用Dijkstra算法的鲁棒基线策略；3）使用两种现成的多智能体强化学习算法进行实验比较；4）在不同智能体数量下进行性能评估。

Result: 实验结果显示：1）两种现成的MARL算法在小规模智能体数量下与基线策略表现相当；2）随着智能体数量增加，MARL算法出现明显的扩展性问题；3）基线策略在多智能体场景中表现出更好的鲁棒性和可扩展性。

Conclusion: 结论表明，虽然现成的多智能体强化学习算法在小规模无人机控制任务中表现良好，但在大规模多智能体系统中面临扩展性挑战。需要开发更高效的MARL算法来处理复杂多智能体协同控制问题，特别是针对无人机数据包中继等实际应用场景。

Abstract: This work presents a conceptual study on the application of Multi-Agent Reinforcement Learning (MARL) for decentralized control of unmanned aerial vehicles to relay a critical data package to a known position. For this purpose, a family of deterministic games is introduced, designed for scaling studies for MARL. A robust baseline policy is proposed, which is based on restricting agent motion envelopes and applying Dijkstra's algorithm. Experimental results show that two off-the-shelf MARL algorithms perform competitively with the baseline for a small number of agents, but scalability issues arise as the number of agents increase.

</details>
