<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 22]
- [cs.CL](#cs.CL) [Total: 5]
- [cs.RO](#cs.RO) [Total: 3]
- [cs.LG](#cs.LG) [Total: 1]
- [cs.AI](#cs.AI) [Total: 4]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [GeoPredict: Leveraging Predictive Kinematics and 3D Gaussian Geometry for Precise VLA Manipulation](https://arxiv.org/abs/2512.16811)
*Jingjing Qian,Boyao Han,Chen Shi,Lei Xiao,Long Yang,Shaoshuai Shi,Li Jiang*

Main category: cs.CV

TL;DR: GeoPredict是一个几何感知的视觉-语言-动作模型框架，通过预测3D关键点轨迹和几何信息来增强机器人操作的3D推理能力，在训练时使用深度渲染监督，推理时仅需轻量级查询标记。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言-动作模型在机器人操作中表现出良好的泛化能力，但主要是反应式的且以2D为中心，在需要精确3D推理的任务中可靠性不足。

Method: GeoPredict框架包含两个预测模块：1) 轨迹级模块编码运动历史并预测机器人手臂的多步3D关键点轨迹；2) 预测性3D高斯几何模块预测工作空间几何，并沿未来关键点轨迹进行轨迹引导细化。这些模块仅作为训练时的监督，通过基于深度的渲染实现，推理时仅需轻量级额外查询标记，无需调用任何3D解码。

Result: 在RoboCasa Human-50、LIBERO和真实世界操作任务上的实验表明，GeoPredict始终优于强大的VLA基线模型，特别是在几何密集和空间要求高的场景中表现更佳。

Conclusion: GeoPredict通过引入几何感知的预测模块，显著提升了VLA模型在需要精确3D推理的机器人操作任务中的性能，同时保持了推理效率。

Abstract: Vision-Language-Action (VLA) models achieve strong generalization in robotic manipulation but remain largely reactive and 2D-centric, making them unreliable in tasks that require precise 3D reasoning. We propose GeoPredict, a geometry-aware VLA framework that augments a continuous-action policy with predictive kinematic and geometric priors. GeoPredict introduces a trajectory-level module that encodes motion history and predicts multi-step 3D keypoint trajectories of robot arms, and a predictive 3D Gaussian geometry module that forecasts workspace geometry with track-guided refinement along future keypoint trajectories. These predictive modules serve exclusively as training-time supervision through depth-based rendering, while inference requires only lightweight additional query tokens without invoking any 3D decoding. Experiments on RoboCasa Human-50, LIBERO, and real-world manipulation tasks show that GeoPredict consistently outperforms strong VLA baselines, especially in geometry-intensive and spatially demanding scenarios.

</details>


### [2] [OPENTOUCH: Bringing Full-Hand Touch to Real-World Interaction](https://arxiv.org/abs/2512.16842)
*Yuxin Ray Song,Jinzhou Li,Rao Fu,Devin Murphy,Kaichen Zhou,Rishi Shiv,Yaqi Li,Haoyu Xiong,Crystal Elaine Owens,Yilun Du,Yiyue Luo,Xianyi Cheng,Antonio Torralba,Wojciech Matusik,Paul Pu Liang*

Main category: cs.CV

TL;DR: OpenTouch是首个野外环境下的自我中心视角全手触觉数据集，包含5.1小时的同步视频-触觉-姿态数据，用于推进多模态感知和机器人操作研究。


<details>
  <summary>Details</summary>
Motivation: 人类手是我们与物理世界的主要交互界面，但现有的自我中心视角感知很少能准确识别接触的时间、位置和力度。缺乏鲁棒的穿戴式触觉传感器，也没有野外环境下对齐第一人称视频和全手接触的数据集。

Method: 提出了OpenTouch数据集，包含5.1小时的同步视频-触觉-姿态数据，以及2,900个精心挑选的片段和详细的文本标注。基于此数据集建立了检索和分类基准，探索触觉如何支撑感知和行动。

Result: 研究表明触觉信号为抓握理解提供了紧凑而强大的线索，增强了跨模态对齐，并且可以从野外视频查询中可靠地检索出来。

Conclusion: 通过发布这个标注好的视觉-触觉-姿态数据集和基准，旨在推进多模态自我中心感知、具身学习以及接触丰富的机器人操作研究。

Abstract: The human hand is our primary interface to the physical world, yet egocentric perception rarely knows when, where, or how forcefully it makes contact. Robust wearable tactile sensors are scarce, and no existing in-the-wild datasets align first-person video with full-hand touch. To bridge the gap between visual perception and physical interaction, we present OpenTouch, the first in-the-wild egocentric full-hand tactile dataset, containing 5.1 hours of synchronized video-touch-pose data and 2,900 curated clips with detailed text annotations. Using OpenTouch, we introduce retrieval and classification benchmarks that probe how touch grounds perception and action. We show that tactile signals provide a compact yet powerful cue for grasp understanding, strengthen cross-modal alignment, and can be reliably retrieved from in-the-wild video queries. By releasing this annotated vision-touch-pose dataset and benchmark, we aim to advance multimodal egocentric perception, embodied learning, and contact-rich robotic manipulation.

</details>


### [3] [Flowing from Reasoning to Motion: Learning 3D Hand Trajectory Prediction from Egocentric Human Interaction Videos](https://arxiv.org/abs/2512.16907)
*Mingfei Chen,Yifan Wang,Zhengqin Li,Homanga Bharadhwaj,Yujin Chen,Chuan Qin,Ziyi Kou,Yuan Tian,Eric Whitmire,Rajinder Sodhi,Hrvoje Benko,Eli Shlizerman,Yue Liu*

Main category: cs.CV

TL;DR: 提出EgoMAN数据集和模型，用于解决现有3D手部轨迹预测中运动与语义监督解耦、推理与动作弱关联的问题


<details>
  <summary>Details</summary>
Motivation: 现有3D手部轨迹预测研究存在两个主要局限：1) 数据集将运动与语义监督解耦；2) 模型对推理和动作的关联较弱。需要同时解决数据和模型层面的问题

Method: 首先构建EgoMAN数据集（大规模第一人称数据集，包含21.9万条6DoF轨迹和300万结构化QA对），然后提出EgoMAN模型，这是一个推理到运动的框架，通过轨迹-令牌接口连接视觉语言推理和运动生成，采用渐进式训练对齐推理与运动动态

Result: 该方法能够生成准确且具有阶段感知的轨迹，并在真实场景中展现出良好的泛化能力

Conclusion: 通过结合大规模语义增强数据集和推理到运动的框架，成功解决了3D手部轨迹预测中运动与语义的关联问题，实现了更准确和可泛化的预测

Abstract: Prior works on 3D hand trajectory prediction are constrained by datasets that decouple motion from semantic supervision and by models that weakly link reasoning and action. To address these, we first present the EgoMAN dataset, a large-scale egocentric dataset for interaction stage-aware 3D hand trajectory prediction with 219K 6DoF trajectories and 3M structured QA pairs for semantic, spatial, and motion reasoning. We then introduce the EgoMAN model, a reasoning-to-motion framework that links vision-language reasoning and motion generation via a trajectory-token interface. Trained progressively to align reasoning with motion dynamics, our approach yields accurate and stage-aware trajectories with generalization across real-world scenes.

</details>


### [4] [MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning](https://arxiv.org/abs/2512.16909)
*Yuanchen Ju,Yongyuan Liang,Yen-Jen Wang,Nandiraju Gireesh,Yuanliang Ju,Seungjae Lee,Qiao Gu,Elvis Hsieh,Furong Huang,Koushil Sreenath*

Main category: cs.CV

TL;DR: MomaGraph：面向家庭移动操作机器人的统一场景图表示，包含数据集、评估基准和7B视觉语言模型，在任务规划中达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 家庭移动操作机器人需要既能导航又能操作，这要求紧凑且语义丰富的场景表示。现有方法存在以下问题：1）空间和功能关系分离；2）将场景视为静态快照，缺乏对象状态和时间更新；3）忽视与当前任务最相关的信息

Method: 提出MomaGraph统一场景表示，整合空间-功能关系和部件级交互元素。创建MomaGraph-Scenes数据集（大规模任务驱动场景图）和MomaGraph-Bench评估套件。开发MomaGraph-R1（7B视觉语言模型），通过强化学习训练，采用"先图后规划"框架进行零样本任务规划

Result: 模型在基准测试中达到71.6%准确率（比最佳基线提升11.4%），在开源模型中达到最先进水平。能够泛化到公共基准测试，并有效迁移到真实机器人实验

Conclusion: MomaGraph为具身智能体提供了统一的场景表示方法，通过整合数据集、评估基准和模型训练，显著提升了家庭环境中任务规划和场景理解的能力

Abstract: Mobile manipulators in households must both navigate and manipulate. This requires a compact, semantically rich scene representation that captures where objects are, how they function, and which parts are actionable. Scene graphs are a natural choice, yet prior work often separates spatial and functional relations, treats scenes as static snapshots without object states or temporal updates, and overlooks information most relevant for accomplishing the current task. To address these limitations, we introduce MomaGraph, a unified scene representation for embodied agents that integrates spatial-functional relationships and part-level interactive elements. However, advancing such a representation requires both suitable data and rigorous evaluation, which have been largely missing. We thus contribute MomaGraph-Scenes, the first large-scale dataset of richly annotated, task-driven scene graphs in household environments, along with MomaGraph-Bench, a systematic evaluation suite spanning six reasoning capabilities from high-level planning to fine-grained scene understanding. Built upon this foundation, we further develop MomaGraph-R1, a 7B vision-language model trained with reinforcement learning on MomaGraph-Scenes. MomaGraph-R1 predicts task-oriented scene graphs and serves as a zero-shot task planner under a Graph-then-Plan framework. Extensive experiments demonstrate that our model achieves state-of-the-art results among open-source models, reaching 71.6% accuracy on the benchmark (+11.4% over the best baseline), while generalizing across public benchmarks and transferring effectively to real-robot experiments.

</details>


### [5] [Next-Generation License Plate Detection and Recognition System using YOLOv8](https://arxiv.org/abs/2512.16826)
*Arslan Amin,Rafia Mumtaz,Muhammad Jawad Bashir,Syed Mohammad Hassan Zaidi*

Main category: cs.CV

TL;DR: 该研究评估了YOLOv8变体在车牌识别和字符识别任务中的性能，提出了一种优化的识别流程，在保持计算效率的同时实现了高精度。


<details>
  <summary>Details</summary>
Motivation: 在交通管理和车辆监控领域，高效的车牌检测和识别至关重要。尽管已有多种方法，但在多样化环境中保持实时准确性仍然具有挑战性。本研究旨在通过评估YOLOv8变体来推进智能交通系统的发展。

Method: 使用两个不同的数据集进行训练和评估，测试YOLOv8变体在车牌识别和字符识别任务中的性能。引入了一种基于x轴位置的自定义字符排序方法，并提出了一个优化的识别流程：使用YOLOv8 Nano进行车牌识别，使用YOLOv8 Small进行字符识别。

Result: YOLOv8 Nano在车牌识别任务中实现了0.964的精确度和0.918的mAP50；YOLOv8 Small在字符识别任务中实现了0.92的精确度和0.91的mAP50。提出的优化流程在保持计算效率的同时确保了高准确性。

Conclusion: 该研究提出的YOLOv8 Nano和Small组合的优化识别流程，为智能交通系统中边缘设备的实际部署建立了坚实的基础，标志着向更智能、更高效城市基础设施发展的重要进展。

Abstract: In the evolving landscape of traffic management and vehicle surveillance, efficient license plate detection and recognition are indispensable. Historically, many methodologies have tackled this challenge, but consistent real-time accuracy, especially in diverse environments, remains elusive. This study examines the performance of YOLOv8 variants on License Plate Recognition (LPR) and Character Recognition tasks, crucial for advancing Intelligent Transportation Systems. Two distinct datasets were employed for training and evaluation, yielding notable findings. The YOLOv8 Nano variant demonstrated a precision of 0.964 and mAP50 of 0.918 on the LPR task, while the YOLOv8 Small variant exhibited a precision of 0.92 and mAP50 of 0.91 on the Character Recognition task. A custom method for character sequencing was introduced, effectively sequencing the detected characters based on their x-axis positions. An optimized pipeline, utilizing YOLOv8 Nano for LPR and YOLOv8 Small for Character Recognition, is proposed. This configuration not only maintains computational efficiency but also ensures high accuracy, establishing a robust foundation for future real-world deployments on edge devices within Intelligent Transportation Systems. This effort marks a significant stride towards the development of smarter and more efficient urban infrastructures.

</details>


### [6] [Radiology Report Generation with Layer-Wise Anatomical Attention](https://arxiv.org/abs/2512.16841)
*Emmanuel D. Muñiz-De-León,Jorge A. Rosales-de-Golferichs,Ana S. Muñoz-Rodríguez,Alejandro I. Trejo-Castro,Eduardo de Avila-Armenta,Antonio Martínez-Torteya*

Main category: cs.CV

TL;DR: 提出一种紧凑的图像到文本架构，仅使用单张正面胸片生成放射学报告发现部分，通过层间解剖注意力机制整合肺部和心脏分割掩码，无需额外可训练参数即可提升临床相关区域的注意力。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的放射学报告生成系统依赖大规模多模态训练、临床元数据和多视角图像，资源密集且难以普及。需要开发更紧凑、仅依赖单张图像的方法来降低应用门槛。

Method: 结合冻结的DINOv3 ViT编码器和GPT-2解码器，通过层间解剖注意力机制整合肺部和心脏分割掩码，使用分层高斯平滑技术将注意力偏向临床相关区域，不增加可训练参数。

Result: 在MIMIC-CXR数据集上评估，5种关键病理的CheXpert Macro-F1提升168%，Micro-F1提升146%；14种观察指标的总体性能提升86%；结构一致性方面RadGraph F1提升9.7%。

Conclusion: 尽管模型规模小且仅依赖图像条件，但解码器级别的解剖引导能够改善空间定位并增强临床相关区域的连贯性，为资源受限环境提供了可行的解决方案。

Abstract: Automatic radiology report generation is a promising application of multimodal deep learning, aiming to reduce reporting workload and improve consistency. However, current state-of-the-art (SOTA) systems - such as Multimodal AI for Radiology Applications (MAIRA-2) and Medical Pathways Language Model-Multimodal (MedPaLM-M) - depend on large-scale multimodal training, clinical metadata, and multiple imaging views, making them resource-intensive and inaccessible for most settings. We introduce a compact image-to-text architecture that generates the Findings section of chest X-ray reports from a single frontal image. The model combines a frozen Self-Distillation with No Labels v3 (DINOv3) Vision Transformer (ViT) encoder with a Generative Pre-trained Transformer 2 (GPT-2) decoder enhanced by layer-wise anatomical attention. This mechanism integrates lung and heart segmentation masks through hierarchical Gaussian smoothing, biasing attention toward clinically relevant regions without adding trainable parameters. Evaluated on the official Medical Information Mart for Intensive Care-Chest X-ray (MIMIC-CXR) dataset using Chest Radiograph Expert (CheXpert) and Radiology Graph (RadGraph) metrics, our approach achieved substantial gains: CheXpert Macro-F1 for five key pathologies increased by 168% (0.083 -> 0.238) and Micro-F1 by 146% (0.137 -> 0.337), while broader performance across 14 observations improved by 86% (0.170 -> 0.316). Structural coherence also improved, with RadGraph F1 rising by 9.7%. Despite its small size and purely image-conditioned design, the model demonstrates that decoder-level anatomical guidance improves spatial grounding and enhances coherence in clinically relevant regions. The source code is publicly available at: https://github.com/devMuniz02/UDEM-CXR-Reporting-Thesis-2025.

</details>


### [7] [GenEval 2: Addressing Benchmark Drift in Text-to-Image Evaluation](https://arxiv.org/abs/2512.16853)
*Amita Kamath,Kai-Wei Chang,Ranjay Krishna,Luke Zettlemoyer,Yushi Hu,Marjan Ghazvininejad*

Main category: cs.CV

TL;DR: 本文指出文本到图像模型评估中的基准漂移问题，发现GenEval基准已与人类判断严重脱节，提出新基准GenEval 2和改进的评估方法Soft-TIFA。


<details>
  <summary>Details</summary>
Motivation: 自动化文本到图像模型评估面临挑战：需要评分模型评估正确性，测试提示需对当前模型具有挑战性但不能难倒评分模型。这些约束可能导致基准随时间漂移，静态基准无法跟上新模型能力的发展。

Method: 1) 分析GenEval基准的漂移问题，通过大规模人类研究验证；2) 提出新基准GenEval 2，改进原始视觉概念覆盖并提高组合性；3) 提出Soft-TIFA评估方法，结合视觉原始概念的判断。

Result: GenEval基准已严重漂移，与人类判断的绝对误差高达17.7%，表明该基准已饱和。GenEval 2对当前模型更具挑战性，Soft-TIFA评估方法更符合人类判断且不易随时间漂移。

Conclusion: 基准漂移是文本到图像模型评估的重要问题，需要持续审计和改进。GenEval 2和Soft-TIFA提供了更好的评估框架，但避免基准漂移仍需持续努力。

Abstract: Automating Text-to-Image (T2I) model evaluation is challenging; a judge model must be used to score correctness, and test prompts must be selected to be challenging for current T2I models but not the judge. We argue that satisfying these constraints can lead to benchmark drift over time, where the static benchmark judges fail to keep up with newer model capabilities. We show that benchmark drift is a significant problem for GenEval, one of the most popular T2I benchmarks. Although GenEval was well-aligned with human judgment at the time of its release, it has drifted far from human judgment over time -- resulting in an absolute error of as much as 17.7% for current models. This level of drift strongly suggests that GenEval has been saturated for some time, as we verify via a large-scale human study. To help fill this benchmarking gap, we introduce a new benchmark, GenEval 2, with improved coverage of primitive visual concepts and higher degrees of compositionality, which we show is more challenging for current models. We also introduce Soft-TIFA, an evaluation method for GenEval 2 that combines judgments for visual primitives, which we show is more well-aligned with human judgment and argue is less likely to drift from human-alignment over time (as compared to more holistic judges such as VQAScore). Although we hope GenEval 2 will provide a strong benchmark for many years, avoiding benchmark drift is far from guaranteed and our work, more generally, highlights the importance of continual audits and improvement for T2I and related automated model evaluation benchmarks.

</details>


### [8] [RePlan: Reasoning-guided Region Planning for Complex Instruction-based Image Editing](https://arxiv.org/abs/2512.16864)
*Tianyuan Qu,Lei Ke,Xiaohang Zhan,Longxiang Tang,Yuqi Liu,Bohao Peng,Bei Yu,Dong Yu,Jiaya Jia*

Main category: cs.CV

TL;DR: RePlan是一个用于复杂指令图像编辑的框架，通过区域对齐规划实现精确的多区域编辑，无需迭代修复


<details>
  <summary>Details</summary>
Motivation: 现有基于指令的图像编辑模型在处理指令-视觉复杂性（IV-Complexity）时表现不佳，即当复杂指令遇到杂乱或模糊场景时难以准确执行编辑

Method: 提出RePlan框架，包含视觉语言规划器和扩散编辑器：规划器通过逐步推理分解指令并显式定位到目标区域；编辑器使用无需训练的注意力区域注入机制应用更改，实现精确的并行多区域编辑

Result: 在IV-Complex设置下，RePlan持续优于基于更大数据集训练的基线模型，显著提高了区域精度和整体保真度

Conclusion: RePlan通过规划-执行框架有效解决了复杂指令图像编辑问题，在区域精度和编辑保真度方面表现出色，为指令-视觉复杂场景下的图像编辑提供了有效解决方案

Abstract: Instruction-based image editing enables natural-language control over visual modifications, yet existing models falter under Instruction-Visual Complexity (IV-Complexity), where intricate instructions meet cluttered or ambiguous scenes. We introduce RePlan (Region-aligned Planning), a plan-then-execute framework that couples a vision-language planner with a diffusion editor. The planner decomposes instructions via step-by-step reasoning and explicitly grounds them to target regions; the editor then applies changes using a training-free attention-region injection mechanism, enabling precise, parallel multi-region edits without iterative inpainting. To strengthen planning, we apply GRPO-based reinforcement learning using 1K instruction-only examples, yielding substantial gains in reasoning fidelity and format reliability. We further present IV-Edit, a benchmark focused on fine-grained grounding and knowledge-intensive edits. Across IV-Complex settings, RePlan consistently outperforms strong baselines trained on far larger datasets, improving regional precision and overall fidelity. Our project page: https://replan-iv-edit.github.io

</details>


### [9] [Memory-Enhanced SAM3 for Occlusion-Robust Surgical Instrument Segmentation](https://arxiv.org/abs/2512.16880)
*Valay Bundele,Mehran Hosseinzadeh,Hendrik P. A. Lensch*

Main category: cs.CV

TL;DR: ReMeDI-SAM3：一种无需训练的内存增强型SAM3扩展，用于内窥镜视频中的手术器械分割，通过相关性感知内存过滤、分段插值和特征重识别模块，显著提升了遮挡后的分割性能。


<details>
  <summary>Details</summary>
Motivation: 内窥镜视频中的手术器械分割对计算机辅助干预至关重要，但由于频繁遮挡、快速运动、镜面伪影和长期器械重新进入等挑战，现有方法性能受限。SAM3虽然提供了强大的时空视频对象分割框架，但在手术场景中受到内存更新无差别、内存容量固定和遮挡后身份恢复能力弱等限制。

Method: 提出了ReMeDI-SAM3，包含三个核心组件：1）相关性感知内存过滤，配备专门的遮挡感知内存存储遮挡前帧；2）分段插值方案，扩展有效内存容量；3）基于特征的重识别模块，通过时间投票实现可靠的遮挡后身份消歧。

Result: 在EndoVis17和EndoVis18数据集上的零样本评估显示，相对于原始SAM3分别获得了约7%和16%的绝对mcIoU提升，甚至超过了先前基于训练的方法。

Conclusion: ReMeDI-SAM3通过内存增强机制有效解决了SAM3在手术场景中的局限性，显著提升了遮挡后的分割性能，为内窥镜视频中的手术器械分割提供了有效的训练免费解决方案。

Abstract: Accurate surgical instrument segmentation in endoscopic videos is crucial for computer-assisted interventions, yet remains challenging due to frequent occlusions, rapid motion, specular artefacts, and long-term instrument re-entry. While SAM3 provides a powerful spatio-temporal framework for video object segmentation, its performance in surgical scenes is limited by indiscriminate memory updates, fixed memory capacity, and weak identity recovery after occlusions. We propose ReMeDI-SAM3, a training-free memory-enhanced extension of SAM3, that addresses these limitations through three components: (i) relevance-aware memory filtering with a dedicated occlusion-aware memory for storing pre-occlusion frames, (ii) a piecewise interpolation scheme that expands the effective memory capacity, and (iii) a feature-based re-identification module with temporal voting for reliable post-occlusion identity disambiguation. Together, these components mitigate error accumulation and enable reliable recovery after occlusions. Evaluations on EndoVis17 and EndoVis18 under a zero-shot setting show absolute mcIoU improvements of around 7% and 16%, respectively, over vanilla SAM3, outperforming even prior training-based approaches. Project page: https://valaybundele.github.io/remedi-sam3/.

</details>


### [10] [M-PhyGs: Multi-Material Object Dynamics from Video](https://arxiv.org/abs/2512.16885)
*Norika Wada,Kohei Yamashita,Ryo Kawahara,Ko Nishino*

Main category: cs.CV

TL;DR: M-PhyGs方法从视频中估计复杂自然物体（如花朵）的材料组成和物理参数，解决了现有方法假设单一材料或简单几何的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有方法估计物理材料参数时假设物体是单一材料、预先学习动力学或简单拓扑，但真实物体（如花朵）通常具有复杂的材料组成和几何结构，超出了这些假设范围。

Method: 提出Multi-material Physical Gaussians (M-PhyGs)，从自然环境中拍摄的短视频中联合分割物体的相似材料并恢复其连续力学参数，同时考虑重力影响。采用新的级联3D和2D损失函数，并利用时间小批量处理提高效率。

Result: 在Phlowers数据集（人们与花朵交互的视频数据集）上的实验结果表明，M-PhyGs及其组件在多材料物理参数估计这一挑战性任务上具有准确性和有效性。

Conclusion: M-PhyGs能够从视频中准确估计复杂自然物体的材料组成和物理参数，为多材料物理参数估计提供了有效的解决方案，特别是在花朵这类代表性常见物体上表现出色。

Abstract: Knowledge of the physical material properties governing the dynamics of a real-world object becomes necessary to accurately anticipate its response to unseen interactions. Existing methods for estimating such physical material parameters from visual data assume homogeneous single-material objects, pre-learned dynamics, or simplistic topologies. Real-world objects, however, are often complex in material composition and geometry lying outside the realm of these assumptions. In this paper, we particularly focus on flowers as a representative common object. We introduce Multi-material Physical Gaussians (M-PhyGs) to estimate the material composition and parameters of such multi-material complex natural objects from video. From a short video captured in a natural setting, M-PhyGs jointly segments the object into similar materials and recovers their continuum mechanical parameters while accounting for gravity. M-PhyGs achieves this efficiently with newly introduced cascaded 3D and 2D losses, and by leveraging temporal mini-batching. We introduce a dataset, Phlowers, of people interacting with flowers as a novel platform to evaluate the accuracy of this challenging task of multi-material physical parameter estimation. Experimental results on Phlowers dataset demonstrate the accuracy and effectiveness of M-PhyGs and its components.

</details>


### [11] [LinkedOut: Linking World Knowledge Representation Out of Video LLM for Next-Generation Video Recommendation](https://arxiv.org/abs/2512.16891)
*Haichao Zhang,Yao Lu,Lichen Wang,Yunzhe Li,Daiwei Chen,Yunpeng Xu,Yun Fu*

Main category: cs.CV

TL;DR: LinkedOut提出了一种从视频大语言模型中提取世界知识表示的方法，用于视频推荐任务，解决了传统VLLM在部署时面临的多视频输入、高延迟和语言瓶颈等问题。


<details>
  <summary>Details</summary>
Motivation: 当前视频大语言模型在视频理解方面表现出色，但部署到下游任务如视频推荐时面临挑战：解码生成导致高延迟、不支持多视频输入、语言输出丢弃了视觉细节。这些限制源于缺乏既能保留像素级细节又能利用世界知识的表示方法。

Method: LinkedOut从原始视频帧中提取语义基础、知识感知的token，使用VLLM并通过可提示查询和可选辅助模态进行指导。引入跨层知识融合MoE，从丰富的VLLM特征中选择适当的抽象层次，实现个性化、可解释和低延迟的推荐。

Result: LinkedOut是首个基于VLLM且无需手工标签的视频推荐方法，在标准基准测试中达到最先进结果。可解释性研究和消融实验证实了层多样性和分层融合的好处。

Conclusion: LinkedOut为充分利用VLLM世界知识先验和视觉推理进行下游视觉任务（如推荐）提供了一条实用路径，解决了VLLM部署中的关键限制。

Abstract: Video Large Language Models (VLLMs) unlock world-knowledge-aware video understanding through pretraining on internet-scale data and have already shown promise on tasks such as movie analysis and video question answering. However, deploying VLLMs for downstream tasks such as video recommendation remains challenging, since real systems require multi-video inputs, lightweight backbones, low-latency sequential inference, and rapid response. In practice, (1) decode-only generation yields high latency for sequential inference, (2) typical interfaces do not support multi-video inputs, and (3) constraining outputs to language discards fine-grained visual details that matter for downstream vision tasks. We argue that these limitations stem from the absence of a representation that preserves pixel-level detail while leveraging world knowledge. We present LinkedOut, a representation that extracts VLLM world knowledge directly from video to enable fast inference, supports multi-video histories, and removes the language bottleneck. LinkedOut extracts semantically grounded, knowledge-aware tokens from raw frames using VLLMs, guided by promptable queries and optional auxiliary modalities. We introduce a cross-layer knowledge fusion MoE that selects the appropriate level of abstraction from the rich VLLM features, enabling personalized, interpretable, and low-latency recommendation. To our knowledge, LinkedOut is the first VLLM-based video recommendation method that operates on raw frames without handcrafted labels, achieving state-of-the-art results on standard benchmarks. Interpretability studies and ablations confirm the benefits of layer diversity and layer-wise fusion, pointing to a practical path that fully leverages VLLM world-knowledge priors and visual reasoning for downstream vision tasks such as recommendation.

</details>


### [12] [FlashPortrait: 6x Faster Infinite Portrait Animation with Adaptive Latent Prediction](https://arxiv.org/abs/2512.16900)
*Shuyuan Tu,Yueming Pan,Yinming Huang,Xintong Han,Zhen Xing,Qi Dai,Kai Qiu,Chong Luo,Zuxuan Wu*

Main category: cs.CV

TL;DR: FlashPortrait是一种端到端视频扩散变换器，能够合成保持身份一致性的无限长度肖像动画视频，同时实现高达6倍的推理加速。


<details>
  <summary>Details</summary>
Motivation: 当前基于扩散模型的长肖像动画加速方法难以保证身份一致性，需要一种既能保持身份特征又能加速推理的解决方案。

Method: 1) 使用现成提取器计算身份无关的面部表情特征；2) 引入归一化面部表情块，通过均值和方差归一化对齐面部特征与扩散潜在空间；3) 采用动态滑动窗口方案，在重叠区域进行加权融合；4) 基于特定时间步的潜在变化率和扩散层间导数幅度比，使用当前时间步的高阶潜在导数直接预测未来时间步的潜在表示，跳过多个去噪步骤。

Result: 在基准测试中，FlashPortrait在定性和定量评估中都表现出有效性，能够合成身份保持的无限长度视频，同时实现高达6倍的推理加速。

Conclusion: FlashPortrait通过创新的归一化面部特征对齐、动态滑动窗口和高阶潜在导数预测方法，成功解决了长肖像动画中的身份一致性问题和推理速度瓶颈。

Abstract: Current diffusion-based acceleration methods for long-portrait animation struggle to ensure identity (ID) consistency. This paper presents FlashPortrait, an end-to-end video diffusion transformer capable of synthesizing ID-preserving, infinite-length videos while achieving up to 6x acceleration in inference speed. In particular, FlashPortrait begins by computing the identity-agnostic facial expression features with an off-the-shelf extractor. It then introduces a Normalized Facial Expression Block to align facial features with diffusion latents by normalizing them with their respective means and variances, thereby improving identity stability in facial modeling. During inference, FlashPortrait adopts a dynamic sliding-window scheme with weighted blending in overlapping areas, ensuring smooth transitions and ID consistency in long animations. In each context window, based on the latent variation rate at particular timesteps and the derivative magnitude ratio among diffusion layers, FlashPortrait utilizes higher-order latent derivatives at the current timestep to directly predict latents at future timesteps, thereby skipping several denoising steps and achieving 6x speed acceleration. Experiments on benchmarks show the effectiveness of FlashPortrait both qualitatively and quantitatively.

</details>


### [13] [VIVA: VLM-Guided Instruction-Based Video Editing with Reward Optimization](https://arxiv.org/abs/2512.16906)
*Xiaoyan Cong,Haotian Yang,Angtian Wang,Yizhi Wang,Yiding Yang,Canyu Zhang,Chongyang Ma*

Main category: cs.CV

TL;DR: VIVA是一个基于指令的视频编辑框架，通过VLM引导编码和奖励优化解决现有方法对复杂真实世界指令泛化能力不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散的视频编辑方法通常在简单编辑操作的配对数据上训练，这从根本上限制了它们对多样复杂真实世界指令的泛化能力。

Method: 1. 引入基于VLM的指导器，将文本指令、源视频首帧和可选参考图像编码为视觉基础指令表示；2. 提出后训练阶段Edit-GRPO，将组相对策略优化应用于视频编辑领域；3. 设计数据构建管道，合成生成多样高保真的基本编辑操作配对视频-指令数据。

Result: 广泛实验表明，VIVA在指令遵循、泛化能力和编辑质量方面优于最先进方法。

Conclusion: VIVA通过VLM引导编码和奖励优化的可扩展框架，显著提升了基于指令的视频编辑的泛化能力和编辑质量。

Abstract: Instruction-based video editing aims to modify an input video according to a natural-language instruction while preserving content fidelity and temporal coherence. However, existing diffusion-based approaches are often trained on paired data of simple editing operations, which fundamentally limits their ability to generalize to diverse and complex, real-world instructions. To address this generalization gap, we propose VIVA, a scalable framework for instruction-based video editing that leverages VLM-guided encoding and reward optimization. First, we introduce a VLM-based instructor that encodes the textual instruction, the first frame of the source video, and an optional reference image into visually-grounded instruction representations, providing fine-grained spatial and semantic context for the diffusion transformer backbone. Second, we propose a post-training stage, Edit-GRPO, which adapts Group Relative Policy Optimization to the domain of video editing, directly optimizing the model for instruction-faithful, content-preserving, and aesthetically pleasing edits using relative rewards. Furthermore, we propose a data construction pipeline designed to synthetically generate diverse, high-fidelity paired video-instruction data of basic editing operations. Extensive experiments show that VIVA achieves superior instruction following, generalization, and editing quality over state-of-the-art methods. Website: https://viva-paper.github.io

</details>


### [14] [SceneDiff: A Benchmark and Method for Multiview Object Change Detection](https://arxiv.org/abs/2512.16908)
*Yuqun Wu,Chih-hao Lin,Henry Che,Aditi Tiwari,Chuhang Zou,Shenlong Wang,Derek Hoiem*

Main category: cs.CV

TL;DR: 提出SceneDiff基准和方法，用于多视角场景中的物体变化检测，包括添加、移除或移动的物体识别，在多个基准测试中大幅优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决在不同时间拍摄的同一场景图像/视频中检测物体变化的问题，这对机器人整理、施工进度监控等应用很重要。主要挑战是视角变化可能导致物体被错误识别为发生变化。

Method: 提出SceneDiff方法：1) 在3D中对齐捕获的场景；2) 提取物体区域；3) 比较空间和语义区域特征来检测变化。该方法无需训练，利用预训练的3D、分割和图像编码模型。

Result: 在多视角和双视角基准测试中，该方法大幅优于现有方法（相对AP提升94%和37.4%）。同时创建了SceneDiff基准，包含350个多样化视频对和数千个变化物体实例标注。

Conclusion: 提出的SceneDiff方法和基准为多视角物体变化检测提供了有效的解决方案，显著提升了检测性能，相关代码和基准将公开发布。

Abstract: We investigate the problem of identifying objects that have been added, removed, or moved between a pair of captures (images or videos) of the same scene at different times. Detecting such changes is important for many applications, such as robotic tidying or construction progress and safety monitoring. A major challenge is that varying viewpoints can cause objects to falsely appear changed. We introduce SceneDiff Benchmark, the first multiview change detection benchmark with object instance annotations, comprising 350 diverse video pairs with thousands of changed objects. We also introduce the SceneDiff method, a new training-free approach for multiview object change detection that leverages pretrained 3D, segmentation, and image encoding models to robustly predict across multiple benchmarks. Our method aligns the captures in 3D, extracts object regions, and compares spatial and semantic region features to detect changes. Experiments on multi-view and two-view benchmarks demonstrate that our method outperforms existing approaches by large margins (94% and 37.4% relative AP improvements). The benchmark and code will be publicly released.

</details>


### [15] [SFTok: Bridging the Performance Gap in Discrete Tokenizers](https://arxiv.org/abs/2512.16910)
*Qihang Rao,Borui Zhang,Wenzhao Zheng,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: SFTok是一种新型离散图像分词器，通过多步迭代机制和自强制引导视觉重建技术，在高压缩率下实现卓越的图像重建质量。


<details>
  <summary>Details</summary>
Motivation: 当前离散分词器在图像重建质量上仍落后于连续分词器，限制了其在多模态系统中的采用。需要开发既能保持计算效率又能提供高质量重建的离散分词器。

Method: 提出SFTok离散分词器，采用多步迭代重建机制，结合自强制引导视觉重建和去偏拟合训练策略，解决多步过程中的训练-推理不一致问题。

Result: 在每张图像仅用64个token的高压缩率下，SFTok在ImageNet上达到最先进的重建质量（rFID = 1.21），在类别到图像生成任务中表现优异（gFID = 2.29）。

Conclusion: SFTok通过创新的多步迭代机制和训练策略，显著提升了离散分词器的图像重建能力，为高效多模态系统提供了高质量的图像表示方案。

Abstract: Recent advances in multimodal models highlight the pivotal role of image tokenization in high-resolution image generation. By compressing images into compact latent representations, tokenizers enable generative models to operate in lower-dimensional spaces, thereby improving computational efficiency and reducing complexity. Discrete tokenizers naturally align with the autoregressive paradigm but still lag behind continuous ones, limiting their adoption in multimodal systems. To address this, we propose \textbf{SFTok}, a discrete tokenizer that incorporates a multi-step iterative mechanism for precise reconstruction. By integrating \textbf{self-forcing guided visual reconstruction} and \textbf{debias-and-fitting training strategy}, SFTok resolves the training-inference inconsistency in multi-step process, significantly enhancing image reconstruction quality. At a high compression rate of only 64 tokens per image, SFTok achieves state-of-the-art reconstruction quality on ImageNet (rFID = 1.21) and demonstrates exceptional performance in class-to-image generation tasks (gFID = 2.29).

</details>


### [16] [StereoPilot: Learning Unified and Efficient Stereo Conversion via Generative Priors](https://arxiv.org/abs/2512.16915)
*Guibao Shen,Yihua Du,Wenhang Ge,Jing He,Chirui Chang,Donghao Zhou,Zhen Yang,Luozhou Wang,Xin Tao,Ying-Cong Chen*

Main category: cs.CV

TL;DR: 提出UniStereo数据集和StereoPilot模型，解决单目到立体视频转换中的深度歧义和格式不一致问题，实现高效高质量转换


<details>
  <summary>Details</summary>
Motivation: 立体显示设备快速增长，但高质量立体视频内容制作成本高且复杂；现有的多阶段"深度-扭曲-修复"流水线存在误差传播、深度歧义和并行/汇聚立体格式不一致等问题

Method: 1) 构建UniStereo首个大规模统一立体视频转换数据集，涵盖两种立体格式；2) 提出StereoPilot前馈模型，直接合成目标视图，无需显式深度图或迭代扩散采样；3) 配备可学习域切换器和循环一致性损失，适应不同立体格式

Result: StereoPilot在视觉保真度和计算效率方面显著优于现有最先进方法，能够无缝适应不同立体格式并实现改进的一致性

Conclusion: 通过UniStereo数据集和StereoPilot模型，成功解决了单目到立体视频转换的关键挑战，为高质量立体内容生成提供了高效解决方案

Abstract: The rapid growth of stereoscopic displays, including VR headsets and 3D cinemas, has led to increasing demand for high-quality stereo video content. However, producing 3D videos remains costly and complex, while automatic Monocular-to-Stereo conversion is hindered by the limitations of the multi-stage ``Depth-Warp-Inpaint'' (DWI) pipeline. This paradigm suffers from error propagation, depth ambiguity, and format inconsistency between parallel and converged stereo configurations. To address these challenges, we introduce UniStereo, the first large-scale unified dataset for stereo video conversion, covering both stereo formats to enable fair benchmarking and robust model training. Building upon this dataset, we propose StereoPilot, an efficient feed-forward model that directly synthesizes the target view without relying on explicit depth maps or iterative diffusion sampling. Equipped with a learnable domain switcher and a cycle consistency loss, StereoPilot adapts seamlessly to different stereo formats and achieves improved consistency. Extensive experiments demonstrate that StereoPilot significantly outperforms state-of-the-art methods in both visual fidelity and computational efficiency. Project page: https://hit-perfect.github.io/StereoPilot/.

</details>


### [17] [AdaTooler-V: Adaptive Tool-Use for Images and Videos](https://arxiv.org/abs/2512.16918)
*Chaoyang Wang,Kaituo Feng,Dongyang Chen,Zhongyu Wang,Zhixun Li,Sicheng Gao,Meng Meng,Xu Zhou,Manyuan Zhang,Yuzhang Shang,Xiangyu Yue*

Main category: cs.CV

TL;DR: AdaTooler-V是一个多模态大语言模型，通过自适应工具使用机制，仅在视觉问题真正需要工具时才调用视觉工具，避免了不必要的工具调用开销，在多个基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有开源多模态大语言模型存在盲目使用视觉工具的问题，即使不需要工具时也会调用，这显著增加了推理开销并降低了模型性能。需要开发能够自适应判断何时需要工具使用的模型。

Method: 提出了AdaTooler-V模型，采用AT-GRPO强化学习算法，基于每个样本的工具效益分数自适应调整奖励尺度，鼓励模型仅在工具能真正提供改进时才调用。构建了两个训练数据集：AdaTooler-V-CoT-100k用于SFT冷启动，AdaTooler-V-300k用于具有可验证奖励的强化学习。

Result: 在12个基准测试中表现出强大的推理能力，在多样化的视觉推理任务中优于现有方法。AdaTooler-V-7B在V*高分辨率基准测试中达到89.8%的准确率，超越了商业专有模型GPT-4o和Gemini 1.5 Pro。

Conclusion: AdaTooler-V通过自适应工具使用机制有效解决了现有模型盲目调用视觉工具的问题，在保持高性能的同时显著降低了推理开销，为多模态大语言模型的工具使用提供了新的解决方案。

Abstract: Recent advances have shown that multimodal large language models (MLLMs) benefit from multimodal interleaved chain-of-thought (CoT) with vision tool interactions. However, existing open-source models often exhibit blind tool-use reasoning patterns, invoking vision tools even when they are unnecessary, which significantly increases inference overhead and degrades model performance. To this end, we propose AdaTooler-V, an MLLM that performs adaptive tool-use by determining whether a visual problem truly requires tools. First, we introduce AT-GRPO, a reinforcement learning algorithm that adaptively adjusts reward scales based on the Tool Benefit Score of each sample, encouraging the model to invoke tools only when they provide genuine improvements. Moreover, we construct two datasets to support training: AdaTooler-V-CoT-100k for SFT cold start and AdaTooler-V-300k for RL with verifiable rewards across single-image, multi-image, and video data. Experiments across twelve benchmarks demonstrate the strong reasoning capability of AdaTooler-V, outperforming existing methods in diverse visual reasoning tasks. Notably, AdaTooler-V-7B achieves an accuracy of 89.8\% on the high-resolution benchmark V*, surpassing the commercial proprietary model GPT-4o and Gemini 1.5 Pro. All code, models, and data are released.

</details>


### [18] [EasyV2V: A High-quality Instruction-based Video Editing Framework](https://arxiv.org/abs/2512.16920)
*Jinjie Mai,Chaoyang Wang,Guocheng Gordon Qian,Willi Menapace,Sergey Tulyakov,Bernard Ghanem,Peter Wonka,Ashkan Mirzaei*

Main category: cs.CV

TL;DR: EasyV2V是一个简单有效的基于指令的视频编辑框架，通过数据构建、架构简化和控制统一三方面创新，在视频编辑任务上达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 图像编辑技术发展迅速，但视频编辑仍面临一致性、控制和泛化性等挑战，需要探索数据、架构和控制的设计空间。

Method: 1) 数据方面：组合现有专家模型构建多样化视频对，通过单帧监督和仿射运动伪对将图像编辑对提升为视频，挖掘密集标注的视频片段，添加过渡监督；2) 模型方面：利用预训练文本到视频模型的编辑能力，采用简单的序列拼接条件和轻量LoRA微调；3) 控制方面：通过单一掩码机制统一时空控制，支持可选参考图像。

Result: EasyV2V在视频编辑任务上达到最先进水平，超越了同期和商业系统，支持灵活输入（视频+文本、视频+掩码+文本、视频+掩码+参考图像+文本）。

Conclusion: EasyV2V通过系统性的数据、架构和控制设计，提供了一个简单而强大的视频编辑框架，有效解决了视频编辑中的关键挑战。

Abstract: While image editing has advanced rapidly, video editing remains less explored, facing challenges in consistency, control, and generalization. We study the design space of data, architecture, and control, and introduce \emph{EasyV2V}, a simple and effective framework for instruction-based video editing. On the data side, we compose existing experts with fast inverses to build diverse video pairs, lift image edit pairs into videos via single-frame supervision and pseudo pairs with shared affine motion, mine dense-captioned clips for video pairs, and add transition supervision to teach how edits unfold. On the model side, we observe that pretrained text-to-video models possess editing capability, motivating a simplified design. Simple sequence concatenation for conditioning with light LoRA fine-tuning suffices to train a strong model. For control, we unify spatiotemporal control via a single mask mechanism and support optional reference images. Overall, EasyV2V works with flexible inputs, e.g., video+text, video+mask+text, video+mask+reference+text, and achieves state-of-the-art video editing results, surpassing concurrent and commercial systems. Project page: https://snap-research.github.io/easyv2v/

</details>


### [19] [Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification](https://arxiv.org/abs/2512.16921)
*Qihao Liu,Chengzhi Mao,Yaojie Liu,Alan Yuille,Wen-Sheng Chu*

Main category: cs.CV

TL;DR: AuditDM是一个自动化框架，通过主动发现和纠正多模态大语言模型的失败模式来评估模型能力。它训练一个审计模型生成具有挑战性的问题和反事实图像，揭示目标模型的弱点，并作为无标注数据用于模型改进。


<details>
  <summary>Details</summary>
Motivation: 传统的多模态大语言模型评估方法缺乏可解释性，且往往无法充分揭示不同模型之间的显著能力差距。需要一种更主动、可解释的方法来发现模型弱点并促进改进。

Method: AuditDM通过强化学习微调一个MLLM作为审计器，使其生成能够最大化目标模型之间分歧的挑战性问题和反事实图像。训练完成后，审计器能够发现多样化的、可解释的失败案例，这些案例既揭示了模型弱点，又作为无标注数据用于模型纠正。

Result: 在Gemma-3和PaliGemma-2等最先进模型上应用AuditDM，发现了超过20种不同的失败类型。基于这些发现进行微调后，所有模型在16个基准测试中均得到一致提升，甚至使一个3B参数的模型超越了其28B参数的对应模型。

Conclusion: 随着数据扩展达到收益递减点，针对性的模型审计为模型诊断和改进提供了一条有效途径。AuditDM框架能够主动发现模型弱点并促进性能提升。

Abstract: Conventional evaluation methods for multimodal LLMs (MLLMs) lack interpretability and are often insufficient to fully disclose significant capability gaps across models. To address this, we introduce AuditDM, an automated framework that actively discovers and rectifies MLLM failure modes by auditing their divergence. AuditDM fine-tunes an MLLM as an auditor via reinforcement learning to generate challenging questions and counterfactual images that maximize disagreement among target models. Once trained, the auditor uncovers diverse, interpretable exemplars that reveal model weaknesses and serve as annotation-free data for rectification. When applied to SoTA models like Gemma-3 and PaliGemma-2, AuditDM discovers more than 20 distinct failure types. Fine-tuning on these discoveries consistently improves all models across 16 benchmarks, and enables a 3B model to surpass its 28B counterpart. Our results suggest that as data scaling hits diminishing returns, targeted model auditing offers an effective path to model diagnosis and improvement.

</details>


### [20] [Next-Embedding Prediction Makes Strong Vision Learners](https://arxiv.org/abs/2512.16922)
*Sihan Xu,Ziqiao Ma,Wenhao Chai,Xuweiyi Chen,Weiyang Jin,Joyce Chai,Saining Xie,Stella X. Yu*

Main category: cs.CV

TL;DR: 论文提出NEPA方法，通过预测未来图像块嵌入而非像素重建，实现视觉自监督学习，在ImageNet和ADE20K上取得良好效果


<details>
  <summary>Details</summary>
Motivation: 受自然语言生成式预训练成功的启发，探索是否可以将相同原理应用于视觉自监督学习，从学习表示转向学习模型

Method: 提出Next-Embedding Predictive Autoregression (NEPA)，使用因果掩码和停止梯度，让模型学习基于过去图像块嵌入预测未来嵌入，无需像素重建、离散标记、对比损失或任务特定头

Result: 在ImageNet-1K上，ViT-B和ViT-L分别达到83.8%和85.3%的top-1准确率；在ADE20K语义分割任务上有效迁移

Conclusion: 基于嵌入的生成式预训练为视觉自监督学习提供了一种简单、可扩展且可能模态无关的替代方案

Abstract: Inspired by the success of generative pretraining in natural language, we ask whether the same principles can yield strong self-supervised visual learners. Instead of training models to output features for downstream use, we train them to generate embeddings to perform predictive tasks directly. This work explores such a shift from learning representations to learning models. Specifically, models learn to predict future patch embeddings conditioned on past ones, using causal masking and stop gradient, which we refer to as Next-Embedding Predictive Autoregression (NEPA). We demonstrate that a simple Transformer pretrained on ImageNet-1k with next embedding prediction as its sole learning objective is effective - no pixel reconstruction, discrete tokens, contrastive loss, or task-specific heads. This formulation retains architectural simplicity and scalability, without requiring additional design complexity. NEPA achieves strong results across tasks, attaining 83.8% and 85.3% top-1 accuracy on ImageNet-1K with ViT-B and ViT-L backbones after fine-tuning, and transferring effectively to semantic segmentation on ADE20K. We believe generative pretraining from embeddings provides a simple, scalable, and potentially modality-agnostic alternative to visual self-supervised learning.

</details>


### [21] [Generative Refocusing: Flexible Defocus Control from a Single Image](https://arxiv.org/abs/2512.16923)
*Chun-Wei Tuan Mu,Jia-Bin Huang,Yu-Lun Liu*

Main category: cs.CV

TL;DR: 提出Generative Refocusing方法，通过DeblurNet恢复全焦图像和BokehNet生成可控散景，采用半监督训练结合合成数据和真实散景图像，实现单图像重对焦和文本引导调整。


<details>
  <summary>Details</summary>
Motivation: 景深控制在摄影中很重要，但获得完美对焦通常需要多次尝试或特殊设备。现有单图像重对焦方法存在显著缺陷：需要全焦输入、依赖模拟器合成数据、孔径控制有限。

Method: 提出Generative Refocusing两阶段方法：1) DeblurNet从各种输入恢复全焦图像；2) BokehNet生成可控散景。采用半监督训练，结合合成配对数据和未配对的真实散景图像，利用EXIF元数据捕捉真实光学特性。

Result: 在散焦去模糊、散景合成和重对焦基准测试中达到最佳性能。方法支持文本引导调整和自定义孔径形状。

Conclusion: Generative Refocusing方法通过半监督训练有效结合合成和真实数据，克服了现有方法的局限性，实现了高质量的单图像重对焦和灵活的散景控制。

Abstract: Depth-of-field control is essential in photography, but getting the perfect focus often takes several tries or special equipment. Single-image refocusing is still difficult. It involves recovering sharp content and creating realistic bokeh. Current methods have significant drawbacks. They need all-in-focus inputs, depend on synthetic data from simulators, and have limited control over aperture. We introduce Generative Refocusing, a two-step process that uses DeblurNet to recover all-in-focus images from various inputs and BokehNet for creating controllable bokeh. Our main innovation is semi-supervised training. This method combines synthetic paired data with unpaired real bokeh images, using EXIF metadata to capture real optical characteristics beyond what simulators can provide. Our experiments show we achieve top performance in defocus deblurring, bokeh synthesis, and refocusing benchmarks. Additionally, our Generative Refocusing allows text-guided adjustments and custom aperture shapes.

</details>


### [22] [The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text](https://arxiv.org/abs/2512.16924)
*Hanlin Wang,Hao Ouyang,Qiuyu Wang,Yue Yu,Yihao Meng,Wen Wang,Ka Leong Cheng,Shuailei Ma,Qingyan Bai,Yixuan Li,Cheng Chen,Yanhong Zeng,Xing Zhu,Yujun Shen,Qifeng Chen*

Main category: cs.CV

TL;DR: WorldCanvas是一个支持提示的世界事件生成框架，通过结合文本、轨迹和参考图像实现丰富的用户导向模拟，能够生成包含多智能体交互、对象进出、参考引导外观和反直觉事件的可控视频。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在局限性：纯文本方法表达能力有限，现有的轨迹控制图像到视频方法缺乏语义意图和视觉基础。需要一种多模态方法来生成连贯、可控的世界事件，支持复杂的交互和动态场景。

Method: 采用多模态方法，结合三种输入：轨迹（编码运动、时序和可见性）、自然语言（表达语义意图）和参考图像（提供对象身份的视觉基础）。这种组合使得系统能够生成包含多智能体交互、对象进出、参考引导外观和反直觉事件的连贯可控事件。

Result: 生成的视频不仅具有时间连贯性，还展现出涌现一致性，能够在对象暂时消失时保持对象身份和场景一致性。系统能够生成各种复杂的世界事件，包括多智能体交互和反直觉场景。

Conclusion: WorldCanvas通过支持富有表现力的世界事件生成，将世界模型从被动预测器推进为交互式、用户可塑造的模拟器，为可控视频生成提供了新的可能性。

Abstract: We present WorldCanvas, a framework for promptable world events that enables rich, user-directed simulation by combining text, trajectories, and reference images. Unlike text-only approaches and existing trajectory-controlled image-to-video methods, our multimodal approach combines trajectories -- encoding motion, timing, and visibility -- with natural language for semantic intent and reference images for visual grounding of object identity, enabling the generation of coherent, controllable events that include multi-agent interactions, object entry/exit, reference-guided appearance and counterintuitive events. The resulting videos demonstrate not only temporal coherence but also emergent consistency, preserving object identity and scene despite temporary disappearance. By supporting expressive world events generation, WorldCanvas advances world models from passive predictors to interactive, user-shaped simulators. Our project page is available at: https://worldcanvas.github.io/.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [23] [Grammar-Forced Translation of Natural Language to Temporal Logic using LLMs](https://arxiv.org/abs/2512.16814)
*William English,Dominic Simon,Sumit Kumar Jha,Rickard Ewetz*

Main category: cs.CL

TL;DR: GraFT框架通过限制每一步的有效输出标记来简化自然语言到时序逻辑的翻译任务，相比现有方法提高了翻译准确率


<details>
  <summary>Details</summary>
Motivation: 现有方法在原子命题提取、共指消解和有限数据学习方面存在困难，需要更有效的自然语言到时序逻辑翻译框架

Method: 提出Grammar Forced Translation (GraFT)框架，通过限制每一步的有效输出标记来简化原子命题提取和翻译两个子任务

Result: 在CW、GLTL和Navi基准测试中，GraFT将端到端翻译准确率平均提高5.49%，域外翻译准确率平均提高14.06%

Conclusion: GraFT通过减少解决方案空间显著提高了自然语言到时序逻辑翻译的准确性和效率，并提供了理论支持

Abstract: Translating natural language (NL) into a formal language such as temporal logic (TL) is integral for human communication with robots and autonomous systems. State-of-the-art approaches decompose the task into a lifting of atomic propositions (APs) phase and a translation phase. However, existing methods struggle with accurate lifting, the existence of co-references, and learning from limited data. In this paper, we propose a framework for NL to TL translation called Grammar Forced Translation (GraFT). The framework is based on the observation that previous work solves both the lifting and translation steps by letting a language model iteratively predict tokens from its full vocabulary. In contrast, GraFT reduces the complexity of both tasks by restricting the set of valid output tokens from the full vocabulary to only a handful in each step. The solution space reduction is obtained by exploiting the unique properties of each problem. We also provide a theoretical justification for why the solution space reduction leads to more efficient learning. We evaluate the effectiveness of GraFT using the CW, GLTL, and Navi benchmarks. Compared with state-of-the-art translation approaches, it can be observed that GraFT the end-to-end translation accuracy by 5.49% and out-of-domain translation accuracy by 14.06% on average.

</details>


### [24] [What Do Prosody and Text Convey? Characterizing How Meaningful Information is Distributed Across Multiple Channels](https://arxiv.org/abs/2512.16832)
*Aditya Yadavalli,Tiago Pimentel,Tamar I Regev,Ethan Wilcox,Alex Warstadt*

Main category: cs.CL

TL;DR: 本文提出了一种信息论方法来量化语音中仅由韵律（而非文本）传递的信息量及其内容，发现韵律在传达讽刺和情感方面比文本多传递一个数量级的信息。


<details>
  <summary>Details</summary>
Motivation: 韵律（语音的旋律）传达了文本或文字通常无法捕捉的关键信息。研究者希望量化仅由韵律传递的信息量，并确定这些信息的具体内容。

Method: 采用信息论方法，利用大型语音和语言模型估计话语意义维度（如情感）与其通信通道（如音频或文本）之间的互信息。应用该方法量化音频和文本在讽刺、情感和疑问性方面传递的信息量，使用电视和播客的语音数据。

Result: 对于讽刺和情感，音频通道（即韵律通道）传递的信息量比纯文本通道多一个数量级以上，至少在缺乏当前句子之外的长期上下文时如此。对于疑问性，韵律提供的额外信息相对较少。

Conclusion: 研究提出了一个计划，将该方法应用于更多意义维度、通信通道和语言，以进一步探索韵律在人类交流中的作用。

Abstract: Prosody -- the melody of speech -- conveys critical information often not captured by the words or text of a message. In this paper, we propose an information-theoretic approach to quantify how much information is expressed by prosody alone and not by text, and crucially, what that information is about. Our approach applies large speech and language models to estimate the mutual information between a particular dimension of an utterance's meaning (e.g., its emotion) and any of its communication channels (e.g., audio or text). We then use this approach to quantify how much information is conveyed by audio and text about sarcasm, emotion, and questionhood, using speech from television and podcasts. We find that for sarcasm and emotion the audio channel -- and by implication the prosodic channel -- transmits over an order of magnitude more information about these features than the text channel alone, at least when long-term context beyond the current sentence is unavailable. For questionhood, prosody provides comparatively less additional information. We conclude by outlining a program applying our approach to more dimensions of meaning, communication channels, and languages.

</details>


### [25] [LLMCache: Layer-Wise Caching Strategies for Accelerated Reuse in Transformer Inference](https://arxiv.org/abs/2512.16843)
*Harsh Vardhan Bansal*

Main category: cs.CL

TL;DR: LLMCache是一个层级的缓存框架，通过重用基于输入序列语义相似度的中间激活来加速Transformer推理，在BERT和GPT-2上实现最高3.1倍加速，精度损失小于0.5%。


<details>
  <summary>Details</summary>
Motivation: Transformer语言模型虽然性能优异，但推理延迟高，限制了实时和大规模部署。现有的token级键值缓存机制适用范围有限，需要更通用的加速方案。

Method: 提出LLMCache框架：1）模型无关，支持编码器和解码器架构；2）可在任意Transformer层缓存；3）轻量级指纹机制匹配语义相似输入；4）自适应淘汰策略管理缓存陈旧性。

Result: 在BERT和GPT-2模型上，在SQuAD、WikiText-103和OpenBookQA数据集上测试，推理时间最高加速3.1倍，精度下降小于0.5%。

Conclusion: LLMCache是一个实用且通用的解决方案，能够优化Transformer在实际应用中的推理性能，为实时和大规模部署提供了有效途径。

Abstract: Transformer-based language models have achieved remarkable performance across a wide range of tasks, yet their high inference latency poses a significant challenge for real-timeand large-scale deployment. While existing caching mechanisms,such as token-level key-value caches, offer speedups in autore-gressive decoding, they are limited in scope and applicability. In this paper, we present LLMCache, a novel layer-wise caching framework that accelerates transformer inference by reusing intermediate activations based on semantic similarity of input sequences. Unlike prior work, LLMCache is model-agnostic,operates across both encoder and decoder architectures, and supports caching at arbitrary transformer layers. We introduce a lightweight fingerprinting mechanism for matching seman-tically similar inputs and propose adaptive eviction strategies to manage cache staleness. Experiments on BERT and GPT-2 across SQuAD, WikiText-103, and OpenBookQA show up to 3.1 X speedup in inference time with <0.5% accuracy degradation. Our results highlight LLMCache as a practical and general-purpose solution for optimizing transformer inference in real-world applications

</details>


### [26] [Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image](https://arxiv.org/abs/2512.16899)
*Yushi Hu,Reyhane Askari-Hemmat,Melissa Hall,Emily Dinan,Luke Zettlemoyer,Marjan Ghazvininejad*

Main category: cs.CL

TL;DR: MMRB2是首个针对多模态奖励模型的综合基准，涵盖文本到图像、图像编辑、交错生成和多模态推理四大任务，包含4000个专家标注的偏好对，用于评估现有评判方法。


<details>
  <summary>Details</summary>
Motivation: 奖励模型对于训练大语言模型至关重要，但在处理交错图像和文本序列的全能模型方面研究不足，缺乏全面的评估基准。

Method: 构建MMRB2基准：1) 设计实用且具有挑战性的提示；2) 收集最先进模型和智能体的响应；3) 通过集成过滤策略筛选具有强专家共识的偏好对，包含4000个标注样本。

Result: Gemini 3 Pro准确率75-80%，GPT-5和Gemini 2.5 Pro为66-75%，GPT-4o为59%，开源模型Qwen3-VL-32B与Gemini 2.5 Flash相当(64%)，人类准确率>90%。MMRB2性能与下游任务成功强相关。

Conclusion: MMRB2填补了多模态奖励模型评估的空白，揭示了现有模型与人类专家之间的显著差距，为未来奖励模型的改进指明了方向。

Abstract: Reward models (RMs) are essential for training large language models (LLMs), but remain underexplored for omni models that handle interleaved image and text sequences. We introduce Multimodal RewardBench 2 (MMRB2), the first comprehensive benchmark for reward models on multimodal understanding and (interleaved) generation. MMRB2 spans four tasks: text-to-image, image editing, interleaved generation, and multimodal reasoning ("thinking-with-images"), providing 1,000 expert-annotated preference pairs per task from 23 models and agents across 21 source tasks. MMRB2 is designed with: (1) practical but challenging prompts; (2) responses from state-of-the-art models and agents; and (3) preference pairs with strong human-expert consensus, curated via an ensemble filtering strategy. Using MMRB2, we study existing judges for each subtask, including multimodal LLM-as-a-judge and models trained with human preferences. The latest Gemini 3 Pro attains 75-80% accuracy. GPT-5 and Gemini 2.5 Pro reach 66-75% accuracy, compared to >90% for humans, yet surpass the widely used GPT-4o (59%). The best performing open-source model Qwen3-VL-32B achieves similar accuracies as Gemini 2.5 Flash (64%). We also show that MMRB2 performance strongly correlates with downstream task success using Best-of-N sampling and conduct an in-depth analysis that shows key areas to improve the reward models going forward.

</details>


### [27] [In-Context Algebra](https://arxiv.org/abs/2512.16902)
*Eric Todd,Jannik Brinkmann,Rohit Gandikota,David Bau*

Main category: cs.CL

TL;DR: Transformer模型在变量符号含义不固定的算术任务中，能够学习符号推理机制而非几何嵌入，实现近乎完美的准确率并泛化到未见过的代数群。


<details>
  <summary>Details</summary>
Motivation: 先前研究发现transformer在固定符号含义的算术任务中会发展出反映代数结构的几何嵌入，但本研究探索当符号含义随序列变化时（即符号作为变量），模型会发展出何种机制。

Method: 设计新任务：符号到特定代数群元素的分配随序列变化；创建有针对性的数据分布进行因果测试；隔离并分析模型学习到的机制。

Result: 模型在挑战性设置下达到近乎完美的准确率，甚至能泛化到未见过的代数群；识别出三种一致学习的机制：交换复制（专用头复制答案）、单位元识别（区分包含单位元的事实）、基于闭包的消去（跟踪群成员资格以约束有效答案）。

Conclusion: 与固定符号设置中的几何表示互补，当训练模型在上下文推理中处理含义不固定的变量时，模型会发展出符号推理机制。

Abstract: We investigate the mechanisms that arise when transformers are trained to solve arithmetic on sequences where tokens are variables whose meaning is determined only through their interactions. While prior work has found that transformers develop geometric embeddings that mirror algebraic structure, those previous findings emerge from settings where arithmetic-valued tokens have fixed meanings. We devise a new task in which the assignment of symbols to specific algebraic group elements varies from one sequence to another. Despite this challenging setup, transformers achieve near-perfect accuracy on the task and even generalize to unseen algebraic groups. We develop targeted data distributions to create causal tests of a set of hypothesized mechanisms, and we isolate three mechanisms models consistently learn: commutative copying where a dedicated head copies answers, identity element recognition that distinguishes identity-containing facts, and closure-based cancellation that tracks group membership to constrain valid answers. Complementary to the geometric representations found in fixed-symbol settings, our findings show that models develop symbolic reasoning mechanisms when trained to reason in-context with variables whose meanings are not fixed.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [28] [ReinforceGen: Hybrid Skill Policies with Automated Data Generation and Reinforcement Learning](https://arxiv.org/abs/2512.16861)
*Zihan Zhou,Animesh Garg,Ajay Mandlekar,Caelan Garrett*

Main category: cs.RO

TL;DR: ReinforceGen系统通过任务分解、数据生成、模仿学习和运动规划的组合方法，结合强化学习微调，实现了长时程机器人操作任务的高成功率。


<details>
  <summary>Details</summary>
Motivation: 长时程机器人操作一直是机器人领域的长期挑战，需要解决复杂任务的分解、技能学习和规划问题。

Method: 1. 将任务分解为多个局部技能；2. 通过运动规划连接技能；3. 使用10个人类演示生成的数据集进行模仿学习；4. 通过在线适应和强化学习进行微调。

Result: 在Robosuite数据集上，ReinforceGen在最高重置范围设置下达到80%的成功率，微调方法带来平均89%的性能提升。

Conclusion: ReinforceGen通过结合任务分解、模仿学习和强化学习微调的方法，有效解决了长时程机器人操作问题，在基准测试中表现出色。

Abstract: Long-horizon manipulation has been a long-standing challenge in the robotics community. We propose ReinforceGen, a system that combines task decomposition, data generation, imitation learning, and motion planning to form an initial solution, and improves each component through reinforcement-learning-based fine-tuning. ReinforceGen first segments the task into multiple localized skills, which are connected through motion planning. The skills and motion planning targets are trained with imitation learning on a dataset generated from 10 human demonstrations, and then fine-tuned through online adaptation and reinforcement learning. When benchmarked on the Robosuite dataset, ReinforceGen reaches 80% success rate on all tasks with visuomotor controls in the highest reset range setting. Additional ablation studies show that our fine-tuning approaches contributes to an 89% average performance increase. More results and videos available in https://reinforcegen.github.io/

</details>


### [29] [PolaRiS: Scalable Real-to-Sim Evaluations for Generalist Robot Policies](https://arxiv.org/abs/2512.16881)
*Arhan Jain,Mingtong Zhang,Kanav Arora,William Chen,Marcel Torne,Muhammad Zubair Irshad,Sergey Zakharov,Yue Wang,Sergey Levine,Chelsea Finn,Wei-Chiu Ma,Dhruv Shah,Abhishek Gupta,Karl Pertsch*

Main category: cs.RO

TL;DR: PolaRiS是一个可扩展的真实到仿真框架，通过神经重建方法将真实场景视频扫描转换为交互式仿真环境，用于机器人策略评估，显著提高了仿真评估与真实世界性能的相关性。


<details>
  <summary>Details</summary>
Motivation: 机器人学习研究中准确测量和比较策略性能面临挑战：真实世界评估具有随机性、难以复现且耗时；现有仿真基准与真实世界存在视觉和物理领域差距；构建真实多样的仿真环境需要大量人力和专业知识。

Method: PolaRiS框架：1) 使用神经重建方法将真实世界场景的短视频扫描转换为交互式仿真环境；2) 开发简单的仿真数据协同训练方法，弥合剩余的真实到仿真差距，实现未见仿真环境的零样本评估。

Result: 通过仿真与真实世界的广泛配对评估，PolaRiS评估与真实世界通用策略性能的相关性显著强于现有仿真基准。其简单性还支持快速创建多样化的仿真环境。

Conclusion: PolaRiS为下一代机器人基础模型迈出了分布式和民主化评估的一步，通过高保真仿真评估弥合了真实世界与仿真之间的差距。

Abstract: A significant challenge for robot learning research is our ability to accurately measure and compare the performance of robot policies. Benchmarking in robotics is historically challenging due to the stochasticity, reproducibility, and time-consuming nature of real-world rollouts. This challenge is exacerbated for recent generalist policies, which has to be evaluated across a wide variety of scenes and tasks. Evaluation in simulation offers a scalable complement to real world evaluations, but the visual and physical domain gap between existing simulation benchmarks and the real world has made them an unreliable signal for policy improvement. Furthermore, building realistic and diverse simulated environments has traditionally required significant human effort and expertise. To bridge the gap, we introduce Policy Evaluation and Environment Reconstruction in Simulation (PolaRiS), a scalable real-to-sim framework for high-fidelity simulated robot evaluation. PolaRiS utilizes neural reconstruction methods to turn short video scans of real-world scenes into interactive simulation environments. Additionally, we develop a simple simulation data co-training recipe that bridges remaining real-to-sim gaps and enables zero-shot evaluation in unseen simulation environments. Through extensive paired evaluations between simulation and the real world, we demonstrate that PolaRiS evaluations provide a much stronger correlation to real world generalist policy performance than existing simulated benchmarks. Its simplicity also enables rapid creation of diverse simulated environments. As such, this work takes a step towards distributed and democratized evaluation for the next generation of robotic foundation models.

</details>


### [30] [Sceniris: A Fast Procedural Scene Generation Framework](https://arxiv.org/abs/2512.16896)
*Jinghuan Shang,Harsh Patel,Ran Gong,Karl Schmeckpeper*

Main category: cs.RO

TL;DR: Sceniris是一个高效的程序化场景生成框架，用于快速生成大规模、无碰撞的场景变体，相比现有方法实现了至少234倍的加速。


<details>
  <summary>Details</summary>
Motivation: 现有的程序化生成方法输出吞吐量低，在扩展数据集创建时形成显著瓶颈，需要更高效的场景生成框架来支持物理AI和生成模型的发展。

Method: 通过批量采样和cuRobo中更快的碰撞检测来优化性能，同时扩展了对象间空间关系以支持多样化的场景需求，还提供了可选的机器人可达性检查。

Result: 相比之前的Scene Synthesizer方法，Sceniris实现了至少234倍的加速，能够高效生成大规模、无碰撞的场景变体，并提供机器人操作可行的场景。

Conclusion: Sceniris是一个高效的程序化场景生成框架，解决了现有方法的性能瓶颈，为物理AI和生成模型的大规模数据集创建提供了重要工具。

Abstract: Synthetic 3D scenes are essential for developing Physical AI and generative models. Existing procedural generation methods often have low output throughput, creating a significant bottleneck in scaling up dataset creation. In this work, we introduce Sceniris, a highly efficient procedural scene generation framework for rapidly generating large-scale, collision-free scene variations. Sceniris also provides an optional robot reachability check, providing manipulation-feasible scenes for robot tasks. Sceniris is designed for maximum efficiency by addressing the primary performance limitations of the prior method, Scene Synthesizer. Leveraging batch sampling and faster collision checking in cuRobo, Sceniris achieves at least 234x speed-up over Scene Synthesizer. Sceniris also expands the object-wise spatial relationships available in prior work to support diverse scene requirements. Our code is available at https://github.com/rai-inst/sceniris

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [31] [Training Together, Diagnosing Better: Federated Learning for Collagen VI-Related Dystrophies](https://arxiv.org/abs/2512.16876)
*Astrid Brull,Sara Aguti,Véronique Bolduc,Ying Hu,Daniel M. Jimenez-Gutierrez,Enrique Zuazua,Joaquin Del-Rio,Oleksii Sliusarenko,Haiyan Zhou,Francesco Muntoni,Carsten G. Bönnemann,Xabi Uribe-Etxebarria*

Main category: cs.LG

TL;DR: 该研究利用联邦学习技术，通过跨机构协作训练机器学习模型，显著提高了罕见病COL6-RD的诊断准确性，解决了数据稀缺和隐私保护问题。


<details>
  <summary>Details</summary>
Motivation: 罕见病如胶原VI相关肌营养不良症（COL6-RD）的诊断面临数据稀缺、分散且难以共享的挑战。跨医院、机构或国家的数据共享存在严重的隐私、监管和物流障碍，限制了机器学习在罕见病诊断中的应用。

Method: 研究采用联邦学习（FL）方法，使用Sherpa.ai FL平台在两个国际组织的分布式数据集上进行协作训练。利用患者来源成纤维细胞培养物的胶原VI免疫荧光显微镜图像，训练能够将胶原VI患者图像分类到COL6-RD三个主要致病机制组（外显子跳跃、甘氨酸替代和假外显子插入）的机器学习模型。

Result: 联邦学习方法取得了F1分数0.82的优异表现，显著优于单个机构模型（0.57-0.75）。这表明FL方法在诊断效用和泛化能力方面相比孤立机构模型有显著提升。

Conclusion: 联邦学习为罕见病诊断提供了一种有效的解决方案，能够在保护患者隐私的同时实现跨机构协作。该方法不仅能提高诊断准确性，还有助于解释意义未明的变异，并指导测序策略的优先级排序以识别新的致病变异。

Abstract: The application of Machine Learning (ML) to the diagnosis of rare diseases, such as collagen VI-related dystrophies (COL6-RD), is fundamentally limited by the scarcity and fragmentation of available data. Attempts to expand sampling across hospitals, institutions, or countries with differing regulations face severe privacy, regulatory, and logistical obstacles that are often difficult to overcome. The Federated Learning (FL) provides a promising solution by enabling collaborative model training across decentralized datasets while keeping patient data local and private. Here, we report a novel global FL initiative using the Sherpa.ai FL platform, which leverages FL across distributed datasets in two international organizations for the diagnosis of COL6-RD, using collagen VI immunofluorescence microscopy images from patient-derived fibroblast cultures. Our solution resulted in an ML model capable of classifying collagen VI patient images into the three primary pathogenic mechanism groups associated with COL6-RD: exon skipping, glycine substitution, and pseudoexon insertion. This new approach achieved an F1-score of 0.82, outperforming single-organization models (0.57-0.75). These results demonstrate that FL substantially improves diagnostic utility and generalizability compared to isolated institutional models. Beyond enabling more accurate diagnosis, we anticipate that this approach will support the interpretation of variants of uncertain significance and guide the prioritization of sequencing strategies to identify novel pathogenic variants.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [32] [Generative Adversarial Reasoner: Enhancing LLM Reasoning with Adversarial Reinforcement Learning](https://arxiv.org/abs/2512.16917)
*Qihao Liu,Luoxin Ye,Wufei Ma,Yu-Cheng Chou,Alan Yuille*

Main category: cs.AI

TL;DR: 提出Generative Adversarial Reasoner框架，通过对抗强化学习联合训练LLM推理器和判别器，提升数学推理能力


<details>
  <summary>Details</summary>
Motivation: 尽管具备显式推理能力的大语言模型在数学推理方面表现出色，但仍存在过程错误，如计算错误、逻辑脆弱、表面合理但无效的步骤。需要改进推理质量。

Method: 提出Generative Adversarial Reasoner框架：1) 计算高效的审查计划将推理链划分为逻辑完整的片段；2) 判别器评估每个片段的合理性并提供结构化理由；3) 对抗强化学习联合训练推理器和判别器，推理器获得逻辑一致步骤的奖励，判别器获得正确检测错误的奖励。

Result: 在多个数学基准测试中取得一致提升：AIME24上，DeepSeek-R1-Distill-Qwen-7B从54.0提升到61.3(+7.3)，DeepSeek-R1-Distill-Llama-8B从43.7提升到53.7(+10.0)。模块化判别器支持灵活奖励塑造。

Conclusion: Generative Adversarial Reasoner框架通过对抗强化学习产生密集、校准良好的步级奖励，补充稀疏的精确匹配信号，改善信用分配，提高样本效率，增强LLM的整体推理质量。

Abstract: Large language models (LLMs) with explicit reasoning capabilities excel at mathematical reasoning yet still commit process errors, such as incorrect calculations, brittle logic, and superficially plausible but invalid steps. In this paper, we introduce Generative Adversarial Reasoner, an on-policy joint training framework designed to enhance reasoning by co-evolving an LLM reasoner and an LLM-based discriminator through adversarial reinforcement learning. A compute-efficient review schedule partitions each reasoning chain into logically complete slices of comparable length, and the discriminator evaluates each slice's soundness with concise, structured justifications. Learning couples complementary signals: the LLM reasoner is rewarded for logically consistent steps that yield correct answers, while the discriminator earns rewards for correctly detecting errors or distinguishing traces in the reasoning process. This produces dense, well-calibrated, on-policy step-level rewards that supplement sparse exact-match signals, improving credit assignment, increasing sample efficiency, and enhancing overall reasoning quality of LLMs. Across various mathematical benchmarks, the method delivers consistent gains over strong baselines with standard RL post-training. Specifically, on AIME24, we improve DeepSeek-R1-Distill-Qwen-7B from 54.0 to 61.3 (+7.3) and DeepSeek-R1-Distill-Llama-8B from 43.7 to 53.7 (+10.0). The modular discriminator also enables flexible reward shaping for objectives such as teacher distillation, preference alignment, and mathematical proof-based reasoning.

</details>


### [33] [TOGGLE: Temporal Logic-Guided Large Language Model Compression for Edge](https://arxiv.org/abs/2512.16855)
*Khurram Khalil,Khaza Anuarul Hoque*

Main category: cs.AI

TL;DR: TOGGLE框架首次将形式化方法引入LLM压缩，使用信号时序逻辑（STL）在压缩过程中形式化指定并强制执行语言特性，无需重新训练或微调即可生成满足约束的压缩模型。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然性能卓越，但计算资源需求大，难以部署在资源受限的边缘设备上。现有的压缩技术（如量化和剪枝）往往会损害关键的语言特性，且缺乏保持模型行为的正式保证。

Method: 提出TOGGLE框架，利用信号时序逻辑（STL）形式化指定语言特性，采用STL鲁棒性引导的贝叶斯优化，系统探索层级的量化和剪枝配置，生成满足语言约束的压缩模型。

Result: 在四种LLM架构（GPT-2、DeepSeek-V2 7B、LLaMA 3 8B和Mistral 7B）上评估，实现了高达3.3倍的计算成本（FLOPs）降低和高达68.8%的模型大小缩减，同时满足所有语言特性。

Conclusion: TOGGLE首次将形式化方法集成到LLM压缩中，实现了在边缘硬件上高效、可验证的LLM部署，为资源受限环境下的模型压缩提供了新的形式化保证方法。

Abstract: Large Language Models (LLMs) deliver exceptional performance across natural language tasks but demand substantial computational resources, limiting their deployment on resource-constrained edge devices. Existing compression techniques, such as quantization and pruning, often degrade critical linguistic properties and lack formal guarantees for preserving model behavior. We propose Temporal Logic-Guided Large Language Model Compression (TOGGLE), a novel framework that leverages Signal Temporal Logic (STL) to formally specify and enforce linguistic properties during compression. TOGGLE employs an STL robustness-guided Bayesian optimization to systematically explore layer-wise quantization and pruning configurations, generating compressed models that formally satisfy specified linguistic constraints without retraining or fine-tuning. Evaluating TOGGLE on four LLM architectures (GPT-2, DeepSeek-V2 7B, LLaMA 3 8B, and Mistral 7B), we achieve up to 3.3x reduction in computational costs (FLOPs) and up to a 68.8% reduction in model size while satisfying all linguistic properties. TOGGLE represents the first integration of formal methods into LLM compression, enabling efficient, verifiable deployment of LLMs on edge hardware.

</details>


### [34] [Distributional AGI Safety](https://arxiv.org/abs/2512.16856)
*Nenad Tomašev,Matija Franklin,Julian Jacobs,Sébastien Krier,Simon Osindero*

Main category: cs.AI

TL;DR: 论文提出"拼凑式AGI"假说，认为通用智能可能首先通过多个子AGI智能体的协调协作实现，而非单一系统。为此需要超越个体对齐，建立分布式AGI安全框架，通过虚拟智能体沙盒经济来管理智能体间交互风险。


<details>
  <summary>Details</summary>
Motivation: 当前AI安全和对齐研究主要关注单一AI系统的保障，假设最终会出现单一的通用人工智能。但另一种可能性是通用能力首先通过多个子AGI智能体的协调协作实现，这种"拼凑式AGI"假说需要得到重视并开发相应的安全保障措施。

Method: 提出分布式AGI安全框架，核心是设计和实施虚拟智能体沙盒经济系统（不透或半透），通过稳健的市场机制管理智能体间交易，配合适当的可审计性、声誉管理和监督机制来缓解集体风险。

Result: 论文提出了一个超越个体智能体评估和对齐的新安全框架，强调需要为智能体群体协调可能带来的集体风险建立系统性保障机制。

Conclusion: 随着具有工具使用能力和协调能力的先进AI智能体快速部署，"拼凑式AGI"假说需要被认真考虑，必须开发相应的分布式安全框架来应对智能体群体协调带来的新风险。

Abstract: AI safety and alignment research has predominantly been focused on methods for safeguarding individual AI systems, resting on the assumption of an eventual emergence of a monolithic Artificial General Intelligence (AGI). The alternative AGI emergence hypothesis, where general capability levels are first manifested through coordination in groups of sub-AGI individual agents with complementary skills and affordances, has received far less attention. Here we argue that this patchwork AGI hypothesis needs to be given serious consideration, and should inform the development of corresponding safeguards and mitigations. The rapid deployment of advanced AI agents with tool-use capabilities and the ability to communicate and coordinate makes this an urgent safety consideration. We therefore propose a framework for distributional AGI safety that moves beyond evaluating and aligning individual agents. This framework centers on the design and implementation of virtual agentic sandbox economies (impermeable or semi-permeable), where agent-to-agent transactions are governed by robust market mechanisms, coupled with appropriate auditability, reputation management, and oversight to mitigate collective risks.

</details>


### [35] [The Social Responsibility Stack: A Control-Theoretic Architecture for Governing Socio-Technical AI](https://arxiv.org/abs/2512.16873)
*Otman A. Basir*

Main category: cs.AI

TL;DR: 本文提出了社会责任感堆栈（SRS）框架，将社会价值观嵌入AI系统作为显式约束、保障机制和行为接口，通过六层架构实现AI系统的全生命周期责任治理。


<details>
  <summary>Details</summary>
Motivation: 当前负责任AI和治理工作提供了重要的规范原则，但缺乏可执行的工程机制来贯穿系统生命周期。需要将社会价值观转化为可操作的工程控制，实现AI系统的责任治理。

Method: 提出社会责任感堆栈（SRS）六层架构框架，将责任建模为对技术社会系统的闭环监督控制问题，整合设计时保障与运行时监控，采用统一的约束公式化方法，引入安全包络和反馈解释。

Result: SRS框架能够将公平性、自主性、认知负担和解释质量等社会价值目标转化为可连续监控和执行的工程控制，通过临床决策支持、协作自动驾驶和公共部门系统等案例验证了其可行性。

Conclusion: SRS框架连接了伦理学、控制理论和AI治理，为可问责、自适应和可审计的技术社会AI系统提供了实用基础，实现了从规范目标到可操作工程控制的转化。

Abstract: Artificial intelligence systems are increasingly deployed in domains that shape human behaviour, institutional decision-making, and societal outcomes. Existing responsible AI and governance efforts provide important normative principles but often lack enforceable engineering mechanisms that operate throughout the system lifecycle. This paper introduces the Social Responsibility Stack (SRS), a six-layer architectural framework that embeds societal values into AI systems as explicit constraints, safeguards, behavioural interfaces, auditing mechanisms, and governance processes. SRS models responsibility as a closed-loop supervisory control problem over socio-technical systems, integrating design-time safeguards with runtime monitoring and institutional oversight. We develop a unified constraint-based formulation, introduce safety-envelope and feedback interpretations, and show how fairness, autonomy, cognitive burden, and explanation quality can be continuously monitored and enforced. Case studies in clinical decision support, cooperative autonomous vehicles, and public-sector systems illustrate how SRS translates normative objectives into actionable engineering and operational controls. The framework bridges ethics, control theory, and AI governance, providing a practical foundation for accountable, adaptive, and auditable socio-technical AI systems.

</details>
