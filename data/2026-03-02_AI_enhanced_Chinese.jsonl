{"id": "2602.23370", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.23370", "abs": "https://arxiv.org/abs/2602.23370", "authors": ["Kaifeng Wu", "Junyan Wu", "Qiang Liu", "Jiarui Zhang", "Wen Xu"], "title": "Toward General Semantic Chunking: A Discriminative Framework for Ultra-Long Documents", "comment": null, "summary": "Long-document topic segmentation plays an important role in information retrieval and document understanding, yet existing methods still show clear shortcomings in ultra-long text settings. Traditional discriminative models are constrained by fixed windows and cannot model document-level semantics; generative large language models can output paragraph boundaries, but inference is expensive and long inputs are difficult to support. To address these issues, we propose a discriminative segmentation model based on Qwen3-0.6B. On top of the backbone network, we add a cross-window context fusion layer and a boundary classification head, and combine them with an overlapping sliding-window strategy. Our model supports single-pass inputs of up to 13k tokens and can be extended to ultra-long documents for paragraph boundary detection. To further enhance downstream retrieval efficiency, we derive a vector fusion method with scalar correction, which compresses the representation of ultra-long segments into a single vector without semantic loss. Experiments on the Wikipedia long-document topic segmentation dataset WIKI-727K show that, compared with three generative models based on Qwen2-0.5B released by Jina, our method achieves a better macro-averaged F1 and delivers two orders of magnitude faster inference, substantially improving the practicality and scalability of long-document processing.", "AI": {"tldr": "\u57fa\u4e8eQwen3-0.6B\u7684\u957f\u6587\u6863\u4e3b\u9898\u5206\u5272\u6a21\u578b\uff0c\u901a\u8fc7\u8de8\u7a97\u53e3\u4e0a\u4e0b\u6587\u878d\u5408\u5c42\u548c\u8fb9\u754c\u5206\u7c7b\u5934\uff0c\u652f\u630113k\u4ee4\u724c\u5355\u6b21\u8f93\u5165\uff0c\u76f8\u6bd4\u751f\u6210\u5f0f\u6a21\u578b\u5728WIKI-727K\u6570\u636e\u96c6\u4e0a\u83b7\u5f97\u66f4\u597d\u7684F1\u5206\u6570\u548c\u5feb\u4e24\u4e2a\u6570\u91cf\u7ea7\u7684\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u957f\u6587\u6863\u4e3b\u9898\u5206\u5272\u65b9\u6cd5\u5b58\u5728\u660e\u663e\u4e0d\u8db3\uff1a\u4f20\u7edf\u5224\u522b\u5f0f\u6a21\u578b\u53d7\u9650\u4e8e\u56fa\u5b9a\u7a97\u53e3\u65e0\u6cd5\u5efa\u6a21\u6587\u6863\u7ea7\u8bed\u4e49\uff1b\u751f\u6210\u5f0f\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u652f\u6301\u957f\u8f93\u5165\u3002\u9700\u8981\u4e00\u79cd\u65e2\u9ad8\u6548\u53c8\u80fd\u5904\u7406\u8d85\u957f\u6587\u6863\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u57fa\u4e8eQwen3-0.6B\u6784\u5efa\u5224\u522b\u5f0f\u5206\u5272\u6a21\u578b\uff0c\u6dfb\u52a0\u8de8\u7a97\u53e3\u4e0a\u4e0b\u6587\u878d\u5408\u5c42\u548c\u8fb9\u754c\u5206\u7c7b\u5934\uff0c\u7ed3\u5408\u91cd\u53e0\u6ed1\u52a8\u7a97\u53e3\u7b56\u7565\u3002\u652f\u630113k\u4ee4\u724c\u5355\u6b21\u8f93\u5165\uff0c\u53ef\u6269\u5c55\u5230\u8d85\u957f\u6587\u6863\u3002\u8fd8\u63d0\u51fa\u5e26\u6807\u91cf\u6821\u6b63\u7684\u5411\u91cf\u878d\u5408\u65b9\u6cd5\uff0c\u5c06\u8d85\u957f\u6bb5\u843d\u8868\u793a\u538b\u7f29\u4e3a\u5355\u4e2a\u5411\u91cf\u800c\u65e0\u8bed\u4e49\u635f\u5931\u3002", "result": "\u5728WIKI-727K\u6570\u636e\u96c6\u4e0a\uff0c\u76f8\u6bd4Jina\u53d1\u5e03\u7684\u4e09\u4e2a\u57fa\u4e8eQwen2-0.5B\u7684\u751f\u6210\u5f0f\u6a21\u578b\uff0c\u8be5\u65b9\u6cd5\u83b7\u5f97\u4e86\u66f4\u597d\u7684\u5b8f\u89c2\u5e73\u5747F1\u5206\u6570\uff0c\u63a8\u7406\u901f\u5ea6\u5feb\u4e24\u4e2a\u6570\u91cf\u7ea7\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u6587\u6863\u5904\u7406\u7684\u5b9e\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u5224\u522b\u5f0f\u5206\u5272\u6a21\u578b\u6709\u6548\u89e3\u51b3\u4e86\u957f\u6587\u6863\u4e3b\u9898\u5206\u5272\u7684\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u9ad8\u51c6\u786e\u7387\u7684\u540c\u65f6\u5927\u5e45\u63d0\u5347\u63a8\u7406\u901f\u5ea6\uff0c\u4e3a\u4fe1\u606f\u68c0\u7d22\u548c\u6587\u6863\u7406\u89e3\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.23388", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2602.23388", "abs": "https://arxiv.org/abs/2602.23388", "authors": ["Swati Sharma", "Divya V. Sharma", "Anubha Gupta"], "title": "Task-Lens: Cross-Task Utility Based Speech Dataset Profiling for Low-Resource Indian Languages", "comment": "Accepted at LREC 2026", "summary": "The rising demand for inclusive speech technologies amplifies the need for multilingual datasets for Natural Language Processing (NLP) research. However, limited awareness of existing task-specific resources in low-resource languages hinders research. This challenge is especially acute in linguistically diverse countries, such as India. Cross-task profiling of existing Indian speech datasets can alleviate the data scarcity challenge. This involves investigating the utility of datasets across multiple downstream tasks rather than focusing on a single task. Prior surveys typically catalogue datasets for a single task, leaving comprehensive cross-task profiling as an open opportunity. Therefore, we propose Task-Lens, a cross-task survey that assesses the readiness of 50 Indian speech datasets spanning 26 languages for nine downstream speech tasks. First, we analyze which datasets contain metadata and properties suitable for specific tasks. Next, we propose task-aligned enhancements to unlock datasets to their full downstream potential. Finally, we identify tasks and Indian languages that are critically underserved by current resources. Our findings reveal that many Indian speech datasets contain untapped metadata that can support multiple downstream tasks. By uncovering cross-task linkages and gaps, Task-Lens enables researchers to explore the broader applicability of existing datasets and to prioritize dataset creation for underserved tasks and languages.", "AI": {"tldr": "Task-Lens\u662f\u4e00\u4e2a\u8de8\u4efb\u52a1\u8c03\u67e5\uff0c\u8bc4\u4f30\u4e8650\u4e2a\u5370\u5ea6\u8bed\u97f3\u6570\u636e\u96c6\u57289\u4e2a\u4e0b\u6e38\u8bed\u97f3\u4efb\u52a1\u4e2d\u7684\u51c6\u5907\u60c5\u51b5\uff0c\u63ed\u793a\u4e86\u672a\u5145\u5206\u5229\u7528\u7684\u5143\u6570\u636e\u53ef\u4ee5\u652f\u6301\u591a\u4e2a\u4efb\u52a1\uff0c\u5e76\u8bc6\u522b\u4e86\u670d\u52a1\u4e0d\u8db3\u7684\u4efb\u52a1\u548c\u8bed\u8a00\u3002", "motivation": "\u5305\u5bb9\u6027\u8bed\u97f3\u6280\u672f\u9700\u6c42\u589e\u957f\u9700\u8981\u591a\u8bed\u8a00\u6570\u636e\u96c6\uff0c\u4f46\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u73b0\u6709\u4efb\u52a1\u7279\u5b9a\u8d44\u6e90\u610f\u8bc6\u6709\u9650\uff0c\u7279\u522b\u662f\u5728\u5370\u5ea6\u8fd9\u6837\u7684\u8bed\u8a00\u591a\u6837\u6027\u56fd\u5bb6\u3002\u8de8\u4efb\u52a1\u5206\u6790\u73b0\u6709\u6570\u636e\u96c6\u53ef\u4ee5\u7f13\u89e3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002", "method": "\u63d0\u51faTask-Lens\u8de8\u4efb\u52a1\u8c03\u67e5\u65b9\u6cd5\uff1a1) \u5206\u6790\u54ea\u4e9b\u6570\u636e\u96c6\u5305\u542b\u9002\u5408\u7279\u5b9a\u4efb\u52a1\u7684\u5143\u6570\u636e\u548c\u5c5e\u6027\uff1b2) \u63d0\u51fa\u4efb\u52a1\u5bf9\u9f50\u7684\u589e\u5f3a\u5efa\u8bae\u4ee5\u91ca\u653e\u6570\u636e\u96c6\u7684\u4e0b\u6e38\u6f5c\u529b\uff1b3) \u8bc6\u522b\u5f53\u524d\u8d44\u6e90\u4e25\u91cd\u4e0d\u8db3\u7684\u4efb\u52a1\u548c\u5370\u5ea6\u8bed\u8a00\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u8bb8\u591a\u5370\u5ea6\u8bed\u97f3\u6570\u636e\u96c6\u5305\u542b\u672a\u5145\u5206\u5229\u7528\u7684\u5143\u6570\u636e\uff0c\u53ef\u4ee5\u652f\u6301\u591a\u4e2a\u4e0b\u6e38\u4efb\u52a1\u3002\u901a\u8fc7\u63ed\u793a\u8de8\u4efb\u52a1\u8054\u7cfb\u548c\u5dee\u8ddd\uff0cTask-Lens\u4f7f\u7814\u7a76\u4eba\u5458\u80fd\u591f\u63a2\u7d22\u73b0\u6709\u6570\u636e\u96c6\u7684\u66f4\u5e7f\u6cdb\u5e94\u7528\uff0c\u5e76\u4f18\u5148\u4e3a\u670d\u52a1\u4e0d\u8db3\u7684\u4efb\u52a1\u548c\u8bed\u8a00\u521b\u5efa\u6570\u636e\u96c6\u3002", "conclusion": "\u8de8\u4efb\u52a1\u5206\u6790\u73b0\u6709\u8bed\u97f3\u6570\u636e\u96c6\u53ef\u4ee5\u7f13\u89e3\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0cTask-Lens\u65b9\u6cd5\u901a\u8fc7\u8bc4\u4f30\u6570\u636e\u96c6\u7684\u591a\u4efb\u52a1\u9002\u7528\u6027\uff0c\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u5145\u5206\u5229\u7528\u73b0\u6709\u8d44\u6e90\u5e76\u786e\u5b9a\u4f18\u5148\u53d1\u5c55\u65b9\u5411\u3002"}}
{"id": "2602.23440", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.23440", "abs": "https://arxiv.org/abs/2602.23440", "authors": ["Chris Samarinas", "Haw-Shiuan Chang", "Hamed Zamani"], "title": "Truncated Step-Level Sampling with Process Rewards for Retrieval-Augmented Reasoning", "comment": null, "summary": "Training large language models to reason with search engines via reinforcement learning is hindered by a fundamental credit assignment problem: existing methods such as Search-R1 provide only a sparse outcome reward after an entire multi-step trajectory, making it infeasible to attribute success or failure to individual reasoning and retrieval decisions. Process-reward methods like StepSearch alleviate this by introducing step-level supervision, but rely on heuristic rewards such as TF-IDF overlap with gold documents, and still sample k complete trajectories per example, retaining high gradient variance. We propose SLATE, a framework built on two complementary ideas: (1) truncated step-level sampling, which generates k trajectories that share a common prefix and differ only at the next step, and (2) dense LLM-as-judge rewards, which replace heuristic scoring with a capable LLM evaluator that assesses the quality of each reasoning step, search query, and answer, providing richer and more reliable supervision. We theoretically prove that under the same dense reward structure, truncated sampling reduces the variance of advantage estimates by up to a factor of T compared to full-trajectory sampling for T-step trajectories, yielding lower-variance, better-targeted policy gradients. Experiments on seven QA benchmarks confirm that SLATE consistently outperforms both sparse-reward and process-reward baselines, with the largest gains on harder multi-hop tasks and smaller models.", "AI": {"tldr": "SLATE\u6846\u67b6\u901a\u8fc7\u622a\u65ad\u6b65\u7ea7\u91c7\u6837\u548c\u5bc6\u96c6LLM\u8bc4\u5224\u5956\u52b1\uff0c\u89e3\u51b3\u4e86\u8bed\u8a00\u6a21\u578b\u641c\u7d22\u63a8\u7406\u4e2d\u7684\u4fe1\u7528\u5206\u914d\u95ee\u9898\uff0c\u76f8\u6bd4\u4f20\u7edf\u7a00\u758f\u5956\u52b1\u548c\u8fc7\u7a0b\u5956\u52b1\u65b9\u6cd5\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u641c\u7d22\u63a8\u7406\u65f6\u5b58\u5728\u4fe1\u7528\u5206\u914d\u95ee\u9898\uff1a\u7a00\u758f\u5956\u52b1\u65b9\u6cd5\u96be\u4ee5\u5c06\u6210\u529f\u6216\u5931\u8d25\u5f52\u56e0\u4e8e\u5355\u4e2a\u63a8\u7406\u548c\u68c0\u7d22\u51b3\u7b56\uff0c\u800c\u8fc7\u7a0b\u5956\u52b1\u65b9\u6cd5\u867d\u7136\u5f15\u5165\u6b65\u7ea7\u76d1\u7763\u4f46\u4f9d\u8d56\u542f\u53d1\u5f0f\u5956\u52b1\u4e14\u68af\u5ea6\u65b9\u5dee\u9ad8\u3002", "method": "\u63d0\u51faSLATE\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u601d\u60f3\uff1a(1) \u622a\u65ad\u6b65\u7ea7\u91c7\u6837\uff1a\u751f\u6210k\u6761\u5171\u4eab\u5171\u540c\u524d\u7f00\u3001\u4ec5\u5728\u4e0b\u4e00\u6b65\u4e0d\u540c\u7684\u8f68\u8ff9\uff1b(2) \u5bc6\u96c6LLM\u8bc4\u5224\u5956\u52b1\uff1a\u7528\u80fd\u529b\u5f3a\u7684LLM\u8bc4\u4f30\u5668\u66ff\u4ee3\u542f\u53d1\u5f0f\u8bc4\u5206\uff0c\u8bc4\u4f30\u6bcf\u4e2a\u63a8\u7406\u6b65\u9aa4\u3001\u641c\u7d22\u67e5\u8be2\u548c\u56de\u7b54\u7684\u8d28\u91cf\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u5728\u76f8\u540c\u5bc6\u96c6\u5956\u52b1\u7ed3\u6784\u4e0b\uff0c\u622a\u65ad\u91c7\u6837\u53ef\u5c06\u4f18\u52bf\u4f30\u8ba1\u65b9\u5dee\u964d\u4f4e\u6700\u591aT\u500d\uff08T\u6b65\u8f68\u8ff9\uff09\uff0c\u4ea7\u751f\u66f4\u4f4e\u65b9\u5dee\u3001\u66f4\u7cbe\u51c6\u7684\u7b56\u7565\u68af\u5ea6\u3002\u5728\u4e03\u4e2aQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSLATE\u59cb\u7ec8\u4f18\u4e8e\u7a00\u758f\u5956\u52b1\u548c\u8fc7\u7a0b\u5956\u52b1\u57fa\u7ebf\uff0c\u5728\u66f4\u96be\u7684multi-hop\u4efb\u52a1\u548c\u5c0f\u6a21\u578b\u4e0a\u63d0\u5347\u6700\u5927\u3002", "conclusion": "SLATE\u901a\u8fc7\u622a\u65ad\u91c7\u6837\u548c\u5bc6\u96c6LLM\u5956\u52b1\u7684\u7ec4\u5408\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u641c\u7d22\u63a8\u7406\u4e2d\u7684\u4fe1\u7528\u5206\u914d\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u3001\u4f4e\u65b9\u5dee\u7684\u76d1\u7763\u4fe1\u53f7\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bed\u8a00\u6a21\u578b\u7684\u641c\u7d22\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2602.23452", "categories": ["cs.CL", "cs.DL"], "pdf": "https://arxiv.org/pdf/2602.23452", "abs": "https://arxiv.org/abs/2602.23452", "authors": ["Zhengqing Yuan", "Kaiwen Shi", "Zheyuan Zhang", "Lichao Sun", "Nitesh V. Chawla", "Yanfang Ye"], "title": "CiteAudit: You Cited It, But Did You Read It? A Benchmark for Verifying Scientific References in the LLM Era", "comment": null, "summary": "Scientific research relies on accurate citation for attribution and integrity, yet large language models (LLMs) introduce a new risk: fabricated references that appear plausible but correspond to no real publications. Such hallucinated citations have already been observed in submissions and accepted papers at major machine learning venues, exposing vulnerabilities in peer review. Meanwhile, rapidly growing reference lists make manual verification impractical, and existing automated tools remain fragile to noisy and heterogeneous citation formats and lack standardized evaluation. We present the first comprehensive benchmark and detection framework for hallucinated citations in scientific writing. Our multi-agent verification pipeline decomposes citation checking into claim extraction, evidence retrieval, passage matching, reasoning, and calibrated judgment to assess whether a cited source truly supports its claim. We construct a large-scale human-validated dataset across domains and define unified metrics for citation faithfulness and evidence alignment. Experiments with state-of-the-art LLMs reveal substantial citation errors and show that our framework significantly outperforms prior methods in both accuracy and interpretability. This work provides the first scalable infrastructure for auditing citations in the LLM era and practical tools to improve the trustworthiness of scientific references.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u9488\u5bf9\u79d1\u5b66\u5199\u4f5c\u4e2d\u5e7b\u89c9\u5f15\u7528\u7684\u7efc\u5408\u57fa\u51c6\u548c\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u9a8c\u8bc1\u7ba1\u9053\u5206\u89e3\u5f15\u7528\u68c0\u67e5\u8fc7\u7a0b\uff0c\u6784\u5efa\u5927\u89c4\u6a21\u4eba\u5de5\u9a8c\u8bc1\u6570\u636e\u96c6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u79d1\u5b66\u5199\u4f5c\u4e2d\u4f1a\u4ea7\u751f\u770b\u4f3c\u5408\u7406\u4f46\u5b9e\u9645\u4e0d\u5b58\u5728\u7684\u5e7b\u89c9\u5f15\u7528\uff0c\u8fd9\u79cd\u73b0\u8c61\u5df2\u5728\u4e3b\u8981\u673a\u5668\u5b66\u4e60\u4f1a\u8bae\u4e2d\u88ab\u89c2\u5bdf\u5230\uff0c\u66b4\u9732\u4e86\u540c\u884c\u8bc4\u5ba1\u7684\u8106\u5f31\u6027\u3002\u540c\u65f6\uff0c\u5feb\u901f\u589e\u957f\u7684\u53c2\u8003\u6587\u732e\u5217\u8868\u4f7f\u5f97\u624b\u52a8\u9a8c\u8bc1\u53d8\u5f97\u4e0d\u5207\u5b9e\u9645\uff0c\u73b0\u6709\u81ea\u52a8\u5316\u5de5\u5177\u5bf9\u566a\u58f0\u548c\u5f02\u6784\u5f15\u7528\u683c\u5f0f\u7684\u9c81\u68d2\u6027\u4e0d\u8db3\uff0c\u4e14\u7f3a\u4e4f\u6807\u51c6\u5316\u8bc4\u4f30\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u9a8c\u8bc1\u7ba1\u9053\uff0c\u5c06\u5f15\u7528\u68c0\u67e5\u5206\u89e3\u4e3a\uff1a1\uff09\u4e3b\u5f20\u63d0\u53d6\uff0c2\uff09\u8bc1\u636e\u68c0\u7d22\uff0c3\uff09\u6bb5\u843d\u5339\u914d\uff0c4\uff09\u63a8\u7406\uff0c5\uff09\u6821\u51c6\u5224\u65ad\u3002\u6784\u5efa\u4e86\u8de8\u9886\u57df\u7684\u5927\u89c4\u6a21\u4eba\u5de5\u9a8c\u8bc1\u6570\u636e\u96c6\uff0c\u5e76\u5b9a\u4e49\u4e86\u7edf\u4e00\u7684\u5f15\u7528\u5fe0\u5b9e\u5ea6\u548c\u8bc1\u636e\u5bf9\u9f50\u5ea6\u91cf\u6807\u51c6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6700\u5148\u8fdb\u7684LLMs\u5b58\u5728\u5927\u91cf\u5f15\u7528\u9519\u8bef\uff0c\u800c\u8be5\u6846\u67b6\u5728\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\u3002\u8fd9\u4e3aLLM\u65f6\u4ee3\u7684\u5f15\u7528\u5ba1\u8ba1\u63d0\u4f9b\u4e86\u9996\u4e2a\u53ef\u6269\u5c55\u7684\u57fa\u7840\u8bbe\u65bd\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u63d0\u4f9b\u4e86\u9996\u4e2a\u53ef\u6269\u5c55\u7684\u57fa\u7840\u8bbe\u65bd\u6765\u5ba1\u8ba1LLM\u65f6\u4ee3\u7684\u5f15\u7528\uff0c\u5e76\u4e3a\u63d0\u9ad8\u79d1\u5b66\u53c2\u8003\u6587\u732e\u7684\u53ef\u4fe1\u5ea6\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u589e\u5f3a\u79d1\u5b66\u7814\u7a76\u7684\u5b8c\u6574\u6027\u548c\u53ef\u4fe1\u5ea6\u3002"}}
{"id": "2602.23438", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23438", "abs": "https://arxiv.org/abs/2602.23438", "authors": ["Varun Gopal", "Rishabh Jain", "Aradhya Mathur", "Nikitha SR", "Sohan Patnaik", "Sudhir Yarram", "Mayur Hemani", "Balaji Krishnamurthy", "Mausoom Sarkar"], "title": "DesignSense: A Human Preference Dataset and Reward Modeling Framework for Graphic Layout Generation", "comment": "14 pages, 3 figures", "summary": "Graphic layouts serve as an important and engaging medium for visual communication across different channels. While recent layout generation models have demonstrated impressive capabilities, they frequently fail to align with nuanced human aesthetic judgment. Existing preference datasets and reward models trained on text-to-image generation do not generalize to layout evaluation, where the spatial arrangement of identical elements determines quality. To address this critical gap, we introduce DesignSense-10k, a large-scale dataset of 10,235 human-annotated preference pairs for graphic layout evaluation. We propose a five-stage curation pipeline that generates visually coherent layout transformations across diverse aspect ratios, using semantic grouping, layout prediction, filtering, clustering, and VLM-based refinement to produce high-quality comparison pairs. Human preferences are annotated using a 4-class scheme (left, right, both good, both bad) to capture subjective ambiguity. Leveraging this dataset, we train DesignSense, a vision-language model-based classifier that substantially outperforms existing open-source and proprietary models across comprehensive evaluation metrics (54.6% improvement in Macro F1 over the strongest proprietary baseline). Our analysis shows that frontier VLMs remain unreliable overall and fail catastrophically on the full four-class task, underscoring the need for specialized, preference-aware models. Beyond the dataset, our reward model DesignSense yields tangible downstream gains in layout generation. Using our judge during RL based training improves generator win rate by about 3%, while inference-time scaling, which involves generating multiple candidates and selecting the best one, provides a 3.6% improvement. These results highlight the practical impact of specialized, layout-aware preference modeling on real-world layout generation quality.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86DesignSense-10k\u6570\u636e\u96c6\u548cDesignSense\u6a21\u578b\uff0c\u4e13\u95e8\u7528\u4e8e\u56fe\u5f62\u5e03\u5c40\u7684\u5ba1\u7f8e\u504f\u597d\u8bc4\u4f30\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5e03\u5c40\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u5e03\u5c40\u751f\u6210\u6a21\u578b\u867d\u7136\u529f\u80fd\u5f3a\u5927\uff0c\u4f46\u5e38\u5e38\u65e0\u6cd5\u4e0e\u4eba\u7c7b\u7ec6\u5fae\u7684\u5ba1\u7f8e\u5224\u65ad\u4fdd\u6301\u4e00\u81f4\u3002\u73b0\u6709\u7684\u504f\u597d\u6570\u636e\u96c6\u548c\u5956\u52b1\u6a21\u578b\u4e3b\u8981\u9488\u5bf9\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\uff0c\u65e0\u6cd5\u63a8\u5e7f\u5230\u5e03\u5c40\u8bc4\u4f30\u9886\u57df\uff0c\u56e0\u4e3a\u5e03\u5c40\u8d28\u91cf\u53d6\u51b3\u4e8e\u76f8\u540c\u5143\u7d20\u7684\u7a7a\u95f4\u6392\u5217\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e94\u9636\u6bb5\u7b56\u5212\u6d41\u7a0b\uff1a\u8bed\u4e49\u5206\u7ec4\u3001\u5e03\u5c40\u9884\u6d4b\u3001\u8fc7\u6ee4\u3001\u805a\u7c7b\u548c\u57fa\u4e8eVLM\u7684\u7ec6\u5316\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u6bd4\u8f83\u5bf9\u3002\u4f7f\u75284\u7c7b\u6807\u6ce8\u65b9\u6848\uff08\u5de6\u597d\u3001\u53f3\u597d\u3001\u90fd\u597d\u3001\u90fd\u5dee\uff09\u6355\u6349\u4e3b\u89c2\u6a21\u7cca\u6027\u3002\u57fa\u4e8e\u6b64\u6570\u636e\u96c6\u8bad\u7ec3\u4e86DesignSense\uff0c\u4e00\u4e2a\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5206\u7c7b\u5668\u3002", "result": "DesignSense\u5728\u7efc\u5408\u8bc4\u4f30\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u5f00\u6e90\u548c\u4e13\u6709\u6a21\u578b\uff08\u5728\u6700\u5f3a\u4e13\u6709\u57fa\u7ebf\u4e0aMacro F1\u63d0\u534754.6%\uff09\u3002\u524d\u6cbfVLM\u5728\u56db\u7c7b\u4efb\u52a1\u4e0a\u6574\u4f53\u4e0d\u53ef\u9760\u4e14\u8868\u73b0\u707e\u96be\u6027\u3002\u4f7f\u7528DesignSense\u8fdb\u884cRL\u8bad\u7ec3\u65f6\u751f\u6210\u5668\u80dc\u7387\u63d0\u5347\u7ea63%\uff0c\u63a8\u7406\u65f6\u7f29\u653e\uff08\u751f\u6210\u591a\u4e2a\u5019\u9009\u5e76\u9009\u62e9\u6700\u4f73\uff09\u63d0\u4f9b3.6%\u7684\u6539\u8fdb\u3002", "conclusion": "\u4e13\u4e1a\u5316\u7684\u3001\u5e03\u5c40\u611f\u77e5\u7684\u504f\u597d\u5efa\u6a21\u5bf9\u5b9e\u9645\u5e03\u5c40\u751f\u6210\u8d28\u91cf\u5177\u6709\u91cd\u8981\u5f71\u54cd\uff0cDesignSense-10k\u6570\u636e\u96c6\u548cDesignSense\u6a21\u578b\u586b\u8865\u4e86\u56fe\u5f62\u5e03\u5c40\u8bc4\u4f30\u7684\u5173\u952e\u7a7a\u767d\u3002"}}
{"id": "2602.23391", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23391", "abs": "https://arxiv.org/abs/2602.23391", "authors": ["Nazanin Mohammadi Sepahvand", "Eleni Triantafillou", "Hugo Larochelle", "Doina Precup", "Daniel M. Roy", "Gintare Karolina Dziugaite"], "title": "Detoxifying LLMs via Representation Erasure-Based Preference Optimization", "comment": null, "summary": "Large language models (LLMs) trained on webscale data can produce toxic outputs, raising concerns for safe deployment. Prior defenses, based on applications of DPO, NPO, and similar algorithms, reduce the likelihood of harmful continuations, but not robustly so: they are vulnerable to adversarial prompting and easily undone by fine-tuning-based relearning attacks. Indeed, research has shown that these edits to the model are superficial: linear probing reveals that harmful \"directions\" remain present in representations. To address this, we propose Representation Erasure-based Preference Optimization (REPO), reformulating detoxification as a token-level preference problem. Using a novel objective with preference data, we force the representations of toxic continuations to converge toward their benign counterparts. Our mechanistic analysis reveals that this granular approach is critical: unlike baselines, REPO induces deep, localized edits to toxicity-encoding neurons while preserving general model utility. Exhaustive evaluations show that REPO achieves state-of-the-art robustness, stopping sophisticated threats-including relearning attacks and enhanced GCG jailbreaks-where existing representation- and output-based methods fail.", "AI": {"tldr": "REPO\u662f\u4e00\u79cd\u65b0\u7684\u8bed\u8a00\u6a21\u578b\u53bb\u6bd2\u65b9\u6cd5\uff0c\u901a\u8fc7\u8868\u793a\u64e6\u9664\u504f\u597d\u4f18\u5316\u5728token\u7ea7\u522b\u5904\u7406\u6bd2\u6027\u95ee\u9898\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u80fd\u66f4\u6df1\u5ea6\u5730\u7f16\u8f91\u6bd2\u6027\u7f16\u7801\u795e\u7ecf\u5143\uff0c\u5b9e\u73b0\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u9632\u5fa1\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eDPO\u3001NPO\u7b49\u7b97\u6cd5\u7684\u53bb\u6bd2\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u5b83\u4eec\u53ea\u80fd\u964d\u4f4e\u6709\u5bb3\u8f93\u51fa\u7684\u53ef\u80fd\u6027\uff0c\u4f46\u4e0d\u591f\u9c81\u68d2\uff0c\u5bb9\u6613\u53d7\u5230\u5bf9\u6297\u6027\u63d0\u793a\u548c\u5fae\u8c03\u518d\u5b66\u4e60\u653b\u51fb\u7684\u5f71\u54cd\u3002\u7814\u7a76\u8868\u660e\u8fd9\u4e9b\u7f16\u8f91\u662f\u8868\u9762\u7684\uff0c\u6709\u5bb3\"\u65b9\u5411\"\u4ecd\u7136\u5b58\u5728\u4e8e\u8868\u793a\u4e2d\u3002", "method": "\u63d0\u51fa\u8868\u793a\u64e6\u9664\u504f\u597d\u4f18\u5316\uff08REPO\uff09\uff0c\u5c06\u53bb\u6bd2\u91cd\u65b0\u5b9a\u4e49\u4e3atoken\u7ea7\u522b\u7684\u504f\u597d\u95ee\u9898\u3002\u4f7f\u7528\u5e26\u6709\u504f\u597d\u6570\u636e\u7684\u65b0\u76ee\u6807\u51fd\u6570\uff0c\u5f3a\u5236\u6709\u6bd2\u5ef6\u7eed\u7684\u8868\u793a\u5411\u826f\u6027\u5bf9\u5e94\u7269\u6536\u655b\u3002\u8fd9\u79cd\u65b9\u6cd5\u5728\u673a\u5236\u4e0a\u5b9e\u73b0\u6df1\u5ea6\u3001\u5c40\u90e8\u5316\u7684\u6bd2\u6027\u7f16\u7801\u795e\u7ecf\u5143\u7f16\u8f91\u3002", "result": "REPO\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u9c81\u68d2\u6027\uff0c\u80fd\u591f\u963b\u6b62\u590d\u6742\u7684\u5a01\u80c1\uff0c\u5305\u62ec\u518d\u5b66\u4e60\u653b\u51fb\u548c\u589e\u5f3a\u7684GCG\u8d8a\u72f1\u653b\u51fb\uff0c\u800c\u73b0\u6709\u7684\u57fa\u4e8e\u8868\u793a\u548c\u8f93\u51fa\u7684\u65b9\u6cd5\u90fd\u5931\u8d25\u4e86\u3002\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7684\u4e00\u822c\u6548\u7528\u3002", "conclusion": "REPO\u901a\u8fc7token\u7ea7\u522b\u7684\u8868\u793a\u64e6\u9664\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u6df1\u5ea6\u7684\u53bb\u6bd2\u6548\u679c\uff0c\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u7684\u9632\u5fa1\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u8bed\u8a00\u6a21\u578b\u5b89\u5168\u90e8\u7f72\u4e2d\u7684\u5173\u952e\u95ee\u9898\u3002"}}
{"id": "2602.23367", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.23367", "abs": "https://arxiv.org/abs/2602.23367", "authors": ["Shubh Laddha", "Lucas Changbencharoen", "Win Kuptivej", "Surya Shringla", "Archana Vaidheeswaran", "Yash Bhaskar"], "title": "HumanMCP: A Human-Like Query Dataset for Evaluating MCP Tool Retrieval Performance", "comment": "4 pages, 2 figures, 3 tables", "summary": "Model Context Protocol (MCP) servers contain a collection of thousands of open-source standardized tools, linking LLMs to external systems; however, existing datasets and benchmarks lack realistic, human-like user queries, remaining a critical gap in evaluating the tool usage and ecosystems of MCP servers. Existing datasets often do contain tool descriptions but fail to represent how different users portray their requests, leading to poor generalization and inflated reliability of certain benchmarks. This paper introduces the first large-scale MCP dataset featuring diverse, high-quality diverse user queries generated specifically to match 2800 tools across 308 MCP servers, developing on the MCP Zero dataset. Each tool is paired with multiple unique user personas that we have generated, to capture varying levels of user intent ranging from precise task requests, and ambiguous, exploratory commands, reflecting the complexity of real-world interaction patterns.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u5927\u89c4\u6a21MCP\u6570\u636e\u96c6\uff0c\u5305\u542b\u9488\u5bf92800\u4e2a\u5de5\u5177\u751f\u6210\u7684\u591a\u6837\u5316\u3001\u9ad8\u8d28\u91cf\u7528\u6237\u67e5\u8be2\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6570\u636e\u96c6\u7f3a\u4e4f\u771f\u5b9e\u7528\u6237\u67e5\u8be2\u6a21\u5f0f\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709MCP\u670d\u52a1\u5668\u6570\u636e\u96c6\u548c\u57fa\u51c6\u6d4b\u8bd5\u7f3a\u4e4f\u771f\u5b9e\u3001\u4eba\u6027\u5316\u7684\u7528\u6237\u67e5\u8be2\uff0c\u65e0\u6cd5\u53cd\u6620\u4e0d\u540c\u7528\u6237\u5982\u4f55\u8868\u8fbe\u8bf7\u6c42\uff0c\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u5dee\u548c\u57fa\u51c6\u6d4b\u8bd5\u53ef\u9760\u6027\u88ab\u5938\u5927\u3002", "method": "\u57fa\u4e8eMCP Zero\u6570\u636e\u96c6\uff0c\u4e3a308\u4e2aMCP\u670d\u52a1\u5668\u4e2d\u76842800\u4e2a\u5de5\u5177\u751f\u6210\u591a\u6837\u5316\u3001\u9ad8\u8d28\u91cf\u7684\u7528\u6237\u67e5\u8be2\uff0c\u6bcf\u4e2a\u5de5\u5177\u914d\u4ee5\u591a\u4e2a\u72ec\u7279\u7684\u7528\u6237\u89d2\u8272\uff0c\u6355\u6349\u4ece\u7cbe\u786e\u4efb\u52a1\u8bf7\u6c42\u5230\u6a21\u7cca\u63a2\u7d22\u6027\u547d\u4ee4\u7684\u4e0d\u540c\u7528\u6237\u610f\u56fe\u3002", "result": "\u521b\u5efa\u4e86\u9996\u4e2a\u5927\u89c4\u6a21MCP\u6570\u636e\u96c6\uff0c\u5305\u542b\u9488\u5bf92800\u4e2a\u5de5\u5177\u751f\u6210\u7684\u591a\u6837\u5316\u7528\u6237\u67e5\u8be2\uff0c\u53cd\u6620\u4e86\u771f\u5b9e\u4e16\u754c\u4ea4\u4e92\u6a21\u5f0f\u7684\u590d\u6742\u6027\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u586b\u8865\u4e86MCP\u670d\u52a1\u5668\u5de5\u5177\u4f7f\u7528\u548c\u751f\u6001\u7cfb\u7edf\u8bc4\u4f30\u7684\u5173\u952e\u7a7a\u767d\uff0c\u4e3a\u66f4\u51c6\u786e\u5730\u8bc4\u4f30\u5de5\u5177\u4f7f\u7528\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2602.23479", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.23479", "abs": "https://arxiv.org/abs/2602.23479", "authors": ["Michael Frew", "Nishit Bheda", "Bryan Tripp"], "title": "FHIRPath-QA: Executable Question Answering over FHIR Electronic Health Records", "comment": "Submitted to LREC 2026 CL4Health Workshop", "summary": "Though patients are increasingly granted digital access to their electronic health records (EHRs), existing interfaces may not support precise, trustworthy answers to patient-specific questions. Large language models (LLM) show promise in clinical question answering (QA), but retrieval-based approaches are computationally inefficient, prone to hallucination, and difficult to deploy over real-life EHRs. In this work, we introduce FHIRPath-QA, the first open dataset and benchmark for patient-specific QA that includes open-standard FHIRPath queries over real-world clinical data. We propose a text-to-FHIRPath QA paradigm that shifts reasoning from free-text generation to FHIRPath query synthesis, significantly reducing LLM usage. Built on MIMIC-IV on FHIR Demo, the dataset pairs over 14k natural language questions in patient and clinician phrasing with validated FHIRPath queries and answers. Further, we demonstrate that state-of-the-art LLMs struggle to deal with ambiguity in patient language and perform poorly in FHIRPath query synthesis. However, they benefit strongly from supervised fine-tuning. Our results highlight that text-to-FHIRPath synthesis has the potential to serve as a practical foundation for safe, efficient, and interoperable consumer health applications, and our dataset and benchmark serve as a starting point for future research on the topic. The full dataset and generation code is available at: https://github.com/mooshifrew/fhirpath-qa.", "AI": {"tldr": "FHIRPath-QA\u662f\u9996\u4e2a\u9488\u5bf9\u60a3\u8005\u7279\u5b9a\u95ee\u9898\u7684\u5f00\u653e\u6570\u636e\u96c6\u548c\u57fa\u51c6\uff0c\u91c7\u7528text-to-FHIRPath\u67e5\u8be2\u5408\u6210\u8303\u5f0f\uff0c\u5c06\u4e34\u5e8a\u95ee\u7b54\u4ece\u81ea\u7531\u6587\u672c\u751f\u6210\u8f6c\u5411\u7ed3\u6784\u5316\u67e5\u8be2\uff0c\u663e\u8457\u51cf\u5c11LLM\u4f7f\u7528\u5e76\u63d0\u9ad8\u5b89\u5168\u6027\u548c\u6548\u7387\u3002", "motivation": "\u5c3d\u7ba1\u60a3\u8005\u8d8a\u6765\u8d8a\u591a\u5730\u83b7\u5f97\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u7684\u6570\u5b57\u8bbf\u95ee\u6743\u9650\uff0c\u4f46\u73b0\u6709\u754c\u9762\u53ef\u80fd\u65e0\u6cd5\u63d0\u4f9b\u7cbe\u786e\u3001\u53ef\u4fe1\u7684\u60a3\u8005\u7279\u5b9a\u95ee\u9898\u7b54\u6848\u3002\u57fa\u4e8e\u68c0\u7d22\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u65b9\u6cd5\u5b58\u5728\u8ba1\u7b97\u6548\u7387\u4f4e\u3001\u6613\u4ea7\u751f\u5e7b\u89c9\u3001\u96be\u4ee5\u5728\u5b9e\u9645EHR\u4e0a\u90e8\u7f72\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51fatext-to-FHIRPath\u95ee\u7b54\u8303\u5f0f\uff0c\u5c06\u63a8\u7406\u4ece\u81ea\u7531\u6587\u672c\u751f\u6210\u8f6c\u5411FHIRPath\u67e5\u8be2\u5408\u6210\u3002\u57fa\u4e8eMIMIC-IV on FHIR Demo\u6784\u5efa\u6570\u636e\u96c6\uff0c\u5305\u542b\u8d85\u8fc714k\u4e2a\u81ea\u7136\u8bed\u8a00\u95ee\u9898\uff08\u60a3\u8005\u548c\u4e34\u5e8a\u533b\u751f\u8868\u8ff0\uff09\u4e0e\u7ecf\u8fc7\u9a8c\u8bc1\u7684FHIRPath\u67e5\u8be2\u548c\u7b54\u6848\u3002", "result": "\u6700\u5148\u8fdb\u7684LLM\u5728\u5904\u7406\u60a3\u8005\u8bed\u8a00\u6a21\u7cca\u6027\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u5728FHIRPath\u67e5\u8be2\u5408\u6210\u4e2d\u8868\u73b0\u8f83\u5dee\uff0c\u4f46\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u540e\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002text-to-FHIRPath\u5408\u6210\u6709\u6f5c\u529b\u4f5c\u4e3a\u5b89\u5168\u3001\u9ad8\u6548\u3001\u53ef\u4e92\u64cd\u4f5c\u7684\u6d88\u8d39\u8005\u5065\u5eb7\u5e94\u7528\u57fa\u7840\u3002", "conclusion": "FHIRPath-QA\u6570\u636e\u96c6\u548c\u57fa\u51c6\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u8d77\u70b9\uff0ctext-to-FHIRPath\u5408\u6210\u8303\u5f0f\u6709\u671b\u6210\u4e3a\u5b9e\u7528\u3001\u5b89\u5168\u3001\u9ad8\u6548\u7684\u4e34\u5e8a\u95ee\u7b54\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u5408\u6d88\u8d39\u8005\u5065\u5eb7\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2602.24247", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.24247", "abs": "https://arxiv.org/abs/2602.24247", "authors": ["Mihir Sinha", "Kriti Thakur", "Prasanta K. Panigrahi", "Alivelu Manga Parimi", "Mayukha Pal"], "title": "Data-Driven Linearization based Arc Fault Prediction in Medium Voltage Electrical Distribution System", "comment": null, "summary": "High-impedance arc faults (HIAFs) in medium-voltage electrical distribution systems are difficult to detect due to their low fault current levels and nonlinear transient behavior. Traditional detection algorithms generally struggle with predictions under dynamic waveform scenarios. This research provides our approach of using a unique data-driven linearization (DDL) framework for early prediction of HIAFs, giving both interpretability and scalability. The proposed method translates nonlinear current waveforms into a linearized space using coordinate embeddings and polynomial transformation, enabling precise modelling of fault precursors.The total duration of the test waveform is 0.5 seconds, within which the arc fault occurs between 0.2 seconds to 0.3 seconds. Our proposed approach using DDL, trained solely on the pre-fault healthy region (0.10 seconds to 0.18 seconds) effectively captures certain invisible fault precursors, to accurately predict the onset of fault at 0.189 seconds, which is approximately 0.011 seconds (i.e., 11 milliseconds) earlier than the actual fault occurrence. In particular, the framework predicts the start of arc faults at 0.189 seconds, significantly earlier of the actual fault incidence at 0.200 seconds, demonstrating substantial early warning capability. Performance evaluation comprises eigenvalue analysis, prediction error measures, error growth rate and waveform regeneration fidelity. Such early prediction proves that the model is capable of correctly foreseeing faults which is especially helpful in preventing real-world faults and accidents. It confirms that our proposed approach reliably predicts arc faults in medium-voltage power distribution systems", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u6570\u636e\u9a71\u52a8\u7ebf\u6027\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u4e2d\u538b\u914d\u7535\u7cfb\u7edf\u4e2d\u9ad8\u963b\u6297\u7535\u5f27\u6545\u969c\u7684\u65e9\u671f\u9884\u6d4b\uff0c\u4ec5\u4f7f\u7528\u6545\u969c\u524d\u5065\u5eb7\u533a\u57df\u6570\u636e\u5373\u53ef\u63d0\u524d11\u6beb\u79d2\u9884\u6d4b\u6545\u969c\u53d1\u751f", "motivation": "\u4e2d\u538b\u914d\u7535\u7cfb\u7edf\u4e2d\u7684\u9ad8\u963b\u6297\u7535\u5f27\u6545\u969c\u7531\u4e8e\u6545\u969c\u7535\u6d41\u6c34\u5e73\u4f4e\u548c\u975e\u7ebf\u6027\u77ac\u6001\u884c\u4e3a\u800c\u96be\u4ee5\u68c0\u6d4b\uff0c\u4f20\u7edf\u68c0\u6d4b\u7b97\u6cd5\u5728\u52a8\u6001\u6ce2\u5f62\u573a\u666f\u4e0b\u9884\u6d4b\u56f0\u96be\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u65e9\u671f\u9884\u6d4b\u6545\u969c\u5e76\u63d0\u4f9b\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u6269\u5c55\u6027\u7684\u65b9\u6cd5", "method": "\u91c7\u7528\u6570\u636e\u9a71\u52a8\u7ebf\u6027\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5750\u6807\u5d4c\u5165\u548c\u591a\u9879\u5f0f\u53d8\u6362\u5c06\u975e\u7ebf\u6027\u7535\u6d41\u6ce2\u5f62\u8f6c\u6362\u4e3a\u7ebf\u6027\u5316\u7a7a\u95f4\uff0c\u4ec5\u4f7f\u7528\u6545\u969c\u524d\u5065\u5eb7\u533a\u57df\uff080.10\u79d2\u81f30.18\u79d2\uff09\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u6709\u6548\u6355\u6349\u4e0d\u53ef\u89c1\u7684\u6545\u969c\u524d\u5146", "result": "\u8be5\u65b9\u6cd5\u57280.189\u79d2\u9884\u6d4b\u5230\u7535\u5f27\u6545\u969c\u5f00\u59cb\uff0c\u6bd4\u5b9e\u9645\u6545\u969c\u53d1\u751f\u65f6\u95f40.200\u79d2\u63d0\u524d11\u6beb\u79d2\uff0c\u8868\u73b0\u51fa\u663e\u8457\u7684\u65e9\u671f\u9884\u8b66\u80fd\u529b\u3002\u901a\u8fc7\u7279\u5f81\u503c\u5206\u6790\u3001\u9884\u6d4b\u8bef\u5dee\u6d4b\u91cf\u3001\u8bef\u5dee\u589e\u957f\u7387\u548c\u6ce2\u5f62\u518d\u751f\u4fdd\u771f\u5ea6\u7b49\u6027\u80fd\u8bc4\u4f30\uff0c\u8bc1\u5b9e\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027", "conclusion": "\u63d0\u51fa\u7684\u6570\u636e\u9a71\u52a8\u7ebf\u6027\u5316\u6846\u67b6\u80fd\u591f\u53ef\u9760\u9884\u6d4b\u4e2d\u538b\u914d\u7535\u7cfb\u7edf\u4e2d\u7684\u7535\u5f27\u6545\u969c\uff0c\u7279\u522b\u662f\u80fd\u591f\u63d0\u524d\u9884\u6d4b\u6545\u969c\uff0c\u8fd9\u5bf9\u4e8e\u9884\u9632\u5b9e\u9645\u6545\u969c\u548c\u4e8b\u6545\u7279\u522b\u6709\u5e2e\u52a9\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u4ef7\u503c"}}
{"id": "2602.23408", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23408", "abs": "https://arxiv.org/abs/2602.23408", "authors": ["Yuchun Feng", "Jinliang Zheng", "Zhihao Wang", "Dongxiu Liu", "Jianxiong Li", "Jiangmiao Pang", "Tai Wang", "Xianyuan Zhan"], "title": "Demystifying Action Space Design for Robotic Manipulation Policies", "comment": null, "summary": "The specification of the action space plays a pivotal role in imitation-based robotic manipulation policy learning, fundamentally shaping the optimization landscape of policy learning. While recent advances have focused heavily on scaling training data and model capacity, the choice of action space remains guided by ad-hoc heuristics or legacy designs, leading to an ambiguous understanding of robotic policy design philosophies. To address this ambiguity, we conducted a large-scale and systematic empirical study, confirming that the action space does have significant and complex impacts on robotic policy learning. We dissect the action design space along temporal and spatial axes, facilitating a structured analysis of how these choices govern both policy learnability and control stability. Based on 13,000+ real-world rollouts on a bimanual robot and evaluation on 500+ trained models over four scenarios, we examine the trade-offs between absolute vs. delta representations, and joint-space vs. task-space parameterizations. Our large-scale results suggest that properly designing the policy to predict delta actions consistently improves performance, while joint-space and task-space representations offer complementary strengths, favoring control stability and generalization, respectively.", "AI": {"tldr": "\u52a8\u4f5c\u7a7a\u95f4\u8bbe\u8ba1\u5bf9\u57fa\u4e8e\u6a21\u4eff\u5b66\u4e60\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u5b66\u4e60\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u672c\u6587\u901a\u8fc7\u5927\u89c4\u6a21\u5b9e\u8bc1\u7814\u7a76\u5206\u6790\u4e86\u52a8\u4f5c\u7a7a\u95f4\u5728\u65f6\u95f4\u548c\u7a7a\u95f4\u7ef4\u5ea6\u4e0a\u7684\u8bbe\u8ba1\u9009\u62e9\u5bf9\u7b56\u7565\u5b66\u4e60\u7684\u5f71\u54cd\u3002", "motivation": "\u5f53\u524d\u673a\u5668\u4eba\u7b56\u7565\u5b66\u4e60\u4e2d\uff0c\u52a8\u4f5c\u7a7a\u95f4\u7684\u9009\u62e9\u4e3b\u8981\u57fa\u4e8e\u7ecf\u9a8c\u6cd5\u5219\u6216\u4f20\u7edf\u8bbe\u8ba1\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7684\u7406\u89e3\u3002\u867d\u7136\u8fd1\u671f\u7814\u7a76\u96c6\u4e2d\u5728\u6269\u5c55\u8bad\u7ec3\u6570\u636e\u548c\u6a21\u578b\u5bb9\u91cf\u4e0a\uff0c\u4f46\u52a8\u4f5c\u7a7a\u95f4\u8bbe\u8ba1\u5bf9\u4f18\u5316\u666f\u89c2\u7684\u5f71\u54cd\u4ecd\u4e0d\u660e\u786e\u3002", "method": "\u8fdb\u884c\u4e86\u5927\u89c4\u6a21\u7cfb\u7edf\u6027\u5b9e\u8bc1\u7814\u7a76\uff0c\u5c06\u52a8\u4f5c\u8bbe\u8ba1\u7a7a\u95f4\u6cbf\u65f6\u95f4\u548c\u7a7a\u95f4\u7ef4\u5ea6\u5206\u89e3\uff0c\u5206\u6790\u4e86\u7edd\u5bf9vs.\u589e\u91cf\u8868\u793a\u3001\u5173\u8282\u7a7a\u95f4vs.\u4efb\u52a1\u7a7a\u95f4\u53c2\u6570\u5316\u7684\u6743\u8861\u3002\u57fa\u4e8e\u53cc\u81c2\u673a\u5668\u4eba\u768413,000+\u6b21\u771f\u5b9e\u4e16\u754c\u6d4b\u8bd5\u548c500+\u4e2a\u8bad\u7ec3\u6a21\u578b\u5728\u56db\u4e2a\u573a\u666f\u4e2d\u7684\u8bc4\u4f30\u3002", "result": "\u52a8\u4f5c\u7a7a\u95f4\u786e\u5b9e\u5bf9\u673a\u5668\u4eba\u7b56\u7565\u5b66\u4e60\u6709\u663e\u8457\u4e14\u590d\u6742\u7684\u5f71\u54cd\u3002\u8bbe\u8ba1\u7b56\u7565\u9884\u6d4b\u589e\u91cf\u52a8\u4f5c\u80fd\u6301\u7eed\u63d0\u5347\u6027\u80fd\uff0c\u800c\u5173\u8282\u7a7a\u95f4\u548c\u4efb\u52a1\u7a7a\u95f4\u8868\u793a\u5404\u6709\u4f18\u52bf\uff1a\u5173\u8282\u7a7a\u95f4\u6709\u5229\u4e8e\u63a7\u5236\u7a33\u5b9a\u6027\uff0c\u4efb\u52a1\u7a7a\u95f4\u6709\u5229\u4e8e\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u52a8\u4f5c\u7a7a\u95f4\u8bbe\u8ba1\u5728\u673a\u5668\u4eba\u7b56\u7565\u5b66\u4e60\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u5730\u8003\u8651\u65f6\u95f4\u548c\u7a7a\u95f4\u7ef4\u5ea6\u7684\u8bbe\u8ba1\u9009\u62e9\u3002\u589e\u91cf\u52a8\u4f5c\u8868\u793a\u901a\u5e38\u66f4\u4f18\uff0c\u800c\u5173\u8282\u7a7a\u95f4\u548c\u4efb\u52a1\u7a7a\u95f4\u53c2\u6570\u5316\u5404\u6709\u9002\u7528\u573a\u666f\uff0c\u5e94\u6839\u636e\u5177\u4f53\u9700\u6c42\u5e73\u8861\u63a7\u5236\u7a33\u5b9a\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2602.23514", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23514", "abs": "https://arxiv.org/abs/2602.23514", "authors": ["Mike Middleton", "Teymoor Ali", "Hakan Kayan", "Basabdatta Sen Bhattacharya", "Charith Perera", "Oliver Rhodes", "Elena Gheorghiu", "Mark Vousden", "Martin A. Trefzer"], "title": "Modelling and Simulation of Neuromorphic Datasets for Anomaly Detection in Computer Vision", "comment": "draft paper", "summary": "Limitations on the availability of Dynamic Vision Sensors (DVS) present a fundamental challenge to researchers of neuromorphic computer vision applications. In response, datasets have been created by the research community, but often contain a limited number of samples or scenarios. To address the lack of a comprehensive simulator of neuromorphic vision datasets, we introduce the Anomalous Neuromorphic Tool for Shapes (ANTShapes), a novel dataset simulation framework. Built in the Unity engine, ANTShapes simulates abstract, configurable 3D scenes populated by objects displaying randomly-generated behaviours describing attributes such as motion and rotation. The sampling of object behaviours, and the labelling of anomalously-acting objects, is a statistical process following central limit theorem principles. Datasets containing an arbitrary number of samples can be created and exported from ANTShapes, along with accompanying label and frame data, through the adjustment of a limited number of parameters within the software. ANTShapes addresses the limitations of data availability to researchers of event-based computer vision by allowing for the simulation of bespoke datasets to suit purposes including object recognition and localisation alongside anomaly detection.", "AI": {"tldr": "ANTShapes\u662f\u4e00\u4e2a\u57fa\u4e8eUnity\u5f15\u64ce\u7684\u795e\u7ecf\u5f62\u6001\u89c6\u89c9\u6570\u636e\u96c6\u6a21\u62df\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u53ef\u914d\u7f6e\u76843D\u573a\u666f\u548c\u5f02\u5e38\u884c\u4e3a\u5bf9\u8c61\uff0c\u89e3\u51b3DVS\u4f20\u611f\u5668\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002", "motivation": "\u52a8\u6001\u89c6\u89c9\u4f20\u611f\u5668\uff08DVS\uff09\u7684\u53ef\u7528\u6027\u6709\u9650\uff0c\u73b0\u6709\u6570\u636e\u96c6\u6837\u672c\u6570\u91cf\u5c11\u3001\u573a\u666f\u5355\u4e00\uff0c\u7f3a\u4e4f\u5168\u9762\u7684\u795e\u7ecf\u5f62\u6001\u89c6\u89c9\u6570\u636e\u96c6\u6a21\u62df\u5de5\u5177\u3002", "method": "\u57fa\u4e8eUnity\u5f15\u64ce\u6784\u5efa\u62bd\u8c61\u53ef\u914d\u7f6e\u76843D\u573a\u666f\uff0c\u5bf9\u8c61\u5177\u6709\u968f\u673a\u751f\u6210\u7684\u8fd0\u52a8\u548c\u65cb\u8f6c\u7b49\u884c\u4e3a\u5c5e\u6027\uff0c\u901a\u8fc7\u4e2d\u5fc3\u6781\u9650\u5b9a\u7406\u539f\u7406\u8fdb\u884c\u884c\u4e3a\u91c7\u6837\u548c\u5f02\u5e38\u5bf9\u8c61\u6807\u8bb0\u3002", "result": "ANTShapes\u80fd\u591f\u901a\u8fc7\u8c03\u6574\u5c11\u91cf\u53c2\u6570\u521b\u5efa\u4efb\u610f\u6570\u91cf\u7684\u6570\u636e\u96c6\u6837\u672c\uff0c\u5e76\u5bfc\u51fa\u76f8\u5e94\u7684\u6807\u7b7e\u548c\u5e27\u6570\u636e\uff0c\u652f\u6301\u5bf9\u8c61\u8bc6\u522b\u3001\u5b9a\u4f4d\u548c\u5f02\u5e38\u68c0\u6d4b\u7b49\u591a\u79cd\u5e94\u7528\u3002", "conclusion": "ANTShapes\u89e3\u51b3\u4e86\u4e8b\u4ef6\u578b\u8ba1\u7b97\u673a\u89c6\u89c9\u7814\u7a76\u4e2d\u7684\u6570\u636e\u53ef\u7528\u6027\u95ee\u9898\uff0c\u5141\u8bb8\u7814\u7a76\u4eba\u5458\u6a21\u62df\u5b9a\u5236\u5316\u6570\u636e\u96c6\u4ee5\u6ee1\u8db3\u7279\u5b9a\u7814\u7a76\u9700\u6c42\u3002"}}
{"id": "2602.23400", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23400", "abs": "https://arxiv.org/abs/2602.23400", "authors": ["Zezheng Wu", "Rui Wang", "Xinghe Cheng", "Yang Shao", "Qing Yang", "Jiapu Wang", "Jingwei Zhang"], "title": "U-CAN: Utility-Aware Contrastive Attenuation for Efficient Unlearning in Generative Recommendation", "comment": null, "summary": "Generative Recommendation (GenRec) typically leverages Large Language Models (LLMs) to redefine personalization as an instruction-driven sequence generation task. However, fine-tuning on user logs inadvertently encodes sensitive attributes into model parameters, raising critical privacy concerns. Existing Machine Unlearning (MU) techniques struggle to navigate this tension due to the Polysemy Dilemma, where neurons superimpose sensitive data with general reasoning patterns, leading to catastrophic utility loss under traditional gradient or pruning methods. To address this, we propose Utility-aware Contrastive AttenuatioN (U-CAN), a precision unlearning framework that operates on low-rank adapters. U-CAN quantifies risk by contrasting activations and focuses on neurons with asymmetric responses that are highly sensitive to the forgetting set but suppressed on the retention set. To safeguard performance, we introduce a utility-aware calibration mechanism that combines weight magnitudes with retention-set activation norms, assigning higher utility scores to dimensions that contribute strongly to retention performance. Unlike binary pruning, which often fragments network structure, U-CAN develop adaptive soft attenuation with a differentiable decay function to selectively down-scale high-risk parameters on LoRA adapters, suppressing sensitive retrieval pathways and preserving the topological connectivity of reasoning circuits. Experiments on two public datasets across seven metrics demonstrate that U-CAN achieves strong privacy forgetting, utility retention, and computational efficiency.", "AI": {"tldr": "U-CAN\u662f\u4e00\u4e2a\u7528\u4e8e\u751f\u6210\u5f0f\u63a8\u8350\u7cfb\u7edf\u7684\u9690\u79c1\u4fdd\u62a4\u6846\u67b6\uff0c\u901a\u8fc7\u4f4e\u79e9\u9002\u914d\u5668\u7684\u5bf9\u6bd4\u8870\u51cf\u673a\u5236\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\u9009\u62e9\u6027\u9057\u5fd8\u654f\u611f\u6570\u636e\u3002", "motivation": "\u751f\u6210\u5f0f\u63a8\u8350\u7cfb\u7edf\u5728\u5fae\u8c03\u65f6\u4f1a\u65e0\u610f\u4e2d\u5c06\u654f\u611f\u5c5e\u6027\u7f16\u7801\u5230\u6a21\u578b\u53c2\u6570\u4e2d\uff0c\u5f15\u53d1\u9690\u79c1\u62c5\u5fe7\u3002\u73b0\u6709\u7684\u673a\u5668\u5b66\u4e60\u9057\u5fd8\u6280\u672f\u9762\u4e34\u591a\u4e49\u6027\u56f0\u5883\uff0c\u5bfc\u81f4\u707e\u96be\u6027\u7684\u6548\u7528\u635f\u5931\u3002", "method": "\u63d0\u51faUtility-aware Contrastive AttenuatioN (U-CAN)\u6846\u67b6\uff0c\u5728\u4f4e\u79e9\u9002\u914d\u5668\u4e0a\u64cd\u4f5c\u3002\u901a\u8fc7\u5bf9\u6bd4\u6fc0\u6d3b\u91cf\u5316\u98ce\u9669\uff0c\u8bc6\u522b\u5bf9\u9057\u5fd8\u96c6\u654f\u611f\u4f46\u5bf9\u4fdd\u7559\u96c6\u6291\u5236\u7684\u795e\u7ecf\u5143\u3002\u5f15\u5165\u6548\u7528\u611f\u77e5\u6821\u51c6\u673a\u5236\uff0c\u7ed3\u5408\u6743\u91cd\u5927\u5c0f\u548c\u4fdd\u7559\u96c6\u6fc0\u6d3b\u89c4\u8303\uff0c\u4e3a\u5bf9\u4fdd\u7559\u6027\u80fd\u8d21\u732e\u5927\u7684\u7ef4\u5ea6\u5206\u914d\u66f4\u9ad8\u7684\u6548\u7528\u5206\u6570\u3002\u91c7\u7528\u81ea\u9002\u5e94\u8f6f\u8870\u51cf\u548c\u53ef\u5fae\u8870\u51cf\u51fd\u6570\uff0c\u9009\u62e9\u6027\u964d\u4f4eLoRA\u9002\u914d\u5668\u4e0a\u7684\u9ad8\u98ce\u9669\u53c2\u6570\u3002", "result": "\u5728\u4e24\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u7684\u4e03\u4e2a\u6307\u6807\u5b9e\u9a8c\u8868\u660e\uff0cU-CAN\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u9690\u79c1\u9057\u5fd8\u3001\u6548\u7528\u4fdd\u7559\u548c\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "U-CAN\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u751f\u6210\u5f0f\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u9690\u79c1\u4fdd\u62a4\u95ee\u9898\uff0c\u901a\u8fc7\u7cbe\u786e\u7684\u5bf9\u6bd4\u8870\u51cf\u673a\u5236\u5728\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u654f\u611f\u6570\u636e\u9057\u5fd8\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u68af\u5ea6\u6216\u526a\u679d\u65b9\u6cd5\u5e26\u6765\u7684\u707e\u96be\u6027\u6548\u7528\u635f\u5931\u3002"}}
{"id": "2602.23457", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.23457", "abs": "https://arxiv.org/abs/2602.23457", "authors": ["Annan Zhang", "Hanna Matusik", "Miguel Flores-Acton", "Emily R. Sologuren", "Joshua Jacob", "Daniela Rus"], "title": "Printed helicoids with embedded air channels make sensorized segments for soft continuum robots", "comment": "Accepted for publication in the proceedings of the 2026 IEEE 9th International Conference on Soft Robotics (RoboSoft)", "summary": "Soft robots enable safe, adaptive interaction with complex environments but remain difficult to sense and control due to their highly deformable structures. Architected soft materials such as helicoid lattices offer tunable stiffness and strength but are challenging to instrument because of their sparse geometry. We introduce a fabrication method for embedding air channels into helicoid-based soft continuum robots. Multi-material segments fabricated via vision-controlled jetting in a single print interface with PCBs housing miniature pressure sensors and IMUs for distributed deformation sensing. We characterize the mechanical properties of four helicoid designs and validate the sensor response to fundamental deformation modes. To demonstrate the platform's scalability, we construct and mechanically evaluate a meter-scale, 14-DoF cable-driven soft arm capable of open-loop trajectory tracking and object grasping, with tactile-based stiffness detection demonstrated using the gripper sensors. This approach establishes a scalable fabrication strategy for sensorized architected materials in large-scale soft robotic systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u87ba\u65cb\u6676\u683c\u8f6f\u4f53\u673a\u5668\u4eba\u4e2d\u5d4c\u5165\u7a7a\u6c14\u901a\u9053\u7684\u5236\u9020\u65b9\u6cd5\uff0c\u5b9e\u73b0\u5206\u5e03\u5f0f\u53d8\u5f62\u611f\u77e5\uff0c\u5e76\u6784\u5efa\u4e86\u7c73\u7ea714\u81ea\u7531\u5ea6\u8f6f\u4f53\u81c2\u8fdb\u884c\u9a8c\u8bc1", "motivation": "\u8f6f\u4f53\u673a\u5668\u4eba\u867d\u7136\u80fd\u4e0e\u590d\u6742\u73af\u5883\u5b89\u5168\u4ea4\u4e92\uff0c\u4f46\u7531\u4e8e\u9ad8\u5ea6\u53ef\u53d8\u5f62\u7684\u7ed3\u6784\uff0c\u4f20\u611f\u548c\u63a7\u5236\u4ecd\u7136\u56f0\u96be\u3002\u87ba\u65cb\u6676\u683c\u7b49\u7ed3\u6784\u5316\u8f6f\u6750\u6599\u5177\u6709\u53ef\u8c03\u521a\u5ea6\u548c\u5f3a\u5ea6\uff0c\u4f46\u56e0\u5176\u7a00\u758f\u51e0\u4f55\u7ed3\u6784\u800c\u96be\u4ee5\u96c6\u6210\u4f20\u611f\u5668", "method": "\u91c7\u7528\u89c6\u89c9\u63a7\u5236\u55b7\u5c04\u7684\u591a\u6750\u65993D\u6253\u5370\u6280\u672f\uff0c\u5728\u87ba\u65cb\u6676\u683c\u8f6f\u4f53\u673a\u5668\u4eba\u4e2d\u5d4c\u5165\u7a7a\u6c14\u901a\u9053\uff0c\u5e76\u4e0ePCB\u677f\u96c6\u6210\uff0cPCB\u677f\u4e0a\u88c5\u6709\u5fae\u578b\u538b\u529b\u4f20\u611f\u5668\u548cIMU\uff0c\u5b9e\u73b0\u5206\u5e03\u5f0f\u53d8\u5f62\u611f\u77e5", "result": "\u8868\u5f81\u4e86\u56db\u79cd\u87ba\u65cb\u6676\u683c\u8bbe\u8ba1\u7684\u673a\u68b0\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u4f20\u611f\u5668\u5bf9\u57fa\u672c\u53d8\u5f62\u6a21\u5f0f\u7684\u54cd\u5e94\u3002\u6784\u5efa\u4e86\u7c73\u7ea714\u81ea\u7531\u5ea6\u7535\u7f06\u9a71\u52a8\u8f6f\u4f53\u81c2\uff0c\u80fd\u591f\u8fdb\u884c\u5f00\u73af\u8f68\u8ff9\u8ddf\u8e2a\u548c\u7269\u4f53\u6293\u53d6\uff0c\u5e76\u5229\u7528\u5939\u6301\u5668\u4f20\u611f\u5668\u5b9e\u73b0\u4e86\u57fa\u4e8e\u89e6\u89c9\u7684\u521a\u5ea6\u68c0\u6d4b", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5927\u89c4\u6a21\u8f6f\u4f53\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u7684\u4f20\u611f\u5668\u5316\u7ed3\u6784\u5316\u6750\u6599\u5efa\u7acb\u4e86\u53ef\u6269\u5c55\u7684\u5236\u9020\u7b56\u7565"}}
{"id": "2602.23523", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23523", "abs": "https://arxiv.org/abs/2602.23523", "authors": ["Junjiang Wu", "Liejun Wang", "Zhiqing Guo"], "title": "All in One: Unifying Deepfake Detection, Tampering Localization, and Source Tracing with a Robust Landmark-Identity Watermark", "comment": "Accepted by CVPR 2026", "summary": "With the rapid advancement of deepfake technology, malicious face manipulations pose a significant threat to personal privacy and social security. However, existing proactive forensics methods typically treat deepfake detection, tampering localization, and source tracing as independent tasks, lacking a unified framework to address them jointly. To bridge this gap, we propose a unified proactive forensics framework that jointly addresses these three core tasks. Our core framework adopts an innovative 152-dimensional landmark-identity watermark termed LIDMark, which structurally interweaves facial landmarks with a unique source identifier. To robustly extract the LIDMark, we design a novel Factorized-Head Decoder (FHD). Its architecture factorizes the shared backbone features into two specialized heads (i.e., regression and classification), robustly reconstructing the embedded landmarks and identifier, respectively, even when subjected to severe distortion or tampering. This design realizes an \"all-in-one\" trifunctional forensic solution: the regression head underlies an \"intrinsic-extrinsic\" consistency check for detection and localization, while the classification head robustly decodes the source identifier for tracing. Extensive experiments show that the proposed LIDMark framework provides a unified, robust, and imperceptible solution for the detection, localization, and tracing of deepfake content. The code is available at https://github.com/vpsg-research/LIDMark.", "AI": {"tldr": "\u63d0\u51faLIDMark\u6846\u67b6\uff0c\u901a\u8fc7152\u7ef4\u5730\u6807-\u8eab\u4efd\u6c34\u5370\u7edf\u4e00\u89e3\u51b3\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u3001\u7be1\u6539\u5b9a\u4f4d\u548c\u6765\u6e90\u8ffd\u8e2a\u4e09\u4e2a\u4efb\u52a1\uff0c\u4f7f\u7528\u56e0\u5b50\u5316\u5934\u90e8\u89e3\u7801\u5668\u5b9e\u73b0\u9c81\u68d2\u63d0\u53d6\u3002", "motivation": "\u73b0\u6709\u4e3b\u52a8\u53d6\u8bc1\u65b9\u6cd5\u901a\u5e38\u5c06\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u3001\u7be1\u6539\u5b9a\u4f4d\u548c\u6765\u6e90\u8ffd\u8e2a\u89c6\u4e3a\u72ec\u7acb\u4efb\u52a1\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u8054\u5408\u5904\u7406\u6846\u67b6\u3002\u968f\u7740\u6df1\u5ea6\u4f2a\u9020\u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u6076\u610f\u4eba\u8138\u64cd\u7eb5\u5bf9\u4e2a\u4eba\u9690\u79c1\u548c\u793e\u4f1a\u5b89\u5168\u6784\u6210\u91cd\u5927\u5a01\u80c1\u3002", "method": "\u63d0\u51faLIDMark\u6846\u67b6\uff0c\u91c7\u7528\u521b\u65b0\u7684152\u7ef4\u5730\u6807-\u8eab\u4efd\u6c34\u5370\uff0c\u5c06\u9762\u90e8\u5730\u6807\u4e0e\u552f\u4e00\u6765\u6e90\u6807\u8bc6\u7b26\u7ed3\u6784\u4ea4\u7ec7\u3002\u8bbe\u8ba1\u56e0\u5b50\u5316\u5934\u90e8\u89e3\u7801\u5668\uff0c\u5c06\u5171\u4eab\u9aa8\u5e72\u7279\u5f81\u5206\u89e3\u4e3a\u56de\u5f52\u548c\u5206\u7c7b\u4e24\u4e2a\u4e13\u95e8\u5934\u90e8\uff0c\u5206\u522b\u9c81\u68d2\u91cd\u5efa\u5d4c\u5165\u7684\u5730\u6807\u548c\u6807\u8bc6\u7b26\u3002", "result": "\u5b9e\u9a8c\u8868\u660eLIDMark\u6846\u67b6\u4e3a\u6df1\u5ea6\u4f2a\u9020\u5185\u5bb9\u7684\u68c0\u6d4b\u3001\u5b9a\u4f4d\u548c\u8ffd\u8e2a\u63d0\u4f9b\u4e86\u7edf\u4e00\u3001\u9c81\u68d2\u4e14\u4e0d\u53ef\u5bdf\u89c9\u7684\u89e3\u51b3\u65b9\u6848\u3002\u56de\u5f52\u5934\u90e8\u652f\u6301\"\u5185\u5728-\u5916\u5728\"\u4e00\u81f4\u6027\u68c0\u67e5\u7528\u4e8e\u68c0\u6d4b\u548c\u5b9a\u4f4d\uff0c\u5206\u7c7b\u5934\u90e8\u9c81\u68d2\u89e3\u7801\u6765\u6e90\u6807\u8bc6\u7b26\u7528\u4e8e\u8ffd\u8e2a\u3002", "conclusion": "LIDMark\u6846\u67b6\u901a\u8fc7\u7edf\u4e00\u7684\u6c34\u5370\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u3001\u7be1\u6539\u5b9a\u4f4d\u548c\u6765\u6e90\u8ffd\u8e2a\u4e09\u4e2a\u6838\u5fc3\u53d6\u8bc1\u4efb\u52a1\uff0c\u5b9e\u73b0\u4e86\"\u4e00\u4f53\u5316\"\u7684\u4e09\u529f\u80fd\u53d6\u8bc1\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.23409", "categories": ["cs.LG", "cs.AI", "cs.ET", "quant-ph"], "pdf": "https://arxiv.org/pdf/2602.23409", "abs": "https://arxiv.org/abs/2602.23409", "authors": ["Michael Poppel", "Jonas Stein", "Sebastian W\u00f6lckert", "Markus Baumann", "Claudia Linnhoff-Popien"], "title": "Long Range Frequency Tuning for QML", "comment": null, "summary": "Quantum machine learning models using angle encoding naturally represent truncated Fourier series, providing universal function approximation capabilities with sufficient circuit depth. For unary fixed-frequency encodings, circuit depth scales as O(omega_max * (omega_max + epsilon^{-2})) with target frequency magnitude omega_max and precision epsilon. Trainable-frequency approaches theoretically reduce this to match the target spectrum size, requiring only as many encoding gates as frequencies in the target spectrum. Despite this compelling efficiency, their practical effectiveness hinges on a key assumption: that gradient-based optimization can drive prefactors to arbitrary target values. We demonstrate through systematic experiments that frequency prefactors exhibit limited trainability: movement is constrained to approximately +/-1 units with typical learning rates. When target frequencies lie outside this reachable range, optimization frequently fails. To overcome this frequency reachability limitation, we propose grid-based initialization using ternary encodings, which generate dense integer frequency spectra. While this approach requires O(log_3(omega_max)) encoding gates -- more than the theoretical optimum but exponentially fewer than fixed-frequency methods -- it ensures target frequencies lie within the locally reachable range. On synthetic targets with three shifted high frequencies, ternary grid initialization achieves a median R^2 score of 0.9969, compared to 0.1841 for the trainable-frequency baseline. For the real-world Flight Passengers dataset, ternary grid initialization achieves a median R^2 score of 0.9671, representing a 22.8% improvement over trainable-frequency initialization (median R^2 = 0.7876).", "AI": {"tldr": "\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u89d2\u5ea6\u7f16\u7801\u65b9\u6cd5\u5b58\u5728\u9891\u7387\u53ef\u8bad\u7ec3\u6027\u9650\u5236\uff0c\u4f20\u7edf\u53ef\u8bad\u7ec3\u9891\u7387\u65b9\u6cd5\u5728\u5b9e\u9645\u4e2d\u96be\u4ee5\u4f18\u5316\u5230\u76ee\u6807\u9891\u7387\u503c\u3002\u8bba\u6587\u63d0\u51fa\u57fa\u4e8e\u4e09\u5143\u7f16\u7801\u7684\u7f51\u683c\u521d\u59cb\u5316\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u9891\u7387\u53ef\u8fbe\u8303\u56f4\u548c\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u4e2d\u4f7f\u7528\u89d2\u5ea6\u7f16\u7801\u7684\u65b9\u6cd5\u7406\u8bba\u4e0a\u53ef\u4ee5\u901a\u8fc7\u53ef\u8bad\u7ec3\u9891\u7387\u5b9e\u73b0\u9ad8\u6548\u5b66\u4e60\uff0c\u4f46\u5b9e\u9645\u4f18\u5316\u8fc7\u7a0b\u4e2d\u53d1\u73b0\u9891\u7387\u9884\u56e0\u5b50\u5b58\u5728\u53ef\u8bad\u7ec3\u6027\u9650\u5236\uff1a\u68af\u5ea6\u4f18\u5316\u53ea\u80fd\u5c06\u9891\u7387\u503c\u8c03\u6574\u5230\u7ea6\u00b11\u5355\u4f4d\u8303\u56f4\u5185\uff0c\u5f53\u76ee\u6807\u9891\u7387\u8d85\u51fa\u6b64\u8303\u56f4\u65f6\u4f18\u5316\u7ecf\u5e38\u5931\u8d25\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u4e09\u5143\u7f16\u7801\u7684\u7f51\u683c\u521d\u59cb\u5316\u65b9\u6cd5\uff0c\u751f\u6210\u5bc6\u96c6\u7684\u6574\u6570\u9891\u7387\u8c31\u3002\u8be5\u65b9\u6cd5\u9700\u8981O(log_3(\u03c9_max))\u4e2a\u7f16\u7801\u95e8\uff0c\u867d\u7136\u6bd4\u7406\u8bba\u6700\u4f18\u65b9\u6848\u591a\uff0c\u4f46\u6bd4\u56fa\u5b9a\u9891\u7387\u65b9\u6cd5\u6307\u6570\u7ea7\u51cf\u5c11\uff0c\u786e\u4fdd\u76ee\u6807\u9891\u7387\u4f4d\u4e8e\u5c40\u90e8\u53ef\u8fbe\u8303\u56f4\u5185\u3002", "result": "\u5728\u4e09\u9891\u79fb\u9ad8\u9891\u7387\u5408\u6210\u76ee\u6807\u4e0a\uff0c\u4e09\u5143\u7f51\u683c\u521d\u59cb\u5316\u83b7\u5f97\u4e2d\u4f4d\u6570R\u00b2\u5206\u65700.9969\uff0c\u800c\u53ef\u8bad\u7ec3\u9891\u7387\u57fa\u7ebf\u4ec5\u4e3a0.1841\u3002\u5728\u5b9e\u9645Flight Passengers\u6570\u636e\u96c6\u4e0a\uff0c\u4e09\u5143\u7f51\u683c\u521d\u59cb\u5316\u83b7\u5f97\u4e2d\u4f4d\u6570R\u00b2\u5206\u65700.9671\uff0c\u6bd4\u53ef\u8bad\u7ec3\u9891\u7387\u521d\u59cb\u5316\uff080.7876\uff09\u63d0\u534722.8%\u3002", "conclusion": "\u9891\u7387\u9884\u56e0\u5b50\u7684\u53ef\u8bad\u7ec3\u6027\u9650\u5236\u662f\u91cf\u5b50\u673a\u5668\u5b66\u4e60\u4e2d\u89d2\u5ea6\u7f16\u7801\u65b9\u6cd5\u7684\u5173\u952e\u74f6\u9888\u3002\u4e09\u5143\u7f51\u683c\u521d\u59cb\u5316\u65b9\u6cd5\u901a\u8fc7\u786e\u4fdd\u76ee\u6807\u9891\u7387\u4f4d\u4e8e\u68af\u5ea6\u4f18\u5316\u53ef\u8fbe\u8303\u56f4\u5185\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.23541", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23541", "abs": "https://arxiv.org/abs/2602.23541", "authors": ["Arvind Raghavan", "Elias Bareinboim"], "title": "Causal Identification from Counterfactual Data: Completeness and Bounding Results", "comment": null, "summary": "Previous work establishing completeness results for $\\textit{counterfactual identification}$ has been circumscribed to the setting where the input data belongs to observational or interventional distributions (Layers 1 and 2 of Pearl's Causal Hierarchy), since it was generally presumed impossible to obtain data from counterfactual distributions, which belong to Layer 3. However, recent work (Raghavan & Bareinboim, 2025) has formally characterized a family of counterfactual distributions which can be directly estimated via experimental methods - a notion they call $\\textit{counterfactual realizabilty}$. This leaves open the question of what $\\textit{additional}$ counterfactual quantities now become identifiable, given this new access to (some) Layer 3 data. To answer this question, we develop the CTFIDU+ algorithm for identifying counterfactual queries from an arbitrary set of Layer 3 distributions, and prove that it is complete for this task. Building on this, we establish the theoretical limit of which counterfactuals can be identified from physically realizable distributions, thus implying the $\\textit{fundamental limit to exact causal inference in the non-parametric setting}$. Finally, given the impossibility of identifying certain critical types of counterfactuals, we derive novel analytic bounds for such quantities using realizable counterfactual data, and corroborate using simulations that counterfactual data helps tighten the bounds for non-identifiable quantities in practice.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86CTFIDU+\u7b97\u6cd5\uff0c\u7528\u4e8e\u4ece\u4efb\u610fLayer 3\u5206\u5e03\u4e2d\u8bc6\u522b\u53cd\u4e8b\u5b9e\u67e5\u8be2\uff0c\u8bc1\u660e\u4e86\u5176\u5b8c\u5907\u6027\uff0c\u5e76\u5efa\u7acb\u4e86\u975e\u53c2\u6570\u8bbe\u7f6e\u4e2d\u7cbe\u786e\u56e0\u679c\u63a8\u7406\u7684\u7406\u8bba\u6781\u9650\u3002", "motivation": "\u5148\u524d\u5173\u4e8e\u53cd\u4e8b\u5b9e\u8bc6\u522b\u5b8c\u5907\u6027\u7684\u7814\u7a76\u4ec5\u9650\u4e8e\u89c2\u6d4b\u6216\u5e72\u9884\u5206\u5e03\uff08\u56e0\u679c\u5c42\u7ea7\u7684\u7b2c1\u30012\u5c42\uff09\uff0c\u56e0\u4e3a\u4e00\u822c\u8ba4\u4e3a\u65e0\u6cd5\u83b7\u5f97\u7b2c3\u5c42\u7684\u53cd\u4e8b\u5b9e\u5206\u5e03\u6570\u636e\u3002\u4f46\u6700\u8fd1\u7814\u7a76\u53d1\u73b0\u67d0\u4e9b\u53cd\u4e8b\u5b9e\u5206\u5e03\u53ef\u4ee5\u901a\u8fc7\u5b9e\u9a8c\u65b9\u6cd5\u76f4\u63a5\u4f30\u8ba1\uff08\u53cd\u4e8b\u5b9e\u53ef\u5b9e\u73b0\u6027\uff09\uff0c\u8fd9\u5f15\u53d1\u4e86\u65b0\u7684\u95ee\u9898\uff1a\u5728\u80fd\u591f\u83b7\u53d6\u90e8\u5206Layer 3\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u54ea\u4e9b\u989d\u5916\u7684\u53cd\u4e8b\u5b9e\u91cf\u53d8\u5f97\u53ef\u8bc6\u522b\uff1f", "method": "\u5f00\u53d1\u4e86CTFIDU+\u7b97\u6cd5\uff0c\u7528\u4e8e\u4ece\u4efb\u610fLayer 3\u5206\u5e03\u96c6\u5408\u4e2d\u8bc6\u522b\u53cd\u4e8b\u5b9e\u67e5\u8be2\uff0c\u5e76\u8bc1\u660e\u4e86\u8be5\u7b97\u6cd5\u5bf9\u6b64\u4efb\u52a1\u7684\u5b8c\u5907\u6027\u3002\u57fa\u4e8e\u6b64\uff0c\u5efa\u7acb\u4e86\u4ece\u7269\u7406\u53ef\u5b9e\u73b0\u5206\u5e03\u4e2d\u8bc6\u522b\u53cd\u4e8b\u5b9e\u7684\u7406\u8bba\u6781\u9650\u3002\u5bf9\u4e8e\u4e0d\u53ef\u8bc6\u522b\u7684\u53cd\u4e8b\u5b9e\u91cf\uff0c\u63a8\u5bfc\u4e86\u4f7f\u7528\u53ef\u5b9e\u73b0\u53cd\u4e8b\u5b9e\u6570\u636e\u7684\u65b0\u89e3\u6790\u8fb9\u754c\u3002", "result": "CTFIDU+\u7b97\u6cd5\u88ab\u8bc1\u660e\u662f\u5b8c\u5907\u7684\uff0c\u80fd\u591f\u8bc6\u522b\u4ece\u4efb\u610fLayer 3\u5206\u5e03\u96c6\u5408\u4e2d\u53ef\u8bc6\u522b\u7684\u6240\u6709\u53cd\u4e8b\u5b9e\u67e5\u8be2\u3002\u7814\u7a76\u786e\u5b9a\u4e86\u975e\u53c2\u6570\u8bbe\u7f6e\u4e2d\u7cbe\u786e\u56e0\u679c\u63a8\u7406\u7684\u57fa\u672c\u6781\u9650\u3002\u901a\u8fc7\u6a21\u62df\u9a8c\u8bc1\uff0c\u53cd\u4e8b\u5b9e\u6570\u636e\u5728\u5b9e\u8df5\u4e2d\u786e\u5b9e\u6709\u52a9\u4e8e\u6536\u7d27\u4e0d\u53ef\u8bc6\u522b\u91cf\u7684\u8fb9\u754c\u3002", "conclusion": "\u672c\u6587\u5efa\u7acb\u4e86\u53cd\u4e8b\u5b9e\u8bc6\u522b\u7684\u65b0\u7406\u8bba\u6846\u67b6\uff0c\u660e\u786e\u4e86\u5728\u80fd\u591f\u83b7\u53d6\u90e8\u5206Layer 3\u6570\u636e\u60c5\u51b5\u4e0b\u7684\u8bc6\u522b\u80fd\u529b\u6781\u9650\uff0c\u5e76\u4e3a\u4e0d\u53ef\u8bc6\u522b\u7684\u53cd\u4e8b\u5b9e\u91cf\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u8fb9\u754c\u4f30\u8ba1\u65b9\u6cd5\uff0c\u63a8\u8fdb\u4e86\u56e0\u679c\u63a8\u7406\u7684\u7406\u8bba\u548c\u5b9e\u8df5\u53d1\u5c55\u3002"}}
{"id": "2602.23546", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23546", "abs": "https://arxiv.org/abs/2602.23546", "authors": ["Gaurav Kamath", "Sreenath Madathil", "Sebastian Schuster", "Marie-Catherine de Marneffe", "Siva Reddy"], "title": "Humans and LLMs Diverge on Probabilistic Inferences", "comment": null, "summary": "Human reasoning often involves working over limited information to arrive at probabilistic conclusions. In its simplest form, this involves making an inference that is not strictly entailed by a premise, but rather only likely given the premise. While reasoning LLMs have demonstrated strong performance on logical and mathematical tasks, their behavior on such open-ended, non-deterministic inferences remains largely unexplored. We introduce ProbCOPA, a dataset of 210 handcrafted probabilistic inferences in English, each annotated for inference likelihood by 25--30 human participants. We find that human responses are graded and varied, revealing probabilistic judgments of the inferences in our dataset. Comparing these judgments with responses from eight state-of-the-art reasoning LLMs, we show that models consistently fail to produce human-like distributions. Finally, analyzing LLM reasoning chains, we find evidence of a common reasoning pattern used to evaluate such inferences. Our findings reveal persistent differences between humans and LLMs, and underscore the need to evaluate reasoning beyond deterministic settings.", "AI": {"tldr": "\u8be5\u7814\u7a76\u521b\u5efa\u4e86ProbCOPA\u6570\u636e\u96c6\uff0c\u5305\u542b210\u4e2a\u4eba\u5de5\u6807\u6ce8\u7684\u6982\u7387\u63a8\u7406\u95ee\u9898\uff0c\u6bd4\u8f83\u4e86\u4eba\u7c7b\u4e0e8\u4e2a\u5148\u8fdb\u63a8\u7406LLM\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u6a21\u578b\u65e0\u6cd5\u4ea7\u751f\u7c7b\u4f3c\u4eba\u7c7b\u7684\u6982\u7387\u5206\u5e03\u3002", "motivation": "\u4eba\u7c7b\u63a8\u7406\u7ecf\u5e38\u57fa\u4e8e\u6709\u9650\u4fe1\u606f\u5f97\u51fa\u6982\u7387\u6027\u7ed3\u8bba\uff0c\u800c\u5f53\u524dLLM\u5728\u903b\u8f91\u548c\u6570\u5b66\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u8fd9\u79cd\u5f00\u653e\u5f0f\u7684\u3001\u975e\u786e\u5b9a\u6027\u7684\u6982\u7387\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u884c\u4e3a\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u521b\u5efaProbCOPA\u6570\u636e\u96c6\uff0c\u5305\u542b210\u4e2a\u624b\u5de5\u5236\u4f5c\u7684\u82f1\u8bed\u6982\u7387\u63a8\u7406\u95ee\u9898\uff0c\u6bcf\u4e2a\u95ee\u9898\u753125-30\u540d\u4eba\u7c7b\u53c2\u4e0e\u8005\u6807\u6ce8\u63a8\u7406\u53ef\u80fd\u6027\u3002\u6bd4\u8f83\u4eba\u7c7b\u5224\u65ad\u4e0e8\u4e2a\u6700\u5148\u8fdb\u7684\u63a8\u7406LLM\u7684\u54cd\u5e94\uff0c\u5e76\u5206\u6790LLM\u7684\u63a8\u7406\u94fe\u3002", "result": "\u4eba\u7c7b\u54cd\u5e94\u5448\u73b0\u5206\u7ea7\u548c\u591a\u6837\u5316\uff0c\u63ed\u793a\u4e86\u6570\u636e\u96c6\u4e2d\u7684\u6982\u7387\u5224\u65ad\u3002\u6a21\u578b\u65e0\u6cd5\u4ea7\u751f\u7c7b\u4f3c\u4eba\u7c7b\u7684\u5206\u5e03\uff0c\u5206\u6790LLM\u63a8\u7406\u94fe\u53d1\u73b0\u5b83\u4eec\u4f7f\u7528\u5171\u540c\u7684\u63a8\u7406\u6a21\u5f0f\u6765\u8bc4\u4f30\u6b64\u7c7b\u63a8\u7406\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u4eba\u7c7b\u4e0eLLM\u4e4b\u95f4\u6301\u7eed\u5b58\u5728\u7684\u5dee\u5f02\uff0c\u5f3a\u8c03\u9700\u8981\u5728\u975e\u786e\u5b9a\u6027\u73af\u5883\u4e2d\u8bc4\u4f30\u63a8\u7406\u80fd\u529b\uff0c\u8d85\u8d8a\u4f20\u7edf\u7684\u786e\u5b9a\u6027\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2602.24254", "categories": ["eess.SY", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.24254", "abs": "https://arxiv.org/abs/2602.24254", "authors": ["Kriti Thakur", "Alivelu Manga Parimi", "Mayukha Pal"], "title": "FaultXformer: A Transformer-Encoder Based Fault Classification and Location Identification model in PMU-Integrated Active Electrical Distribution System", "comment": null, "summary": "Accurate fault detection and localization in electrical distribution systems is crucial, especially with the increasing integration of distributed energy resources (DERs), which inject greater variability and complexity into grid operations. In this study, FaultXformer is proposed, a Transformer encoder-based architecture developed for automatic fault analysis using real-time current data obtained from phasor measurement unit (PMU). The approach utilizes time-series current data to initially extract rich temporal information in stage 1, which is crucial for identifying the fault type and precisely determining its location across multiple nodes. In Stage 2, these extracted features are processed to differentiate among distinct fault types and identify the respective fault location within the distribution system. Thus, this dual-stage transformer encoder pipeline enables high-fidelity representation learning, considerably boosting the performance of the work. The model was validated on a dataset generated from the IEEE 13-node test feeder, simulated with 20 separate fault locations and several DER integration scenarios, utilizing current measurements from four strategically located PMUs. To demonstrate robust performance evaluation, stratified 10-fold cross-validation is performed. FaultXformer achieved average accuracies of 98.76% in fault type classification and 98.92% in fault location identification across cross-validation, consistently surpassing conventional deep learning baselines convolutional neural network (CNN), recurrent neural network (RNN). long short-term memory (LSTM) by 1.70%, 34.95%, and 2.04% in classification accuracy and by 10.82%, 40.89%, and 6.27% in location accuracy, respectively. These results demonstrate the efficacy of the proposed model with significant DER penetration.", "AI": {"tldr": "FaultXformer\uff1a\u57fa\u4e8eTransformer\u7f16\u7801\u5668\u7684\u53cc\u9636\u6bb5\u67b6\u6784\uff0c\u7528\u4e8e\u7535\u529b\u914d\u7535\u7f51\u7684\u6545\u969c\u7c7b\u578b\u5206\u7c7b\u548c\u4f4d\u7f6e\u8bc6\u522b\uff0c\u5728DER\u9ad8\u6e17\u900f\u7387\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u968f\u7740\u5206\u5e03\u5f0f\u80fd\u6e90\u8d44\u6e90\uff08DERs\uff09\u5728\u7535\u529b\u914d\u7535\u7f51\u4e2d\u7684\u96c6\u6210\u5ea6\u4e0d\u65ad\u63d0\u9ad8\uff0c\u7535\u7f51\u8fd0\u884c\u53d8\u5f97\u66f4\u52a0\u590d\u6742\u548c\u591a\u53d8\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u51c6\u786e\u7684\u6545\u969c\u68c0\u6d4b\u548c\u5b9a\u4f4d\u65b9\u6cd5\u3002\u4f20\u7edf\u65b9\u6cd5\u5728\u5904\u7406\u8fd9\u79cd\u590d\u6742\u6027\u548c\u53d8\u5f02\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51faFaultXformer\uff0c\u4e00\u79cd\u57fa\u4e8eTransformer\u7f16\u7801\u5668\u7684\u53cc\u9636\u6bb5\u67b6\u6784\u3002\u7b2c\u4e00\u9636\u6bb5\u5229\u7528PMU\u83b7\u53d6\u7684\u5b9e\u65f6\u7535\u6d41\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u63d0\u53d6\u4e30\u5bcc\u7684\u65f6\u5e8f\u4fe1\u606f\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5904\u7406\u8fd9\u4e9b\u7279\u5f81\u4ee5\u533a\u5206\u4e0d\u540c\u7684\u6545\u969c\u7c7b\u578b\u5e76\u786e\u5b9a\u6545\u969c\u4f4d\u7f6e\u3002\u4f7f\u7528IEEE 13\u8282\u70b9\u6d4b\u8bd5\u9988\u7ebf\u751f\u6210\u6570\u636e\u96c6\uff0c\u5305\u542b20\u4e2a\u6545\u969c\u4f4d\u7f6e\u548c\u591a\u79cdDER\u96c6\u6210\u573a\u666f\uff0c\u91c7\u7528\u5206\u5c4210\u6298\u4ea4\u53c9\u9a8c\u8bc1\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "FaultXformer\u5728\u6545\u969c\u7c7b\u578b\u5206\u7c7b\u4e0a\u5e73\u5747\u51c6\u786e\u7387\u8fbe\u523098.76%\uff0c\u5728\u6545\u969c\u4f4d\u7f6e\u8bc6\u522b\u4e0a\u8fbe\u523098.92%\u3002\u76f8\u6bd4\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u57fa\u7ebf\uff08CNN\u3001RNN\u3001LSTM\uff09\uff0c\u5728\u5206\u7c7b\u51c6\u786e\u7387\u4e0a\u5206\u522b\u9ad8\u51fa1.70%\u300134.95%\u30012.04%\uff0c\u5728\u4f4d\u7f6e\u51c6\u786e\u7387\u4e0a\u5206\u522b\u9ad8\u51fa10.82%\u300140.89%\u30016.27%\u3002", "conclusion": "FaultXformer\u5728DER\u9ad8\u6e17\u900f\u7387\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u8bc1\u660e\u4e86\u57fa\u4e8eTransformer\u7684\u67b6\u6784\u5728\u7535\u529b\u7cfb\u7edf\u6545\u969c\u5206\u6790\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2602.23478", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.23478", "abs": "https://arxiv.org/abs/2602.23478", "authors": ["Sander Tonkens", "Sosuke Kojima", "Chenhao Liu", "Judy Masri", "Sylvia Herbert"], "title": "Refining Almost-Safe Value Functions on the Fly", "comment": null, "summary": "Control Barrier Functions (CBFs) are a powerful tool for ensuring robotic safety, but designing or learning valid CBFs for complex systems is a significant challenge. While Hamilton-Jacobi Reachability provides a formal method for synthesizing safe value functions, it scales poorly and is typically performed offline, limiting its applicability in dynamic environments. This paper bridges the gap between offline synthesis and online adaptation. We introduce refineCBF for refining an approximate CBF - whether analytically derived, learned, or even unsafe - via warm-started HJ reachability. We then present its computationally efficient successor, HJ-Patch, which accelerates this process through localized updates. Both methods guarantee the recovery of a safe value function and can ensure monotonic safety improvements during adaptation. Our experiments validate our framework's primary contribution: in-the-loop, real-time adaptation, in simulation (with detailed value function analysis) and on physical hardware. Our experiments on ground vehicles and quadcopters show that our framework can successfully adapt to sudden environmental changes, such as new obstacles and unmodeled wind disturbances, providing a practical path toward deploying formally guaranteed safety in real-world settings.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51farefineCBF\u548cHJ-Patch\u4e24\u79cd\u65b9\u6cd5\uff0c\u901a\u8fc7\u70ed\u542f\u52a8\u7684Hamilton-Jacobi\u53ef\u8fbe\u6027\u5206\u6790\u5728\u7ebf\u7cbe\u5316\u8fd1\u4f3c\u63a7\u5236\u5c4f\u969c\u51fd\u6570\uff0c\u5b9e\u73b0\u5b9e\u65f6\u5b89\u5168\u9002\u5e94\uff0c\u5e94\u7528\u4e8e\u5730\u9762\u8f66\u8f86\u548c\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u5e94\u5bf9\u73af\u5883\u53d8\u5316\u3002", "motivation": "\u63a7\u5236\u5c4f\u969c\u51fd\u6570\u662f\u786e\u4fdd\u673a\u5668\u4eba\u5b89\u5168\u7684\u6709\u529b\u5de5\u5177\uff0c\u4f46\u4e3a\u590d\u6742\u7cfb\u7edf\u8bbe\u8ba1\u6216\u5b66\u4e60\u6709\u6548\u7684CBF\u5177\u6709\u6311\u6218\u6027\u3002Hamilton-Jacobi\u53ef\u8fbe\u6027\u5206\u6790\u867d\u7136\u80fd\u5f62\u5f0f\u5316\u5408\u6210\u5b89\u5168\u503c\u51fd\u6570\uff0c\u4f46\u6269\u5c55\u6027\u5dee\u4e14\u901a\u5e38\u79bb\u7ebf\u6267\u884c\uff0c\u9650\u5236\u4e86\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5e94\u7528\u3002\u9700\u8981\u5f25\u5408\u79bb\u7ebf\u5408\u6210\u4e0e\u5728\u7ebf\u9002\u5e94\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "1. \u63d0\u51farefineCBF\u65b9\u6cd5\uff1a\u901a\u8fc7\u70ed\u542f\u52a8\u7684HJ\u53ef\u8fbe\u6027\u5206\u6790\u7cbe\u5316\u8fd1\u4f3cCBF\uff08\u65e0\u8bba\u662f\u89e3\u6790\u63a8\u5bfc\u3001\u5b66\u4e60\u5f97\u5230\u8fd8\u662f\u751a\u81f3\u4e0d\u5b89\u5168\u7684CBF\uff09\uff1b2. \u63d0\u51faHJ-Patch\u65b9\u6cd5\uff1a\u901a\u8fc7\u5c40\u90e8\u5316\u66f4\u65b0\u52a0\u901f\u8fd9\u4e00\u8fc7\u7a0b\uff1b3. \u4e24\u79cd\u65b9\u6cd5\u90fd\u80fd\u4fdd\u8bc1\u6062\u590d\u5b89\u5168\u503c\u51fd\u6570\uff0c\u5e76\u786e\u4fdd\u9002\u5e94\u8fc7\u7a0b\u4e2d\u7684\u5355\u8c03\u5b89\u5168\u6539\u8fdb\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u4e3b\u8981\u8d21\u732e\uff1a\u5728\u56de\u8def\u4e2d\u7684\u5b9e\u65f6\u9002\u5e94\u80fd\u529b\uff0c\u5305\u62ec\u4eff\u771f\uff08\u8be6\u7ec6\u503c\u51fd\u6570\u5206\u6790\uff09\u548c\u7269\u7406\u786c\u4ef6\u6d4b\u8bd5\u3002\u5730\u9762\u8f66\u8f86\u548c\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u6210\u529f\u9002\u5e94\u7a81\u7136\u7684\u73af\u5883\u53d8\u5316\uff0c\u5982\u65b0\u969c\u788d\u7269\u548c\u672a\u5efa\u6a21\u7684\u98ce\u6270\u52a8\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u5b9e\u73b0\u5f62\u5f0f\u5316\u4fdd\u8bc1\u7684\u5b89\u5168\u63d0\u4f9b\u4e86\u4e00\u6761\u5b9e\u7528\u8def\u5f84\uff0c\u901a\u8fc7\u5728\u7ebf\u7cbe\u5316CBF\u7684\u65b9\u6cd5\uff0c\u4f7f\u673a\u5668\u4eba\u7cfb\u7edf\u80fd\u591f\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5b9e\u65f6\u9002\u5e94\u5e76\u4fdd\u6301\u5b89\u5168\u3002"}}
{"id": "2602.23543", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23543", "abs": "https://arxiv.org/abs/2602.23543", "authors": ["Ziqi Gao", "Jieyu Zhang", "Wisdom Oluchi Ikezogwo", "Jae Sung Park", "Tario G. You", "Daniel Ogbu", "Chenhao Zheng", "Weikai Huang", "Yinuo Yang", "Winson Han", "Quan Kong", "Rajat Saini", "Ranjay Krishna"], "title": "Synthetic Visual Genome 2: Extracting Large-scale Spatio-Temporal Scene Graphs from Videos", "comment": null, "summary": "We introduce Synthetic Visual Genome 2 (SVG2), a large-scale panoptic video scene graph dataset. SVG2 contains over 636K videos with 6.6M objects, 52.0M attributes, and 6.7M relations, providing an order-of-magnitude increase in scale and diversity over prior spatio-temporal scene graph datasets. To create SVG2, we design a fully automated pipeline that combines multi-scale panoptic segmentation, online-offline trajectory tracking with automatic new-object discovery, per-trajectory semantic parsing, and GPT-5-based spatio-temporal relation inference. Building on this resource, we train TRaSER, a video scene graph generation model. TRaSER augments VLMs with a trajectory-aligned token arrangement mechanism and new modules: an object-trajectory resampler and a temporal-window resampler to convert raw videos and panoptic trajectories into compact spatio-temporal scene graphs in a single forward pass. The temporal-window resampler binds visual tokens to short trajectory segments to preserve local motion and temporal semantics, while the object-trajectory resampler aggregates entire trajectories to maintain global context for objects. On the PVSG, VIPSeg, VidOR and SVG2 test datasets, TRaSER improves relation detection by +15 to 20%, object prediction by +30 to 40% over the strongest open-source baselines and by +13% over GPT-5, and attribute prediction by +15%. When TRaSER's generated scene graphs are sent to a VLM for video question answering, it delivers a +1.5 to 4.6% absolute accuracy gain over using video only or video augmented with Qwen2.5-VL's generated scene graphs, demonstrating the utility of explicit spatio-temporal scene graphs as an intermediate representation.", "AI": {"tldr": "SVG2\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u5168\u666f\u89c6\u9891\u573a\u666f\u56fe\u6570\u636e\u96c6\uff0c\u5305\u542b63.6\u4e07\u4e2a\u89c6\u9891\u3001660\u4e07\u4e2a\u5bf9\u8c61\u30015200\u4e07\u4e2a\u5c5e\u6027\u548c670\u4e07\u4e2a\u5173\u7cfb\uff0c\u89c4\u6a21\u6bd4\u73b0\u6709\u6570\u636e\u96c6\u5927\u4e00\u4e2a\u6570\u91cf\u7ea7\u3002\u57fa\u4e8e\u6b64\u6570\u636e\u96c6\u8bad\u7ec3\u7684TRaSER\u6a21\u578b\u5728\u89c6\u9891\u573a\u666f\u56fe\u751f\u6210\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65f6\u7a7a\u573a\u666f\u56fe\u6570\u636e\u96c6\u89c4\u6a21\u6709\u9650\uff0c\u7f3a\u4e4f\u591a\u6837\u6027\u548c\u5927\u89c4\u6a21\u6807\u6ce8\uff0c\u9650\u5236\u4e86\u89c6\u9891\u573a\u666f\u56fe\u751f\u6210\u6a21\u578b\u7684\u53d1\u5c55\u3002\u9700\u8981\u6784\u5efa\u66f4\u5927\u89c4\u6a21\u3001\u66f4\u591a\u6837\u5316\u7684\u6570\u636e\u96c6\u6765\u63a8\u52a8\u8be5\u9886\u57df\u7814\u7a76\u3002", "method": "1) \u521b\u5efaSVG2\u6570\u636e\u96c6\uff1a\u91c7\u7528\u5168\u81ea\u52a8\u6d41\u6c34\u7ebf\uff0c\u7ed3\u5408\u591a\u5c3a\u5ea6\u5168\u666f\u5206\u5272\u3001\u5728\u7ebf-\u79bb\u7ebf\u8f68\u8ff9\u8ddf\u8e2a\uff08\u542b\u81ea\u52a8\u65b0\u5bf9\u8c61\u53d1\u73b0\uff09\u3001\u6bcf\u8f68\u8ff9\u8bed\u4e49\u89e3\u6790\u548cGPT-5\u65f6\u7a7a\u5173\u7cfb\u63a8\u7406\u30022) \u8bad\u7ec3TRaSER\u6a21\u578b\uff1a\u901a\u8fc7\u8f68\u8ff9\u5bf9\u9f50\u7684token\u6392\u5217\u673a\u5236\uff0c\u65b0\u589e\u5bf9\u8c61\u8f68\u8ff9\u91cd\u91c7\u6837\u5668\u548c\u65f6\u95f4\u7a97\u53e3\u91cd\u91c7\u6837\u5668\uff0c\u5c06\u539f\u59cb\u89c6\u9891\u548c\u5168\u666f\u8f68\u8ff9\u8f6c\u6362\u4e3a\u7d27\u51d1\u7684\u65f6\u7a7a\u573a\u666f\u56fe\u3002", "result": "TRaSER\u5728PVSG\u3001VIPSeg\u3001VidOR\u548cSVG2\u6d4b\u8bd5\u96c6\u4e0a\uff1a\u5173\u7cfb\u68c0\u6d4b\u63d0\u534715-20%\uff0c\u5bf9\u8c61\u9884\u6d4b\u6bd4\u6700\u5f3a\u5f00\u6e90\u57fa\u7ebf\u63d0\u534730-40%\uff08\u6bd4GPT-5\u63d0\u534713%\uff09\uff0c\u5c5e\u6027\u9884\u6d4b\u63d0\u534715%\u3002\u5c06TRaSER\u751f\u6210\u7684\u573a\u666f\u56fe\u7528\u4e8e\u89c6\u9891\u95ee\u7b54\u65f6\uff0c\u76f8\u6bd4\u4ec5\u4f7f\u7528\u89c6\u9891\u6216\u89c6\u9891\u52a0Qwen2.5-VL\u751f\u6210\u7684\u573a\u666f\u56fe\uff0c\u51c6\u786e\u7387\u7edd\u5bf9\u63d0\u53471.5-4.6%\u3002", "conclusion": "SVG2\u6570\u636e\u96c6\u663e\u8457\u6269\u5c55\u4e86\u65f6\u7a7a\u573a\u666f\u56fe\u6570\u636e\u7684\u89c4\u6a21\u548c\u591a\u6837\u6027\uff0cTRaSER\u6a21\u578b\u901a\u8fc7\u521b\u65b0\u7684\u8f68\u8ff9\u5bf9\u9f50\u67b6\u6784\u5728\u89c6\u9891\u573a\u666f\u56fe\u751f\u6210\u4efb\u52a1\u4e0a\u53d6\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u8bc1\u660e\u4e86\u663e\u5f0f\u65f6\u7a7a\u573a\u666f\u56fe\u4f5c\u4e3a\u4e2d\u95f4\u8868\u793a\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2602.23545", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23545", "abs": "https://arxiv.org/abs/2602.23545", "authors": ["Matteo Ceriscioli", "Karthika Mohan"], "title": "Planning under Distribution Shifts with Causal POMDPs", "comment": "To appear at the 36th International Conference on Automated Planning and Scheduling (ICAPS-26)", "summary": "In the real world, planning is often challenged by distribution shifts. As such, a model of the environment obtained under one set of conditions may no longer remain valid as the distribution of states or the environment dynamics change, which in turn causes previously learned strategies to fail. In this work, we propose a theoretical framework for planning under partial observability using Partially Observable Markov Decision Processes (POMDPs) formulated using causal knowledge. By representing shifts in the environment as interventions on this causal POMDP, the framework enables evaluating plans under hypothesized changes and actively identifying which components of the environment have been altered. We show how to maintain and update a belief over both the latent state and the underlying domain, and we prove that the value function remains piecewise linear and convex (PWLC) in this augmented belief space. Preservation of PWLC under distribution shifts has the advantage of maintaining the tractability of planning via $\u03b1$-vector-based POMDP methods.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u56e0\u679c\u77e5\u8bc6\u7684POMDP\u6846\u67b6\uff0c\u7528\u4e8e\u5904\u7406\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e0b\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u901a\u8fc7\u5c06\u73af\u5883\u53d8\u5316\u8868\u793a\u4e3a\u56e0\u679c\u5e72\u9884\uff0c\u4fdd\u6301\u503c\u51fd\u6570\u7684PWLC\u7279\u6027\uff0c\u786e\u4fdd\u89c4\u5212\u7684\u53ef\u5904\u7406\u6027\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u89c4\u5212\u5e38\u9762\u4e34\u5206\u5e03\u504f\u79fb\u7684\u6311\u6218\uff0c\u73af\u5883\u6a21\u578b\u5728\u72b6\u6001\u5206\u5e03\u6216\u73af\u5883\u52a8\u6001\u53d8\u5316\u65f6\u53ef\u80fd\u5931\u6548\uff0c\u5bfc\u81f4\u5148\u524d\u5b66\u4e60\u7b56\u7565\u5931\u8d25\u3002\u9700\u8981\u5904\u7406\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u4e0b\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u56e0\u679c\u77e5\u8bc6\u7684POMDP\u7406\u8bba\u6846\u67b6\uff0c\u5c06\u73af\u5883\u504f\u79fb\u8868\u793a\u4e3a\u56e0\u679cPOMDP\u4e0a\u7684\u5e72\u9884\uff0c\u80fd\u591f\u8bc4\u4f30\u5047\u8bbe\u53d8\u5316\u4e0b\u7684\u8ba1\u5212\u5e76\u4e3b\u52a8\u8bc6\u522b\u73af\u5883\u53d8\u5316\u7ec4\u4ef6\u3002\u7ef4\u62a4\u548c\u66f4\u65b0\u6f5c\u5728\u72b6\u6001\u548c\u5e95\u5c42\u9886\u57df\u7684\u4fe1\u5ff5\u3002", "result": "\u8bc1\u660e\u4e86\u503c\u51fd\u6570\u5728\u589e\u5f3a\u4fe1\u5ff5\u7a7a\u95f4\u4e2d\u4fdd\u6301\u5206\u6bb5\u7ebf\u6027\u51f8(PWLC)\u7279\u6027\uff0c\u8fd9\u4e00\u7279\u6027\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u4fdd\u6301\u4f7f\u5f97\u57fa\u4e8e\u03b1\u5411\u91cf\u7684POMDP\u65b9\u6cd5\u4fdd\u6301\u53ef\u5904\u7406\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e0b\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u901a\u8fc7\u56e0\u679c\u8868\u793a\u548cPWLC\u7279\u6027\u7684\u4fdd\u6301\uff0c\u786e\u4fdd\u4e86\u89c4\u5212\u65b9\u6cd5\u5728\u73af\u5883\u53d8\u5316\u4e0b\u7684\u6709\u6548\u6027\u548c\u53ef\u5904\u7406\u6027\u3002"}}
{"id": "2602.23547", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.23547", "abs": "https://arxiv.org/abs/2602.23547", "authors": ["Sasha Boguraev", "Qing Yao", "Kyle Mahowald"], "title": "France or Spain or Germany or France: A Neural Account of Non-Redundant Redundant Disjunctions", "comment": "7 pages, 6 figures", "summary": "Sentences like \"She will go to France or Spain, or perhaps to Germany or France.\" appear formally redundant, yet become acceptable in contexts such as \"Mary will go to a philosophy program in France or Spain, or a mathematics program in Germany or France.\" While this phenomenon has typically been analyzed using symbolic formal representations, we aim to provide a complementary account grounded in artificial neural mechanisms. We first present new behavioral evidence from humans and large language models demonstrating the robustness of this apparent non-redundancy across contexts. We then show that, in language models, redundancy avoidance arises from two interacting mechanisms: models learn to bind contextually relevant information to repeated lexical items, and Transformer induction heads selectively attend to these context-licensed representations. We argue that this neural explanation sheds light on the mechanisms underlying context-sensitive semantic interpretation, and that it complements existing symbolic analyses.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u53e5\u5b50\u4e2d\u770b\u4f3c\u5197\u4f59\u7684\"\u6216\"\u8fde\u63a5\u8bcd\u5728\u7279\u5b9a\u8bed\u5883\u4e0b\u53d8\u5f97\u53ef\u63a5\u53d7\u7684\u73b0\u8c61\uff0c\u901a\u8fc7\u4eba\u5de5\u548c\u8bed\u8a00\u6a21\u578b\u5b9e\u9a8c\uff0c\u63ed\u793a\u4e86Transformer\u6a21\u578b\u901a\u8fc7\u7ed1\u5b9a\u4e0a\u4e0b\u6587\u4fe1\u606f\u548c\u9009\u62e9\u6027\u6ce8\u610f\u529b\u673a\u5236\u907f\u514d\u5197\u4f59\u7684\u795e\u7ecf\u673a\u5236\u3002", "motivation": "\u7814\u7a76\u50cf\"\u5979\u5c06\u53bb\u6cd5\u56fd\u6216\u897f\u73ed\u7259\uff0c\u6216\u8005\u53ef\u80fd\u53bb\u5fb7\u56fd\u6216\u6cd5\u56fd\"\u8fd9\u6837\u770b\u4f3c\u5f62\u5f0f\u5197\u4f59\u7684\u53e5\u5b50\uff0c\u5728\u7279\u5b9a\u8bed\u5883\u4e0b\u53d8\u5f97\u53ef\u63a5\u53d7\u7684\u73b0\u8c61\u3002\u4f20\u7edf\u5206\u6790\u4e3b\u8981\u4f7f\u7528\u7b26\u53f7\u5f62\u5f0f\u8868\u793a\uff0c\u672c\u6587\u65e8\u5728\u63d0\u4f9b\u57fa\u4e8e\u4eba\u5de5\u795e\u7ecf\u673a\u5236\u7684\u8865\u5145\u89e3\u91ca\u3002", "method": "\u9996\u5148\u6536\u96c6\u4eba\u7c7b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u884c\u4e3a\u8bc1\u636e\uff0c\u5c55\u793a\u8fd9\u79cd\u8868\u9762\u975e\u5197\u4f59\u6027\u5728\u4e0d\u540c\u8bed\u5883\u4e0b\u7684\u9c81\u68d2\u6027\u3002\u7136\u540e\u5206\u6790\u8bed\u8a00\u6a21\u578b\u4e2d\uff0c\u5197\u4f59\u907f\u514d\u901a\u8fc7\u4e24\u79cd\u4ea4\u4e92\u673a\u5236\u5b9e\u73b0\uff1a\u6a21\u578b\u5b66\u4e60\u5c06\u4e0a\u4e0b\u6587\u76f8\u5173\u4fe1\u606f\u7ed1\u5b9a\u5230\u91cd\u590d\u7684\u8bcd\u6c47\u9879\uff0cTransformer\u7684\u5f52\u7eb3\u5934\u9009\u62e9\u6027\u5173\u6ce8\u8fd9\u4e9b\u8bed\u5883\u8bb8\u53ef\u7684\u8868\u793a\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8fd9\u79cd\u73b0\u8c61\u5728\u4eba\u7c7b\u548c\u8bed\u8a00\u6a21\u578b\u4e2d\u90fd\u5177\u6709\u9c81\u68d2\u6027\u3002\u5728\u8bed\u8a00\u6a21\u578b\u4e2d\uff0c\u5197\u4f59\u907f\u514d\u6e90\u4e8e\u4e0a\u4e0b\u6587\u4fe1\u606f\u7ed1\u5b9a\u548cTransformer\u5f52\u7eb3\u5934\u7684\u9009\u62e9\u6027\u6ce8\u610f\u529b\u673a\u5236\u3002\u6a21\u578b\u80fd\u591f\u5b66\u4e60\u5c06\u4e0d\u540c\u4e0a\u4e0b\u6587\u4fe1\u606f\u4e0e\u91cd\u590d\u8bcd\u6c47\u5173\u8054\uff0c\u4ece\u800c\u907f\u514d\u611f\u77e5\u5197\u4f59\u3002", "conclusion": "\u8fd9\u79cd\u795e\u7ecf\u673a\u5236\u89e3\u91ca\u4e3a\u8bed\u5883\u654f\u611f\u8bed\u4e49\u89e3\u91ca\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u89e3\u89c6\u89d2\uff0c\u8865\u5145\u4e86\u73b0\u6709\u7684\u7b26\u53f7\u5206\u6790\u3002\u7814\u7a76\u63ed\u793a\u4e86Transformer\u6a21\u578b\u5982\u4f55\u5904\u7406\u8bed\u5883\u4f9d\u8d56\u7684\u8bed\u4e49\u89e3\u91ca\uff0c\u4e3a\u7406\u89e3\u8bed\u8a00\u7406\u89e3\u548c\u751f\u6210\u4e2d\u7684\u5197\u4f59\u73b0\u8c61\u63d0\u4f9b\u4e86\u795e\u7ecf\u57fa\u7840\u3002"}}
{"id": "2602.24259", "categories": ["eess.SY"], "pdf": "https://arxiv.org/pdf/2602.24259", "abs": "https://arxiv.org/abs/2602.24259", "authors": ["Shihao Li", "Jiachen Li", "Christopher Martin", "Zijun Chen", "Dongmei Chen", "Wei Li"], "title": "Curriculum-Based Soft Actor-Critic for Multi-Section R2R Tension Control", "comment": null, "summary": "Precise tension control in roll-to-roll (R2R) manufacturing is difficult under varying operating conditions and process uncertainty. This paper presents a curriculum-based Soft Actor-Critic (SAC) controller for multi-section R2R tension control. The policy is trained in three phases with progressively wider reference ranges, from 27 to 33 N to the full operating envelope of 20 to 40 N, so it can generalize across nominal and disturbed conditions. On a three-section R2R benchmark, the learned controller achieves accurate tracking in nominal operation and handles large disturbances, including 20 N to 40 N step changes, with a single policy and no scenario-specific retuning. These results indicate that curriculum-trained SAC is a practical alternative to model-based control when system parameters vary and process uncertainty is significant.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bfe\u7a0b\u5b66\u4e60\u7684Soft Actor-Critic\u63a7\u5236\u5668\uff0c\u7528\u4e8e\u591a\u6bb5\u5377\u5bf9\u5377\u5236\u9020\u4e2d\u7684\u5f20\u529b\u63a7\u5236\uff0c\u80fd\u591f\u5728\u5bbd\u8303\u56f4\u64cd\u4f5c\u6761\u4ef6\u4e0b\u5b9e\u73b0\u7cbe\u786e\u8ddf\u8e2a\u5e76\u5904\u7406\u5927\u6270\u52a8\u3002", "motivation": "\u5377\u5bf9\u5377\u5236\u9020\u4e2d\u7684\u7cbe\u786e\u5f20\u529b\u63a7\u5236\u5728\u53d8\u5316\u7684\u64cd\u4f5c\u6761\u4ef6\u548c\u8fc7\u7a0b\u4e0d\u786e\u5b9a\u6027\u4e0b\u975e\u5e38\u56f0\u96be\uff0c\u9700\u8981\u80fd\u591f\u9002\u5e94\u53c2\u6570\u53d8\u5316\u548c\u663e\u8457\u4e0d\u786e\u5b9a\u6027\u7684\u63a7\u5236\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u8bfe\u7a0b\u5b66\u4e60\u7684Soft Actor-Critic\u63a7\u5236\u5668\uff0c\u5206\u4e09\u4e2a\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff1a\u4ece27-33N\u7684\u7a84\u8303\u56f4\u5f00\u59cb\uff0c\u9010\u6b65\u6269\u5c55\u523020-40N\u7684\u5b8c\u6574\u64cd\u4f5c\u8303\u56f4\uff0c\u4f7f\u7b56\u7565\u80fd\u591f\u5728\u6807\u79f0\u548c\u6270\u52a8\u6761\u4ef6\u4e0b\u6cdb\u5316\u3002", "result": "\u5728\u4e09\u6bb5\u5377\u5bf9\u5377\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5b66\u4e60\u5230\u7684\u63a7\u5236\u5668\u5728\u6807\u79f0\u64cd\u4f5c\u4e2d\u5b9e\u73b0\u7cbe\u786e\u8ddf\u8e2a\uff0c\u5e76\u80fd\u5904\u7406\u5927\u6270\u52a8\uff08\u5305\u62ec20N\u523040N\u7684\u9636\u8dc3\u53d8\u5316\uff09\uff0c\u4f7f\u7528\u5355\u4e00\u7b56\u7565\u4e14\u65e0\u9700\u9488\u5bf9\u7279\u5b9a\u573a\u666f\u91cd\u65b0\u8c03\u6574\u3002", "conclusion": "\u5f53\u7cfb\u7edf\u53c2\u6570\u53d8\u5316\u4e14\u8fc7\u7a0b\u4e0d\u786e\u5b9a\u6027\u663e\u8457\u65f6\uff0c\u8bfe\u7a0b\u8bad\u7ec3\u7684SAC\u63a7\u5236\u5668\u662f\u57fa\u4e8e\u6a21\u578b\u63a7\u5236\u7684\u5b9e\u9645\u66ff\u4ee3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u5bbd\u64cd\u4f5c\u8303\u56f4\u5185\u5b9e\u73b0\u9c81\u68d2\u63a7\u5236\u3002"}}
{"id": "2602.23499", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23499", "abs": "https://arxiv.org/abs/2602.23499", "authors": ["Tugrul Gorgulu", "Atakan Dag", "M. Esat Kalfaoglu", "Halil Ibrahim Kuru", "Baris Can Cam", "Ozsel Kilinc"], "title": "TaCarla: A comprehensive benchmarking dataset for end-to-end autonomous driving", "comment": null, "summary": "Collecting a high-quality dataset is a critical task that demands meticulous attention to detail, as overlooking certain aspects can render the entire dataset unusable. Autonomous driving challenges remain a prominent area of research, requiring further exploration to enhance the perception and planning performance of vehicles. However, existing datasets are often incomplete. For instance, datasets that include perception information generally lack planning data, while planning datasets typically consist of extensive driving sequences where the ego vehicle predominantly drives forward, offering limited behavioral diversity. In addition, many real datasets struggle to evaluate their models, especially for planning tasks, since they lack a proper closed-loop evaluation setup. The CARLA Leaderboard 2.0 challenge, which provides a diverse set of scenarios to address the long-tail problem in autonomous driving, has emerged as a valuable alternative platform for developing perception and planning models in both open-loop and closed-loop evaluation setups. Nevertheless, existing datasets collected on this platform present certain limitations. Some datasets appear to be tailored primarily for limited sensor configuration, with particular sensor configurations. To support end-to-end autonomous driving research, we have collected a new dataset comprising over 2.85 million frames using the CARLA simulation environment for the diverse Leaderboard 2.0 challenge scenarios. Our dataset is designed not only for planning tasks but also supports dynamic object detection, lane divider detection, centerline detection, traffic light recognition, prediction tasks and visual language action models . Furthermore, we demonstrate its versatility by training various models using our dataset. Moreover, we also provide numerical rarity scores to understand how rarely the current state occurs in the dataset.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684CARLA\u4eff\u771f\u6570\u636e\u96c6\uff0c\u5305\u542b285\u4e07\u5e27\u6570\u636e\uff0c\u4e13\u95e8\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7814\u7a76\u8bbe\u8ba1\uff0c\u652f\u6301\u611f\u77e5\u3001\u89c4\u5212\u3001\u9884\u6d4b\u7b49\u591a\u79cd\u4efb\u52a1\uff0c\u5e76\u63d0\u4f9b\u4e86\u573a\u666f\u7a00\u6709\u5ea6\u8bc4\u5206\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u9a7e\u9a76\u6570\u636e\u96c6\u5b58\u5728\u5c40\u9650\u6027\uff1a\u611f\u77e5\u6570\u636e\u96c6\u7f3a\u4e4f\u89c4\u5212\u6570\u636e\uff0c\u89c4\u5212\u6570\u636e\u96c6\u884c\u4e3a\u591a\u6837\u6027\u6709\u9650\uff0c\u4e14\u7f3a\u4e4f\u95ed\u73af\u8bc4\u4f30\u8bbe\u7f6e\u3002CARLA Leaderboard 2.0\u867d\u7136\u63d0\u4f9b\u4e86\u591a\u6837\u5316\u573a\u666f\uff0c\u4f46\u73b0\u6709\u6570\u636e\u96c6\u4f20\u611f\u5668\u914d\u7f6e\u6709\u9650\uff0c\u65e0\u6cd5\u6ee1\u8db3\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u7814\u7a76\u9700\u6c42\u3002", "method": "\u4f7f\u7528CARLA\u4eff\u771f\u73af\u5883\u6536\u96c6\u65b0\u6570\u636e\u96c6\uff0c\u8986\u76d6Leaderboard 2.0\u6311\u6218\u7684\u591a\u6837\u5316\u573a\u666f\uff0c\u5305\u542b\u8d85\u8fc7285\u4e07\u5e27\u6570\u636e\u3002\u6570\u636e\u96c6\u4e0d\u4ec5\u652f\u6301\u89c4\u5212\u4efb\u52a1\uff0c\u8fd8\u652f\u6301\u52a8\u6001\u7269\u4f53\u68c0\u6d4b\u3001\u8f66\u9053\u7ebf\u68c0\u6d4b\u3001\u4e2d\u5fc3\u7ebf\u68c0\u6d4b\u3001\u4ea4\u901a\u706f\u8bc6\u522b\u3001\u9884\u6d4b\u4efb\u52a1\u548c\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u6a21\u578b\u3002\u540c\u65f6\u63d0\u4f9b\u6570\u503c\u7a00\u6709\u5ea6\u8bc4\u5206\u6765\u8861\u91cf\u5f53\u524d\u72b6\u6001\u5728\u6570\u636e\u96c6\u4e2d\u7684\u7f55\u89c1\u7a0b\u5ea6\u3002", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b285\u4e07\u5e27\u7684\u7efc\u5408\u6027\u81ea\u52a8\u9a7e\u9a76\u6570\u636e\u96c6\uff0c\u8986\u76d6\u591a\u79cd\u4f20\u611f\u5668\u914d\u7f6e\u548c\u4efb\u52a1\u9700\u6c42\u3002\u901a\u8fc7\u5728\u8be5\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u591a\u79cd\u6a21\u578b\uff0c\u8bc1\u660e\u4e86\u5176\u591a\u529f\u80fd\u6027\u548c\u5b9e\u7528\u6027\u3002\u7a00\u6709\u5ea6\u8bc4\u5206\u673a\u5236\u6709\u52a9\u4e8e\u7406\u89e3\u6570\u636e\u5206\u5e03\u7279\u6027\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u586b\u8865\u4e86\u73b0\u6709\u81ea\u52a8\u9a7e\u9a76\u6570\u636e\u96c6\u7684\u7a7a\u767d\uff0c\u4e3a\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u7814\u7a76\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u6570\u636e\u652f\u6301\uff0c\u7279\u522b\u662f\u5728\u611f\u77e5-\u89c4\u5212\u4e00\u4f53\u5316\u3001\u95ed\u73af\u8bc4\u4f30\u548c\u957f\u5c3e\u95ee\u9898\u5904\u7406\u65b9\u9762\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2602.23553", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23553", "abs": "https://arxiv.org/abs/2602.23553", "authors": ["Shawn Liang", "Sahil Shah", "Chengwei Zhou", "SP Sharan", "Harsh Goel", "Arnab Sanyal", "Sandeep Chinchali", "Gourav Datta"], "title": "LE-NeuS: Latency-Efficient Neuro-Symbolic Video Understanding via Adaptive Temporal Verification", "comment": "Under review", "summary": "Neuro-symbolic approaches to long-form video question answering (LVQA) have demonstrated significant accuracy improvements by grounding temporal reasoning in formal verification. However, existing methods incur prohibitive latency overheads, up to 90x slower than base VLM prompting, rendering them impractical for latency-sensitive edge deployments. We present LE-NeuS, a latency-efficient neuro-symbolic framework that preserves the accuracy benefits of temporal logic-guided video understanding while drastically reducing inference latency. Our key insight is that the dominant computational bottleneck arises from sequential and dense proposition detection across video frames during automaton construction. We address this through two principled optimizations: (1) CLIP guided two-stage adaptive sampling that exploits visual redundancy to skip semantically similar frames while preserving temporal boundaries, and (2) batched proposition detection that parallelizes VLM inference across temporal windows. Theoretically, we derive latency bounds as a function of video length, proposition complexity, and sampling density, establishing conditions under which latency efficiency is achievable. Empirically, on LongVideoBench and Video-MME benchmarks deployed on NVIDIA H100 GPUs, LE-NeuS reduces the latency gap from 90x to approximately 10x while maintaining >10% accuracy gains on temporally complex queries.", "AI": {"tldr": "LE-NeuS\u662f\u4e00\u4e2a\u5ef6\u8fdf\u9ad8\u6548\u7684\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u91c7\u6837\u548c\u6279\u91cf\u547d\u9898\u68c0\u6d4b\u4f18\u5316\uff0c\u5c06LVQA\u7684\u63a8\u7406\u5ef6\u8fdf\u4ece90\u500d\u964d\u4f4e\u5230\u7ea610\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u65f6\u95f4\u590d\u6742\u67e5\u8be2\u7684\u51c6\u786e\u7387\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u5728\u957f\u89c6\u9891\u95ee\u7b54\u4e2d\u867d\u7136\u51c6\u786e\u7387\u663e\u8457\u63d0\u5347\uff0c\u4f46\u63a8\u7406\u5ef6\u8fdf\u8fc7\u9ad8\uff08\u6bd4\u57fa\u7840VLM\u63d0\u793a\u616290\u500d\uff09\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5ef6\u8fdf\u654f\u611f\u7684\u8fb9\u7f18\u90e8\u7f72\u9700\u6c42\u3002", "method": "\u91c7\u7528\u4e24\u79cd\u4f18\u5316\uff1a1) CLIP\u5f15\u5bfc\u7684\u4e24\u9636\u6bb5\u81ea\u9002\u5e94\u91c7\u6837\uff0c\u5229\u7528\u89c6\u89c9\u5197\u4f59\u8df3\u8fc7\u8bed\u4e49\u76f8\u4f3c\u5e27\u540c\u65f6\u4fdd\u7559\u65f6\u95f4\u8fb9\u754c\uff1b2) \u6279\u91cf\u547d\u9898\u68c0\u6d4b\uff0c\u5728\u65f6\u95f4\u7a97\u53e3\u4e0a\u5e76\u884c\u5316VLM\u63a8\u7406\u3002", "result": "\u5728LongVideoBench\u548cVideo-MME\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLE-NeuS\u5c06\u5ef6\u8fdf\u5dee\u8ddd\u4ece90\u500d\u964d\u4f4e\u5230\u7ea610\u500d\uff0c\u540c\u65f6\u5728\u65f6\u95f4\u590d\u6742\u67e5\u8be2\u4e0a\u4fdd\u6301\u8d85\u8fc710%\u7684\u51c6\u786e\u7387\u63d0\u5347\u3002", "conclusion": "LE-NeuS\u5728\u4fdd\u6301\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u51c6\u786e\u7387\u4f18\u52bf\u7684\u540c\u65f6\uff0c\u5927\u5e45\u964d\u4f4e\u4e86\u63a8\u7406\u5ef6\u8fdf\uff0c\u4e3a\u5ef6\u8fdf\u654f\u611f\u7684\u8fb9\u7f18\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.23413", "categories": ["cs.LG", "cs.CL", "cs.NE"], "pdf": "https://arxiv.org/pdf/2602.23413", "abs": "https://arxiv.org/abs/2602.23413", "authors": ["Shu Liu", "Shubham Agarwal", "Monishwaran Maheswaran", "Mert Cemri", "Zhifei Li", "Qiuyang Mang", "Ashwin Naren", "Ethan Boneh", "Audrey Cheng", "Melissa Z. Pan", "Alexander Du", "Kurt Keutzer", "Alexandros G. Dimakis", "Koushik Sen", "Matei Zaharia", "Ion Stoica"], "title": "EvoX: Meta-Evolution for Automated Discovery", "comment": null, "summary": "Recent work such as AlphaEvolve has shown that combining LLM-driven optimization with evolutionary search can effectively improve programs, prompts, and algorithms across domains. In this paradigm, previously evaluated solutions are reused to guide the model toward new candidate solutions. Crucially, the effectiveness of this evolution process depends on the search strategy: how prior solutions are selected and varied to generate new candidates. However, most existing methods rely on fixed search strategies with predefined knobs (e.g., explore-exploit ratios) that remain static throughout execution. While effective in some settings, these approaches often fail to adapt across tasks, or even within the same task as the search space changes over time. We introduce EvoX, an adaptive evolution method that optimizes its own evolution process. EvoX jointly evolves candidate solutions and the search strategies used to generate them, continuously updating how prior solutions are selected and varied based on progress. This enables the system to dynamically shift between different search strategies during the optimization process. Across nearly 200 real-world optimization tasks, EvoX outperforms existing AI-driven evolutionary methods including AlphaEvolve, OpenEvolve, GEPA, and ShinkaEvolve on the majority of tasks.", "AI": {"tldr": "EvoX\u662f\u4e00\u79cd\u81ea\u9002\u5e94\u8fdb\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u8fdb\u5316\u5019\u9009\u89e3\u548c\u641c\u7d22\u7b56\u7565\uff0c\u52a8\u6001\u8c03\u6574\u8fdb\u5316\u8fc7\u7a0b\uff0c\u5728\u8fd1200\u4e2a\u771f\u5b9e\u4e16\u754c\u4f18\u5316\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709AI\u9a71\u52a8\u7684\u8fdb\u5316\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5982AlphaEvolve\u867d\u7136\u7ed3\u5408\u4e86LLM\u9a71\u52a8\u7684\u4f18\u5316\u548c\u8fdb\u5316\u641c\u7d22\uff0c\u4f46\u4f9d\u8d56\u56fa\u5b9a\u7684\u641c\u7d22\u7b56\u7565\u548c\u9884\u5b9a\u4e49\u7684\u53c2\u6570\uff0c\u65e0\u6cd5\u9002\u5e94\u4e0d\u540c\u4efb\u52a1\u6216\u4efb\u52a1\u4e2d\u641c\u7d22\u7a7a\u95f4\u968f\u65f6\u95f4\u53d8\u5316\u7684\u60c5\u51b5\u3002", "method": "EvoX\u8054\u5408\u8fdb\u5316\u5019\u9009\u89e3\u548c\u751f\u6210\u8fd9\u4e9b\u89e3\u7684\u641c\u7d22\u7b56\u7565\uff0c\u6301\u7eed\u66f4\u65b0\u5982\u4f55\u9009\u62e9\u548c\u53d8\u5f02\u5148\u524d\u89e3\u7684\u65b9\u6cd5\uff0c\u4f7f\u7cfb\u7edf\u80fd\u591f\u5728\u4f18\u5316\u8fc7\u7a0b\u4e2d\u52a8\u6001\u5207\u6362\u4e0d\u540c\u7684\u641c\u7d22\u7b56\u7565\u3002", "result": "\u5728\u8fd1200\u4e2a\u771f\u5b9e\u4e16\u754c\u4f18\u5316\u4efb\u52a1\u4e2d\uff0cEvoX\u5728\u5927\u591a\u6570\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u7684AI\u9a71\u52a8\u8fdb\u5316\u65b9\u6cd5\uff0c\u5305\u62ecAlphaEvolve\u3001OpenEvolve\u3001GEPA\u548cShinkaEvolve\u3002", "conclusion": "EvoX\u901a\u8fc7\u81ea\u9002\u5e94\u4f18\u5316\u81ea\u8eab\u7684\u8fdb\u5316\u8fc7\u7a0b\uff0c\u80fd\u591f\u52a8\u6001\u8c03\u6574\u641c\u7d22\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8fdb\u5316\u65b9\u6cd5\u5728\u5404\u79cd\u4f18\u5316\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2602.23579", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23579", "abs": "https://arxiv.org/abs/2602.23579", "authors": ["Guillem Rodr\u00edguez-Corominas", "Maria J. Blesa", "Christian Blum"], "title": "Construct, Merge, Solve & Adapt with Reinforcement Learning for the min-max Multiple Traveling Salesman Problem", "comment": null, "summary": "The Multiple Traveling Salesman Problem (mTSP) extends the Traveling Salesman Problem to m tours that start and end at a common depot and jointly visit all customers exactly once. In the min-max variant, the objective is to minimize the longest tour, reflecting workload balance. We propose a hybrid approach, Construct, Merge, Solve & Adapt with Reinforcement Learning (RL-CMSA), for the symmetric single-depot min-max mTSP. The method iteratively constructs diverse solutions using probabilistic clustering guided by learned pairwise q-values, merges routes into a compact pool, solves a restricted set-covering MILP, and refines solutions via inter-route remove, shift, and swap moves. The q-values are updated by reinforcing city-pair co-occurrences in high-quality solutions, while the pool is adapted through ageing and pruning. This combination of exact optimization and reinforcement-guided construction balances exploration and exploitation. Computational results on random and TSPLIB instances show that RL-CMSA consistently finds (near-)best solutions and outperforms a state-of-the-art hybrid genetic algorithm under comparable time limits, especially as instance size and the number of salesmen increase.", "AI": {"tldr": "\u63d0\u51faRL-CMSA\u6df7\u5408\u65b9\u6cd5\u89e3\u51b3\u5bf9\u79f0\u5355\u4ed3\u5e93min-max mTSP\u95ee\u9898\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u5f15\u5bfc\u7684\u6784\u9020\u3001\u7cbe\u786e\u4f18\u5316\u548c\u81ea\u9002\u5e94\u6c60\u7ba1\u7406\uff0c\u5728\u5e73\u8861\u5de5\u4f5c\u8d1f\u8f7d\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u591a\u65c5\u884c\u5546\u95ee\u9898\u4e2d\u7684min-max\u53d8\u4f53\uff0c\u76ee\u6807\u662f\u5e73\u8861\u591a\u4e2a\u9500\u552e\u5458\u7684\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u6700\u5c0f\u5316\u6700\u957f\u8def\u7ebf\uff0c\u8fd9\u5728\u7269\u6d41\u8c03\u5ea6\u548c\u8d44\u6e90\u5206\u914d\u4e2d\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002", "method": "\u63d0\u51faRL-CMSA\u6df7\u5408\u65b9\u6cd5\uff1a1) \u4f7f\u7528\u57fa\u4e8e\u5b66\u4e60\u5230\u7684\u6210\u5bf9q\u503c\u7684\u6982\u7387\u805a\u7c7b\u6784\u9020\u591a\u6837\u5316\u89e3\uff1b2) \u5c06\u8def\u7ebf\u5408\u5e76\u5230\u7d27\u51d1\u6c60\u4e2d\uff1b3) \u6c42\u89e3\u53d7\u9650\u96c6\u5408\u8986\u76d6MILP\uff1b4) \u901a\u8fc7\u8de8\u8def\u7ebf\u79fb\u9664\u3001\u79fb\u52a8\u548c\u4ea4\u6362\u64cd\u4f5c\u6539\u8fdb\u89e3\uff1b5) \u901a\u8fc7\u5f3a\u5316\u9ad8\u8d28\u91cf\u89e3\u4e2d\u7684\u57ce\u5e02\u5bf9\u5171\u73b0\u66f4\u65b0q\u503c\uff1b6) \u901a\u8fc7\u8001\u5316\u548c\u526a\u679d\u81ea\u9002\u5e94\u7ba1\u7406\u6c60\u3002", "result": "\u5728\u968f\u673a\u548cTSPLIB\u5b9e\u4f8b\u4e0a\u7684\u8ba1\u7b97\u7ed3\u679c\u8868\u660e\uff0cRL-CMSA\u80fd\u6301\u7eed\u627e\u5230\uff08\u63a5\u8fd1\uff09\u6700\u4f18\u89e3\uff0c\u5728\u53ef\u6bd4\u65f6\u95f4\u9650\u5236\u4e0b\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u6df7\u5408\u9057\u4f20\u7b97\u6cd5\uff0c\u7279\u522b\u662f\u5728\u5b9e\u4f8b\u89c4\u6a21\u548c\u9500\u552e\u5458\u6570\u91cf\u589e\u52a0\u65f6\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "RL-CMSA\u65b9\u6cd5\u6210\u529f\u7ed3\u5408\u4e86\u7cbe\u786e\u4f18\u5316\u548c\u5f3a\u5316\u5b66\u4e60\u5f15\u5bfc\u7684\u6784\u9020\uff0c\u5e73\u8861\u4e86\u63a2\u7d22\u548c\u5229\u7528\uff0c\u4e3a\u89e3\u51b3min-max mTSP\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\uff0c\u5728\u5e73\u8861\u5de5\u4f5c\u8d1f\u8f7d\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2602.23577", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.23577", "abs": "https://arxiv.org/abs/2602.23577", "authors": ["Jun Li", "Xiangmeng Wang", "Haoyang Li", "Yifei Yan", "Shijie Zhang", "Hong Va Leong", "Ling Feng", "Nancy Xiaonan Yu", "Qing Li"], "title": "Multi-Agent Causal Reasoning for Suicide Ideation Detection Through Online Conversations", "comment": null, "summary": "Suicide remains a pressing global public health concern. While social media platforms offer opportunities for early risk detection through online conversation trees, existing approaches face two major limitations: (1) They rely on predefined rules (e.g., quotes or relies) to log conversations that capture only a narrow spectrum of user interactions, and (2) They overlook hidden influences such as user conformity and suicide copycat behavior, which can significantly affect suicidal expression and propagation in online communities. To address these limitations, we propose a Multi-Agent Causal Reasoning (MACR) framework that collaboratively employs a Reasoning Agent to scale user interactions and a Bias-aware Decision-Making Agent to mitigate harmful biases arising from hidden influences. The Reasoning Agent integrates cognitive appraisal theory to generate counterfactual user reactions to posts, thereby scaling user interactions. It analyses these reactions through structured dimensions, i.e., cognitive, emotional, and behavioral patterns, with a dedicated sub-agent responsible for each dimension. The Bias-aware Decision-Making Agent mitigates hidden biases through a front-door adjustment strategy, leveraging the counterfactual user reactions produced by the Reasoning Agent. Through the collaboration of reasoning and bias-aware decision making, the proposed MACR framework not only alleviates hidden biases, but also enriches contextual information of user interactions with counterfactual knowledge. Extensive experiments on real-world conversational datasets demonstrate the effectiveness and robustness of MACR in identifying suicide risk.", "AI": {"tldr": "\u63d0\u51faMACR\u591a\u667a\u80fd\u4f53\u56e0\u679c\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u63a8\u7406\u667a\u80fd\u4f53\u6269\u5c55\u7528\u6237\u4ea4\u4e92\uff0c\u504f\u7f6e\u611f\u77e5\u51b3\u7b56\u667a\u80fd\u4f53\u7f13\u89e3\u9690\u85cf\u504f\u7f6e\uff0c\u63d0\u5347\u793e\u4ea4\u5a92\u4f53\u81ea\u6740\u98ce\u9669\u68c0\u6d4b\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u793e\u4ea4\u5a92\u4f53\u81ea\u6740\u98ce\u9669\u68c0\u6d4b\u65b9\u6cd5\u5b58\u5728\u4e24\u5927\u5c40\u9650\uff1a1) \u4f9d\u8d56\u9884\u5b9a\u4e49\u89c4\u5219\uff08\u5982\u5f15\u7528\u6216\u56de\u590d\uff09\u8bb0\u5f55\u5bf9\u8bdd\uff0c\u53ea\u80fd\u6355\u6349\u6709\u9650\u7684\u7528\u6237\u4ea4\u4e92\uff1b2) \u5ffd\u89c6\u7528\u6237\u4ece\u4f17\u548c\u81ea\u6740\u6a21\u4eff\u884c\u4e3a\u7b49\u9690\u85cf\u5f71\u54cd\u56e0\u7d20\uff0c\u8fd9\u4e9b\u56e0\u7d20\u663e\u8457\u5f71\u54cd\u5728\u7ebf\u793e\u533a\u4e2d\u7684\u81ea\u6740\u8868\u8fbe\u548c\u4f20\u64ad\u3002", "method": "\u63d0\u51fa\u591a\u667a\u80fd\u4f53\u56e0\u679c\u63a8\u7406\uff08MACR\uff09\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u534f\u4f5c\u667a\u80fd\u4f53\uff1a\u63a8\u7406\u667a\u80fd\u4f53\u6574\u5408\u8ba4\u77e5\u8bc4\u4ef7\u7406\u8bba\u751f\u6210\u7528\u6237\u5bf9\u5e16\u5b50\u7684\u53cd\u4e8b\u5b9e\u53cd\u5e94\uff0c\u4ece\u800c\u6269\u5c55\u7528\u6237\u4ea4\u4e92\uff1b\u504f\u7f6e\u611f\u77e5\u51b3\u7b56\u667a\u80fd\u4f53\u901a\u8fc7\u524d\u95e8\u8c03\u6574\u7b56\u7565\u7f13\u89e3\u9690\u85cf\u504f\u7f6e\uff0c\u5229\u7528\u63a8\u7406\u667a\u80fd\u4f53\u4ea7\u751f\u7684\u53cd\u4e8b\u5b9e\u7528\u6237\u53cd\u5e94\u3002\u63a8\u7406\u667a\u80fd\u4f53\u901a\u8fc7\u8ba4\u77e5\u3001\u60c5\u611f\u548c\u884c\u4e3a\u4e09\u4e2a\u7ef4\u5ea6\u5206\u6790\u7528\u6237\u53cd\u5e94\uff0c\u6bcf\u4e2a\u7ef4\u5ea6\u6709\u4e13\u95e8\u7684\u5b50\u667a\u80fd\u4f53\u8d1f\u8d23\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u5bf9\u8bdd\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cMACR\u5728\u8bc6\u522b\u81ea\u6740\u98ce\u9669\u65b9\u9762\u5177\u6709\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "MACR\u6846\u67b6\u901a\u8fc7\u63a8\u7406\u548c\u504f\u7f6e\u611f\u77e5\u51b3\u7b56\u7684\u534f\u4f5c\uff0c\u4e0d\u4ec5\u7f13\u89e3\u4e86\u9690\u85cf\u504f\u7f6e\uff0c\u8fd8\u901a\u8fc7\u53cd\u4e8b\u5b9e\u77e5\u8bc6\u4e30\u5bcc\u4e86\u7528\u6237\u4ea4\u4e92\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u63d0\u5347\u4e86\u793e\u4ea4\u5a92\u4f53\u81ea\u6740\u98ce\u9669\u68c0\u6d4b\u80fd\u529b\u3002"}}
{"id": "2602.23524", "categories": ["cs.RO", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23524", "abs": "https://arxiv.org/abs/2602.23524", "authors": ["Faiz Aladin", "Ashwin Balasubramanian", "Lars Lindemann", "Daniel Seita"], "title": "V-MORALS: Visual Morse Graph-Aided Estimation of Regions of Attraction in a Learned Latent Space", "comment": null, "summary": "Reachability analysis has become increasingly important in robotics to distinguish safe from unsafe states. Unfortunately, existing reachability and safety analysis methods often fall short, as they typically require known system dynamics or large datasets to estimate accurate system models, are computationally expensive, and assume full state information. A recent method, called MORALS, aims to address these shortcomings by using topological tools to estimate3DR-eEgnciodnesr of Attraction (ROA) in a low-dimensional latent space. However, MORALS still relies on full state knowledge and has not been studied when only sensor measurements are available. This paper presents Visual Morse Graph-Aided Estimation of Regions of Attraction in a Learned Latent Space (V- MORALS). V-MORALS takes in a dataset of image-based trajectories of a system under a given controller, and learns a latent space for reachability analysis. Using this learned latent space, our method is able to generate well-defined Morse Graphs, from which we can compute ROAs for various systems and controllers. V-MORALS provides capabilities similar to the original MORALS architecture without relying on state knowledge, and using only high-level sensor data. Our project website is at: https://v-morals.onrender.com.", "AI": {"tldr": "V-MORALS\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u7684\u5438\u5f15\u57df\u4f30\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u56fe\u50cf\u8f68\u8ff9\u6570\u636e\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5b66\u4e60\u53ef\u8fbe\u6027\u5206\u6790\uff0c\u65e0\u9700\u7cfb\u7edf\u52a8\u529b\u5b66\u6a21\u578b\u6216\u5b8c\u6574\u72b6\u6001\u4fe1\u606f\u3002", "motivation": "\u73b0\u6709\u53ef\u8fbe\u6027\u548c\u5b89\u5168\u6027\u5206\u6790\u65b9\u6cd5\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u9700\u8981\u5df2\u77e5\u7cfb\u7edf\u52a8\u529b\u5b66\u6216\u5927\u91cf\u6570\u636e\u6765\u4f30\u8ba1\u51c6\u786e\u6a21\u578b\uff1b2\uff09\u8ba1\u7b97\u6210\u672c\u9ad8\uff1b3\uff09\u5047\u8bbe\u5b8c\u6574\u72b6\u6001\u4fe1\u606f\u3002MORALS\u65b9\u6cd5\u867d\u7136\u4f7f\u7528\u62d3\u6251\u5de5\u5177\u5728\u4f4e\u7ef4\u6f5c\u5728\u7a7a\u95f4\u4e2d\u4f30\u8ba1\u5438\u5f15\u57df\uff0c\u4f46\u4ecd\u4f9d\u8d56\u5b8c\u6574\u72b6\u6001\u77e5\u8bc6\uff0c\u4e14\u672a\u7814\u7a76\u4ec5\u4f7f\u7528\u4f20\u611f\u5668\u6d4b\u91cf\u7684\u60c5\u51b5\u3002", "method": "V-MORALS\u63a5\u6536\u7ed9\u5b9a\u63a7\u5236\u5668\u4e0b\u7cfb\u7edf\u7684\u56fe\u50cf\u8f68\u8ff9\u6570\u636e\u96c6\uff0c\u5b66\u4e60\u7528\u4e8e\u53ef\u8fbe\u6027\u5206\u6790\u7684\u6f5c\u5728\u7a7a\u95f4\u3002\u5229\u7528\u8fd9\u4e2a\u5b66\u4e60\u5230\u7684\u6f5c\u5728\u7a7a\u95f4\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u5b9a\u4e49\u826f\u597d\u7684\u83ab\u5c14\u65af\u56fe\uff0c\u4ece\u4e2d\u53ef\u4ee5\u8ba1\u7b97\u5404\u79cd\u7cfb\u7edf\u548c\u63a7\u5236\u5668\u7684\u5438\u5f15\u57df\u3002", "result": "V-MORALS\u63d0\u4f9b\u4e86\u4e0e\u539f\u59cbMORALS\u67b6\u6784\u7c7b\u4f3c\u7684\u80fd\u529b\uff0c\u4f46\u65e0\u9700\u4f9d\u8d56\u72b6\u6001\u77e5\u8bc6\uff0c\u4ec5\u4f7f\u7528\u9ad8\u7ea7\u4f20\u611f\u5668\u6570\u636e\uff08\u56fe\u50cf\uff09\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u89e3\u51b3\u4e86\u73b0\u6709\u53ef\u8fbe\u6027\u5206\u6790\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5728\u4ec5\u4f7f\u7528\u4f20\u611f\u5668\u6d4b\u91cf\uff08\u5982\u56fe\u50cf\uff09\u7684\u60c5\u51b5\u4e0b\uff0c\u4e3a\u673a\u5668\u4eba\u5b89\u5168\u5206\u6790\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.23559", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23559", "abs": "https://arxiv.org/abs/2602.23559", "authors": ["Cho-Ying Wu", "Zixun Huang", "Xinyu Huang", "Liu Ren"], "title": "No Calibration, No Depth, No Problem: Cross-Sensor View Synthesis with 3D Consistency", "comment": "CVPR 2026 Main Conference. Project page: https://choyingw.github.io/3d-rgbx.github.io/", "summary": "We present the first study of cross-sensor view synthesis across different modalities. We examine a practical, fundamental, yet widely overlooked problem: getting aligned RGB-X data, where most RGB-X prior work assumes such pairs exist and focuses on modality fusion, but it empirically requires huge engineering effort in calibration. We propose a match-densify-consolidate method. First, we perform RGB-X image matching followed by guided point densification. Using the proposed confidence-aware densification and self-matching filtering, we attain better view synthesis and later consolidate them in 3D Gaussian Splatting (3DGS). Our method uses no 3D priors for X-sensor and only assumes nearly no-cost COLMAP for RGB. We aim to remove the cumbersome calibration for various RGB-X sensors and advance the popularity of cross-sensor learning by a scalable solution that breaks through the bottleneck in large-scale real-world RGB-X data collection.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u8de8\u4f20\u611f\u5668\u6a21\u6001\u7684\u89c6\u56fe\u5408\u6210\u7814\u7a76\uff0c\u89e3\u51b3RGB-X\u6570\u636e\u5bf9\u9f50\u7684\u6821\u51c6\u96be\u9898\uff0c\u901a\u8fc7\u5339\u914d-\u7a20\u5bc6\u5316-\u6574\u5408\u65b9\u6cd5\u5b9e\u73b0\u65e0\u97003D\u5148\u9a8c\u7684\u8de8\u4f20\u611f\u5668\u5b66\u4e60", "motivation": "\u73b0\u6709RGB-X\u7814\u7a76\u5927\u591a\u5047\u8bbe\u5bf9\u9f50\u7684RGB-X\u6570\u636e\u5bf9\u5b58\u5728\uff0c\u4e13\u6ce8\u4e8e\u6a21\u6001\u878d\u5408\uff0c\u4f46\u5b9e\u9645\u6821\u51c6\u9700\u8981\u5927\u91cf\u5de5\u7a0b\u52aa\u529b\u3002\u672c\u6587\u65e8\u5728\u6d88\u9664\u5404\u79cdRGB-X\u4f20\u611f\u5668\u7684\u7e41\u7410\u6821\u51c6\uff0c\u901a\u8fc7\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\u7a81\u7834\u5927\u89c4\u6a21\u771f\u5b9e\u4e16\u754cRGB-X\u6570\u636e\u6536\u96c6\u7684\u74f6\u9888", "method": "\u63d0\u51fa\u5339\u914d-\u7a20\u5bc6\u5316-\u6574\u5408\u65b9\u6cd5\uff1a1) RGB-X\u56fe\u50cf\u5339\u914d\uff1b2) \u5f15\u5bfc\u70b9\u7a20\u5bc6\u5316\uff08\u4f7f\u7528\u7f6e\u4fe1\u5ea6\u611f\u77e5\u7a20\u5bc6\u5316\u548c\u81ea\u5339\u914d\u8fc7\u6ee4\uff09\uff1b3) \u57283D\u9ad8\u65af\u6cfc\u6e85\u4e2d\u6574\u5408\u89c6\u56fe\u5408\u6210\u7ed3\u679c\u3002\u8be5\u65b9\u6cd5\u5bf9X\u4f20\u611f\u5668\u4e0d\u4f7f\u75283D\u5148\u9a8c\uff0c\u4ec5\u5047\u8bbeRGB\u4f7f\u7528\u8fd1\u4e4e\u96f6\u6210\u672c\u7684COLMAP", "result": "\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u89c6\u56fe\u5408\u6210\u6548\u679c\uff0c\u80fd\u591f\u5904\u7406\u5404\u79cdRGB-X\u4f20\u611f\u5668\u7ec4\u5408\uff0c\u4e3a\u8de8\u4f20\u611f\u5668\u5b66\u4e60\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848", "conclusion": "\u8be5\u65b9\u6cd5\u6d88\u9664\u4e86RGB-X\u4f20\u611f\u5668\u7684\u7e41\u7410\u6821\u51c6\u9700\u6c42\uff0c\u901a\u8fc7\u7a81\u7834\u5927\u89c4\u6a21\u771f\u5b9e\u4e16\u754cRGB-X\u6570\u636e\u6536\u96c6\u7684\u74f6\u9888\uff0c\u63a8\u52a8\u4e86\u8de8\u4f20\u611f\u5668\u5b66\u4e60\u7684\u666e\u53ca"}}
{"id": "2602.23446", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23446", "abs": "https://arxiv.org/abs/2602.23446", "authors": ["Alejandro Rodriguez Dominguez"], "title": "Human Supervision as an Information Bottleneck: A Unified Theory of Error Floors in Human-Guided Learning", "comment": "Proceedings from IEEE CAI 2026, Conference on Artificial Intelligence, 8-10 May, Granada, Spain. 8 Pages, 3 Figures, 7 Tables", "summary": "Large language models are trained primarily on human-generated data and feedback, yet they exhibit persistent errors arising from annotation noise, subjective preferences, and the limited expressive bandwidth of natural language. We argue that these limitations reflect structural properties of the supervision channel rather than model scale or optimization. We develop a unified theory showing that whenever the human supervision channel is not sufficient for a latent evaluation target, it acts as an information-reducing channel that induces a strictly positive excess-risk floor for any learner dominated by it. We formalize this Human-Bounded Intelligence limit and show that across six complementary frameworks (operator theory, PAC-Bayes, information theory, causal inference, category theory, and game-theoretic analyses of reinforcement learning from human feedback), non-sufficiency yields strictly positive lower bounds arising from the same structural decomposition into annotation noise, preference distortion, and semantic compression. The theory explains why scaling alone cannot eliminate persistent human-aligned errors and characterizes conditions under which auxiliary non-human signals (e.g., retrieval, program execution, tools) increase effective supervision capacity and collapse the floor by restoring information about the latent target. Experiments on real preference data, synthetic known-target tasks, and externally verifiable benchmarks confirm the predicted structural signatures: human-only supervision exhibits a persistent floor, while sufficiently informative auxiliary channels strictly reduce or eliminate excess error.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\"\u4eba\u7c7b\u6709\u754c\u667a\u80fd\u6781\u9650\"\u7406\u8bba\uff0c\u8ba4\u4e3a\u4ec5\u4f9d\u8d56\u4eba\u7c7b\u76d1\u7763\u7684LLMs\u5b58\u5728\u56fa\u6709\u8bef\u5dee\u4e0b\u9650\uff0c\u6e90\u4e8e\u6807\u6ce8\u566a\u58f0\u3001\u504f\u597d\u626d\u66f2\u548c\u8bed\u4e49\u538b\u7f29\u7b49\u76d1\u7763\u901a\u9053\u7684\u7ed3\u6784\u6027\u9650\u5236\uff0c\u800c\u975e\u6a21\u578b\u89c4\u6a21\u6216\u4f18\u5316\u95ee\u9898\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u4e3b\u8981\u57fa\u4e8e\u4eba\u7c7b\u751f\u6210\u7684\u6570\u636e\u548c\u53cd\u9988\u8fdb\u884c\u8bad\u7ec3\uff0c\u4f46\u8868\u73b0\u51fa\u6301\u7eed\u7684\u8bef\u5dee\uff0c\u8fd9\u4e9b\u8bef\u5dee\u6e90\u4e8e\u6807\u6ce8\u566a\u58f0\u3001\u4e3b\u89c2\u504f\u597d\u548c\u81ea\u7136\u8bed\u8a00\u7684\u6709\u9650\u8868\u8fbe\u80fd\u529b\u3002\u4f5c\u8005\u8ba4\u4e3a\u8fd9\u4e9b\u9650\u5236\u53cd\u6620\u4e86\u76d1\u7763\u901a\u9053\u7684\u7ed3\u6784\u7279\u6027\uff0c\u800c\u975e\u6a21\u578b\u89c4\u6a21\u6216\u4f18\u5316\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u7edf\u4e00\u7406\u8bba\u6846\u67b6\uff0c\u8bc1\u660e\u5f53\u4eba\u7c7b\u76d1\u7763\u901a\u9053\u4e0d\u8db3\u4ee5\u6355\u6349\u6f5c\u5728\u8bc4\u4f30\u76ee\u6807\u65f6\uff0c\u5b83\u4f1a\u4f5c\u4e3a\u4fe1\u606f\u51cf\u5c11\u901a\u9053\uff0c\u4e3a\u4efb\u4f55\u53d7\u5176\u652f\u914d\u7684\u5b66\u4e60\u5668\u5f15\u5165\u4e25\u683c\u6b63\u7684\u8d85\u989d\u98ce\u9669\u4e0b\u9650\u3002\u901a\u8fc7\u516d\u4e2a\u4e92\u8865\u6846\u67b6\uff08\u7b97\u5b50\u7406\u8bba\u3001PAC-Bayes\u3001\u4fe1\u606f\u8bba\u3001\u56e0\u679c\u63a8\u65ad\u3001\u8303\u7574\u8bba\u548cRLHF\u7684\u535a\u5f08\u8bba\u5206\u6790\uff09\u5f62\u5f0f\u5316\u8fd9\u4e00\"\u4eba\u7c7b\u6709\u754c\u667a\u80fd\u6781\u9650\"\u3002", "result": "\u7406\u8bba\u89e3\u91ca\u4e86\u4e3a\u4ec0\u4e48\u4ec5\u9760\u6269\u5c55\u65e0\u6cd5\u6d88\u9664\u6301\u7eed\u7684\u4eba\u7c7b\u5bf9\u9f50\u8bef\u5dee\uff0c\u5e76\u63cf\u8ff0\u4e86\u8f85\u52a9\u975e\u4eba\u7c7b\u4fe1\u53f7\uff08\u5982\u68c0\u7d22\u3001\u7a0b\u5e8f\u6267\u884c\u3001\u5de5\u5177\uff09\u589e\u52a0\u6709\u6548\u76d1\u7763\u5bb9\u91cf\u5e76\u6d88\u9664\u8bef\u5dee\u4e0b\u9650\u7684\u6761\u4ef6\u3002\u5728\u771f\u5b9e\u504f\u597d\u6570\u636e\u3001\u5408\u6210\u5df2\u77e5\u76ee\u6807\u4efb\u52a1\u548c\u5916\u90e8\u53ef\u9a8c\u8bc1\u57fa\u51c6\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u5b9e\u4e86\u9884\u6d4b\u7684\u7ed3\u6784\u7279\u5f81\u3002", "conclusion": "\u4ec5\u4f9d\u8d56\u4eba\u7c7b\u76d1\u7763\u5b58\u5728\u56fa\u6709\u8bef\u5dee\u4e0b\u9650\uff0c\u800c\u8db3\u591f\u4fe1\u606f\u4e30\u5bcc\u7684\u8f85\u52a9\u901a\u9053\u53ef\u4ee5\u4e25\u683c\u51cf\u5c11\u6216\u6d88\u9664\u8d85\u989d\u8bef\u5dee\u3002\u8fd9\u4e3a\u7406\u89e3LLMs\u7684\u5c40\u9650\u6027\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u5e76\u6307\u51fa\u4e86\u901a\u8fc7\u7ed3\u5408\u975e\u4eba\u7c7b\u4fe1\u53f7\u6765\u8d85\u8d8a\u4eba\u7c7b\u76d1\u7763\u9650\u5236\u7684\u9014\u5f84\u3002"}}
{"id": "2602.23605", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23605", "abs": "https://arxiv.org/abs/2602.23605", "authors": ["Zongzhe Xu", "Zitao Shuai", "Eideen Mozaffari", "Ravi S. Aysola", "Rajesh Kumar", "Yuzhe Yang"], "title": "SleepLM: Natural-Language Intelligence for Human Sleep", "comment": null, "summary": "We present SleepLM, a family of sleep-language foundation models that enable human sleep alignment, interpretation, and interaction with natural language. Despite the critical role of sleep, learning-based sleep analysis systems operate in closed label spaces (e.g., predefined stages or events) and fail to describe, query, or generalize to novel sleep phenomena. SleepLM bridges natural language and multimodal polysomnography, enabling language-grounded representations of sleep physiology. To support this alignment, we introduce a multilevel sleep caption generation pipeline that enables the curation of the first large-scale sleep-text dataset, comprising over 100K hours of data from more than 10,000 individuals. Furthermore, we present a unified pretraining objective that combines contrastive alignment, caption generation, and signal reconstruction to better capture physiological fidelity and cross-modal interactions. Extensive experiments on real-world sleep understanding tasks verify that SleepLM outperforms state-of-the-art in zero-shot and few-shot learning, cross-modal retrieval, and sleep captioning. Importantly, SleepLM also exhibits intriguing capabilities including language-guided event localization, targeted insight generation, and zero-shot generalization to unseen tasks. All code and data will be open-sourced.", "AI": {"tldr": "SleepLM\u662f\u4e00\u4e2a\u7761\u7720-\u8bed\u8a00\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u4e0e\u591a\u6a21\u6001\u591a\u5bfc\u7761\u7720\u56fe\u5bf9\u9f50\uff0c\u5b9e\u73b0\u4e86\u4eba\u7c7b\u7761\u7720\u7684\u5bf9\u9f50\u3001\u89e3\u91ca\u548c\u4ea4\u4e92\uff0c\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u5b66\u4e60\u3001\u8de8\u6a21\u6001\u68c0\u7d22\u7b49\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5b66\u4e60\u7684\u7761\u7720\u5206\u6790\u7cfb\u7edf\u5728\u5c01\u95ed\u6807\u7b7e\u7a7a\u95f4\uff08\u5982\u9884\u5b9a\u4e49\u9636\u6bb5\u6216\u4e8b\u4ef6\uff09\u4e2d\u8fd0\u884c\uff0c\u65e0\u6cd5\u63cf\u8ff0\u3001\u67e5\u8be2\u6216\u6cdb\u5316\u5230\u65b0\u7684\u7761\u7720\u73b0\u8c61\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u8fde\u63a5\u81ea\u7136\u8bed\u8a00\u548c\u7761\u7720\u751f\u7406\u5b66\u7684\u7cfb\u7edf\u3002", "method": "1. \u5f15\u5165\u591a\u7ea7\u7761\u7720\u63cf\u8ff0\u751f\u6210\u6d41\u7a0b\uff0c\u521b\u5efa\u9996\u4e2a\u5927\u89c4\u6a21\u7761\u7720-\u6587\u672c\u6570\u636e\u96c6\uff08\u8d85\u8fc710\u4e07\u5c0f\u65f6\u6570\u636e\uff0c\u6765\u81ea1\u4e07\u591a\u4e2a\u4e2a\u4f53\uff09\uff1b2. \u63d0\u51fa\u7edf\u4e00\u7684\u9884\u8bad\u7ec3\u76ee\u6807\uff0c\u7ed3\u5408\u5bf9\u6bd4\u5bf9\u9f50\u3001\u63cf\u8ff0\u751f\u6210\u548c\u4fe1\u53f7\u91cd\u5efa\uff0c\u4ee5\u66f4\u597d\u5730\u6355\u6349\u751f\u7406\u4fdd\u771f\u5ea6\u548c\u8de8\u6a21\u6001\u4ea4\u4e92\u3002", "result": "SleepLM\u5728\u771f\u5b9e\u4e16\u754c\u7761\u7720\u7406\u89e3\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u5b66\u4e60\u3001\u8de8\u6a21\u6001\u68c0\u7d22\u548c\u7761\u7720\u63cf\u8ff0\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002\u6a21\u578b\u8fd8\u5c55\u73b0\u51fa\u8bed\u8a00\u5f15\u5bfc\u4e8b\u4ef6\u5b9a\u4f4d\u3001\u9488\u5bf9\u6027\u6d1e\u5bdf\u751f\u6210\u548c\u96f6\u6837\u672c\u6cdb\u5316\u5230\u672a\u89c1\u4efb\u52a1\u7684\u80fd\u529b\u3002", "conclusion": "SleepLM\u6210\u529f\u6784\u5efa\u4e86\u81ea\u7136\u8bed\u8a00\u4e0e\u591a\u6a21\u6001\u591a\u5bfc\u7761\u7720\u56fe\u4e4b\u95f4\u7684\u6865\u6881\uff0c\u5b9e\u73b0\u4e86\u8bed\u8a00\u57fa\u7840\u7684\u7761\u7720\u751f\u7406\u5b66\u8868\u793a\uff0c\u4e3a\u7761\u7720\u5206\u6790\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u548c\u901a\u7528\u7684\u6846\u67b6\u3002\u6240\u6709\u4ee3\u7801\u548c\u6570\u636e\u5c06\u5f00\u6e90\u3002"}}
{"id": "2602.23580", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23580", "abs": "https://arxiv.org/abs/2602.23580", "authors": ["Yun Wang", "Xuansheng Wu", "Jingyuan Huang", "Lei Liu", "Xiaoming Zhai", "Ninghao Liu"], "title": "BRIDGE the Gap: Mitigating Bias Amplification in Automated Scoring of English Language Learners via Inter-group Data Augmentation", "comment": "15 pages, 1 figure", "summary": "In the field of educational assessment, automated scoring systems increasingly rely on deep learning and large language models (LLMs). However, these systems face significant risks of bias amplification, where model prediction gaps between student groups become larger than those observed in training data. This issue is especially severe for underrepresented groups such as English Language Learners (ELLs), as models may inherit and further magnify existing disparities in the data. We identify that this issue is closely tied to representation bias: the scarcity of minority (high-scoring ELL) samples makes models trained with empirical risk minimization favor majority (non-ELL) linguistic patterns. Consequently, models tend to under-predict ELL students who even demonstrate comparable domain knowledge but use different linguistic patterns, thereby undermining the fairness of automated scoring outcomes. To mitigate this, we propose BRIDGE, a Bias-Reducing Inter-group Data GEneration framework designed for low-resource assessment settings. Instead of relying on the limited minority samples, BRIDGE synthesizes high-scoring ELL samples by \"pasting\" construct-relevant (i.e., rubric-aligned knowledge and evidence) content from abundant high-scoring non-ELL samples into authentic ELL linguistic patterns. We further introduce a discriminator model to ensure the quality of synthetic samples. Experiments on California Science Test (CAST) datasets demonstrate that BRIDGE effectively reduces prediction bias for high-scoring ELL students while maintaining overall scoring performance. Notably, our method achieves fairness gains comparable to using additional real human data, offering a cost-effective solution for ensuring equitable scoring in large-scale assessments.", "AI": {"tldr": "BRIDGE\u6846\u67b6\u901a\u8fc7\u5408\u6210\u9ad8\u8d28\u91cf\u82f1\u8bed\u5b66\u4e60\u8005\u6837\u672c\u6765\u51cf\u5c11\u81ea\u52a8\u8bc4\u5206\u7cfb\u7edf\u4e2d\u7684\u504f\u89c1\u653e\u5927\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u6574\u4f53\u8bc4\u5206\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u5bf9\u9ad8\u5206\u6570\u82f1\u8bed\u5b66\u4e60\u8005\u7684\u9884\u6d4b\u516c\u5e73\u6027\u3002", "motivation": "\u6559\u80b2\u8bc4\u4f30\u4e2d\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u548c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u52a8\u8bc4\u5206\u7cfb\u7edf\u5b58\u5728\u504f\u89c1\u653e\u5927\u98ce\u9669\uff0c\u7279\u522b\u662f\u5bf9\u82f1\u8bed\u5b66\u4e60\u8005\u7b49\u5c11\u6570\u7fa4\u4f53\u3002\u7531\u4e8e\u5c11\u6570\u7fa4\u4f53\uff08\u9ad8\u5206\u6570\u82f1\u8bed\u5b66\u4e60\u8005\uff09\u6837\u672c\u7a00\u7f3a\uff0c\u6a21\u578b\u503e\u5411\u4e8e\u504f\u5411\u591a\u6570\u7fa4\u4f53\uff08\u975e\u82f1\u8bed\u5b66\u4e60\u8005\uff09\u7684\u8bed\u8a00\u6a21\u5f0f\uff0c\u5bfc\u81f4\u5bf9\u82f1\u8bed\u5b66\u4e60\u8005\u8bc4\u5206\u504f\u4f4e\uff0c\u5373\u4f7f\u4ed6\u4eec\u5c55\u793a\u4e86\u76f8\u5f53\u7684\u5b66\u79d1\u77e5\u8bc6\u3002", "method": "\u63d0\u51faBRIDGE\u6846\u67b6\uff1a\u4e00\u79cd\u504f\u89c1\u51cf\u5c11\u7684\u7ec4\u95f4\u6570\u636e\u751f\u6210\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u4e0d\u4f9d\u8d56\u6709\u9650\u7684\u5c11\u6570\u7fa4\u4f53\u6837\u672c\uff0c\u800c\u662f\u901a\u8fc7\u5c06\u4e30\u5bcc\u7684\u9ad8\u5206\u6570\u975e\u82f1\u8bed\u5b66\u4e60\u8005\u6837\u672c\u4e2d\u7684\u5efa\u6784\u76f8\u5173\uff08\u5373\u8bc4\u5206\u6807\u51c6\u5bf9\u9f50\u7684\u77e5\u8bc6\u548c\u8bc1\u636e\uff09\u5185\u5bb9\"\u7c98\u8d34\"\u5230\u771f\u5b9e\u7684\u82f1\u8bed\u5b66\u4e60\u8005\u8bed\u8a00\u6a21\u5f0f\u4e2d\uff0c\u5408\u6210\u9ad8\u8d28\u91cf\u82f1\u8bed\u5b66\u4e60\u8005\u6837\u672c\u3002\u540c\u65f6\u5f15\u5165\u5224\u522b\u5668\u6a21\u578b\u786e\u4fdd\u5408\u6210\u6837\u672c\u8d28\u91cf\u3002", "result": "\u5728\u52a0\u5dde\u79d1\u5b66\u6d4b\u8bd5\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cBRIDGE\u6709\u6548\u51cf\u5c11\u4e86\u9ad8\u5206\u6570\u82f1\u8bed\u5b66\u4e60\u8005\u7684\u9884\u6d4b\u504f\u89c1\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6574\u4f53\u8bc4\u5206\u6027\u80fd\u3002\u8be5\u65b9\u6cd5\u5b9e\u73b0\u7684\u516c\u5e73\u6027\u63d0\u5347\u4e0e\u4f7f\u7528\u989d\u5916\u771f\u5b9e\u4eba\u7c7b\u6570\u636e\u76f8\u5f53\u3002", "conclusion": "BRIDGE\u4e3a\u5927\u89c4\u6a21\u8bc4\u4f30\u4e2d\u7684\u516c\u5e73\u8bc4\u5206\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7ecf\u6d4e\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u5408\u6210\u6570\u636e\u751f\u6210\u65b9\u6cd5\u7f13\u89e3\u4e86\u81ea\u52a8\u8bc4\u5206\u7cfb\u7edf\u4e2d\u7684\u504f\u89c1\u653e\u5927\u95ee\u9898\uff0c\u7279\u522b\u662f\u9488\u5bf9\u8d44\u6e90\u6709\u9650\u7684\u8bc4\u4f30\u73af\u5883\u3002"}}
{"id": "2602.24268", "categories": ["eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.24268", "abs": "https://arxiv.org/abs/2602.24268", "authors": ["Alexandre Anahory Simoes", "Leonardo Colombo", "Juan Giribet", "Efstratios Stratoglou"], "title": "Virtual Constraint for a Quadrotor UAV Enforcing a Body-Axis Pointing Direction", "comment": null, "summary": "We propose a geometric control framework on $SE(3)$ for quadrotors that enforces pointing-driven missions without completing a full attitude reference. The mission is encoded through virtual constraints defining a task manifold and an associated set of admissible velocities, and invariance is achieved by a feedback law obtained from a linear system in selected inputs. Under a transversality condition with the effective actuation distribution, the invariance-enforcing input is uniquely defined, yielding a constructive control law and, for relevant tasks, closed-form expressions. We further derive a local off-manifold stabilization extension. As a case study, we lock a body axis to a prescribed line-of-sight direction while maintaining fixed altitude.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5728SE(3)\u4e0a\u7684\u51e0\u4f55\u63a7\u5236\u6846\u67b6\uff0c\u7528\u4e8e\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u6267\u884c\u6307\u5411\u9a71\u52a8\u4efb\u52a1\uff0c\u65e0\u9700\u5b8c\u6210\u5b8c\u6574\u7684\u59ff\u6001\u53c2\u8003\u3002", "motivation": "\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u5728\u6267\u884c\u6307\u5411\u9a71\u52a8\u4efb\u52a1\uff08\u5982\u76ee\u6807\u8ddf\u8e2a\u3001\u76d1\u89c6\u7b49\uff09\u65f6\uff0c\u901a\u5e38\u9700\u8981\u5b8c\u6574\u7684\u59ff\u6001\u63a7\u5236\uff0c\u4f46\u67d0\u4e9b\u4efb\u52a1\u53ea\u9700\u8981\u7279\u5b9a\u7684\u6307\u5411\u7ea6\u675f\uff0c\u800c\u4e0d\u9700\u8981\u5b8c\u6574\u7684\u59ff\u6001\u53c2\u8003\u3002\u73b0\u6709\u65b9\u6cd5\u53ef\u80fd\u8fc7\u4e8e\u590d\u6742\u6216\u4e0d\u5fc5\u8981\u5730\u9650\u5236\u4e86\u65e0\u4eba\u673a\u7684\u8fd0\u52a8\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u865a\u62df\u7ea6\u675f\u5b9a\u4e49\u4efb\u52a1\u6d41\u5f62\u548c\u76f8\u5173\u7684\u5bb9\u8bb8\u901f\u5ea6\u96c6\uff0c\u5728\u6ee1\u8db3\u6709\u6548\u9a71\u52a8\u5206\u5e03\u6a2a\u622a\u6027\u6761\u4ef6\u4e0b\uff0c\u901a\u8fc7\u7ebf\u6027\u7cfb\u7edf\u9009\u62e9\u8f93\u5165\u83b7\u5f97\u53cd\u9988\u63a7\u5236\u5f8b\uff0c\u786e\u4fdd\u6d41\u5f62\u4e0d\u53d8\u6027\u3002\u8fdb\u4e00\u6b65\u63a8\u5bfc\u4e86\u5c40\u90e8\u79bb\u6d41\u5f62\u7a33\u5b9a\u5316\u6269\u5c55\u3002", "result": "\u5728\u6a2a\u622a\u6027\u6761\u4ef6\u4e0b\uff0c\u4e0d\u53d8\u6027\u5f3a\u5236\u8f93\u5165\u88ab\u552f\u4e00\u786e\u5b9a\uff0c\u4ea7\u751f\u4e86\u6784\u9020\u6027\u63a7\u5236\u5f8b\uff0c\u5bf9\u4e8e\u76f8\u5173\u4efb\u52a1\u83b7\u5f97\u4e86\u95ed\u5f0f\u8868\u8fbe\u5f0f\u3002\u6848\u4f8b\u7814\u7a76\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5c06\u673a\u4f53\u8f74\u9501\u5b9a\u5230\u89c4\u5b9a\u7684\u89c6\u7ebf\u65b9\u5411\uff0c\u540c\u65f6\u4fdd\u6301\u56fa\u5b9a\u9ad8\u5ea6\u3002", "conclusion": "\u63d0\u51fa\u7684\u51e0\u4f55\u63a7\u5236\u6846\u67b6\u80fd\u591f\u6709\u6548\u5904\u7406\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u7684\u6307\u5411\u9a71\u52a8\u4efb\u52a1\uff0c\u901a\u8fc7\u4efb\u52a1\u6d41\u5f62\u548c\u5bb9\u8bb8\u901f\u5ea6\u96c6\u7f16\u7801\u4efb\u52a1\u8981\u6c42\uff0c\u5728\u6ee1\u8db3\u6a2a\u622a\u6027\u6761\u4ef6\u4e0b\u83b7\u5f97\u552f\u4e00\u63a7\u5236\u5f8b\uff0c\u4e3a\u76f8\u5173\u5e94\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u7528\u63a7\u5236\u65b9\u6848\u3002"}}
{"id": "2602.23576", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.23576", "abs": "https://arxiv.org/abs/2602.23576", "authors": ["Anuraj Uthayasooriyan", "Krishna Manaswi Digumarti", "Jack Breward", "Fernando Vanegas", "Julian Galvez-Serna", "Felipe Gonzalez"], "title": "Tilt-X: Enabling Compliant Aerial Manipulation through a Tiltable-Extensible Continuum Manipulator", "comment": "Accepted to IEEE International Conference on Robotics and Automation (ICRA) 2026", "summary": "Aerial manipulators extend the reach and manipulation capabilities of uncrewed multirotor aerial vehicles for inspection, agriculture, sampling, and delivery. Continuum arm aerial manipulation systems offer lightweight, dexterous, and compliant interaction opportunities. Existing designs allow manipulation only below the UAV which restricts their deployability in multiple directions and through clutter. They are also sensitive to propeller downwash. Addressing these limitations, we present Tilt-X, a continuum arm aerial manipulator that integrates a tilting mechanism, a telescopic stage, and a cable-driven continuum section. We present its design and kinematic model and validate it through flight demonstrations. Tilt-X enables a volumetric workspace with up to 75 mm extension and planar orientations between 0$^\\circ$ to 90$^\\circ$. Experiments comparing end effector pose with and without downwash quantitatively measure its accuracy, providing critical evidence to guide the design and control of reliable aerial manipulators. Results show stabilisation of end effector pose as the manipulator extends out of the propeller influence zone.", "AI": {"tldr": "Tilt-X\u662f\u4e00\u79cd\u65b0\u578b\u8fde\u7eed\u81c2\u7a7a\u4e2d\u673a\u68b0\u624b\uff0c\u901a\u8fc7\u503e\u659c\u673a\u6784\u3001\u4f38\u7f29\u5e73\u53f0\u548c\u7f06\u7ef3\u9a71\u52a8\u8fde\u7eed\u81c2\u8bbe\u8ba1\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u7a7a\u4e2d\u673a\u68b0\u624b\u53ea\u80fd\u5411\u4e0b\u64cd\u4f5c\u3001\u6613\u53d7\u87ba\u65cb\u6868\u4e0b\u6d17\u6d41\u5f71\u54cd\u7684\u9650\u5236\uff0c\u5b9e\u73b0\u4e86\u591a\u65b9\u5411\u64cd\u4f5c\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u8fde\u7eed\u81c2\u7a7a\u4e2d\u673a\u68b0\u624b\u7cfb\u7edf\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u9650\u5236\uff1a1\uff09\u53ea\u80fd\u5728\u65e0\u4eba\u673a\u4e0b\u65b9\u8fdb\u884c\u64cd\u4f5c\uff0c\u9650\u5236\u4e86\u591a\u65b9\u5411\u90e8\u7f72\u548c\u901a\u8fc7\u590d\u6742\u73af\u5883\u7684\u80fd\u529b\uff1b2\uff09\u5bf9\u87ba\u65cb\u6868\u4e0b\u6d17\u6d41\u654f\u611f\uff0c\u5f71\u54cd\u64cd\u4f5c\u7cbe\u5ea6\u548c\u7a33\u5b9a\u6027\u3002", "method": "\u63d0\u51faTilt-X\u7cfb\u7edf\uff0c\u6574\u5408\u4e86\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a1\uff09\u503e\u659c\u673a\u6784\u5b9e\u73b0\u5e73\u9762\u65b9\u5411\u8c03\u6574\uff080\u00b0-90\u00b0\uff09\uff1b2\uff09\u4f38\u7f29\u5e73\u53f0\u63d0\u4f9b75mm\u7684\u5ef6\u4f38\u8ddd\u79bb\uff1b3\uff09\u7f06\u7ef3\u9a71\u52a8\u8fde\u7eed\u81c2\u5b9e\u73b0\u8f7b\u91cf\u5316\u548c\u67d4\u987a\u64cd\u4f5c\u3002\u5efa\u7acb\u4e86\u7cfb\u7edf\u7684\u8fd0\u52a8\u5b66\u6a21\u578b\u5e76\u901a\u8fc7\u98de\u884c\u6f14\u793a\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "Tilt-X\u5b9e\u73b0\u4e86\u4f53\u79ef\u5de5\u4f5c\u7a7a\u95f4\uff0c\u5177\u670975mm\u5ef6\u4f38\u80fd\u529b\u548c0\u00b0-90\u00b0\u5e73\u9762\u65b9\u5411\u8c03\u6574\u8303\u56f4\u3002\u5b9e\u9a8c\u6bd4\u8f83\u4e86\u6709/\u65e0\u4e0b\u6d17\u6d41\u60c5\u51b5\u4e0b\u7684\u672b\u7aef\u6267\u884c\u5668\u4f4d\u59ff\u7cbe\u5ea6\uff0c\u7ed3\u679c\u663e\u793a\u5f53\u673a\u68b0\u81c2\u5ef6\u4f38\u51fa\u87ba\u65cb\u6868\u5f71\u54cd\u533a\u57df\u65f6\uff0c\u672b\u7aef\u6267\u884c\u5668\u4f4d\u59ff\u5f97\u5230\u7a33\u5b9a\u3002", "conclusion": "Tilt-X\u7cfb\u7edf\u901a\u8fc7\u521b\u65b0\u7684\u503e\u659c\u548c\u4f38\u7f29\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7a7a\u4e2d\u673a\u68b0\u624b\u7684\u64cd\u4f5c\u7075\u6d3b\u6027\u548c\u6297\u5e72\u6270\u80fd\u529b\uff0c\u4e3a\u53ef\u9760\u7a7a\u4e2d\u673a\u68b0\u624b\u7684\u8bbe\u8ba1\u548c\u63a7\u5236\u63d0\u4f9b\u4e86\u91cd\u8981\u6307\u5bfc\u4f9d\u636e\u3002"}}
{"id": "2602.23574", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23574", "abs": "https://arxiv.org/abs/2602.23574", "authors": ["Ruxiao Duan", "Alex Wong"], "title": "Evidential Neural Radiance Fields", "comment": null, "summary": "Understanding sources of uncertainty is fundamental to trustworthy three-dimensional scene modeling. While recent advances in neural radiance fields (NeRFs) achieve impressive accuracy in scene reconstruction and novel view synthesis, the lack of uncertainty estimation significantly limits their deployment in safety-critical settings. Existing uncertainty quantification methods for NeRFs fail to capture both aleatoric and epistemic uncertainty. Among those that do quantify one or the other, many of them either compromise rendering quality or incur significant computational overhead to obtain uncertainty estimates. To address these issues, we introduce Evidential Neural Radiance Fields, a probabilistic approach that seamlessly integrates with the NeRF rendering process and enables direct quantification of both aleatoric and epistemic uncertainty from a single forward pass. We compare multiple uncertainty quantification methods on three standardized benchmarks, where our approach demonstrates state-of-the-art scene reconstruction fidelity and uncertainty estimation quality.", "AI": {"tldr": "\u63d0\u51faEvidential Neural Radiance Fields\uff0c\u4e00\u79cd\u6982\u7387\u65b9\u6cd5\uff0c\u53ef\u5728\u5355\u6b21\u524d\u5411\u4f20\u64ad\u4e2d\u540c\u65f6\u91cf\u5316\u5076\u7136\u6027\u548c\u8ba4\u77e5\u6027\u4e0d\u786e\u5b9a\u6027\uff0c\u5728\u4fdd\u6301\u6e32\u67d3\u8d28\u91cf\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u6548\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3002", "motivation": "\u795e\u7ecf\u8f90\u5c04\u573a\uff08NeRF\uff09\u5728\u573a\u666f\u91cd\u5efa\u548c\u65b0\u89c6\u89d2\u5408\u6210\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u7f3a\u4e4f\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u9650\u5236\u4e86\u5176\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002\u73b0\u6709\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\u8981\u4e48\u65e0\u6cd5\u540c\u65f6\u6355\u6349\u5076\u7136\u6027\u548c\u8ba4\u77e5\u6027\u4e0d\u786e\u5b9a\u6027\uff0c\u8981\u4e48\u4f1a\u635f\u5bb3\u6e32\u67d3\u8d28\u91cf\u6216\u5e26\u6765\u663e\u8457\u8ba1\u7b97\u5f00\u9500\u3002", "method": "\u63d0\u51faEvidential Neural Radiance Fields\uff0c\u4e00\u79cd\u6982\u7387\u65b9\u6cd5\uff0c\u65e0\u7f1d\u96c6\u6210\u5230NeRF\u6e32\u67d3\u8fc7\u7a0b\u4e2d\uff0c\u901a\u8fc7\u5355\u6b21\u524d\u5411\u4f20\u64ad\u76f4\u63a5\u91cf\u5316\u5076\u7136\u6027\u548c\u8ba4\u77e5\u6027\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u5728\u4e09\u4e2a\u6807\u51c6\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u6700\u5148\u8fdb\u7684\u573a\u666f\u91cd\u5efa\u4fdd\u771f\u5ea6\u548c\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u8d28\u91cf\uff0c\u4f18\u4e8e\u5176\u4ed6\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\u3002", "conclusion": "Evidential Neural Radiance Fields\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\uff0c\u80fd\u591f\u540c\u65f6\u4f30\u8ba1\u5076\u7136\u6027\u548c\u8ba4\u77e5\u6027\u4e0d\u786e\u5b9a\u6027\uff0c\u4e3a\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u7684\u53ef\u4fe1\u4e09\u7ef4\u573a\u666f\u5efa\u6a21\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\u3002"}}
{"id": "2602.23459", "categories": ["cs.LG", "q-bio.QM", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.23459", "abs": "https://arxiv.org/abs/2602.23459", "authors": ["Eric V. Strobl"], "title": "Global Interpretability via Automated Preprocessing: A Framework Inspired by Psychiatric Questionnaires", "comment": null, "summary": "Psychiatric questionnaires are highly context sensitive and often only weakly predict subsequent symptom severity, which makes the prognostic relationship difficult to learn. Although flexible nonlinear models can improve predictive accuracy, their limited interpretability can erode clinical trust. In fields such as imaging and omics, investigators commonly address visit- and instrument-specific artifacts by extracting stable signal through preprocessing and then fitting an interpretable linear model. We adopt the same strategy for questionnaire data by decoupling preprocessing from prediction: we restrict nonlinear capacity to a baseline preprocessing module that estimates stable item values, and then learn a linear mapping from these stabilized baseline items to future severity. We refer to this two-stage method as REFINE (Redundancy-Exploiting Follow-up-Informed Nonlinear Enhancement), which concentrates nonlinearity in preprocessing while keeping the prognostic relationship transparently linear and therefore globally interpretable through a coefficient matrix, rather than through post hoc local attributions. In experiments, REFINE outperforms other interpretable approaches while preserving clear global attribution of prognostic factors across psychiatric and non-psychiatric longitudinal prediction tasks.", "AI": {"tldr": "REFINE\u65b9\u6cd5\u901a\u8fc7\u5c06\u975e\u7ebf\u6027\u5904\u7406\u9650\u5236\u5728\u57fa\u7ebf\u9884\u5904\u7406\u9636\u6bb5\uff0c\u7136\u540e\u4f7f\u7528\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u672a\u6765\u75c7\u72b6\u4e25\u91cd\u7a0b\u5ea6\uff0c\u5728\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u7cbe\u795e\u75c5\u5b66\u95ee\u5377\u5177\u6709\u9ad8\u5ea6\u60c5\u5883\u654f\u611f\u6027\uff0c\u5bf9\u540e\u7eed\u75c7\u72b6\u4e25\u91cd\u7a0b\u5ea6\u7684\u9884\u6d4b\u80fd\u529b\u8f83\u5f31\u3002\u867d\u7136\u975e\u7ebf\u6027\u6a21\u578b\u53ef\u4ee5\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u4f46\u5176\u6709\u9650\u7684\u53ef\u89e3\u91ca\u6027\u4f1a\u524a\u5f31\u4e34\u5e8a\u4fe1\u4efb\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u53c8\u80fd\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a1\uff09\u5c06\u975e\u7ebf\u6027\u80fd\u529b\u9650\u5236\u5728\u57fa\u7ebf\u9884\u5904\u7406\u6a21\u5757\u4e2d\uff0c\u4f30\u8ba1\u7a33\u5b9a\u7684\u9879\u76ee\u503c\uff1b2\uff09\u5b66\u4e60\u4ece\u8fd9\u4e9b\u7a33\u5b9a\u5316\u7684\u57fa\u7ebf\u9879\u76ee\u5230\u672a\u6765\u4e25\u91cd\u7a0b\u5ea6\u7684\u7ebf\u6027\u6620\u5c04\u3002\u8fd9\u79cd\u65b9\u6cd5\u88ab\u79f0\u4e3aREFINE\uff08\u5197\u4f59\u5229\u7528\u968f\u8bbf\u77e5\u60c5\u975e\u7ebf\u6027\u589e\u5f3a\uff09\u3002", "result": "REFINE\u5728\u5b9e\u9a8c\u4e2d\u4f18\u4e8e\u5176\u4ed6\u53ef\u89e3\u91ca\u65b9\u6cd5\uff0c\u540c\u65f6\u5728\u7cbe\u795e\u75c5\u5b66\u548c\u975e\u7cbe\u795e\u75c5\u5b66\u7eb5\u5411\u9884\u6d4b\u4efb\u52a1\u4e2d\u4fdd\u6301\u4e86\u9884\u540e\u56e0\u7d20\u7684\u6e05\u6670\u5168\u5c40\u5f52\u56e0\u3002", "conclusion": "\u901a\u8fc7\u5c06\u975e\u7ebf\u6027\u96c6\u4e2d\u5728\u9884\u5904\u7406\u9636\u6bb5\uff0c\u540c\u65f6\u4fdd\u6301\u9884\u540e\u5173\u7cfb\u7684\u900f\u660e\u7ebf\u6027\uff0cREFINE\u65b9\u6cd5\u5728\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u5168\u5c40\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u4e34\u5e8a\u5b9e\u8df5\u63d0\u4f9b\u4e86\u53ef\u4fe1\u7684\u9884\u6d4b\u5de5\u5177\u3002"}}
{"id": "2602.23632", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23632", "abs": "https://arxiv.org/abs/2602.23632", "authors": ["Lun Zhan", "Feng Xiong", "Huanyong Liu", "Feng Zhang", "Yuhui Yin"], "title": "MMKG-RDS: Reasoning Data Synthesis via Deep Mining of Multimodal Knowledge Graphs", "comment": null, "summary": "Synthesizing high-quality training data is crucial for enhancing domain models' reasoning abilities. Existing methods face limitations in long-tail knowledge coverage, effectiveness verification, and interpretability. Knowledge-graph-based approaches still fall short in functionality, granularity, customizability, and evaluation. To address these issues, we propose MMKG-RDS, a flexible framework for reasoning data synthesis that leverages multimodal knowledge graphs. It supports fine-grained knowledge extraction, customizable path sampling, and multidimensional data quality scoring. We validate MMKG-RDS with the MMKG-RDS-Bench dataset, covering five domains, 17 task types, and 14,950 samples. Experimental results show fine-tuning Qwen3 models (0.6B/8B/32B) on a small number of synthesized samples improves reasoning accuracy by 9.2%. The framework also generates distinct data, challenging existing models on tasks involving tables and formulas, useful for complex benchmark construction. The dataset and code are available at https://github.com/360AILAB-NLP/MMKG-RDS", "AI": {"tldr": "MMKG-RDS\u662f\u4e00\u4e2a\u57fa\u4e8e\u591a\u6a21\u6001\u77e5\u8bc6\u56fe\u8c31\u7684\u63a8\u7406\u6570\u636e\u5408\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u77e5\u8bc6\u63d0\u53d6\u3001\u53ef\u5b9a\u5236\u8def\u5f84\u91c7\u6837\u548c\u591a\u7ef4\u8d28\u91cf\u8bc4\u5206\uff0c\u751f\u6210\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\uff0c\u63d0\u5347\u9886\u57df\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u957f\u5c3e\u77e5\u8bc6\u8986\u76d6\u3001\u6709\u6548\u6027\u9a8c\u8bc1\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u65b9\u6cd5\u5728\u529f\u80fd\u6027\u3001\u7c92\u5ea6\u3001\u53ef\u5b9a\u5236\u6027\u548c\u8bc4\u4f30\u65b9\u9762\u4ecd\u6709\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u7075\u6d3b\u7684\u63a8\u7406\u6570\u636e\u5408\u6210\u6846\u67b6\u3002", "method": "\u63d0\u51faMMKG-RDS\u6846\u67b6\uff0c\u5229\u7528\u591a\u6a21\u6001\u77e5\u8bc6\u56fe\u8c31\u652f\u6301\u7ec6\u7c92\u5ea6\u77e5\u8bc6\u63d0\u53d6\u3001\u53ef\u5b9a\u5236\u8def\u5f84\u91c7\u6837\u548c\u591a\u7ef4\u6570\u636e\u8d28\u91cf\u8bc4\u5206\uff0c\u5e76\u6784\u5efa\u4e86\u5305\u542b5\u4e2a\u9886\u57df\u300117\u79cd\u4efb\u52a1\u7c7b\u578b\u300114,950\u4e2a\u6837\u672c\u7684MMKG-RDS-Bench\u6570\u636e\u96c6\u3002", "result": "\u5728Qwen3\u6a21\u578b\uff080.6B/8B/32B\uff09\u4e0a\u4f7f\u7528\u5c11\u91cf\u5408\u6210\u6837\u672c\u8fdb\u884c\u5fae\u8c03\uff0c\u63a8\u7406\u51c6\u786e\u7387\u63d0\u53479.2%\uff0c\u6846\u67b6\u751f\u6210\u7684\u6570\u636e\u5bf9\u6d89\u53ca\u8868\u683c\u548c\u516c\u5f0f\u7684\u4efb\u52a1\u5177\u6709\u6311\u6218\u6027\uff0c\u53ef\u7528\u4e8e\u590d\u6742\u57fa\u51c6\u6784\u5efa\u3002", "conclusion": "MMKG-RDS\u662f\u4e00\u4e2a\u6709\u6548\u7684\u63a8\u7406\u6570\u636e\u5408\u6210\u6846\u67b6\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u63d0\u5347\u6a21\u578b\u63a8\u7406\u80fd\u529b\uff0c\u5176\u751f\u6210\u7684\u6570\u636e\u5bf9\u73b0\u6709\u6a21\u578b\u5177\u6709\u6311\u6218\u6027\uff0c\u53ef\u7528\u4e8e\u6784\u5efa\u590d\u6742\u57fa\u51c6\u6d4b\u8bd5\u3002"}}
{"id": "2602.23603", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.23603", "abs": "https://arxiv.org/abs/2602.23603", "authors": ["Rafid Ishrak Jahan", "Fahmid Shahriar Iqbal", "Sagnik Ray Choudhury"], "title": "LFQA-HP-1M: A Large-Scale Human Preference Dataset for Long-Form Question Answering", "comment": "LREC 2026 Accepted. https://huggingface.co/datasets/nlpatunt/LFQA-HP-1M", "summary": "Long-form question answering (LFQA) demands nuanced evaluation of multi-sentence explanatory responses, yet existing metrics often fail to reflect human judgment. We present LFQA-HP-1M, a large-scale dataset comprising 1.3M human pairwise preference annotations for LFQA. We propose nine rubrics for answer quality evaluation, and show that simple linear models based on these features perform comparably to state-of-the-art LLM evaluators. We further examine transitivity consistency, positional bias, and verbosity biases in LLM evaluators and demonstrate their vulnerability to adversarial perturbations. Overall, this work provides one of the largest public LFQA preference datasets and a rubric-driven framework for transparent and reliable evaluation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86LFQA-HP-1M\u6570\u636e\u96c6\uff0c\u5305\u542b130\u4e07\u4e2a\u4eba\u7c7b\u6210\u5bf9\u504f\u597d\u6807\u6ce8\uff0c\u7528\u4e8e\u957f\u683c\u5f0f\u95ee\u7b54\u8bc4\u4f30\u3002\u4f5c\u8005\u8bbe\u8ba1\u4e86\u4e5d\u4e2a\u8bc4\u4f30\u6807\u51c6\uff0c\u5e76\u53d1\u73b0\u57fa\u4e8e\u8fd9\u4e9b\u7279\u5f81\u7684\u7b80\u5355\u7ebf\u6027\u6a21\u578b\u4e0e\u6700\u5148\u8fdb\u7684LLM\u8bc4\u4f30\u5668\u6027\u80fd\u76f8\u5f53\u3002\u7814\u7a76\u8fd8\u63ed\u793a\u4e86LLM\u8bc4\u4f30\u5668\u5728\u4f20\u9012\u4e00\u81f4\u6027\u3001\u4f4d\u7f6e\u504f\u89c1\u548c\u5197\u957f\u504f\u89c1\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u957f\u683c\u5f0f\u95ee\u7b54\u9700\u8981\u8bc4\u4f30\u591a\u53e5\u5b50\u89e3\u91ca\u6027\u56de\u7b54\uff0c\u4f46\u73b0\u6709\u8bc4\u4f30\u6307\u6807\u5f80\u5f80\u65e0\u6cd5\u51c6\u786e\u53cd\u6620\u4eba\u7c7b\u5224\u65ad\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f5c\u8005\u65e8\u5728\u521b\u5efa\u4e00\u4e2a\u5927\u89c4\u6a21\u7684\u4eba\u7c7b\u504f\u597d\u6570\u636e\u96c6\uff0c\u5e76\u5f00\u53d1\u66f4\u900f\u660e\u53ef\u9760\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "1. \u6784\u5efaLFQA-HP-1M\u6570\u636e\u96c6\uff0c\u5305\u542b130\u4e07\u4e2a\u4eba\u7c7b\u6210\u5bf9\u504f\u597d\u6807\u6ce8\uff1b2. \u8bbe\u8ba1\u4e5d\u4e2a\u7b54\u6848\u8d28\u91cf\u8bc4\u4f30\u6807\u51c6\uff1b3. \u57fa\u4e8e\u8fd9\u4e9b\u7279\u5f81\u6784\u5efa\u7b80\u5355\u7ebf\u6027\u6a21\u578b\uff1b4. \u5206\u6790LLM\u8bc4\u4f30\u5668\u7684\u4f20\u9012\u4e00\u81f4\u6027\u3001\u4f4d\u7f6e\u504f\u89c1\u548c\u5197\u957f\u504f\u89c1\uff1b5. \u6d4b\u8bd5LLM\u8bc4\u4f30\u5668\u5bf9\u5bf9\u6297\u6027\u6270\u52a8\u7684\u8106\u5f31\u6027\u3002", "result": "1. \u57fa\u4e8e\u4e5d\u4e2a\u8bc4\u4f30\u6807\u51c6\u7684\u7b80\u5355\u7ebf\u6027\u6a21\u578b\u4e0e\u6700\u5148\u8fdb\u7684LLM\u8bc4\u4f30\u5668\u6027\u80fd\u76f8\u5f53\uff1b2. \u53d1\u73b0LLM\u8bc4\u4f30\u5668\u5b58\u5728\u4f20\u9012\u4e00\u81f4\u6027\u4e0d\u8db3\u3001\u4f4d\u7f6e\u504f\u89c1\u548c\u5197\u957f\u504f\u89c1\u95ee\u9898\uff1b3. \u8bc1\u660eLLM\u8bc4\u4f30\u5668\u5bb9\u6613\u53d7\u5230\u5bf9\u6297\u6027\u6270\u52a8\u7684\u5f71\u54cd\uff1b4. \u63d0\u4f9b\u4e86\u76ee\u524d\u6700\u5927\u7684\u516c\u5f00LFQA\u504f\u597d\u6570\u636e\u96c6\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u7684\u957f\u683c\u5f0f\u95ee\u7b54\u504f\u597d\u6570\u636e\u96c6\u548c\u57fa\u4e8e\u8bc4\u4f30\u6807\u51c6\u7684\u6846\u67b6\uff0c\u80fd\u591f\u5b9e\u73b0\u66f4\u900f\u660e\u53ef\u9760\u7684\u8bc4\u4f30\u3002\u7814\u7a76\u63ed\u793a\u4e86\u5f53\u524dLLM\u8bc4\u4f30\u5668\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765\u8bc4\u4f30\u65b9\u6cd5\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\u3002"}}
{"id": "2602.23583", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.23583", "abs": "https://arxiv.org/abs/2602.23583", "authors": ["Donggeon Kim", "Seungwon Jan", "Hyeonjun Park", "Daegyu Lim"], "title": "VCA: Vision-Click-Action Framework for Precise Manipulation of Segmented Objects in Target Ambiguous Environments", "comment": "Submitted to UR 2026", "summary": "The reliance on language in Vision-Language-Action (VLA) models introduces ambiguity, cognitive overhead, and difficulties in precise object identification and sequential task execution, particularly in environments with multiple visually similar objects. To address these limitations, we propose Vision-Click-Action (VCA), a framework that replaces verbose textual commands with direct, click-based visual interaction using pretrained segmentation models. By allowing operators to specify target objects clearly through visual selection in the robot's 2D camera view, VCA reduces interpretation errors, lowers cognitive load, and provides a practical and scalable alternative to language-driven interfaces for real-world robotic manipulation. Experimental results validate that the proposed VCA framework achieves effective instance-level manipulation of specified target objects. Experiment videos are available at https://robrosinc.github.io/vca/.", "AI": {"tldr": "VCA\u6846\u67b6\u7528\u70b9\u51fb\u5f0f\u89c6\u89c9\u4ea4\u4e92\u66ff\u4ee3\u8bed\u8a00\u6307\u4ee4\uff0c\u89e3\u51b3VLA\u6a21\u578b\u4e2d\u8bed\u8a00\u5e26\u6765\u7684\u6a21\u7cca\u6027\u3001\u8ba4\u77e5\u8d1f\u62c5\u548c\u7cbe\u786e\u7269\u4f53\u8bc6\u522b\u95ee\u9898\uff0c\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u673a\u5668\u4eba\u64cd\u4f5c", "motivation": "VLA\u6a21\u578b\u4f9d\u8d56\u8bed\u8a00\u6307\u4ee4\u4f1a\u5f15\u5165\u6a21\u7cca\u6027\u3001\u589e\u52a0\u8ba4\u77e5\u8d1f\u62c5\uff0c\u5728\u5b58\u5728\u591a\u4e2a\u89c6\u89c9\u76f8\u4f3c\u7269\u4f53\u7684\u73af\u5883\u4e2d\u96be\u4ee5\u7cbe\u786e\u8bc6\u522b\u5bf9\u8c61\u548c\u6267\u884c\u987a\u5e8f\u4efb\u52a1", "method": "\u63d0\u51faVision-Click-Action\u6846\u67b6\uff0c\u4f7f\u7528\u9884\u8bad\u7ec3\u5206\u5272\u6a21\u578b\uff0c\u901a\u8fc7\u70b9\u51fb\u673a\u5668\u4eba2D\u76f8\u673a\u89c6\u56fe\u4e2d\u7684\u76ee\u6807\u7269\u4f53\u6765\u66ff\u4ee3\u5197\u957f\u7684\u6587\u672c\u547d\u4ee4\uff0c\u5b9e\u73b0\u76f4\u63a5\u7684\u89c6\u89c9\u4ea4\u4e92", "result": "\u5b9e\u9a8c\u9a8c\u8bc1VCA\u6846\u67b6\u80fd\u591f\u6709\u6548\u5b9e\u73b0\u6307\u5b9a\u76ee\u6807\u7269\u4f53\u7684\u5b9e\u4f8b\u7ea7\u64cd\u4f5c\uff0c\u51cf\u5c11\u89e3\u91ca\u9519\u8bef\uff0c\u964d\u4f4e\u8ba4\u77e5\u8d1f\u8377", "conclusion": "VCA\u4e3a\u73b0\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u66ff\u4ee3\u8bed\u8a00\u9a71\u52a8\u754c\u9762\u7684\u65b9\u6848"}}
{"id": "2602.23495", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23495", "abs": "https://arxiv.org/abs/2602.23495", "authors": ["Yangyi Li", "Mengdi Huai"], "title": "Uncertainty-aware Language Guidance for Concept Bottleneck Models", "comment": null, "summary": "Concept Bottleneck Models (CBMs) provide inherent interpretability by first mapping input samples to high-level semantic concepts, followed by a combination of these concepts for the final classification. However, the annotation of human-understandable concepts requires extensive expert knowledge and labor, constraining the broad adoption of CBMs. On the other hand, there are a few works that leverage the knowledge of large language models (LLMs) to construct concept bottlenecks. Nevertheless, they face two essential limitations: First, they overlook the uncertainty associated with the concepts annotated by LLMs and lack a valid mechanism to quantify uncertainty about the annotated concepts, increasing the risk of errors due to hallucinations from LLMs. Additionally, they fail to incorporate the uncertainty associated with these annotations into the learning process for concept bottleneck models. To address these limitations, we propose a novel uncertainty-aware CBM method, which not only rigorously quantifies the uncertainty of LLM-annotated concept labels with valid and distribution-free guarantees, but also incorporates quantified concept uncertainty into the CBM training procedure to account for varying levels of reliability across LLM-annotated concepts. We also provide the theoretical analysis for our proposed method. Extensive experiments on the real-world datasets validate the desired properties of our proposed methods.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u6982\u5ff5\u74f6\u9888\u6a21\u578b\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cf\u5316LLM\u6807\u6ce8\u6982\u5ff5\u7684\u4e0d\u786e\u5b9a\u6027\u5e76\u7eb3\u5165\u6a21\u578b\u8bad\u7ec3\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6LLM\u6807\u6ce8\u4e0d\u786e\u5b9a\u6027\u548c\u7f3a\u4e4f\u6709\u6548\u91cf\u5316\u673a\u5236\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u6982\u5ff5\u74f6\u9888\u6a21\u578b\u9700\u8981\u5927\u91cf\u4e13\u5bb6\u77e5\u8bc6\u6807\u6ce8\u6982\u5ff5\uff0c\u9650\u5236\u4e86\u5e7f\u6cdb\u5e94\u7528\u3002\u73b0\u6709\u5229\u7528LLM\u6784\u5efa\u6982\u5ff5\u74f6\u9888\u7684\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u5c40\u9650\uff1a\u5ffd\u89c6LLM\u6807\u6ce8\u6982\u5ff5\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u7f3a\u4e4f\u6709\u6548\u7684\u91cf\u5316\u673a\u5236\uff1b\u672a\u80fd\u5c06\u6807\u6ce8\u4e0d\u786e\u5b9a\u6027\u7eb3\u5165\u6a21\u578b\u5b66\u4e60\u8fc7\u7a0b\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u4e0d\u786e\u5b9a\u6027\u611f\u77e5CBM\u65b9\u6cd5\uff0c\u4e0d\u4ec5\u4e25\u683c\u91cf\u5316LLM\u6807\u6ce8\u6982\u5ff5\u6807\u7b7e\u7684\u4e0d\u786e\u5b9a\u6027\uff08\u5177\u6709\u6709\u6548\u4e14\u65e0\u5206\u5e03\u5047\u8bbe\u7684\u4fdd\u8bc1\uff09\uff0c\u8fd8\u5c06\u91cf\u5316\u7684\u6982\u5ff5\u4e0d\u786e\u5b9a\u6027\u7eb3\u5165CBM\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u4ee5\u8003\u8651LLM\u6807\u6ce8\u6982\u5ff5\u7684\u4e0d\u540c\u53ef\u9760\u6027\u6c34\u5e73\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u671f\u671b\u7279\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u91cf\u5316LLM\u6807\u6ce8\u6982\u5ff5\u7684\u4e0d\u786e\u5b9a\u6027\u5e76\u5c06\u5176\u7eb3\u5165\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u6982\u5ff5\u74f6\u9888\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u5b9e\u73b0\u9014\u5f84\u3002"}}
{"id": "2602.23643", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23643", "abs": "https://arxiv.org/abs/2602.23643", "authors": ["Judah Goldfeder", "Philippe Wyder", "Yann LeCun", "Ravid Shwartz Ziv"], "title": "AI Must Embrace Specialization via Superhuman Adaptable Intelligence", "comment": null, "summary": "Everyone from AI executives and researchers to doomsayers, politicians, and activists is talking about Artificial General Intelligence (AGI). Yet, they often don't seem to agree on its exact definition. One common definition of AGI is an AI that can do everything a human can do, but are humans truly general? In this paper, we address what's wrong with our conception of AGI, and why, even in its most coherent formulation, it is a flawed concept to describe the future of AI. We explore whether the most widely accepted definitions are plausible, useful, and truly general. We argue that AI must embrace specialization, rather than strive for generality, and in its specialization strive for superhuman performance, and introduce Superhuman Adaptable Intelligence (SAI). SAI is defined as intelligence that can learn to exceed humans at anything important that we can do, and that can fill in the skill gaps where humans are incapable. We then lay out how SAI can help hone a discussion around AI that was blurred by an overloaded definition of AGI, and extrapolate the implications of using it as a guide for the future.", "AI": {"tldr": "\u8be5\u8bba\u6587\u6279\u5224\u4e86\u5f53\u524d\u5bf9\u901a\u7528\u4eba\u5de5\u667a\u80fd\uff08AGI\uff09\u7684\u5b9a\u4e49\uff0c\u8ba4\u4e3a\u4eba\u7c7b\u672c\u8eab\u5e76\u975e\u771f\u6b63\u7684\"\u901a\u7528\"\uff0c\u56e0\u6b64\u4ee5\u4eba\u7c7b\u4e3a\u6807\u51c6\u7684AGI\u6982\u5ff5\u5b58\u5728\u7f3a\u9677\u3002\u4f5c\u8005\u63d0\u51fa\u7528\"\u8d85\u4eba\u9002\u5e94\u6027\u667a\u80fd\uff08SAI\uff09\"\u66ff\u4ee3AGI\uff0c\u5f3a\u8c03AI\u5e94\u8ffd\u6c42\u4e13\u4e1a\u5316\u800c\u975e\u901a\u7528\u6027\uff0c\u5e76\u5728\u4e13\u4e1a\u9886\u57df\u5b9e\u73b0\u8d85\u8d8a\u4eba\u7c7b\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524dAI\u754c\u5bf9AGI\u7684\u5b9a\u4e49\u5b58\u5728\u6df7\u4e71\u548c\u77db\u76fe\uff0c\u4e0d\u540c\u7fa4\u4f53\u5bf9AGI\u7684\u7406\u89e3\u5404\u4e0d\u76f8\u540c\u3002\u4f5c\u8005\u8ba4\u4e3a\u4ee5\"\u80fd\u505a\u4eba\u7c7b\u80fd\u505a\u7684\u4e00\u5207\"\u4f5c\u4e3aAGI\u5b9a\u4e49\u5b58\u5728\u95ee\u9898\uff0c\u56e0\u4e3a\u4eba\u7c7b\u672c\u8eab\u5e76\u975e\u771f\u6b63\u7684\u901a\u7528\u667a\u80fd\u3002\u9700\u8981\u91cd\u65b0\u5ba1\u89c6AI\u53d1\u5c55\u7684\u65b9\u5411\u548c\u76ee\u6807\uff0c\u5efa\u7acb\u66f4\u6e05\u6670\u3001\u5b9e\u7528\u7684\u6982\u5ff5\u6846\u67b6\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5f53\u524dAGI\u5b9a\u4e49\u7684\u7f3a\u9677\uff0c\u8bba\u8bc1\u4eba\u7c7b\u667a\u80fd\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51faSAI\u6982\u5ff5\u4f5c\u4e3a\u66ff\u4ee3\u6846\u67b6\u3002SAI\u5b9a\u4e49\u4e3a\u80fd\u591f\u5b66\u4e60\u5728\u4eba\u7c7b\u91cd\u8981\u80fd\u529b\u4e0a\u8d85\u8d8a\u4eba\u7c7b\uff0c\u5e76\u586b\u8865\u4eba\u7c7b\u80fd\u529b\u7a7a\u767d\u7684\u667a\u80fd\u7cfb\u7edf\u3002\u8be5\u65b9\u6cd5\u5305\u62ec\u6982\u5ff5\u5206\u6790\u3001\u903b\u8f91\u8bba\u8bc1\u548c\u66ff\u4ee3\u6846\u67b6\u6784\u5efa\u3002", "result": "\u6210\u529f\u8bc6\u522b\u4e86AGI\u6982\u5ff5\u7684\u4e09\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u5b9a\u4e49\u4e0d\u6e05\u6670\u3001\u4ee5\u4eba\u7c7b\u4e3a\u57fa\u51c6\u5b58\u5728\u95ee\u9898\u3001\u5b9e\u7528\u6027\u4e0d\u8db3\u3002\u63d0\u51fa\u4e86SAI\u4f5c\u4e3a\u66f4\u6e05\u6670\u3001\u5b9e\u7528\u7684\u66ff\u4ee3\u6982\u5ff5\uff0c\u80fd\u591f\u66f4\u597d\u5730\u6307\u5bfcAI\u53d1\u5c55\u65b9\u5411\uff0c\u907f\u514dAGI\u6982\u5ff5\u7684\u6a21\u7cca\u6027\u5e26\u6765\u7684\u8ba8\u8bba\u6df7\u4e71\u3002", "conclusion": "AI\u53d1\u5c55\u5e94\u653e\u5f03\u8ffd\u6c42\u901a\u7528\u6027\uff0c\u8f6c\u800c\u4e13\u6ce8\u4e8e\u4e13\u4e1a\u5316\uff0c\u5728\u7279\u5b9a\u9886\u57df\u5b9e\u73b0\u8d85\u8d8a\u4eba\u7c7b\u7684\u6027\u80fd\u3002SAI\u6982\u5ff5\u6bd4AGI\u66f4\u6e05\u6670\u3001\u5b9e\u7528\uff0c\u80fd\u591f\u4e3aAI\u53d1\u5c55\u63d0\u4f9b\u66f4\u597d\u7684\u6307\u5bfc\u6846\u67b6\uff0c\u5e2e\u52a9\u6f84\u6e05\u5f53\u524d\u6a21\u7cca\u7684AI\u8ba8\u8bba\uff0c\u63a8\u52a8AI\u5411\u66f4\u6709\u610f\u4e49\u7684\u65b9\u5411\u53d1\u5c55\u3002"}}
{"id": "2602.23610", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23610", "abs": "https://arxiv.org/abs/2602.23610", "authors": ["Yu Zhu", "Kai Yang"], "title": "LLM-Driven Multi-Turn Task-Oriented Dialogue Synthesis for Realistic Reasoning", "comment": null, "summary": "The reasoning capability of large language models (LLMs), defined as their ability to analyze, infer, and make decisions based on input information, is essential for building intelligent task-oriented dialogue systems. However, existing benchmarks do not sufficiently reflect the complexity of real-world scenarios, which limits their effectiveness in evaluating and enhancing LLM reasoning in practical contexts. Many current reasoning datasets are overly simplistic and abstract, often disconnected from realistic task flows, domain constraints, and operational rules, making it difficult to effectively evaluate LLMs' logical reasoning ability. In addition, data contamination from pretraining corpora undermines the reliability of evaluation results, and traditional crowdsourcing methods for dataset construction are labor-intensive and difficult to scale. To address these challenges, we propose a LLM-driven framework for synthesizing multi-turn, task-oriented dialogues grounded in realistic reasoning scenarios, leveraging trilevel optimization to enhance dialogue quality. Our method generates dialogues grounded in authentic task scenarios, enriched with real-world information, and exhibiting strong contextual coherence. Corresponding reasoning tasks are carefully designed around these dialogues and iteratively refined to continuously improve the tasks' quality and challenge. The resulting dataset serves as a valuable benchmark for assessing and advancing the realistic logical reasoning capabilities of LLMs. Experimental results show that our synthetic data-based reasoning tasks introduce non-trivial reasoning challenges and provide meaningful support for improving the reasoning capabilities of LLMs.", "AI": {"tldr": "\u63d0\u51faLLM\u9a71\u52a8\u7684\u591a\u8f6e\u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\u5408\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u5c42\u4f18\u5316\u751f\u6210\u57fa\u4e8e\u771f\u5b9e\u63a8\u7406\u573a\u666f\u7684\u5bf9\u8bdd\uff0c\u6784\u5efa\u9ad8\u8d28\u91cf\u63a8\u7406\u4efb\u52a1\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u63d0\u5347LLM\u7684\u73b0\u5b9e\u903b\u8f91\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u63a8\u7406\u57fa\u51c6\u65e0\u6cd5\u5145\u5206\u53cd\u6620\u73b0\u5b9e\u573a\u666f\u590d\u6742\u6027\uff0c\u6570\u636e\u96c6\u8fc7\u4e8e\u7b80\u5316\u62bd\u8c61\uff0c\u4e0e\u771f\u5b9e\u4efb\u52a1\u6d41\u7a0b\u8131\u8282\uff0c\u4e14\u5b58\u5728\u6570\u636e\u6c61\u67d3\u95ee\u9898\uff0c\u4f20\u7edf\u4f17\u5305\u65b9\u6cd5\u6210\u672c\u9ad8\u96be\u6269\u5c55\uff0c\u9700\u8981\u66f4\u597d\u7684\u8bc4\u4f30LLM\u73b0\u5b9e\u63a8\u7406\u80fd\u529b\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faLLM\u9a71\u52a8\u7684\u6846\u67b6\uff0c\u5408\u6210\u57fa\u4e8e\u771f\u5b9e\u63a8\u7406\u573a\u666f\u7684\u591a\u8f6e\u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\uff0c\u91c7\u7528\u4e09\u5c42\u4f18\u5316\u63d0\u5347\u5bf9\u8bdd\u8d28\u91cf\uff0c\u751f\u6210\u5305\u542b\u771f\u5b9e\u4e16\u754c\u4fe1\u606f\u7684\u8fde\u8d2f\u5bf9\u8bdd\uff0c\u5e76\u56f4\u7ed5\u5bf9\u8bdd\u8bbe\u8ba1\u8fed\u4ee3\u4f18\u5316\u7684\u63a8\u7406\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e\u5408\u6210\u6570\u636e\u7684\u63a8\u7406\u4efb\u52a1\u5f15\u5165\u4e86\u975e\u5e73\u51e1\u7684\u63a8\u7406\u6311\u6218\uff0c\u4e3a\u63d0\u5347LLM\u7684\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u6709\u610f\u4e49\u7684\u652f\u6301\uff0c\u751f\u6210\u7684\u5bf9\u8bdd\u6570\u636e\u96c6\u53ef\u4f5c\u4e3a\u8bc4\u4f30LLM\u73b0\u5b9e\u903b\u8f91\u63a8\u7406\u80fd\u529b\u7684\u6709\u6548\u57fa\u51c6\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709\u63a8\u7406\u8bc4\u4f30\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u5408\u6210\u771f\u5b9e\u573a\u666f\u7684\u5bf9\u8bdd\u548c\u4efb\u52a1\uff0c\u4e3a\u8bc4\u4f30\u548c\u63d0\u5347LLM\u7684\u73b0\u5b9e\u903b\u8f91\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u63a8\u52a8\u4e86LLM\u5728\u590d\u6742\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53d1\u5c55\u3002"}}
{"id": "2602.23592", "categories": ["cs.RO", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.23592", "abs": "https://arxiv.org/abs/2602.23592", "authors": ["Zebin Yang", "Tong Xie", "Baotong Lu", "Shaoshan Liu", "Bo Yu", "Meng Li"], "title": "KEEP: A KV-Cache-Centric Memory Management System for Efficient Embodied Planning", "comment": "DAC 2026", "summary": "Memory-augmented Large Language Models (LLMs) have demonstrated remarkable capability for complex and long-horizon embodied planning. By keeping track of past experiences and environmental states, memory enables LLMs to maintain a global view, thereby avoiding repetitive exploration. However, existing approaches often store the memory as raw text, leading to excessively long prompts and high prefill latency. While it is possible to store and reuse the KV caches, the efficiency benefits are greatly undermined due to frequent KV cache updates. In this paper, we propose KEEP, a KV-cache-centric memory management system for efficient embodied planning. KEEP features 3 key innovations: (1) a Static-Dynamic Memory Construction algorithm that reduces KV cache recomputation by mixed-granularity memory group; (2) a Multi-hop Memory Re-computation algorithm that dynamically identifies important cross-attention among different memory groups and reconstructs memory interactions iteratively; (3) a Layer-balanced Memory Loading that eliminates unbalanced KV cache loading and cross-attention computation across different layers. Extensive experimental results have demonstrated that KEEP achieves 2.68x speedup with negligible accuracy loss compared with text-based memory methods on ALFRED dataset. Compared with the KV re-computation method CacheBlend (EuroSys'25), KEEP shows 4.13% success rate improvement and 1.90x time-to-first-token (TTFT) reduction. Our code is available on https://github.com/PKU-SEC-Lab/KEEP_Embodied_Memory.", "AI": {"tldr": "KEEP\u662f\u4e00\u4e2a\u57fa\u4e8eKV\u7f13\u5b58\u7684\u8bb0\u5fc6\u7ba1\u7406\u7cfb\u7edf\uff0c\u7528\u4e8e\u9ad8\u6548\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5177\u8eab\u89c4\u5212\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u5185\u5b58\u7ba1\u7406\u6280\u672f\u5b9e\u73b0\u4e862.68\u500d\u52a0\u901f\u548c\u53ef\u5ffd\u7565\u7684\u7cbe\u5ea6\u635f\u5931\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6587\u672c\u7684\u8bb0\u5fc6\u65b9\u6cd5\u4f1a\u5bfc\u81f4\u8fc7\u957f\u7684\u63d0\u793a\u548c\u9ad8\u6602\u7684\u9884\u586b\u5145\u5ef6\u8fdf\uff0c\u800c\u57fa\u4e8eKV\u7f13\u5b58\u7684\u65b9\u6cd5\u7531\u4e8e\u9891\u7e41\u7684\u7f13\u5b58\u66f4\u65b0\u800c\u6548\u7387\u53d7\u9650\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u8bb0\u5fc6\u7ba1\u7406\u7cfb\u7edf\u3002", "method": "\u63d0\u51faKEEP\u7cfb\u7edf\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u6280\u672f\uff1a1\uff09\u9759\u6001-\u52a8\u6001\u5185\u5b58\u6784\u5efa\u7b97\u6cd5\uff0c\u901a\u8fc7\u6df7\u5408\u7c92\u5ea6\u5185\u5b58\u7ec4\u51cf\u5c11KV\u7f13\u5b58\u91cd\u8ba1\u7b97\uff1b2\uff09\u591a\u8df3\u5185\u5b58\u91cd\u8ba1\u7b97\u7b97\u6cd5\uff0c\u52a8\u6001\u8bc6\u522b\u4e0d\u540c\u5185\u5b58\u7ec4\u95f4\u7684\u91cd\u8981\u4ea4\u53c9\u6ce8\u610f\u529b\u5e76\u8fed\u4ee3\u91cd\u5efa\u5185\u5b58\u4ea4\u4e92\uff1b3\uff09\u5c42\u5e73\u8861\u5185\u5b58\u52a0\u8f7d\uff0c\u6d88\u9664\u4e0d\u540c\u5c42\u95f4\u4e0d\u5e73\u8861\u7684KV\u7f13\u5b58\u52a0\u8f7d\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u8ba1\u7b97\u3002", "result": "\u5728ALFRED\u6570\u636e\u96c6\u4e0a\uff0cKEEP\u76f8\u6bd4\u57fa\u4e8e\u6587\u672c\u7684\u8bb0\u5fc6\u65b9\u6cd5\u5b9e\u73b0\u4e862.68\u500d\u52a0\u901f\u4e14\u7cbe\u5ea6\u635f\u5931\u53ef\u5ffd\u7565\uff1b\u76f8\u6bd4KV\u91cd\u8ba1\u7b97\u65b9\u6cd5CacheBlend\uff0c\u63d0\u5347\u4e864.13%\u7684\u6210\u529f\u7387\u5e76\u51cf\u5c11\u4e861.90\u500d\u7684\u9996\u6b21\u4ee4\u724c\u65f6\u95f4\u3002", "conclusion": "KEEP\u901a\u8fc7\u521b\u65b0\u7684KV\u7f13\u5b58\u4e2d\u5fc3\u5316\u5185\u5b58\u7ba1\u7406\u7cfb\u7edf\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5177\u8eab\u89c4\u5212\u4e2d\u8bb0\u5fc6\u7ba1\u7406\u7684\u6548\u7387\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u9ad8\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u901f\u5ea6\u3002"}}
{"id": "2602.23588", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23588", "abs": "https://arxiv.org/abs/2602.23588", "authors": ["Abhishek Dalvi", "Vasant Honavar"], "title": "Hyperdimensional Cross-Modal Alignment of Frozen Language and Image Models for Efficient Image Captioning", "comment": null, "summary": "Large unimodal foundation models for vision and language encode rich semantic structures, yet aligning them typically requires computationally intensive multimodal fine-tuning. Such approaches depend on large-scale parameter updates, are resource intensive, and can perturb pretrained representations. Emerging evidence suggests, however, that independently trained foundation models may already exhibit latent semantic compatibility, reflecting shared structures in the data they model. This raises a fundamental question: can cross-modal alignment be achieved without modifying the models themselves? Here we introduce HDFLIM (HyperDimensional computing with Frozen Language and Image Models), a framework that establishes cross-modal mappings while keeping pretrained vision and language models fully frozen. HDFLIM projects unimodal embeddings into a shared hyperdimensional space and leverages lightweight symbolic operations -- binding, bundling, and similarity-based retrieval to construct associative cross-modal representations in a single pass over the data. Caption generation emerges from high-dimensional memory retrieval rather than iterative gradient-based optimization. We show that HDFLIM achieves performance comparable to end-to-end vision-language training methods and produces captions that are more semantically grounded than zero-shot baselines. By decoupling alignment from parameter tuning, our results suggest that semantic mapping across foundation models can be realized through symbolic operations on hyperdimensional encodings of the respective embeddings. More broadly, this work points toward an alternative paradigm for foundation model alignment in which frozen models are integrated through structured representational mappings rather than through large-scale retraining. The codebase for our implementation can be found at https://github.com/Abhishek-Dalvi410/HDFLIM.", "AI": {"tldr": "HDFLIM\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u5fae\u8c03\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u8de8\u6a21\u6001\u5bf9\u9f50\u65b9\u6cd5\uff0c\u901a\u8fc7\u8d85\u7ef4\u8ba1\u7b97\u5728\u51bb\u7ed3\u7684\u89c6\u89c9\u548c\u8bed\u8a00\u6a21\u578b\u4e4b\u95f4\u5efa\u7acb\u6620\u5c04\u5173\u7cfb", "motivation": "\u4f20\u7edf\u8de8\u6a21\u6001\u5bf9\u9f50\u9700\u8981\u8ba1\u7b97\u5bc6\u96c6\u7684\u591a\u6a21\u6001\u5fae\u8c03\uff0c\u8fd9\u4f1a\u66f4\u65b0\u5927\u91cf\u53c2\u6570\u3001\u6d88\u8017\u8d44\u6e90\u5e76\u53ef\u80fd\u7834\u574f\u9884\u8bad\u7ec3\u8868\u793a\u3002\u7814\u7a76\u53d1\u73b0\u72ec\u7acb\u8bad\u7ec3\u7684\u57fa\u6a21\u578b\u53ef\u80fd\u5df2\u5b58\u5728\u6f5c\u5728\u7684\u8bed\u4e49\u517c\u5bb9\u6027\uff0c\u56e0\u6b64\u63a2\u7d22\u80fd\u5426\u5728\u4e0d\u4fee\u6539\u6a21\u578b\u672c\u8eab\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u8de8\u6a21\u6001\u5bf9\u9f50", "method": "HDFLIM\u5c06\u5355\u6a21\u6001\u5d4c\u5165\u6295\u5f71\u5230\u5171\u4eab\u7684\u8d85\u7ef4\u7a7a\u95f4\uff0c\u5229\u7528\u8f7b\u91cf\u7ea7\u7b26\u53f7\u64cd\u4f5c\uff08\u7ed1\u5b9a\u3001\u6346\u7ed1\u548c\u57fa\u4e8e\u76f8\u4f3c\u6027\u7684\u68c0\u7d22\uff09\u5728\u5355\u6b21\u6570\u636e\u904d\u5386\u4e2d\u6784\u5efa\u5173\u8054\u7684\u8de8\u6a21\u6001\u8868\u793a\u3002\u6807\u9898\u751f\u6210\u901a\u8fc7\u9ad8\u7ef4\u8bb0\u5fc6\u68c0\u7d22\u800c\u975e\u8fed\u4ee3\u7684\u57fa\u4e8e\u68af\u5ea6\u7684\u4f18\u5316\u5b9e\u73b0", "result": "HDFLIM\u5b9e\u73b0\u4e86\u4e0e\u7aef\u5230\u7aef\u89c6\u89c9\u8bed\u8a00\u8bad\u7ec3\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u751f\u6210\u7684\u6807\u9898\u6bd4\u96f6\u6837\u672c\u57fa\u7ebf\u66f4\u5177\u8bed\u4e49\u57fa\u7840", "conclusion": "\u901a\u8fc7\u5c06\u5bf9\u9f50\u4e0e\u53c2\u6570\u8c03\u4f18\u89e3\u8026\uff0c\u7814\u7a76\u8868\u660e\u8de8\u57fa\u6a21\u578b\u7684\u8bed\u4e49\u6620\u5c04\u53ef\u4ee5\u901a\u8fc7\u5bf9\u5404\u81ea\u5d4c\u5165\u7684\u8d85\u7ef4\u7f16\u7801\u8fdb\u884c\u7b26\u53f7\u64cd\u4f5c\u6765\u5b9e\u73b0\u3002\u8fd9\u9879\u5de5\u4f5c\u6307\u5411\u4e86\u4e00\u79cd\u66ff\u4ee3\u8303\u5f0f\uff1a\u901a\u8fc7\u7ed3\u6784\u5316\u8868\u793a\u6620\u5c04\u800c\u975e\u5927\u89c4\u6a21\u91cd\u65b0\u8bad\u7ec3\u6765\u96c6\u6210\u51bb\u7ed3\u6a21\u578b"}}
{"id": "2602.23504", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2602.23504", "abs": "https://arxiv.org/abs/2602.23504", "authors": ["Anik Pramanik", "Murat Kantarcioglu", "Vincent Oria", "Shantanu Sharma"], "title": "FedDAG: Clustered Federated Learning via Global Data and Gradient Integration for Heterogeneous Environments", "comment": "This paper has been accepted in ICLR 2026", "summary": "Federated Learning (FL) enables a group of clients to collaboratively train a model without sharing individual data, but its performance drops when client data are heterogeneous. Clustered FL tackles this by grouping similar clients. However, existing clustered FL approaches rely solely on either data similarity or gradient similarity; however, this results in an incomplete assessment of client similarities. Prior clustered FL approaches also restrict knowledge and representation sharing to clients within the same cluster. This prevents cluster models from benefiting from the diverse client population across clusters. To address these limitations, FedDAG introduces a clustered FL framework, FedDAG, that employs a weighted, class-wise similarity metric that integrates both data and gradient information, providing a more holistic measure of similarity during clustering. In addition, FedDAG adopts a dual-encoder architecture for cluster models, comprising a primary encoder trained on its own clients' data and a secondary encoder refined using gradients from complementary clusters. This enables cross-cluster feature transfer while preserving cluster-specific specialization. Experiments on diverse benchmarks and data heterogeneity settings show that FedDAG consistently outperforms state-of-the-art clustered FL baselines in accuracy.", "AI": {"tldr": "FedDAG\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u805a\u7c7b\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u6570\u636e\u548c\u68af\u5ea6\u4fe1\u606f\u7684\u52a0\u6743\u7c7b\u76f8\u4f3c\u6027\u5ea6\u91cf\u8fdb\u884c\u805a\u7c7b\uff0c\u5e76\u91c7\u7528\u53cc\u7f16\u7801\u5668\u67b6\u6784\u5b9e\u73b0\u8de8\u96c6\u7fa4\u7279\u5f81\u8f6c\u79fb\uff0c\u5728\u5f02\u6784\u6570\u636e\u573a\u666f\u4e0b\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u805a\u7c7b\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1) \u4ec5\u4f9d\u8d56\u6570\u636e\u76f8\u4f3c\u6027\u6216\u68af\u5ea6\u76f8\u4f3c\u6027\u8fdb\u884c\u805a\u7c7b\uff0c\u5bfc\u81f4\u5bf9\u5ba2\u6237\u7aef\u76f8\u4f3c\u6027\u7684\u8bc4\u4f30\u4e0d\u5b8c\u6574\uff1b2) \u9650\u5236\u77e5\u8bc6\u548c\u8868\u5f81\u5171\u4eab\u4ec5\u9650\u4e8e\u540c\u4e00\u96c6\u7fa4\u5185\uff0c\u4f7f\u5f97\u96c6\u7fa4\u6a21\u578b\u65e0\u6cd5\u4ece\u8de8\u96c6\u7fa4\u7684\u591a\u6837\u5316\u5ba2\u6237\u7aef\u7fa4\u4f53\u4e2d\u53d7\u76ca\u3002", "method": "FedDAG\u91c7\u7528\u52a0\u6743\u7c7b\u76f8\u4f3c\u6027\u5ea6\u91cf\uff0c\u6574\u5408\u6570\u636e\u548c\u68af\u5ea6\u4fe1\u606f\u8fdb\u884c\u66f4\u5168\u9762\u7684\u5ba2\u6237\u7aef\u805a\u7c7b\u3002\u540c\u65f6\uff0c\u91c7\u7528\u53cc\u7f16\u7801\u5668\u67b6\u6784\uff1a\u4e3b\u7f16\u7801\u5668\u5728\u81ea\u8eab\u5ba2\u6237\u7aef\u6570\u636e\u4e0a\u8bad\u7ec3\uff0c\u8f85\u52a9\u7f16\u7801\u5668\u5229\u7528\u4e92\u8865\u96c6\u7fa4\u7684\u68af\u5ea6\u8fdb\u884c\u7cbe\u70bc\uff0c\u5b9e\u73b0\u8de8\u96c6\u7fa4\u7279\u5f81\u8f6c\u79fb\u540c\u65f6\u4fdd\u6301\u96c6\u7fa4\u7279\u5b9a\u4e13\u4e1a\u5316\u3002", "result": "\u5728\u591a\u79cd\u57fa\u51c6\u6d4b\u8bd5\u548c\u6570\u636e\u5f02\u6784\u8bbe\u7f6e\u4e0b\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFedDAG\u5728\u51c6\u786e\u7387\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u805a\u7c7b\u8054\u90a6\u5b66\u4e60\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "FedDAG\u901a\u8fc7\u66f4\u5168\u9762\u7684\u76f8\u4f3c\u6027\u5ea6\u91cf\u548c\u8de8\u96c6\u7fa4\u77e5\u8bc6\u5171\u4eab\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u805a\u7c7b\u8054\u90a6\u5b66\u4e60\u5728\u6570\u636e\u5f02\u6784\u573a\u666f\u4e0b\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2602.23668", "categories": ["cs.AI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.23668", "abs": "https://arxiv.org/abs/2602.23668", "authors": ["Yihan", "Wen", "Xin Chen"], "title": "PseudoAct: Leveraging Pseudocode Synthesis for Flexible Planning and Action Control in Large Language Model Agents", "comment": null, "summary": "Large language model (LLM) agents typically rely on reactive decision-making paradigms such as ReAct, selecting actions conditioned on growing execution histories. While effective for short tasks, these approaches often lead to redundant tool usage, unstable reasoning, and high token consumption in complex long-horizon tasks involving branching, iteration, or multi-tool coordination. To address these limitations, this paper introduces PseudoAct, a novel framework for flexible planning and action control in LLM agents through pseudocode synthesis. Leveraging the ability of LLMs to express task-solving strategies as code, PseudoAct synthesizes a structured pseudocode plan that decomposes a task into subtasks and explicitly encodes control flow, including sequencing, conditionals, loops, parallel composition, and combinations of these logic primitives. Actions are then executed by following this global plan, making the decision logic explicit and temporally coherent. This design reduces redundant actions, prevents infinite loops, and avoids uninformative alternative exploration, enabling consistent and efficient long-horizon decision-making. Experiments on benchmark datasets show that our method significantly outperforms existing reactive agent approaches, achieving a 20.93% absolute gain in success rate on FEVER and setting a new state-of-the-art on HotpotQA.", "AI": {"tldr": "PseudoAct\u6846\u67b6\u901a\u8fc7\u4f2a\u4ee3\u7801\u5408\u6210\u5b9e\u73b0LLM\u667a\u80fd\u4f53\u7684\u7075\u6d3b\u89c4\u5212\u548c\u884c\u52a8\u63a7\u5236\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u53cd\u5e94\u5f0f\u51b3\u7b56\u5728\u590d\u6742\u957f\u65f6\u4efb\u52a1\u4e2d\u7684\u5197\u4f59\u5de5\u5177\u4f7f\u7528\u3001\u4e0d\u7a33\u5b9a\u63a8\u7406\u548c\u9ad8token\u6d88\u8017\u95ee\u9898\u3002", "motivation": "\u4f20\u7edfLLM\u667a\u80fd\u4f53\u4f9d\u8d56\u53cd\u5e94\u5f0f\u51b3\u7b56\u8303\u5f0f\uff08\u5982ReAct\uff09\uff0c\u5728\u6d89\u53ca\u5206\u652f\u3001\u8fed\u4ee3\u6216\u591a\u5de5\u5177\u534f\u8c03\u7684\u590d\u6742\u957f\u65f6\u4efb\u52a1\u4e2d\uff0c\u4f1a\u5bfc\u81f4\u5197\u4f59\u5de5\u5177\u4f7f\u7528\u3001\u4e0d\u7a33\u5b9a\u63a8\u7406\u548c\u9ad8token\u6d88\u8017\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u89c4\u5212\u548c\u63a7\u5236\u65b9\u6cd5\u3002", "method": "\u63d0\u51faPseudoAct\u6846\u67b6\uff0c\u5229\u7528LLM\u5c06\u4efb\u52a1\u89e3\u51b3\u7b56\u7565\u8868\u8fbe\u4e3a\u4ee3\u7801\u7684\u80fd\u529b\uff0c\u5408\u6210\u7ed3\u6784\u5316\u4f2a\u4ee3\u7801\u8ba1\u5212\uff0c\u5c06\u4efb\u52a1\u5206\u89e3\u4e3a\u5b50\u4efb\u52a1\u5e76\u663e\u5f0f\u7f16\u7801\u63a7\u5236\u6d41\uff08\u5305\u62ec\u5e8f\u5217\u3001\u6761\u4ef6\u3001\u5faa\u73af\u3001\u5e76\u884c\u7ec4\u5408\u7b49\u903b\u8f91\u539f\u8bed\uff09\uff0c\u7136\u540e\u6309\u7167\u8fd9\u4e2a\u5168\u5c40\u8ba1\u5212\u6267\u884c\u884c\u52a8\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u53cd\u5e94\u5f0f\u667a\u80fd\u4f53\u65b9\u6cd5\uff0c\u5728FEVER\u4e0a\u5b9e\u73b0\u4e8620.93%\u7684\u7edd\u5bf9\u6210\u529f\u7387\u63d0\u5347\uff0c\u5e76\u5728HotpotQA\u4e0a\u521b\u9020\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "PseudoAct\u901a\u8fc7\u4f2a\u4ee3\u7801\u5408\u6210\u5b9e\u73b0\u4e86\u663e\u5f0f\u548c\u65f6\u95f4\u4e00\u81f4\u7684\u51b3\u7b56\u903b\u8f91\uff0c\u51cf\u5c11\u4e86\u5197\u4f59\u884c\u52a8\uff0c\u9632\u6b62\u4e86\u65e0\u9650\u5faa\u73af\uff0c\u907f\u514d\u4e86\u65e0\u4fe1\u606f\u91cf\u7684\u66ff\u4ee3\u63a2\u7d22\uff0c\u5b9e\u73b0\u4e86\u4e00\u81f4\u4e14\u9ad8\u6548\u7684\u957f\u65f6\u51b3\u7b56\u3002"}}
{"id": "2602.23656", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23656", "abs": "https://arxiv.org/abs/2602.23656", "authors": ["Zitong Xu", "Yuqing Wu", "Yue Zhao"], "title": "TRIZ-RAGNER: A Retrieval-Augmented Large Language Model for TRIZ-Aware Named Entity Recognition in Patent-Based Contradiction Mining", "comment": null, "summary": "TRIZ-based contradiction mining is a fundamental task in patent analysis and systematic innovation, as it enables the identification of improving and worsening technical parameters that drive inventive problem solving. However, existing approaches largely rely on rule-based systems or traditional machine learning models, which struggle with semantic ambiguity, domain dependency, and limited generalization when processing complex patent language. Recently, large language models (LLMs) have shown strong semantic understanding capabilities, yet their direct application to TRIZ parameter extraction remains challenging due to hallucination and insufficient grounding in structured TRIZ knowledge. To address these limitations, this paper proposes TRIZ-RAGNER, a retrieval-augmented large language model framework for TRIZ-aware named entity recognition in patent-based contradiction mining. TRIZ-RAGNER reformulates contradiction mining as a semantic-level NER task and integrates dense retrieval over a TRIZ knowledge base, cross-encoder reranking for context refinement, and structured LLM prompting to extract improving and worsening parameters from patent sentences. By injecting domain-specific TRIZ knowledge into the LLM reasoning process, the proposed framework effectively reduces semantic noise and improves extraction consistency. Experiments on the PaTRIZ dataset demonstrate that TRIZ-RAGNER consistently outperforms traditional sequence labeling models and LLM-based baselines. The proposed framework achieves a precision of 85.6%, a recall of 82.9%, and an F1-score of 84.2% in TRIZ contradiction pair identification. Compared with the strongest baseline using prompt-enhanced GPT, TRIZ-RAGNER yields an absolute F1-score improvement of 7.3 percentage points, confirming the effectiveness of retrieval-augmented TRIZ knowledge grounding for robust and accurate patent-based contradiction mining.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faTRIZ-RAGNER\u6846\u67b6\uff0c\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u7684LLM\u65b9\u6cd5\u6539\u8fdb\u4e13\u5229\u4e2d\u7684TRIZ\u77db\u76fe\u6316\u6398\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u6027\u80fd", "motivation": "\u73b0\u6709\u57fa\u4e8e\u89c4\u5219\u6216\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u7684\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u4e13\u5229\u8bed\u8a00\u65f6\u5b58\u5728\u8bed\u4e49\u6a21\u7cca\u3001\u9886\u57df\u4f9d\u8d56\u548c\u6cdb\u5316\u80fd\u529b\u6709\u9650\u7684\u95ee\u9898\uff0c\u800c\u76f4\u63a5\u5e94\u7528\u5927\u8bed\u8a00\u6a21\u578b\u53c8\u9762\u4e34\u5e7b\u89c9\u548c\u7f3a\u4e4f\u7ed3\u6784\u5316TRIZ\u77e5\u8bc6\u57fa\u7840\u7684\u9650\u5236", "method": "\u63d0\u51faTRIZ-RAGNER\u6846\u67b6\uff0c\u5c06\u77db\u76fe\u6316\u6398\u91cd\u65b0\u5b9a\u4e49\u4e3a\u8bed\u4e49\u7ea7\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u4efb\u52a1\uff0c\u96c6\u6210\u5bc6\u96c6\u68c0\u7d22TRIZ\u77e5\u8bc6\u5e93\u3001\u4ea4\u53c9\u7f16\u7801\u5668\u91cd\u6392\u5e8f\u548c\u7ed3\u6784\u5316LLM\u63d0\u793a\uff0c\u4ece\u4e13\u5229\u53e5\u5b50\u4e2d\u63d0\u53d6\u6539\u5584\u548c\u6076\u5316\u53c2\u6570", "result": "\u5728PaTRIZ\u6570\u636e\u96c6\u4e0a\uff0cTRIZ-RAGNER\u5728TRIZ\u77db\u76fe\u5bf9\u8bc6\u522b\u4e2d\u8fbe\u523085.6%\u7cbe\u786e\u7387\u300182.9%\u53ec\u56de\u7387\u548c84.2% F1\u5206\u6570\uff0c\u76f8\u6bd4\u57fa\u4e8e\u63d0\u793a\u589e\u5f3aGPT\u7684\u6700\u5f3a\u57fa\u7ebf\u7edd\u5bf9\u63d0\u53477.3\u4e2a\u767e\u5206\u70b9", "conclusion": "\u68c0\u7d22\u589e\u5f3a\u7684TRIZ\u77e5\u8bc6\u57fa\u7840\u80fd\u6709\u6548\u51cf\u5c11\u8bed\u4e49\u566a\u58f0\u5e76\u63d0\u9ad8\u63d0\u53d6\u4e00\u81f4\u6027\uff0c\u4e3a\u7a33\u5065\u51c6\u786e\u7684\u4e13\u5229\u77db\u76fe\u6316\u6398\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.23607", "categories": ["cs.RO", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.23607", "abs": "https://arxiv.org/abs/2602.23607", "authors": ["Yanda Yang", "Sambeeta Das"], "title": "MicroPush: A Simulator and Benchmark for Contact-Rich Cell Pushing and Assembly with a Magnetic Rolling Microrobot", "comment": "13 pages, 8 figures", "summary": "Magnetic rolling microrobots enable gentle manipulation in confined microfluidic environments, yet autonomy for contact-rich behaviors such as cell pushing and multi-target assembly remains difficult to develop and evaluate reproducibly. We present MicroPush, an open-source simulator and benchmark suite for magnetic rolling microrobots in cluttered 2D scenes. MicroPush combines an overdamped interaction model with contact-aware stick--slip effects, lightweight near-field damping, optional Poiseuille background flow, and a calibrated mapping from actuation frequency to free-space rolling speed. On top of the simulator core, we provide a modular planning--control stack with a two-phase strategy for contact establishment and goal-directed pushing, together with a deterministic benchmark protocol with fixed tasks, staged execution, and unified CSV logging for single-object transport and hexagonal assembly. We report success, time, and tracking metrics, and an actuation-variation measure $E_{\u0394\u03c9}$. Results show that controller stability dominates performance under flow disturbances, while planner choice can influence command smoothness over long-horizon sequences via waypoint progression. MicroPush enables reproducible comparison and ablation of planning, control, and learning methods for microscale contact-rich micromanipulation.", "AI": {"tldr": "MicroPush\u662f\u4e00\u4e2a\u7528\u4e8e\u78c1\u6027\u6eda\u52a8\u5fae\u673a\u5668\u4eba\u5728\u6742\u4e712D\u573a\u666f\u4e2d\u7684\u5f00\u6e90\u6a21\u62df\u5668\u548c\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\uff0c\u4e13\u6ce8\u4e8e\u63a5\u89e6\u4e30\u5bcc\u7684\u5fae\u64cd\u4f5c\u4efb\u52a1\u3002", "motivation": "\u78c1\u6027\u6eda\u52a8\u5fae\u673a\u5668\u4eba\u5728\u53d7\u9650\u5fae\u6d41\u4f53\u73af\u5883\u4e2d\u80fd\u591f\u5b9e\u73b0\u6e29\u548c\u64cd\u4f5c\uff0c\u4f46\u63a5\u89e6\u4e30\u5bcc\u884c\u4e3a\uff08\u5982\u7ec6\u80de\u63a8\u52a8\u548c\u591a\u76ee\u6807\u7ec4\u88c5\uff09\u7684\u81ea\u4e3b\u6027\u96be\u4ee5\u5f00\u53d1\u548c\u53ef\u91cd\u590d\u8bc4\u4f30\u3002", "method": "\u7ed3\u5408\u8fc7\u963b\u5c3c\u76f8\u4e92\u4f5c\u7528\u6a21\u578b\u4e0e\u63a5\u89e6\u611f\u77e5\u7684\u7c98\u6ed1\u6548\u5e94\u3001\u8f7b\u91cf\u7ea7\u8fd1\u573a\u963b\u5c3c\u3001\u53ef\u9009\u6cca\u8083\u53f6\u80cc\u666f\u6d41\uff0c\u4ee5\u53ca\u4ece\u9a71\u52a8\u9891\u7387\u5230\u81ea\u7531\u7a7a\u95f4\u6eda\u52a8\u901f\u5ea6\u7684\u6821\u51c6\u6620\u5c04\u3002\u63d0\u4f9b\u6a21\u5757\u5316\u89c4\u5212-\u63a7\u5236\u5806\u6808\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u7b56\u7565\u8fdb\u884c\u63a5\u89e6\u5efa\u7acb\u548c\u76ee\u6807\u5bfc\u5411\u63a8\u52a8\u3002", "result": "\u7ed3\u679c\u663e\u793a\u63a7\u5236\u5668\u7a33\u5b9a\u6027\u5728\u6d41\u52a8\u6270\u52a8\u4e0b\u4e3b\u5bfc\u6027\u80fd\uff0c\u800c\u89c4\u5212\u5668\u9009\u62e9\u53ef\u4ee5\u901a\u8fc7\u8def\u5f84\u70b9\u8fdb\u5c55\u5f71\u54cd\u957f\u65f6\u57df\u5e8f\u5217\u7684\u547d\u4ee4\u5e73\u6ed1\u5ea6\u3002\u62a5\u544a\u4e86\u6210\u529f\u7387\u3001\u65f6\u95f4\u3001\u8ddf\u8e2a\u6307\u6807\u548c\u9a71\u52a8\u53d8\u5316\u5ea6\u91cfE_\u0394\u03c9\u3002", "conclusion": "MicroPush\u80fd\u591f\u5b9e\u73b0\u5fae\u5c3a\u5ea6\u63a5\u89e6\u4e30\u5bcc\u5fae\u64cd\u4f5c\u7684\u89c4\u5212\u3001\u63a7\u5236\u548c\u5b66\u4e60\u65b9\u6cd5\u7684\u53ef\u91cd\u590d\u6bd4\u8f83\u548c\u6d88\u878d\u7814\u7a76\u3002"}}
{"id": "2602.23589", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23589", "abs": "https://arxiv.org/abs/2602.23589", "authors": ["Hiroshi Sasaki"], "title": "Pseudo Contrastive Learning for Diagram Comprehension in Multimodal Models", "comment": "9 pages, 3 figures", "summary": "Recent multimodal models such as Contrastive Language-Image Pre-training (CLIP) have shown remarkable ability to align visual and linguistic representations. However, domains where small visual differences carry large semantic significance, such as diagram understanding, remain challenging due to the models' limited sensitivity to fine-grained structural variations.\n  We propose a new training paradigm designed to enhance diagram comprehension in vision-language models. Our approach introduces pseudo contrastive samples generated by a diagram renderer that creates synthetic diagrams using randomly picked text elements. These samples highlight structural differences in diagrammatic imagery without requiring any modification or editing of the original data. By incorporating these pseudo contrastive samples into the training objective, the model learns to capture more precise and semantically consistent diagram structures.\n  Empirical evaluations on a benchmark dataset of flowcharts demonstrate substantial improvements over standard CLIP and hard-negative CLIP training in both image-text matching and visual question answering tasks. The results underscore the value of domain-specific training strategies and contribute to advancing diagrammatic understanding within the broader context of vision-language learning.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u8bad\u7ec3\u8303\u5f0f\uff0c\u901a\u8fc7\u751f\u6210\u4f2a\u5bf9\u6bd4\u6837\u672c\u6765\u589e\u5f3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5bf9\u56fe\u8868\u7684\u7406\u89e3\u80fd\u529b\uff0c\u5728\u6d41\u7a0b\u56fe\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u6807\u51c6CLIP\u548chard-negative CLIP\u8bad\u7ec3", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u6a21\u578b\uff08\u5982CLIP\uff09\u5728\u89c6\u89c9\u548c\u8bed\u8a00\u8868\u793a\u5bf9\u9f50\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u9700\u8981\u7cbe\u7ec6\u7ed3\u6784\u5dee\u5f02\u611f\u77e5\u7684\u9886\u57df\uff08\u5982\u56fe\u8868\u7406\u89e3\uff09\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u56e0\u4e3a\u6a21\u578b\u5bf9\u7ec6\u7c92\u5ea6\u7ed3\u6784\u53d8\u5316\u7684\u654f\u611f\u6027\u6709\u9650", "method": "\u63d0\u51fa\u65b0\u7684\u8bad\u7ec3\u8303\u5f0f\uff0c\u4f7f\u7528\u56fe\u8868\u6e32\u67d3\u5668\u751f\u6210\u4f2a\u5bf9\u6bd4\u6837\u672c\uff0c\u8fd9\u4e9b\u6837\u672c\u901a\u8fc7\u968f\u673a\u9009\u53d6\u6587\u672c\u5143\u7d20\u521b\u5efa\u5408\u6210\u56fe\u8868\uff0c\u7a81\u51fa\u56fe\u8868\u56fe\u50cf\u4e2d\u7684\u7ed3\u6784\u5dee\u5f02\uff0c\u65e0\u9700\u4fee\u6539\u539f\u59cb\u6570\u636e\uff0c\u5c06\u8fd9\u4e9b\u4f2a\u5bf9\u6bd4\u6837\u672c\u7eb3\u5165\u8bad\u7ec3\u76ee\u6807", "result": "\u5728\u6d41\u7a0b\u56fe\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u8bc1\u8bc4\u4f30\u663e\u793a\uff0c\u5728\u56fe\u50cf-\u6587\u672c\u5339\u914d\u548c\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u4e2d\uff0c\u76f8\u6bd4\u6807\u51c6CLIP\u548chard-negative CLIP\u8bad\u7ec3\u90fd\u6709\u663e\u8457\u6539\u8fdb", "conclusion": "\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u9886\u57df\u7279\u5b9a\u8bad\u7ec3\u7b56\u7565\u7684\u4ef7\u503c\uff0c\u6709\u52a9\u4e8e\u5728\u66f4\u5e7f\u6cdb\u7684\u89c6\u89c9\u8bed\u8a00\u5b66\u4e60\u80cc\u666f\u4e0b\u63a8\u8fdb\u56fe\u8868\u7406\u89e3\u80fd\u529b\u7684\u53d1\u5c55"}}
{"id": "2602.23507", "categories": ["cs.LG", "stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2602.23507", "abs": "https://arxiv.org/abs/2602.23507", "authors": ["Diana Shamsutdinova", "Felix Zimmer", "Oyebayo Ridwan Olaniran", "Sarah Markham", "Daniel Stahl", "Gordon Forbes", "Ewan Carr"], "title": "Sample Size Calculations for Developing Clinical Prediction Models: Overview and pmsims R package", "comment": "26 pages, 4 figures, 1 table, preprint", "summary": "Background: Clinical prediction models are increasingly used to inform healthcare decisions, but determining the minimum sample size for their development remains a critical and unresolved challenge. Inadequate sample sizes can lead to overfitting, poor generalisability, and biased predictions. Existing approaches, such as heuristic rules, closed-form formulas, and simulation-based methods, vary in flexibility and accuracy, particularly for complex data structures and machine learning models. Methods: We review current methodologies for sample size estimation in prediction modelling and introduce a conceptual framework that distinguishes between mean-based and assurance-based criteria. Building on this, we propose a novel simulation-based approach that integrates learning curves, Gaussian Process optimisation, and assurance principles to identify sample sizes that achieve target performance with high probability. This approach is implemented in pmsims, an open-source, model-agnostic R package. Results: Through case studies, we demonstrate that sample size estimates vary substantially across methods, performance metrics, and modelling strategies. Compared to existing tools, pmsims provides flexible, efficient, and interpretable solutions that accommodate diverse models and user-defined metrics while explicitly accounting for variability in model performance. Conclusions: Our framework and software advance sample size methodology for clinical prediction modelling by combining flexibility with computational efficiency. Future work should extend these methods to hierarchical and multimodal data, incorporate fairness and stability metrics, and address challenges such as missing data and complex dependency structures.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e34\u5e8a\u9884\u6d4b\u6a21\u578b\u6837\u672c\u91cf\u786e\u5b9a\u65b9\u6cd5\uff0c\u7ed3\u5408\u5b66\u4e60\u66f2\u7ebf\u3001\u9ad8\u65af\u8fc7\u7a0b\u4f18\u5316\u548c\u4fdd\u8bc1\u539f\u5219\uff0c\u5f00\u53d1\u4e86\u5f00\u6e90\u7684R\u5305pmsims\uff0c\u4e3a\u590d\u6742\u6570\u636e\u7ed3\u6784\u548c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u63d0\u4f9b\u7075\u6d3b\u9ad8\u6548\u7684\u6837\u672c\u91cf\u4f30\u8ba1\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u4e34\u5e8a\u9884\u6d4b\u6a21\u578b\u5728\u533b\u7597\u51b3\u7b56\u4e2d\u7684\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u4f46\u786e\u5b9a\u5176\u5f00\u53d1\u6240\u9700\u7684\u6700\u5c0f\u6837\u672c\u91cf\u4ecd\u7136\u662f\u4e00\u4e2a\u5173\u952e\u4e14\u672a\u89e3\u51b3\u7684\u6311\u6218\u3002\u6837\u672c\u91cf\u4e0d\u8db3\u4f1a\u5bfc\u81f4\u8fc7\u62df\u5408\u3001\u6cdb\u5316\u80fd\u529b\u5dee\u548c\u9884\u6d4b\u504f\u5dee\u3002\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u7ecf\u9a8c\u6cd5\u5219\u3001\u5c01\u95ed\u516c\u5f0f\u548c\u57fa\u4e8e\u6a21\u62df\u7684\u65b9\u6cd5\uff09\u5728\u7075\u6d3b\u6027\u548c\u51c6\u786e\u6027\u65b9\u9762\u5b58\u5728\u5dee\u5f02\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u590d\u6742\u6570\u636e\u7ed3\u6784\u548c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u3002", "method": "\u672c\u6587\u56de\u987e\u4e86\u9884\u6d4b\u5efa\u6a21\u4e2d\u6837\u672c\u91cf\u4f30\u8ba1\u7684\u73b0\u6709\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u533a\u5206\u5747\u503c\u6807\u51c6\u548c\u4fdd\u8bc1\u6807\u51c6\u7684\u6982\u5ff5\u6846\u67b6\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u6a21\u62df\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u6574\u5408\u4e86\u5b66\u4e60\u66f2\u7ebf\u3001\u9ad8\u65af\u8fc7\u7a0b\u4f18\u5316\u548c\u4fdd\u8bc1\u539f\u5219\uff0c\u4ee5\u786e\u5b9a\u80fd\u591f\u4ee5\u9ad8\u6982\u7387\u8fbe\u5230\u76ee\u6807\u6027\u80fd\u7684\u6837\u672c\u91cf\u3002\u8be5\u65b9\u6cd5\u5728\u5f00\u6e90\u7684\u3001\u6a21\u578b\u65e0\u5173\u7684R\u5305pmsims\u4e2d\u5b9e\u73b0\u3002", "result": "\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u8bc1\u660e\uff0c\u6837\u672c\u91cf\u4f30\u8ba1\u5728\u4e0d\u540c\u65b9\u6cd5\u3001\u6027\u80fd\u6307\u6807\u548c\u5efa\u6a21\u7b56\u7565\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002\u4e0e\u73b0\u6709\u5de5\u5177\u76f8\u6bd4\uff0cpmsims\u63d0\u4f9b\u4e86\u7075\u6d3b\u3001\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u9002\u5e94\u591a\u6837\u5316\u7684\u6a21\u578b\u548c\u7528\u6237\u5b9a\u4e49\u7684\u6307\u6807\uff0c\u540c\u65f6\u660e\u786e\u8003\u8651\u6a21\u578b\u6027\u80fd\u7684\u53d8\u5f02\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u6846\u67b6\u548c\u8f6f\u4ef6\u901a\u8fc7\u5c06\u7075\u6d3b\u6027\u4e0e\u8ba1\u7b97\u6548\u7387\u76f8\u7ed3\u5408\uff0c\u63a8\u8fdb\u4e86\u4e34\u5e8a\u9884\u6d4b\u5efa\u6a21\u7684\u6837\u672c\u91cf\u65b9\u6cd5\u5b66\u3002\u672a\u6765\u5de5\u4f5c\u5e94\u5c06\u8fd9\u4e9b\u65b9\u6cd5\u6269\u5c55\u5230\u5206\u5c42\u548c\u591a\u6a21\u6001\u6570\u636e\uff0c\u7eb3\u5165\u516c\u5e73\u6027\u548c\u7a33\u5b9a\u6027\u6307\u6807\uff0c\u5e76\u89e3\u51b3\u7f3a\u5931\u6570\u636e\u548c\u590d\u6742\u4f9d\u8d56\u7ed3\u6784\u7b49\u6311\u6218\u3002"}}
{"id": "2602.23729", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23729", "abs": "https://arxiv.org/abs/2602.23729", "authors": ["Seungdong Yoa", "Sanghyu Yoon", "Suhee Yoon", "Dongmin Kim", "Ye Seul Sim", "Junhyun Lee", "Woohyung Lim"], "title": "From Static Benchmarks to Dynamic Protocol: Agent-Centric Text Anomaly Detection for Evaluating LLM Reasoning", "comment": "Accepted to ICLR 2026", "summary": "The evaluation of large language models (LLMs) has predominantly relied on static datasets, which offer limited scalability and fail to capture the evolving reasoning capabilities of recent models. To overcome these limitations, we propose an agent-centric benchmarking paradigm that moves beyond static datasets by introducing a dynamic protocol in which autonomous agents iteratively generate, validate, and solve problems. Within this protocol, a teacher agent generates candidate problems, an orchestrator agent rigorously verifies their validity and guards against adversarial attacks, and a student agent attempts to solve the validated problems. An invalid problem is revised by the teacher agent until it passes validation. If the student correctly solves the problem, the orchestrator prompts the teacher to generate more challenging variants. Consequently, the benchmark scales in difficulty automatically as more capable agents are substituted into any role, enabling progressive evaluation of large language models without manually curated datasets. Adopting text anomaly detection as our primary evaluation format, which demands cross-sentence logical inference and resists pattern-matching shortcuts, we demonstrate that this protocol systematically exposes corner-case reasoning errors that conventional benchmarks fail to reveal. We further advocate evaluating systems along several complementary axes including cross-model pairwise performance and progress between the initial and orchestrator-finalized problems. By shifting the focus from fixed datasets to dynamic protocols, our approach offers a sustainable direction for evaluating ever-evolving language models and introduces a research agenda centered on the co-evolution of agent-centric benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u667a\u80fd\u4f53\u7684\u52a8\u6001\u57fa\u51c6\u6d4b\u8bd5\u8303\u5f0f\uff0c\u66ff\u4ee3\u4f20\u7edf\u9759\u6001\u6570\u636e\u96c6\u8bc4\u4f30\u65b9\u6cd5\uff0c\u901a\u8fc7\u6559\u5e08\u3001\u534f\u8c03\u8005\u548c\u5b66\u751f\u667a\u80fd\u4f53\u7684\u534f\u4f5c\u81ea\u52a8\u751f\u6210\u3001\u9a8c\u8bc1\u548c\u89e3\u51b3\u95ee\u9898\uff0c\u5b9e\u73b0\u96be\u5ea6\u81ea\u9002\u5e94\u6269\u5c55\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u9759\u6001\u6570\u636e\u96c6\u7684LLM\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u53ef\u6269\u5c55\u6027\u6709\u9650\u3001\u65e0\u6cd5\u6355\u6349\u6a21\u578b\u6f14\u8fdb\u63a8\u7406\u80fd\u529b\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u52a8\u6001\u3001\u53ef\u6301\u7eed\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u91c7\u7528\u667a\u80fd\u4f53\u4e2d\u5fc3\u8303\u5f0f\uff1a\u6559\u5e08\u667a\u80fd\u4f53\u751f\u6210\u5019\u9009\u95ee\u9898\uff0c\u534f\u8c03\u8005\u667a\u80fd\u4f53\u9a8c\u8bc1\u95ee\u9898\u6709\u6548\u6027\u5e76\u9632\u5fa1\u5bf9\u6297\u653b\u51fb\uff0c\u5b66\u751f\u667a\u80fd\u4f53\u5c1d\u8bd5\u89e3\u51b3\u95ee\u9898\u3002\u65e0\u6548\u95ee\u9898\u7531\u6559\u5e08\u4fee\u8ba2\uff0c\u6b63\u786e\u89e3\u7b54\u540e\u751f\u6210\u66f4\u5177\u6311\u6218\u7684\u53d8\u4f53\u3002", "result": "\u8be5\u534f\u8bae\u80fd\u7cfb\u7edf\u6027\u5730\u66b4\u9732\u4f20\u7edf\u57fa\u51c6\u6d4b\u8bd5\u65e0\u6cd5\u63ed\u793a\u7684\u8fb9\u754c\u60c5\u51b5\u63a8\u7406\u9519\u8bef\uff0c\u5728\u6587\u672c\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\u4e2d\u5c55\u793a\u51fa\u4f18\u8d8a\u6027\uff0c\u652f\u6301\u8de8\u6a21\u578b\u6210\u5bf9\u6027\u80fd\u548c\u95ee\u9898\u6f14\u8fdb\u7684\u591a\u7ef4\u5ea6\u8bc4\u4f30\u3002", "conclusion": "\u4ece\u56fa\u5b9a\u6570\u636e\u96c6\u8f6c\u5411\u52a8\u6001\u534f\u8bae\u4e3a\u8bc4\u4f30\u4e0d\u65ad\u6f14\u8fdb\u7684\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u53ef\u6301\u7eed\u65b9\u5411\uff0c\u5e76\u5f15\u5165\u4e86\u4ee5\u667a\u80fd\u4f53\u57fa\u51c6\u6d4b\u8bd5\u5171\u540c\u8fdb\u5316\u4e3a\u4e2d\u5fc3\u7684\u7814\u7a76\u8bae\u7a0b\u3002"}}
{"id": "2602.23648", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.23648", "abs": "https://arxiv.org/abs/2602.23648", "authors": ["Yao Li", "Peiyuan Tang", "Wuyang Zhang", "Chengyang Zhu", "Yifan Duan", "Weikai Shi", "Xiaodong Zhang", "Zijiang Yang", "Jianmin Ji", "Yanyong Zhang"], "title": "FAVLA: A Force-Adaptive Fast-Slow VLA model for Contact-Rich Robotic Manipulation", "comment": null, "summary": "Force/torque feedback can substantially improve Vision-Language-Action (VLA) models on contact-rich manipulation, but most existing approaches fuse all modalities at a single operating frequency. This design ignores the mismatched sampling rates of real robot sensors, forcing downsampling of the high-frequency contact cues needed for reactive correction. Combined with common VLM-action-expert (AE) pipelines that execute action chunks largely open loop between expensive VLM updates, unified-frequency fusion often yields delayed responses to impacts, stick-slip, and force spikes. We propose FAVLA, a force-adaptive fast-slow VLA that decouples slow perception planning from fast contact-aware control. FAVLA runs a slow VLM at a fixed low frequency to encode modalities to produce latent representations and to predict near-future force variation. A fast AE then executes at a variable high frequency, conditioning on the latest force sequence data to generate reactive actions. We further introduce a force adapter that injects high-frequency force features into multiple AE layers, and adaptively schedules the AE's execution frequency based on the VLM's predicted force variation. Extensive experiments on contact-rich tasks demonstrate that FAVLA significantly outperforms baselines, achieving superior reactivity and success rates, especially with a smaller contact force during manipulation.", "AI": {"tldr": "FAVLA\u63d0\u51fa\u4e86\u4e00\u79cd\u529b\u81ea\u9002\u5e94\u5feb\u6162VLA\u6a21\u578b\uff0c\u901a\u8fc7\u89e3\u8026\u6162\u901f\u611f\u77e5\u89c4\u5212\u548c\u5feb\u901f\u63a5\u89e6\u611f\u77e5\u63a7\u5236\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u7edf\u4e00\u9891\u7387\u878d\u5408\u65b9\u6cd5\u5728\u63a5\u89e6\u4e30\u5bcc\u64cd\u4f5c\u4e2d\u7684\u5ef6\u8fdf\u54cd\u5e94\u95ee\u9898\u3002", "motivation": "\u73b0\u6709VLA\u6a21\u578b\u901a\u5e38\u5c06\u6240\u6709\u6a21\u6001\u4ee5\u5355\u4e00\u9891\u7387\u878d\u5408\uff0c\u5ffd\u7565\u4e86\u673a\u5668\u4eba\u4f20\u611f\u5668\u7684\u4e0d\u540c\u91c7\u6837\u7387\uff0c\u5bfc\u81f4\u9ad8\u9891\u7387\u63a5\u89e6\u4fe1\u53f7\u88ab\u964d\u91c7\u6837\u3002\u7ed3\u5408\u5e38\u89c1\u7684VLM-\u52a8\u4f5c\u4e13\u5bb6\u6d41\u6c34\u7ebf\u5728\u6602\u8d35\u7684VLM\u66f4\u65b0\u4e4b\u95f4\u57fa\u672c\u5f00\u73af\u6267\u884c\u52a8\u4f5c\u5757\uff0c\u7edf\u4e00\u9891\u7387\u878d\u5408\u5f80\u5f80\u5bf9\u51b2\u51fb\u3001\u7c98\u6ed1\u548c\u529b\u5cf0\u503c\u4ea7\u751f\u5ef6\u8fdf\u54cd\u5e94\u3002", "method": "FAVLA\u91c7\u7528\u529b\u81ea\u9002\u5e94\u5feb\u6162\u67b6\u6784\uff1a1) \u6162\u901fVLM\u4ee5\u56fa\u5b9a\u4f4e\u9891\u8fd0\u884c\uff0c\u7f16\u7801\u591a\u6a21\u6001\u4ea7\u751f\u6f5c\u5728\u8868\u793a\u5e76\u9884\u6d4b\u8fd1\u672a\u6765\u529b\u53d8\u5316\uff1b2) \u5feb\u901fAE\u4ee5\u53ef\u53d8\u9ad8\u9891\u6267\u884c\uff0c\u57fa\u4e8e\u6700\u65b0\u529b\u5e8f\u5217\u6570\u636e\u751f\u6210\u53cd\u5e94\u6027\u52a8\u4f5c\uff1b3) \u5f15\u5165\u529b\u9002\u914d\u5668\u5c06\u9ad8\u9891\u529b\u7279\u5f81\u6ce8\u5165\u591a\u4e2aAE\u5c42\uff1b4) \u57fa\u4e8eVLM\u9884\u6d4b\u7684\u529b\u53d8\u5316\u81ea\u9002\u5e94\u8c03\u5ea6AE\u6267\u884c\u9891\u7387\u3002", "result": "\u5728\u63a5\u89e6\u4e30\u5bcc\u7684\u4efb\u52a1\u4e0a\u8fdb\u884c\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cFAVLA\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u53cd\u5e94\u6027\u548c\u6210\u529f\u7387\uff0c\u7279\u522b\u662f\u5728\u64cd\u4f5c\u8fc7\u7a0b\u4e2d\u4f7f\u7528\u66f4\u5c0f\u7684\u63a5\u89e6\u529b\u65f6\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u901a\u8fc7\u89e3\u8026\u6162\u901f\u611f\u77e5\u89c4\u5212\u548c\u5feb\u901f\u63a5\u89e6\u611f\u77e5\u63a7\u5236\uff0cFAVLA\u80fd\u591f\u6709\u6548\u5904\u7406\u673a\u5668\u4eba\u4f20\u611f\u5668\u9891\u7387\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u5728\u63a5\u89e6\u4e30\u5bcc\u7684\u64cd\u4f5c\u4efb\u52a1\u4e2d\u5b9e\u73b0\u66f4\u53ca\u65f6\u7684\u53cd\u5e94\u548c\u66f4\u9ad8\u7684\u6210\u529f\u7387\u3002"}}
{"id": "2602.23528", "categories": ["cs.LG", "cs.CE", "stat.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.23528", "abs": "https://arxiv.org/abs/2602.23528", "authors": ["Yicen Li", "Jose Antonio Lara Benitez", "Ruiyang Hong", "Anastasis Kratsios", "Paul David McNicholas", "Maarten Valentijn de Hoop"], "title": "Neural Operators Can Discover Functional Clusters", "comment": null, "summary": "Operator learning is reshaping scientific computing by amortizing inference across infinite families of problems. While neural operators (NOs) are increasingly well understood for regression, far less is known for classification and its unsupervised analogue: clustering. We prove that sample-based neural operators can learn any finite collection of classes in an infinite-dimensional reproducing kernel Hilbert space, even when the classes are neither convex nor connected, under mild kernel sampling assumptions. Our universal clustering theorem shows that any $K$ closed classes can be approximated to arbitrary precision by NO-parameterized classes in the upper Kuratowski topology on closed sets, a notion that can be interpreted as disallowing false-positive misclassifications.\n  Building on this, we develop an NO-powered clustering pipeline for functional data and apply it to unlabeled families of ordinary differential equation (ODE) trajectories. Discretized trajectories are lifted by a fixed pre-trained encoder into a continuous feature map and mapped to soft assignments by a lightweight trainable head. Experiments on diverse synthetic ODE benchmarks show that the resulting practical SNO recovers latent dynamical structure in regimes where classical methods fail, providing evidence consistent with our universal clustering theory.", "AI": {"tldr": "\u8bba\u6587\u8bc1\u660e\u4e86\u57fa\u4e8e\u6837\u672c\u7684\u795e\u7ecf\u7b97\u5b50\u80fd\u591f\u5b66\u4e60\u65e0\u9650\u7ef4\u518d\u751f\u6838\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u4e2d\u4efb\u610f\u6709\u9650\u7c7b\u522b\u96c6\u5408\uff0c\u5373\u4f7f\u7c7b\u522b\u975e\u51f8\u975e\u8fde\u901a\uff0c\u5e76\u5f00\u53d1\u4e86\u7528\u4e8e\u51fd\u6570\u6570\u636e\u805a\u7c7b\u7684\u795e\u7ecf\u7b97\u5b50\u7ba1\u9053\uff0c\u5728ODE\u8f68\u8ff9\u5206\u6790\u4e2d\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u795e\u7ecf\u7b97\u5b50\u5728\u56de\u5f52\u4efb\u52a1\u4e2d\u5df2\u6709\u6df1\u5165\u7814\u7a76\uff0c\u4f46\u5728\u5206\u7c7b\u53ca\u5176\u65e0\u76d1\u7763\u5bf9\u5e94\u4efb\u52a1\u2014\u2014\u805a\u7c7b\u65b9\u9762\u4e86\u89e3\u751a\u5c11\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63a2\u7d22\u795e\u7ecf\u7b97\u5b50\u5728\u65e0\u9650\u7ef4\u51fd\u6570\u7a7a\u95f4\u4e2d\u7684\u805a\u7c7b\u80fd\u529b\u3002", "method": "1. \u7406\u8bba\u8bc1\u660e\uff1a\u5728\u6e29\u548c\u7684\u6838\u91c7\u6837\u5047\u8bbe\u4e0b\uff0c\u8bc1\u660e\u57fa\u4e8e\u6837\u672c\u7684\u795e\u7ecf\u7b97\u5b50\u80fd\u591f\u5b66\u4e60\u65e0\u9650\u7ef4\u518d\u751f\u6838\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u4e2d\u4efb\u610f\u6709\u9650\u7c7b\u522b\u96c6\u5408\uff1b2. \u5b9e\u7528\u7ba1\u9053\uff1a\u5f00\u53d1\u795e\u7ecf\u7b97\u5b50\u9a71\u52a8\u7684\u51fd\u6570\u6570\u636e\u805a\u7c7b\u7ba1\u9053\uff0c\u5305\u62ec\u9884\u8bad\u7ec3\u7f16\u7801\u5668\u5c06\u79bb\u6563\u8f68\u8ff9\u63d0\u5347\u4e3a\u8fde\u7eed\u7279\u5f81\u6620\u5c04\uff0c\u8f7b\u91cf\u7ea7\u53ef\u8bad\u7ec3\u5934\u90e8\u6620\u5c04\u5230\u8f6f\u5206\u914d\u3002", "result": "1. \u63d0\u51fa\u901a\u7528\u805a\u7c7b\u5b9a\u7406\uff1a\u4efb\u4f55K\u4e2a\u95ed\u7c7b\u90fd\u53ef\u4ee5\u5728\u95ed\u96c6\u7684\u4e0aKuratowski\u62d3\u6251\u4e2d\u88ab\u795e\u7ecf\u7b97\u5b50\u53c2\u6570\u5316\u7684\u7c7b\u4ee5\u4efb\u610f\u7cbe\u5ea6\u903c\u8fd1\uff1b2. \u5728\u5408\u6210ODE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u63d0\u51fa\u7684\u5b9e\u7528SNO\u65b9\u6cd5\u5728\u7ecf\u5178\u65b9\u6cd5\u5931\u8d25\u7684\u573a\u666f\u4e0b\u6210\u529f\u6062\u590d\u6f5c\u5728\u52a8\u6001\u7ed3\u6784\u3002", "conclusion": "\u795e\u7ecf\u7b97\u5b50\u4e0d\u4ec5\u9002\u7528\u4e8e\u56de\u5f52\u4efb\u52a1\uff0c\u4e5f\u80fd\u6709\u6548\u5904\u7406\u65e0\u9650\u7ef4\u51fd\u6570\u7a7a\u95f4\u4e2d\u7684\u805a\u7c7b\u95ee\u9898\u3002\u7406\u8bba\u8bc1\u660e\u548c\u5b9e\u9a8c\u7ed3\u679c\u4e00\u81f4\u8868\u660e\uff0c\u795e\u7ecf\u7b97\u5b50\u80fd\u591f\u5b66\u4e60\u590d\u6742\u975e\u51f8\u975e\u8fde\u901a\u7684\u7c7b\u522b\u7ed3\u6784\uff0c\u4e3a\u51fd\u6570\u6570\u636e\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u7684\u5f3a\u5927\u5de5\u5177\u3002"}}
{"id": "2602.23701", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.23701", "abs": "https://arxiv.org/abs/2602.23701", "authors": ["Yawen Wang", "Wenjie Wu", "Junjie Wang", "Qing Wang"], "title": "From Flat Logs to Causal Graphs: Hierarchical Failure Attribution for LLM-based Multi-Agent Systems", "comment": null, "summary": "LLM-powered Multi-Agent Systems (MAS) have demonstrated remarkable capabilities in complex domains but suffer from inherent fragility and opaque failure mechanisms. Existing failure attribution methods, whether relying on direct prompting, costly replays, or supervised fine-tuning, typically treat execution logs as flat sequences. This linear perspective fails to disentangle the intricate causal links inherent to MAS, leading to weak observability and ambiguous responsibility boundaries. To address these challenges, we propose CHIEF, a novel framework that transforms chaotic trajectories into a structured hierarchical causal graph. It then employs hierarchical oracle-guided backtracking to efficiently prune the search space via sybthesized virtual oracles. Finally, it implements counterfactual attribution via a progressive causal screening strategy to rigorously distinguish true root causes from propagated symptoms. Experiments on Who&When benchmark show that CHIEF outperforms eight strong and state-of-the-art baselines on both agent- and step-level accuracy. Ablation studies further confirm the critical role of each proposed module.", "AI": {"tldr": "CHIEF\u6846\u67b6\u901a\u8fc7\u5c06\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u6df7\u6c8c\u8f68\u8ff9\u8f6c\u6362\u4e3a\u5c42\u6b21\u5316\u56e0\u679c\u56fe\uff0c\u4f7f\u7528\u5c42\u6b21\u5316oracle\u5f15\u5bfc\u56de\u6eaf\u548c\u6e10\u8fdb\u56e0\u679c\u7b5b\u9009\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6545\u969c\u5f52\u56e0\u7684\u51c6\u786e\u6027\u3002", "motivation": "LLM\u9a71\u52a8\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u590d\u6742\u9886\u57df\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b58\u5728\u56fa\u6709\u7684\u8106\u5f31\u6027\u548c\u4e0d\u900f\u660e\u7684\u6545\u969c\u673a\u5236\u3002\u73b0\u6709\u7684\u6545\u969c\u5f52\u56e0\u65b9\u6cd5\u901a\u5e38\u5c06\u6267\u884c\u65e5\u5fd7\u89c6\u4e3a\u6241\u5e73\u5e8f\u5217\uff0c\u8fd9\u79cd\u7ebf\u6027\u89c6\u89d2\u65e0\u6cd5\u89e3\u6784MAS\u4e2d\u590d\u6742\u7684\u56e0\u679c\u8054\u7cfb\uff0c\u5bfc\u81f4\u5f31\u53ef\u89c2\u6d4b\u6027\u548c\u6a21\u7cca\u7684\u8d23\u4efb\u8fb9\u754c\u3002", "method": "\u63d0\u51faCHIEF\u6846\u67b6\uff1a1) \u5c06\u6df7\u6c8c\u8f68\u8ff9\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u5c42\u6b21\u56e0\u679c\u56fe\uff1b2) \u4f7f\u7528\u5c42\u6b21\u5316oracle\u5f15\u5bfc\u56de\u6eaf\uff0c\u901a\u8fc7\u5408\u6210\u865a\u62dforacle\u9ad8\u6548\u526a\u679d\u641c\u7d22\u7a7a\u95f4\uff1b3) \u901a\u8fc7\u6e10\u8fdb\u56e0\u679c\u7b5b\u9009\u7b56\u7565\u5b9e\u73b0\u53cd\u4e8b\u5b9e\u5f52\u56e0\uff0c\u4e25\u683c\u533a\u5206\u771f\u6b63\u7684\u6839\u672c\u539f\u56e0\u548c\u4f20\u64ad\u7684\u75c7\u72b6\u3002", "result": "\u5728Who&When\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCHIEF\u5728\u667a\u80fd\u4f53\u7ea7\u522b\u548c\u6b65\u9aa4\u7ea7\u522b\u51c6\u786e\u6027\u4e0a\u90fd\u4f18\u4e8e\u516b\u4e2a\u5f3a\u5927\u7684\u6700\u5148\u8fdb\u57fa\u7ebf\u65b9\u6cd5\u3002\u6d88\u878d\u7814\u7a76\u8fdb\u4e00\u6b65\u8bc1\u5b9e\u4e86\u6bcf\u4e2a\u63d0\u51fa\u6a21\u5757\u7684\u5173\u952e\u4f5c\u7528\u3002", "conclusion": "CHIEF\u901a\u8fc7\u7ed3\u6784\u5316\u5c42\u6b21\u56e0\u679c\u56fe\u8868\u793a\u548c\u9ad8\u6548\u7684\u56e0\u679c\u5206\u6790\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u6545\u969c\u5f52\u56e0\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u89e3\u51b3MAS\u7684\u8106\u5f31\u6027\u548c\u4e0d\u900f\u660e\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2602.23753", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.23753", "abs": "https://arxiv.org/abs/2602.23753", "authors": ["Jiasen Zheng", "Zijun Zhou", "Huajun Zhang", "Junjiang Lin", "Jingyun Jia", "Qi Wang"], "title": "Structured Prompt Optimization for Few-Shot Text Classification via Semantic Alignment in Latent Space", "comment": null, "summary": "This study addresses the issues of semantic entanglement, unclear label structure, and insufficient feature representation in few-shot text classification, and proposes an optimization framework based on structured prompts to enhance semantic understanding and task adaptation under low-resource conditions. The framework first uses a pretrained language model to encode the input text and obtain basic semantic representations. It then introduces structured prompts composed of multi-dimensional semantic factors and integrates them with text features through a learnable combination mechanism, which forms task-related representations with clear boundaries in the latent space. To further strengthen the consistency between text representations and label semantics, the method constructs a structured label embedding matrix and employs a cross-space alignment mechanism to ensure stable matching between textual features and label attributes. In addition, the model applies prompt orthogonality constraints and a joint optimization objective to maintain independence across different semantic factors in the prompts, allowing the structured prompts to provide transparent and controllable guidance for classification decisions. Three types of sensitivity experiments, including learning rate sensitivity, prompt length sensitivity, and data scale sensitivity, are designed to evaluate the stability and robustness of the framework under different conditions. Experimental results show that the proposed structured prompt optimization framework effectively alleviates semantic conflicts and label ambiguity in few-shot text classification. It significantly improves performance on accuracy, precision, recall, and AUC, and demonstrates strong cross-task applicability.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u7ed3\u6784\u5316\u63d0\u793a\u7684\u4f18\u5316\u6846\u67b6\uff0c\u89e3\u51b3\u5c11\u6837\u672c\u6587\u672c\u5206\u7c7b\u4e2d\u7684\u8bed\u4e49\u7ea0\u7f20\u3001\u6807\u7b7e\u7ed3\u6784\u4e0d\u6e05\u6670\u548c\u7279\u5f81\u8868\u793a\u4e0d\u8db3\u95ee\u9898\uff0c\u901a\u8fc7\u591a\u7ef4\u5ea6\u8bed\u4e49\u56e0\u7d20\u548c\u8de8\u7a7a\u95f4\u5bf9\u9f50\u673a\u5236\u63d0\u5347\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u9488\u5bf9\u5c11\u6837\u672c\u6587\u672c\u5206\u7c7b\u4e2d\u5b58\u5728\u7684\u8bed\u4e49\u7ea0\u7f20\u3001\u6807\u7b7e\u7ed3\u6784\u4e0d\u6e05\u6670\u548c\u7279\u5f81\u8868\u793a\u4e0d\u8db3\u7b49\u95ee\u9898\uff0c\u9700\u8981\u5728\u4f4e\u8d44\u6e90\u6761\u4ef6\u4e0b\u63d0\u5347\u8bed\u4e49\u7406\u89e3\u548c\u4efb\u52a1\u9002\u5e94\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7f16\u7801\u6587\u672c\uff0c\u5f15\u5165\u7531\u591a\u7ef4\u5ea6\u8bed\u4e49\u56e0\u7d20\u7ec4\u6210\u7684\u7ed3\u6784\u5316\u63d0\u793a\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7ec4\u5408\u673a\u5236\u4e0e\u6587\u672c\u7279\u5f81\u878d\u5408\uff0c\u6784\u5efa\u7ed3\u6784\u5316\u6807\u7b7e\u5d4c\u5165\u77e9\u9635\u548c\u8de8\u7a7a\u95f4\u5bf9\u9f50\u673a\u5236\uff0c\u5e94\u7528\u63d0\u793a\u6b63\u4ea4\u6027\u7ea6\u675f\u548c\u8054\u5408\u4f18\u5316\u76ee\u6807\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u6709\u6548\u7f13\u89e3\u4e86\u5c11\u6837\u672c\u6587\u672c\u5206\u7c7b\u4e2d\u7684\u8bed\u4e49\u51b2\u7a81\u548c\u6807\u7b7e\u6a21\u7cca\u95ee\u9898\uff0c\u5728\u51c6\u786e\u7387\u3001\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548cAUC\u7b49\u6307\u6807\u4e0a\u663e\u8457\u63d0\u5347\uff0c\u5e76\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u8de8\u4efb\u52a1\u9002\u7528\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u7ed3\u6784\u5316\u63d0\u793a\u4f18\u5316\u6846\u67b6\u80fd\u591f\u6709\u6548\u89e3\u51b3\u5c11\u6837\u672c\u6587\u672c\u5206\u7c7b\u7684\u5173\u952e\u6311\u6218\uff0c\u901a\u8fc7\u589e\u5f3a\u8bed\u4e49\u7406\u89e3\u548c\u4efb\u52a1\u9002\u5e94\u80fd\u529b\uff0c\u4e3a\u4f4e\u8d44\u6e90\u6761\u4ef6\u4e0b\u7684\u6587\u672c\u5206\u7c7b\u63d0\u4f9b\u4e86\u900f\u660e\u53ef\u63a7\u7684\u6307\u5bfc\u65b9\u6848\u3002"}}
{"id": "2602.23615", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23615", "abs": "https://arxiv.org/abs/2602.23615", "authors": ["Jiacheng Yang", "Anqi Chen", "Yunkai Dang", "Qi Fan", "Cong Wang", "Wenbin Li", "Feng Miao", "Yang Gao"], "title": "Annotation-Free Visual Reasoning for High-Resolution Large Multimodal Models via Reinforcement Learning", "comment": null, "summary": "Current Large Multimodal Models (LMMs) struggle with high-resolution visual inputs during the reasoning process, as the number of image tokens increases quadratically with resolution, introducing substantial redundancy and irrelevant information. A common practice is to identify key image regions and refer to their high-resolution counterparts during reasoning, typically trained with external visual supervision. However, such visual supervision cues require costly grounding labels from human annotators. Meanwhile, it remains an open question how to enhance a model's grounding abilities to support reasoning without relying on additional annotations. In this paper, we propose High-resolution Annotation-free Reasoning Technique (HART), a closed-loop framework that enables LMMs to focus on and self-verify key regions of high-resolution visual inputs. HART incorporates a post-training paradigm in which we design Advantage Preference Group Relative Policy Optimization (AP-GRPO) to encourage accurate localization of key regions. Notably, HART provides explainable reasoning pathways and enables efficient optimization of localization. Extensive experiments demonstrate that HART improves performance across a wide range of high-resolution visual tasks, consistently outperforming strong baselines. When applied to post-train Qwen2.5-VL-7B, HART even surpasses larger-scale models such as Qwen2.5-VL-72B and LLaVA-OneVision-72B on high-resolution, vision-centric benchmarks.", "AI": {"tldr": "HART\u662f\u4e00\u4e2a\u65e0\u9700\u6807\u6ce8\u7684\u9ad8\u5206\u8fa8\u7387\u89c6\u89c9\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9a8c\u8bc1\u5173\u952e\u533a\u57df\u5b9a\u4f4d\u548c\u7b56\u7565\u4f18\u5316\uff0c\u63d0\u5347\u5927\u6a21\u578b\u5728\u9ad8\u5206\u8fa8\u7387\u89c6\u89c9\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u5927\u6a21\u578b\u5904\u7406\u9ad8\u5206\u8fa8\u7387\u89c6\u89c9\u8f93\u5165\u65f6\u9762\u4e34token\u6570\u91cf\u5e73\u65b9\u7ea7\u589e\u957f\u3001\u5197\u4f59\u4fe1\u606f\u591a\u7684\u95ee\u9898\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u7684\u4eba\u5de5\u6807\u6ce8\u6765\u8bc6\u522b\u5173\u952e\u533a\u57df\uff0c\u5982\u4f55\u5728\u4e0d\u4f9d\u8d56\u989d\u5916\u6807\u6ce8\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u6a21\u578b\u7684\u5b9a\u4f4d\u548c\u63a8\u7406\u80fd\u529b\u662f\u4e00\u4e2a\u5f00\u653e\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51faHART\u6846\u67b6\uff0c\u91c7\u7528\u540e\u8bad\u7ec3\u8303\u5f0f\uff0c\u8bbe\u8ba1AP-GRPO\uff08\u4f18\u52bf\u504f\u597d\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff09\u6765\u9f13\u52b1\u51c6\u786e\u7684\u5173\u952e\u533a\u57df\u5b9a\u4f4d\uff0c\u5f62\u6210\u81ea\u9a8c\u8bc1\u7684\u95ed\u73af\u7cfb\u7edf\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u805a\u7126\u5e76\u9a8c\u8bc1\u9ad8\u5206\u8fa8\u7387\u8f93\u5165\u7684\u5173\u952e\u533a\u57df\u3002", "result": "\u5b9e\u9a8c\u8868\u660eHART\u5728\u591a\u79cd\u9ad8\u5206\u8fa8\u7387\u89c6\u89c9\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u6301\u7eed\u8d85\u8d8a\u5f3a\u57fa\u7ebf\u3002\u5e94\u7528\u4e8eQwen2.5-VL-7B\u540e\uff0c\u751a\u81f3\u8d85\u8d8a\u4e86Qwen2.5-VL-72B\u548cLLaVA-OneVision-72B\u7b49\u66f4\u5927\u89c4\u6a21\u6a21\u578b\u5728\u9ad8\u5206\u8fa8\u7387\u89c6\u89c9\u57fa\u51c6\u4e0a\u7684\u8868\u73b0\u3002", "conclusion": "HART\u901a\u8fc7\u65e0\u9700\u6807\u6ce8\u7684\u81ea\u9a8c\u8bc1\u5173\u952e\u533a\u57df\u5b9a\u4f4d\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5927\u6a21\u578b\u5904\u7406\u9ad8\u5206\u8fa8\u7387\u89c6\u89c9\u8f93\u5165\u7684\u6311\u6218\uff0c\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u8def\u5f84\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2602.23529", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23529", "abs": "https://arxiv.org/abs/2602.23529", "authors": ["Martin \u010cern\u00fd", "David Sychrovsk\u00fd", "Filip \u00daradn\u00edk", "Jakub \u010cern\u00fd"], "title": "Active Value Querying to Minimize Additive Error in Subadditive Set Function Learning", "comment": null, "summary": "Subadditive set functions play a pivotal role in computational economics (especially in combinatorial auctions), combinatorial optimization or artificial intelligence applications such as interpretable machine learning. However, specifying a set function requires assigning values to an exponentially large number of subsets in general, a task that is often resource-intensive in practice, particularly when the values derive from external sources such as retraining of machine learning models. A~simple omission of certain values introduces ambiguity that becomes even more significant when the incomplete set function has to be further optimized over. Motivated by the well-known result about inapproximability of subadditive functions using deterministic value queries with respect to a multiplicative error, we study a problem of approximating an unknown subadditive (or a subclass of thereof) set function with respect to an additive error -- i. e., we aim to efficiently close the distance between minimal and maximal completions. Our contributions are threefold: (i) a thorough exploration of minimal and maximal completions of different classes of set functions with missing values and an analysis of their resulting distance; (ii) the development of methods to minimize this distance over classes of set functions with a known prior, achieved by disclosing values of additional subsets in both offline and online manner; and (iii) empirical demonstrations of the algorithms' performance in practical scenarios.", "AI": {"tldr": "\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u6dfb\u52a0\u989d\u5916\u5b50\u96c6\u503c\u6765\u6700\u5c0f\u5316\u4e0d\u5b8c\u5168\u6b21\u53ef\u52a0\u96c6\u51fd\u6570\u7684\u8865\u5168\u8ddd\u79bb\uff0c\u5305\u62ec\u7406\u8bba\u5206\u6790\u548c\u5728\u7ebf/\u79bb\u7ebf\u7b97\u6cd5\u8bbe\u8ba1", "motivation": "\u6b21\u53ef\u52a0\u96c6\u51fd\u6570\u5728\u591a\u4e2a\u9886\u57df\u6709\u91cd\u8981\u5e94\u7528\uff0c\u4f46\u5b8c\u6574\u6307\u5b9a\u9700\u8981\u6307\u6570\u7ea7\u6570\u91cf\u7684\u503c\uff0c\u5b9e\u8df5\u4e2d\u5f80\u5f80\u4e0d\u5b8c\u6574\u3002\u4e0d\u5b8c\u5168\u96c6\u51fd\u6570\u5728\u4f18\u5316\u65f6\u4f1a\u4ea7\u751f\u6b67\u4e49\uff0c\u9700\u8981\u7814\u7a76\u5982\u4f55\u6709\u6548\u8fd1\u4f3c\u672a\u77e5\u7684\u6b21\u53ef\u52a0\u96c6\u51fd\u6570", "method": "1) \u63a2\u7d22\u4e0d\u540c\u7c7b\u522b\u96c6\u51fd\u6570\u7684\u6700\u5c0f\u548c\u6700\u5927\u8865\u5168\u53ca\u5176\u8ddd\u79bb\u5206\u6790\uff1b2) \u5f00\u53d1\u5728\u5df2\u77e5\u5148\u9a8c\u4e0b\u901a\u8fc7\u62ab\u9732\u989d\u5916\u5b50\u96c6\u503c\u6765\u6700\u5c0f\u5316\u8ddd\u79bb\u7684\u65b9\u6cd5\uff0c\u5305\u62ec\u79bb\u7ebf\u548c\u5728\u7ebf\u65b9\u5f0f\uff1b3) \u5728\u5b9e\u9645\u573a\u666f\u4e2d\u8fdb\u884c\u7b97\u6cd5\u6027\u80fd\u7684\u5b9e\u8bc1\u6f14\u793a", "result": "\u63d0\u51fa\u4e86\u7cfb\u7edf\u7684\u65b9\u6cd5\u6765\u5904\u7406\u4e0d\u5b8c\u5168\u6b21\u53ef\u52a0\u96c6\u51fd\u6570\u7684\u8fd1\u4f3c\u95ee\u9898\uff0c\u5305\u62ec\u7406\u8bba\u5206\u6790\u548c\u5b9e\u7528\u7b97\u6cd5\uff0c\u5e76\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u6027\u80fd", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u89e3\u51b3\u4e0d\u5b8c\u5168\u6b21\u53ef\u52a0\u96c6\u51fd\u6570\u7684\u8fd1\u4f3c\u95ee\u9898\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u7b97\u6cd5\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u51cf\u5c11\u8865\u5168\u8ddd\u79bb\uff0c\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c"}}
{"id": "2602.23792", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.23792", "abs": "https://arxiv.org/abs/2602.23792", "authors": ["Xiangzhong Luo", "Yilin An", "Zhicheng Yu", "Weichen Liu", "Xu Yang"], "title": "Divide and Conquer: Accelerating Diffusion-Based Large Language Models via Adaptive Parallel Decoding", "comment": "11 pages, 7 figures", "summary": "Diffusion-based large language models (dLLMs) have shown promising performance across various reasoning tasks, establishing themselves as an alternative to autoregressive large language models (LLMs). Unlike autoregressive LLMs that generate one token per step based on all previous tokens, dLLMs theoretically enable parallel generation of multiple tokens at each decoding step. However, recent dLLMs still favor one-token-per-step generation in practice, as directly decoding multiple masked tokens often leads to degraded generation quality and stability. This reveals a substantial gap between the theoretical parallelism and practical performance of dLLMs. To bridge this gap, we introduce an adaptive parallel decoding approach, namely DiCo, which features a three-phase divide-and-conquer paradigm to unleash the inherent parallelism of dLLMs. During the Divide phase, DiCo first explores the input masked sequence and identifies masked tokens as seed tokens, which are then expanded to construct a set of local clusters. During the Conquer phase, DiCo performs parallel decoding across different local clusters constructed in the Divide phase. The divide-and-conquer process repeatedly alternates between the Divide and Conquer phases until convergence. During the Finalize phase, DiCo decodes the remaining few masked tokens using an effective fine-grained compound decoding scheme to finalize the generation. Extensive experiments demonstrate that DiCo can achieve significant inference speedups while maintaining competitive generation quality.", "AI": {"tldr": "DiCo\u662f\u4e00\u79cd\u7528\u4e8e\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u9002\u5e94\u5e76\u884c\u89e3\u7801\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6cbb\u8303\u5f0f\u5b9e\u73b0\u5e76\u884c\u751f\u6210\uff0c\u5728\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u7406\u8bba\u4e0a\u652f\u6301\u5e76\u884c\u751f\u6210\u591a\u4e2atoken\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u4e2d\u4ecd\u503e\u5411\u4e8e\u5355token\u751f\u6210\uff0c\u56e0\u4e3a\u76f4\u63a5\u89e3\u7801\u591a\u4e2a\u63a9\u7801token\u4f1a\u5bfc\u81f4\u751f\u6210\u8d28\u91cf\u548c\u7a33\u5b9a\u6027\u4e0b\u964d\uff0c\u5b58\u5728\u7406\u8bba\u4e0e\u5b9e\u9645\u6027\u80fd\u7684\u5dee\u8ddd\u3002", "method": "\u63d0\u51faDiCo\u81ea\u9002\u5e94\u5e76\u884c\u89e3\u7801\u65b9\u6cd5\uff0c\u91c7\u7528\u4e09\u9636\u6bb5\u5206\u6cbb\u8303\u5f0f\uff1a1) Divide\u9636\u6bb5\uff1a\u63a2\u7d22\u8f93\u5165\u63a9\u7801\u5e8f\u5217\uff0c\u8bc6\u522b\u79cd\u5b50token\u5e76\u6269\u5c55\u6784\u5efa\u5c40\u90e8\u7c07\uff1b2) Conquer\u9636\u6bb5\uff1a\u5728\u4e0d\u540c\u5c40\u90e8\u7c07\u95f4\u5e76\u884c\u89e3\u7801\uff1b3) Finalize\u9636\u6bb5\uff1a\u4f7f\u7528\u7ec6\u7c92\u5ea6\u590d\u5408\u89e3\u7801\u65b9\u6848\u89e3\u7801\u5269\u4f59\u5c11\u91cf\u63a9\u7801token\u5b8c\u6210\u751f\u6210\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cDiCo\u80fd\u591f\u5b9e\u73b0\u663e\u8457\u7684\u63a8\u7406\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u6709\u7ade\u4e89\u529b\u7684\u751f\u6210\u8d28\u91cf\u3002", "conclusion": "DiCo\u901a\u8fc7\u81ea\u9002\u5e94\u5e76\u884c\u89e3\u7801\u65b9\u6cd5\u6210\u529f\u5f25\u5408\u4e86\u6269\u6563\u5927\u8bed\u8a00\u6a21\u578b\u7406\u8bba\u5e76\u884c\u6027\u4e0e\u5b9e\u9645\u6027\u80fd\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u9ad8\u6548\u63a8\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.23618", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23618", "abs": "https://arxiv.org/abs/2602.23618", "authors": ["Peng Dai", "Yu Zhang", "Yiqiang Feng", "Zhen Fan", "Yang Zhang"], "title": "Egocentric Visibility-Aware Human Pose Estimation", "comment": "Conference on Computer Vision and Pattern Recognition 2026", "summary": "Egocentric human pose estimation (HPE) using a head-mounted device is crucial for various VR and AR applications, but it faces significant challenges due to keypoint invisibility. Nevertheless, none of the existing egocentric HPE datasets provide keypoint visibility annotations, and the existing methods often overlook the invisibility problem, treating visible and invisible keypoints indiscriminately during estimation. As a result, their capacity to accurately predict visible keypoints is compromised. In this paper, we first present Eva-3M, a large-scale egocentric visibility-aware HPE dataset comprising over 3.0M frames, with 435K of them annotated with keypoint visibility labels. Additionally, we augment the existing EMHI dataset with keypoint visibility annotations to further facilitate the research in this direction. Furthermore, we propose EvaPose, a novel egocentric visibility-aware HPE method that explicitly incorporates visibility information to enhance pose estimation accuracy. Extensive experiments validate the significant value of ground-truth visibility labels in egocentric HPE settings, and demonstrate that our EvaPose achieves state-of-the-art performance in both Eva-3M and EMHI datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Eva-3M\u6570\u636e\u96c6\u548cEvaPose\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u4e2d\u5173\u952e\u70b9\u4e0d\u53ef\u89c1\u6027\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u6807\u6ce8\u53ef\u89c1\u6027\u6807\u7b7e\u548c\u663e\u5f0f\u5efa\u6a21\u53ef\u89c1\u6027\u4fe1\u606f\u6765\u63d0\u5347\u59ff\u6001\u4f30\u8ba1\u7cbe\u5ea6\u3002", "motivation": "\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u5728VR/AR\u5e94\u7528\u4e2d\u5f88\u91cd\u8981\uff0c\u4f46\u9762\u4e34\u5173\u952e\u70b9\u4e0d\u53ef\u89c1\u6027\u7684\u6311\u6218\u3002\u73b0\u6709\u6570\u636e\u96c6\u7f3a\u4e4f\u5173\u952e\u70b9\u53ef\u89c1\u6027\u6807\u6ce8\uff0c\u73b0\u6709\u65b9\u6cd5\u4e5f\u5ffd\u89c6\u4e0d\u53ef\u89c1\u6027\u95ee\u9898\uff0c\u5bfc\u81f4\u53ef\u89c1\u5173\u952e\u70b9\u7684\u9884\u6d4b\u7cbe\u5ea6\u53d7\u9650\u3002", "method": "1) \u521b\u5efaEva-3M\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff08300\u4e07\u5e27\uff0c\u5176\u4e2d43.5\u4e07\u5e27\u6709\u53ef\u89c1\u6027\u6807\u6ce8\uff09\uff1b2) \u4e3a\u73b0\u6709EMHI\u6570\u636e\u96c6\u6dfb\u52a0\u53ef\u89c1\u6027\u6807\u6ce8\uff1b3) \u63d0\u51faEvaPose\u65b9\u6cd5\uff0c\u663e\u5f0f\u5730\u6574\u5408\u53ef\u89c1\u6027\u4fe1\u606f\u6765\u63d0\u5347\u59ff\u6001\u4f30\u8ba1\u7cbe\u5ea6\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5730\u9762\u771f\u5b9e\u53ef\u89c1\u6027\u6807\u7b7e\u5728\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u59ff\u6001\u4f30\u8ba1\u4e2d\u7684\u91cd\u8981\u6027\uff0cEvaPose\u65b9\u6cd5\u5728Eva-3M\u548cEMHI\u6570\u636e\u96c6\u4e0a\u90fd\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u5173\u952e\u70b9\u53ef\u89c1\u6027\u6807\u6ce8\u5bf9\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u81f3\u5173\u91cd\u8981\uff0c\u63d0\u51fa\u7684Eva-3M\u6570\u636e\u96c6\u548cEvaPose\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u4e0d\u53ef\u89c1\u6027\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u59ff\u6001\u4f30\u8ba1\u7cbe\u5ea6\u3002"}}
{"id": "2602.23826", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23826", "abs": "https://arxiv.org/abs/2602.23826", "authors": ["Sebastian Gerstner", "Hinrich Sch\u00fctze"], "title": "GLUScope: A Tool for Analyzing GLU Neurons in Transformer Language Models", "comment": "6 pages for main body, 9 pages in total. 4 figures", "summary": "We present GLUScope, an open-source tool for analyzing neurons in Transformer-based language models, intended for interpretability researchers. We focus on more recent models than previous tools do; specifically we consider gated activation functions such as SwiGLU. This introduces a new challenge: understanding positive activations is not enough. Instead, both the gate and the in activation of a neuron can be positive or negative, leading to four different possible sign combinations that in some cases have quite different functionalities. Accordingly, for any neuron, our tool shows text examples for each of the four sign combinations, and indicates how often each combination occurs. We describe examples of how our tool can lead to novel insights. A demo is available at https: //sjgerstner.github.io/gluscope.", "AI": {"tldr": "GLUScope\u662f\u4e00\u4e2a\u5f00\u6e90\u5de5\u5177\uff0c\u7528\u4e8e\u5206\u6790\u57fa\u4e8eTransformer\u7684\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u795e\u7ecf\u5143\uff0c\u7279\u522b\u9488\u5bf9\u4f7f\u7528SwiGLU\u7b49\u95e8\u63a7\u6fc0\u6d3b\u51fd\u6570\u7684\u8f83\u65b0\u6a21\u578b\uff0c\u901a\u8fc7\u5c55\u793a\u56db\u79cd\u4e0d\u540c\u7b26\u53f7\u7ec4\u5408\u7684\u6587\u672c\u793a\u4f8b\u6765\u5e2e\u52a9\u7406\u89e3\u795e\u7ecf\u5143\u529f\u80fd\u3002", "motivation": "\u73b0\u6709\u5de5\u5177\u4e3b\u8981\u9488\u5bf9\u65e9\u671fTransformer\u6a21\u578b\uff0c\u800c\u8f83\u65b0\u6a21\u578b\u4f7f\u7528\u95e8\u63a7\u6fc0\u6d3b\u51fd\u6570\uff08\u5982SwiGLU\uff09\u5e26\u6765\u4e86\u65b0\u7684\u5206\u6790\u6311\u6218\uff1a\u4ec5\u7406\u89e3\u6b63\u6fc0\u6d3b\u4e0d\u591f\uff0c\u9700\u8981\u540c\u65f6\u8003\u8651\u95e8\u6fc0\u6d3b\u548c\u8f93\u5165\u6fc0\u6d3b\u7684\u6b63\u8d1f\u7b26\u53f7\u7ec4\u5408\u3002", "method": "\u5f00\u53d1GLUScope\u5de5\u5177\uff0c\u9488\u5bf9\u6bcf\u4e2a\u795e\u7ecf\u5143\u5c55\u793a\u56db\u79cd\u7b26\u53f7\u7ec4\u5408\uff08\u95e8\u6b63/\u8d1f \u00d7 \u8f93\u5165\u6b63/\u8d1f\uff09\u7684\u6587\u672c\u793a\u4f8b\uff0c\u5e76\u7edf\u8ba1\u6bcf\u79cd\u7ec4\u5408\u7684\u51fa\u73b0\u9891\u7387\uff0c\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u7406\u89e3\u795e\u7ecf\u5143\u7684\u4e0d\u540c\u529f\u80fd\u6a21\u5f0f\u3002", "result": "GLUScope\u80fd\u591f\u6709\u6548\u5206\u6790\u4f7f\u7528\u95e8\u63a7\u6fc0\u6d3b\u51fd\u6570\u7684Transformer\u6a21\u578b\u795e\u7ecf\u5143\uff0c\u901a\u8fc7\u5c55\u793a\u56db\u79cd\u7b26\u53f7\u7ec4\u5408\u7684\u793a\u4f8b\uff0c\u63ed\u793a\u4e86\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u8fd9\u4e9b\u7ec4\u5408\u5177\u6709\u76f8\u5f53\u4e0d\u540c\u7684\u529f\u80fd\u7279\u6027\u3002", "conclusion": "GLUScope\u4e3a\u53ef\u89e3\u91ca\u6027\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u5206\u6790\u73b0\u4ee3Transformer\u6a21\u578b\u795e\u7ecf\u5143\u7684\u65b0\u5de5\u5177\uff0c\u7279\u522b\u9002\u7528\u4e8e\u7406\u89e3\u95e8\u63a7\u6fc0\u6d3b\u51fd\u6570\u5e26\u6765\u7684\u590d\u6742\u884c\u4e3a\uff0c\u80fd\u591f\u4ea7\u751f\u65b0\u7684\u7814\u7a76\u89c1\u89e3\u3002"}}
{"id": "2602.23694", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23694", "abs": "https://arxiv.org/abs/2602.23694", "authors": ["Seungyeol Baek", "Jaspreet Singh", "Lala Shakti Swarup Ray", "Hymalai Bello", "Paul Lukowicz", "Sungho Suh"], "title": "Interpretable Multimodal Gesture Recognition for Drone and Mobile Robot Teleoperation via Log-Likelihood Ratio Fusion", "comment": null, "summary": "Human operators are still frequently exposed to hazardous environments such as disaster zones and industrial facilities, where intuitive and reliable teleoperation of mobile robots and Unmanned Aerial Vehicles (UAVs) is essential. In this context, hands-free teleoperation enhances operator mobility and situational awareness, thereby improving safety in hazardous environments. While vision-based gesture recognition has been explored as one method for hands-free teleoperation, its performance often deteriorates under occlusions, lighting variations, and cluttered backgrounds, limiting its applicability in real-world operations. To overcome these limitations, we propose a multimodal gesture recognition framework that integrates inertial data (accelerometer, gyroscope, and orientation) from Apple Watches on both wrists with capacitive sensing signals from custom gloves. We design a late fusion strategy based on the log-likelihood ratio (LLR), which not only enhances recognition performance but also provides interpretability by quantifying modality-specific contributions. To support this research, we introduce a new dataset of 20 distinct gestures inspired by aircraft marshalling signals, comprising synchronized RGB video, IMU, and capacitive sensor data. Experimental results demonstrate that our framework achieves performance comparable to a state-of-the-art vision-based baseline while significantly reducing computational cost, model size, and training time, making it well suited for real-time robot control. We therefore underscore the potential of sensor-based multimodal fusion as a robust and interpretable solution for gesture-driven mobile robot and drone teleoperation.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eApple Watch\u60ef\u6027\u6570\u636e\u548c\u5b9a\u5236\u624b\u5957\u7535\u5bb9\u4f20\u611f\u7684\u591a\u6a21\u6001\u624b\u52bf\u8bc6\u522b\u6846\u67b6\uff0c\u7528\u4e8e\u65e0\u4eba\u673a\u548c\u79fb\u52a8\u673a\u5668\u4eba\u7684\u514d\u624b\u6301\u9065\u64cd\u4f5c\uff0c\u6027\u80fd\u5ab2\u7f8e\u89c6\u89c9\u57fa\u7ebf\u4f46\u8ba1\u7b97\u6210\u672c\u66f4\u4f4e\u3002", "motivation": "\u64cd\u4f5c\u4eba\u5458\u5728\u5371\u9669\u73af\u5883\u4e2d\u9700\u8981\u514d\u624b\u6301\u9065\u64cd\u4f5c\u79fb\u52a8\u673a\u5668\u4eba\u548c\u65e0\u4eba\u673a\uff0c\u73b0\u6709\u89c6\u89c9\u624b\u52bf\u8bc6\u522b\u5728\u906e\u6321\u3001\u5149\u7167\u53d8\u5316\u548c\u6742\u4e71\u80cc\u666f\u4e2d\u6027\u80fd\u4e0b\u964d\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u591a\u6a21\u6001\u624b\u52bf\u8bc6\u522b\u6846\u67b6\uff0c\u96c6\u6210\u53cc\u8155Apple Watch\u7684\u60ef\u6027\u6570\u636e\uff08\u52a0\u901f\u5ea6\u8ba1\u3001\u9640\u87ba\u4eea\u3001\u65b9\u5411\uff09\u548c\u5b9a\u5236\u624b\u5957\u7684\u7535\u5bb9\u4f20\u611f\u4fe1\u53f7\uff0c\u91c7\u7528\u57fa\u4e8e\u5bf9\u6570\u4f3c\u7136\u6bd4\uff08LLR\uff09\u7684\u540e\u671f\u878d\u5408\u7b56\u7565\u3002", "result": "\u6846\u67b6\u6027\u80fd\u4e0e\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u57fa\u7ebf\u76f8\u5f53\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3001\u6a21\u578b\u5927\u5c0f\u548c\u8bad\u7ec3\u65f6\u95f4\uff0c\u9002\u5408\u5b9e\u65f6\u673a\u5668\u4eba\u63a7\u5236\u3002", "conclusion": "\u4f20\u611f\u5668\u591a\u6a21\u6001\u878d\u5408\u4e3a\u624b\u52bf\u9a71\u52a8\u7684\u79fb\u52a8\u673a\u5668\u4eba\u548c\u65e0\u4eba\u673a\u9065\u64cd\u4f5c\u63d0\u4f9b\u4e86\u9c81\u68d2\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.23622", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23622", "abs": "https://arxiv.org/abs/2602.23622", "authors": ["Shibo Hong", "Boxian Ai", "Jun Kuang", "Wei Wang", "FengJiao Chen", "Zhongyuan Peng", "Chenhao Huang", "Yixin Cao"], "title": "DLEBench: Evaluating Small-scale Object Editing Ability for Instruction-based Image Editing Model", "comment": null, "summary": "Significant progress has been made in the field of Instruction-based Image Editing Models (IIEMs). However, while these models demonstrate plausible adherence to instructions and strong reasoning ability on current benchmarks, their ability to edit small objects remains underexplored, despite its importance for precise local editing and refining details in both real and generated images. In this paper, we introduce DeepLookEditBench (DLEBench), the first benchmark dedicated to assessing the abilities of IIEMs in editing small-scale objects. Specifically, we construct a challenging testbed comprising 1889 samples across seven instruction types. In these samples, target objects occupy only 1%-10% of the image area, covering complex scenarios such as partial occlusion and multi-object editing. To ensure robust evaluation on this benchmark, we propose an evaluation protocol with refined score rubrics to minimize subjectivity and ambiguity in two criteria: Instruction Following and Visual Consistency. This protocol also introduces a dual-mode evaluation framework (Tool-driven and Oracle-guided Modes) addressing the misalignment between LMM-as-a-Judge and human judgements on DLEBench. Empirical results on 10 IIEMs reveal significant performance gaps in small-scale object editing, highlighting the need for specialized benchmarks to advance this ability.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u4e13\u95e8\u8bc4\u4f30\u6307\u4ee4\u5f0f\u56fe\u50cf\u7f16\u8f91\u6a21\u578b\u5728\u5c0f\u7269\u4f53\u7f16\u8f91\u80fd\u529b\u4e0a\u7684\u57fa\u51c6\u6d4b\u8bd5DLEBench\uff0c\u5305\u542b1889\u4e2a\u6837\u672c\u548c7\u79cd\u6307\u4ee4\u7c7b\u578b\uff0c\u9488\u5bf9\u56fe\u50cf\u9762\u79ef\u4ec5\u53601%-10%\u7684\u5c0f\u7269\u4f53\u7f16\u8f91\u573a\u666f\u3002", "motivation": "\u5f53\u524d\u6307\u4ee4\u5f0f\u56fe\u50cf\u7f16\u8f91\u6a21\u578b\u5728\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5c0f\u7269\u4f53\u7f16\u8f91\u80fd\u529b\u65b9\u9762\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\uff0c\u800c\u8fd9\u4e00\u80fd\u529b\u5bf9\u4e8e\u7cbe\u786e\u5c40\u90e8\u7f16\u8f91\u548c\u7ec6\u8282\u4f18\u5316\u81f3\u5173\u91cd\u8981\u3002", "method": "\u6784\u5efa\u4e86DeepLookEditBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b1889\u4e2a\u6837\u672c\u8986\u76d67\u79cd\u6307\u4ee4\u7c7b\u578b\uff0c\u76ee\u6807\u7269\u4f53\u4ec5\u5360\u56fe\u50cf\u9762\u79ef\u76841%-10%\uff0c\u6db5\u76d6\u90e8\u5206\u906e\u6321\u548c\u591a\u7269\u4f53\u7f16\u8f91\u7b49\u590d\u6742\u573a\u666f\u3002\u63d0\u51fa\u4e86\u5305\u542b\u7cbe\u5316\u8bc4\u5206\u6807\u51c6\u7684\u8bc4\u4f30\u534f\u8bae\uff0c\u5e76\u8bbe\u8ba1\u4e86\u53cc\u6a21\u5f0f\u8bc4\u4f30\u6846\u67b6\uff08\u5de5\u5177\u9a71\u52a8\u6a21\u5f0f\u548cOracle\u5f15\u5bfc\u6a21\u5f0f\uff09\u6765\u89e3\u51b3LMM-as-a-Judge\u4e0e\u4eba\u7c7b\u5224\u65ad\u4e4b\u95f4\u7684\u4e0d\u4e00\u81f4\u95ee\u9898\u3002", "result": "\u572810\u4e2a\u6307\u4ee4\u5f0f\u56fe\u50cf\u7f16\u8f91\u6a21\u578b\u4e0a\u7684\u5b9e\u8bc1\u7ed3\u679c\u663e\u793a\uff0c\u5728\u5c0f\u7269\u4f53\u7f16\u8f91\u65b9\u9762\u5b58\u5728\u663e\u8457\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u7a81\u663e\u4e86\u4e13\u95e8\u57fa\u51c6\u6d4b\u8bd5\u7684\u5fc5\u8981\u6027\u3002", "conclusion": "\u9700\u8981\u4e13\u95e8\u7684\u57fa\u51c6\u6d4b\u8bd5\u6765\u63a8\u52a8\u6307\u4ee4\u5f0f\u56fe\u50cf\u7f16\u8f91\u6a21\u578b\u5728\u5c0f\u7269\u4f53\u7f16\u8f91\u80fd\u529b\u65b9\u9762\u7684\u53d1\u5c55\uff0cDLEBench\u4e3a\u6b64\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8bc4\u4f30\u5de5\u5177\u3002"}}
{"id": "2602.23565", "categories": ["cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.23565", "abs": "https://arxiv.org/abs/2602.23565", "authors": ["Adhyyan Narang", "Sarah Dean", "Lillian J Ratliff", "Maryam Fazel"], "title": "Dynamics of Learning under User Choice: Overspecialization and Peer-Model Probing", "comment": null, "summary": "In many economically relevant contexts where machine learning is deployed, multiple platforms obtain data from the same pool of users, each of whom selects the platform that best serves them. Prior work in this setting focuses exclusively on the \"local\" losses of learners on the distribution of data that they observe. We find that there exist instances where learners who use existing algorithms almost surely converge to models with arbitrarily poor global performance, even when models with low full-population loss exist. This happens through a feedback-induced mechanism, which we call the overspecialization trap: as learners optimize for users who already prefer them, they become less attractive to users outside this base, which further restricts the data they observe. Inspired by the recent use of knowledge distillation in modern ML, we propose an algorithm that allows learners to \"probe\" the predictions of peer models, enabling them to learn about users who do not select them. Our analysis characterizes when probing succeeds: this procedure converges almost surely to a stationary point with bounded full-population risk when probing sources are sufficiently informative, e.g., a known market leader or a majority of peers with good global performance. We verify our findings with semi-synthetic experiments on the MovieLens, Census, and Amazon Sentiment datasets.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u591a\u5e73\u53f0\u673a\u5668\u5b66\u4e60\u4e2d\u7684\"\u8fc7\u5ea6\u4e13\u4e1a\u5316\u9677\u9631\"\u95ee\u9898\uff0c\u63d0\u51fa\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u6280\u672f\u8ba9\u5e73\u53f0\"\u63a2\u6d4b\"\u540c\u884c\u6a21\u578b\u9884\u6d4b\u6765\u89e3\u51b3\u6570\u636e\u9009\u62e9\u504f\u5dee\uff0c\u786e\u4fdd\u6536\u655b\u5230\u5168\u5c40\u6027\u80fd\u826f\u597d\u7684\u6a21\u578b\u3002", "motivation": "\u5728\u591a\u5e73\u53f0\u673a\u5668\u5b66\u4e60\u90e8\u7f72\u573a\u666f\u4e2d\uff0c\u7528\u6237\u4f1a\u9009\u62e9\u6700\u9002\u5408\u81ea\u5df1\u7684\u5e73\u53f0\uff0c\u5bfc\u81f4\u6bcf\u4e2a\u5e73\u53f0\u53ea\u80fd\u89c2\u5bdf\u5230\u9009\u62e9\u81ea\u5df1\u7684\u7528\u6237\u6570\u636e\u3002\u73b0\u6709\u7b97\u6cd5\u53ea\u5173\u6ce8\u5c40\u90e8\u635f\u5931\uff0c\u53ef\u80fd\u5bfc\u81f4\u6a21\u578b\u5168\u5c40\u6027\u80fd\u6781\u5dee\uff0c\u5373\u4f7f\u5b58\u5728\u4f4e\u5168\u4eba\u53e3\u635f\u5931\u7684\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u77e5\u8bc6\u84b8\u998f\u7684\u7b97\u6cd5\uff0c\u5141\u8bb8\u5b66\u4e60\u8005\"\u63a2\u6d4b\"\u540c\u884c\u6a21\u578b\u7684\u9884\u6d4b\uff0c\u4ece\u800c\u4e86\u89e3\u4e0d\u9009\u62e9\u81ea\u5df1\u7684\u7528\u6237\u4fe1\u606f\u3002\u5f53\u63a2\u6d4b\u6e90\u8db3\u591f\u4fe1\u606f\u4e30\u5bcc\u65f6\uff08\u5982\u5df2\u77e5\u7684\u5e02\u573a\u9886\u5bfc\u8005\u6216\u5927\u591a\u6570\u5177\u6709\u826f\u597d\u5168\u5c40\u6027\u80fd\u7684\u540c\u884c\uff09\uff0c\u8be5\u7b97\u6cd5\u80fd\u6536\u655b\u5230\u5177\u6709\u6709\u754c\u5168\u4eba\u53e3\u98ce\u9669\u7684\u7a33\u5b9a\u70b9\u3002", "result": "\u5206\u6790\u8868\u660e\uff0c\u5f53\u63a2\u6d4b\u6e90\u8db3\u591f\u4fe1\u606f\u4e30\u5bcc\u65f6\uff0c\u63d0\u51fa\u7684\u7b97\u6cd5\u51e0\u4e4e\u5fc5\u7136\u6536\u655b\u5230\u5177\u6709\u6709\u754c\u5168\u4eba\u53e3\u98ce\u9669\u7684\u7a33\u5b9a\u70b9\u3002\u5728MovieLens\u3001Census\u548cAmazon Sentiment\u6570\u636e\u96c6\u4e0a\u7684\u534a\u5408\u6210\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8fd9\u4e00\u53d1\u73b0\u3002", "conclusion": "\u5728\u591a\u5e73\u53f0\u673a\u5668\u5b66\u4e60\u73af\u5883\u4e2d\uff0c\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u8ba9\u5e73\u53f0\u63a2\u6d4b\u540c\u884c\u6a21\u578b\u9884\u6d4b\u53ef\u4ee5\u6709\u6548\u907f\u514d\"\u8fc7\u5ea6\u4e13\u4e1a\u5316\u9677\u9631\"\uff0c\u786e\u4fdd\u6a21\u578b\u5177\u6709\u826f\u597d\u7684\u5168\u5c40\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u63a2\u6d4b\u6e90\u4fe1\u606f\u4e30\u5bcc\u7684\u60c5\u51b5\u4e0b\u3002"}}
{"id": "2602.23730", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23730", "abs": "https://arxiv.org/abs/2602.23730", "authors": ["Longyin Zhang", "Shuo Sun", "Yingxu He", "Won Cheng Yi Lewis", "Muhammad Huzaifah Bin Md Shahrin", "Hardik Bhupendra Sailor", "Heng Meng Jeremy Wong", "Tarun Kumar Vangani", "Yi Ma", "Qiongqiong Wang", "Minh Duc Pham", "Ridong Jiang", "Jingtao Li", "Jingyi Liao", "Zhuohan Liu", "Yanfeng Lu", "Manas Gupta", "Ai Ti Aw"], "title": "Unlocking Cognitive Capabilities and Analyzing the Perception-Logic Trade-off", "comment": null, "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) pursue omni-perception capabilities, yet integrating robust sensory grounding with complex reasoning remains a challenge, particularly for underrepresented regions. In this report, we introduce the research preview of MERaLiON2-Omni (Alpha), a 10B-parameter multilingual omni-perception tailored for Southeast Asia (SEA). We present a progressive training pipeline that explicitly decouples and then integrates \"System 1\" (Perception) and \"System 2\" (Reasoning) capabilities. First, we establish a robust Perception Backbone by aligning region-specific audio-visual cues (e.g., Singlish code-switching, local cultural landmarks) with a multilingual LLM through orthogonal modality adaptation. Second, to inject cognitive capabilities without large-scale supervision, we propose a cost-effective Generate-Judge-Refine pipeline. By utilizing a Super-LLM to filter hallucinations and resolve conflicts via a consensus mechanism, we synthesize high-quality silver data that transfers textual Chain-of-Thought reasoning to multimodal scenarios.\n  Comprehensive evaluation on our newly introduced SEA-Omni Benchmark Suite reveals an Efficiency-Stability Paradox: while reasoning acts as a non-linear amplifier for abstract tasks (boosting mathematical and instruction-following performance significantly), it introduces instability in low-level sensory processing. Specifically, we identify Temporal Drift in long-context audio, where extended reasoning desynchronizes the model from acoustic timestamps, and Visual Over-interpretation, where logic overrides pixel-level reality. This report details the architecture, the data-efficient training recipe, and a diagnostic analysis of the trade-offs between robust perception and structured reasoning.", "AI": {"tldr": "MERaLiON2-Omni (Alpha)\u662f\u4e00\u4e2a\u9488\u5bf9\u4e1c\u5357\u4e9a\u5730\u533a\u768410B\u53c2\u6570\u591a\u8bed\u8a00\u5168\u611f\u77e5MLLM\uff0c\u901a\u8fc7\u5206\u79bb\u548c\u6574\u5408\"\u7cfb\u7edf1\"\uff08\u611f\u77e5\uff09\u4e0e\"\u7cfb\u7edf2\"\uff08\u63a8\u7406\uff09\u80fd\u529b\uff0c\u5728\u533a\u57df\u7279\u5b9a\u97f3\u9891\u89c6\u89c9\u7ebf\u7d22\u5bf9\u9f50\u548c\u4f4e\u6210\u672c\u6570\u636e\u751f\u6210\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u53d1\u73b0\u63a8\u7406\u80fd\u529b\u5728\u63d0\u5347\u62bd\u8c61\u4efb\u52a1\u8868\u73b0\u7684\u540c\u65f6\u4f1a\u964d\u4f4e\u4f4e\u7ea7\u611f\u5b98\u5904\u7406\u7684\u7a33\u5b9a\u6027\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u8ffd\u6c42\u5168\u611f\u77e5\u80fd\u529b\uff0c\u4f46\u5c06\u7a33\u5065\u7684\u611f\u5b98\u57fa\u7840\u4e0e\u590d\u6742\u63a8\u7406\u76f8\u7ed3\u5408\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff0c\u7279\u522b\u662f\u5728\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u5730\u533a\u5982\u4e1c\u5357\u4e9a\u3002\u9700\u8981\u89e3\u51b3\u533a\u57df\u7279\u5b9a\u611f\u77e5\u4e0e\u9ad8\u7ea7\u63a8\u7406\u80fd\u529b\u7684\u6574\u5408\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u6d41\u7a0b\uff1a1\uff09\u901a\u8fc7\u6b63\u4ea4\u6a21\u6001\u9002\u5e94\u5efa\u7acb\u7a33\u5065\u7684\u611f\u77e5\u9aa8\u5e72\uff0c\u5c06\u533a\u57df\u7279\u5b9a\u97f3\u9891\u89c6\u89c9\u7ebf\u7d22\u4e0e\u591a\u8bed\u8a00LLM\u5bf9\u9f50\uff1b2\uff09\u63d0\u51fa\u4f4e\u6210\u672cGenerate-Judge-Refine\u6d41\u7a0b\uff0c\u5229\u7528Super-LLM\u8fc7\u6ee4\u5e7b\u89c9\u5e76\u901a\u8fc7\u5171\u8bc6\u673a\u5236\u89e3\u51b3\u51b2\u7a81\uff0c\u5408\u6210\u9ad8\u8d28\u91cf\u94f6\u6570\u636e\uff0c\u5c06\u6587\u672c\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u8f6c\u79fb\u5230\u591a\u6a21\u6001\u573a\u666f\u3002", "result": "\u5728\u65b0\u5f15\u5165\u7684SEA-Omni\u57fa\u51c6\u5957\u4ef6\u8bc4\u4f30\u4e2d\u53d1\u73b0\u6548\u7387-\u7a33\u5b9a\u6027\u6096\u8bba\uff1a\u63a8\u7406\u80fd\u529b\u4f5c\u4e3a\u62bd\u8c61\u4efb\u52a1\u7684\u975e\u7ebf\u6027\u653e\u5927\u5668\uff08\u663e\u8457\u63d0\u5347\u6570\u5b66\u548c\u6307\u4ee4\u8ddf\u968f\u6027\u80fd\uff09\uff0c\u4f46\u5728\u4f4e\u7ea7\u611f\u5b98\u5904\u7406\u4e2d\u5f15\u5165\u4e0d\u7a33\u5b9a\u6027\uff0c\u5177\u4f53\u8868\u73b0\u4e3a\u957f\u4e0a\u4e0b\u6587\u97f3\u9891\u4e2d\u7684\u65f6\u95f4\u6f02\u79fb\u548c\u89c6\u89c9\u8fc7\u5ea6\u89e3\u91ca\u95ee\u9898\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u9488\u5bf9\u4e1c\u5357\u4e9a\u5730\u533a\u7684\u5168\u611f\u77e5MLLM\u67b6\u6784\u3001\u6570\u636e\u9ad8\u6548\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5e76\u8bca\u65ad\u5206\u6790\u4e86\u7a33\u5065\u611f\u77e5\u4e0e\u7ed3\u6784\u5316\u63a8\u7406\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\uff0c\u4e3a\u533a\u57df\u7279\u5b9a\u591a\u6a21\u6001\u6a21\u578b\u5f00\u53d1\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2602.23845", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.23845", "abs": "https://arxiv.org/abs/2602.23845", "authors": ["Jian Kai", "Zidong Zhang", "Jiwen Chen", "Zhengxiang Wu", "Songtao Sun", "Fuyang Li", "Yang Cao", "Qiang Liu"], "title": "CLFEC: A New Task for Unified Linguistic and Factual Error Correction in paragraph-level Chinese Professional Writing", "comment": null, "summary": "Chinese text correction has traditionally focused on spelling and grammar, while factual error correction is usually treated separately. However, in paragraph-level Chinese professional writing, linguistic (word/grammar/punctuation) and factual errors frequently co-occur and interact, making unified correction both necessary and challenging. This paper introduces CLFEC (Chinese Linguistic & Factual Error Correction), a new task for joint linguistic and factual correction. We construct a mixed, multi-domain Chinese professional writing dataset spanning current affairs, finance, law, and medicine. We then conduct a systematic study of LLM-based correction paradigms, from prompting to retrieval-augmented generation (RAG) and agentic workflows. The analysis reveals practical challenges, including limited generalization of specialized correction models, the need for evidence grounding for factual repair, the difficulty of mixed-error paragraphs, and over-correction on clean inputs. Results further show that handling linguistic and factual Error within the same context outperform decoupled processes, and that agentic workflows can be effective with suitable backbone models. Overall, our dataset and empirical findings provide guidance for building reliable, fully automatic proofreading systems in industrial settings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCLFEC\u4efb\u52a1\uff0c\u65e8\u5728\u8054\u5408\u7ea0\u6b63\u4e2d\u6587\u4e13\u4e1a\u5199\u4f5c\u4e2d\u7684\u8bed\u8a00\u9519\u8bef\u548c\u4e8b\u5b9e\u9519\u8bef\uff0c\u6784\u5efa\u4e86\u591a\u9886\u57df\u6570\u636e\u96c6\u5e76\u7cfb\u7edf\u7814\u7a76\u4e86\u57fa\u4e8eLLM\u7684\u7ea0\u6b63\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u4e2d\u6587\u7ea0\u9519\u4e3b\u8981\u5173\u6ce8\u62fc\u5199\u548c\u8bed\u6cd5\uff0c\u800c\u4e8b\u5b9e\u9519\u8bef\u7ea0\u6b63\u901a\u5e38\u5355\u72ec\u5904\u7406\u3002\u4f46\u5728\u6bb5\u843d\u7ea7\u4e2d\u6587\u4e13\u4e1a\u5199\u4f5c\u4e2d\uff0c\u8bed\u8a00\u9519\u8bef\uff08\u8bcd\u6c47/\u8bed\u6cd5/\u6807\u70b9\uff09\u548c\u4e8b\u5b9e\u9519\u8bef\u7ecf\u5e38\u540c\u65f6\u51fa\u73b0\u4e14\u76f8\u4e92\u5f71\u54cd\uff0c\u4f7f\u5f97\u7edf\u4e00\u7ea0\u6b63\u65e2\u5fc5\u8981\u53c8\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u6784\u5efa\u4e86\u6db5\u76d6\u65f6\u4e8b\u3001\u91d1\u878d\u3001\u6cd5\u5f8b\u548c\u533b\u5b66\u7684\u591a\u9886\u57df\u4e2d\u6587\u4e13\u4e1a\u5199\u4f5c\u6570\u636e\u96c6\uff1b\u7cfb\u7edf\u7814\u7a76\u4e86\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7ea0\u6b63\u8303\u5f0f\uff0c\u5305\u62ec\u63d0\u793a\u5de5\u7a0b\u3001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u548c\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a\u4e13\u4e1a\u7ea0\u6b63\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff1b\u4e8b\u5b9e\u4fee\u590d\u9700\u8981\u8bc1\u636e\u652f\u6301\uff1b\u6df7\u5408\u9519\u8bef\u6bb5\u843d\u5904\u7406\u56f0\u96be\uff1b\u5728\u5e72\u51c0\u8f93\u5165\u4e0a\u5b58\u5728\u8fc7\u5ea6\u7ea0\u6b63\u95ee\u9898\u3002\u5728\u540c\u4e00\u4e0a\u4e0b\u6587\u4e2d\u5904\u7406\u8bed\u8a00\u548c\u4e8b\u5b9e\u9519\u8bef\u4f18\u4e8e\u89e3\u8026\u5904\u7406\uff0c\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u5728\u5408\u9002\u7684\u9aa8\u5e72\u6a21\u578b\u4e0b\u8868\u73b0\u6709\u6548\u3002", "conclusion": "\u672c\u6587\u7684\u6570\u636e\u96c6\u548c\u5b9e\u8bc1\u7ed3\u679c\u4e3a\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\u6784\u5efa\u53ef\u9760\u3001\u5168\u81ea\u52a8\u7684\u6821\u5bf9\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002\u8054\u5408\u5904\u7406\u8bed\u8a00\u548c\u4e8b\u5b9e\u9519\u8bef\u662f\u53ef\u884c\u7684\uff0c\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u5728\u9002\u5f53\u6a21\u578b\u652f\u6301\u4e0b\u5177\u6709\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2602.23706", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23706", "abs": "https://arxiv.org/abs/2602.23706", "authors": ["Vijay U. Rathod", "Manav S. Sharma", "Shambhavi Verma", "Aadi Joshi", "Sachin Aage", "Sujal Shahane"], "title": "A Reliable Indoor Navigation System for Humans Using AR-based Technique", "comment": "6 pages, 6 figures, 2 tables, Presented at 7th International Conference on Advances in Science and Technology (ICAST 2024-25)", "summary": "Reliable navigation systems are not available indoors, such as in campuses and small areas. Users must depend on confusing, time-consuming static signage or floor maps. In this paper, an AR-based technique has been applied to campus and small-site navigation, where Vuforia Area Target is used for environment modeling. AI navigation's NavMesh component is used for navigation purposes, and the A* algorithm is used within this component for shortest path calculation. Compared to Dijkstra's algorithm, it can reach a solution about two to three times faster for smaller search spaces. In many cases, Dijkstra's algorithm has difficulty performing well in high-complexity environments where memory usage grows and processing times increase. Compared to older approaches such as GPS, real-time processing and AR overlays can be combined to provide intuitive directions for users while dynamically updating the path in response to environmental changes. Experimental results indicate significantly improved navigation accuracy, better user experience, and greater efficiency compared to traditional methods. These results show that AR technology integrated with existing pathfinding algorithms is feasible and scalable, making it a user-friendly solution for indoor navigation. Although highly effective in limited and defined indoor spaces, further optimization of NavMesh is required for large or highly dynamic environments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eAR\u7684\u5ba4\u5185\u5bfc\u822a\u7cfb\u7edf\uff0c\u7ed3\u5408Vuforia\u73af\u5883\u5efa\u6a21\u3001AI\u5bfc\u822a\u7684NavMesh\u7ec4\u4ef6\u548cA*\u7b97\u6cd5\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5bfc\u822a\u7cbe\u5ea6\u548c\u7528\u6237\u4f53\u9a8c\u3002", "motivation": "\u5ba4\u5185\u73af\u5883\u5982\u6821\u56ed\u548c\u5c0f\u578b\u533a\u57df\u7f3a\u4e4f\u53ef\u9760\u7684\u5bfc\u822a\u7cfb\u7edf\uff0c\u7528\u6237\u53ea\u80fd\u4f9d\u8d56\u6df7\u4e71\u3001\u8017\u65f6\u7684\u9759\u6001\u6807\u8bc6\u6216\u697c\u5c42\u5730\u56fe\uff0c\u9700\u8981\u66f4\u76f4\u89c2\u3001\u9ad8\u6548\u7684\u5bfc\u822a\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528Vuforia Area Target\u8fdb\u884c\u73af\u5883\u5efa\u6a21\uff0c\u91c7\u7528AI\u5bfc\u822a\u7684NavMesh\u7ec4\u4ef6\u8fdb\u884c\u5bfc\u822a\u89c4\u5212\uff0c\u5728\u7ec4\u4ef6\u5185\u90e8\u4f7f\u7528A*\u7b97\u6cd5\u8ba1\u7b97\u6700\u77ed\u8def\u5f84\u3002\u76f8\u6bd4Dijkstra\u7b97\u6cd5\uff0cA*\u5728\u8f83\u5c0f\u641c\u7d22\u7a7a\u95f4\u4e2d\u5feb2-3\u500d\uff0c\u66f4\u9002\u5408\u9ad8\u590d\u6742\u5ea6\u73af\u5883\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5bfc\u822a\u7cbe\u5ea6\u663e\u8457\u63d0\u5347\uff0c\u7528\u6237\u4f53\u9a8c\u66f4\u597d\uff0c\u6548\u7387\u9ad8\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002AR\u6280\u672f\u4e0e\u73b0\u6709\u8def\u5f84\u89c4\u5212\u7b97\u6cd5\u7ed3\u5408\u53ef\u884c\u4e14\u53ef\u6269\u5c55\uff0c\u4e3a\u7528\u6237\u63d0\u4f9b\u4e86\u53cb\u597d\u7684\u5ba4\u5185\u5bfc\u822a\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "AR\u6280\u672f\u4e0e\u8def\u5f84\u89c4\u5212\u7b97\u6cd5\u96c6\u6210\u5728\u6709\u9650\u5b9a\u4e49\u7684\u5ba4\u5185\u7a7a\u95f4\u4e2d\u975e\u5e38\u6709\u6548\uff0c\u4f46NavMesh\u5728\u5927\u578b\u6216\u9ad8\u5ea6\u52a8\u6001\u73af\u5883\u4e2d\u9700\u8981\u8fdb\u4e00\u6b65\u4f18\u5316\u3002\u8be5\u7cfb\u7edf\u4e3a\u5ba4\u5185\u5bfc\u822a\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u7528\u6237\u53cb\u597d\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.23645", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23645", "abs": "https://arxiv.org/abs/2602.23645", "authors": ["Tongyan Hua", "Haoran Gong", "Yuan Liu", "Di Wang", "Ying-Cong Chen", "Wufan Zhao"], "title": "BuildAnyPoint: 3D Building Structured Abstraction from Diverse Point Clouds", "comment": "CVPR 2026", "summary": "We introduce BuildAnyPoint, a novel generative framework for structured 3D building reconstruction from point clouds with diverse distributions, such as those captured by airborne LiDAR and Structure-from-Motion. To recover artist-created building abstraction in this highly underconstrained setting, we capitalize on the role of explicit 3D generative priors in autoregressive mesh generation. Specifically, we design a Loosely Cascaded Diffusion Transformer (Loca-DiT) that initially recovers the underlying distribution from noisy or sparse points, followed by autoregressively encapsulating them into compact meshes. We first formulate distribution recovery as a conditional generation task by training latent diffusion models conditioned on input point clouds, and then tailor a decoder-only transformer for conditional autoregressive mesh generation based on the recovered point clouds. Our method delivers substantial qualitative and quantitative improvements over prior building abstraction methods. Furthermore, the effectiveness of our approach is evidenced by the strong performance of its recovered point clouds on building point cloud completion benchmarks, which exhibit improved surface accuracy and distribution uniformity.", "AI": {"tldr": "BuildAnyPoint\u662f\u4e00\u4e2a\u4ece\u70b9\u4e91\u8fdb\u884c\u7ed3\u6784\u53163D\u5efa\u7b51\u91cd\u5efa\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u677e\u6563\u7ea7\u8054\u6269\u6563\u53d8\u6362\u5668(Loca-DiT)\u4ece\u566a\u58f0\u6216\u7a00\u758f\u70b9\u4e91\u4e2d\u6062\u590d\u5e95\u5c42\u5206\u5e03\uff0c\u7136\u540e\u81ea\u56de\u5f52\u5730\u5c06\u5176\u5c01\u88c5\u4e3a\u7d27\u51d1\u7f51\u683c\u3002", "motivation": "\u4ece\u5177\u6709\u4e0d\u540c\u5206\u5e03\u7684\u70b9\u4e91\uff08\u5982\u673a\u8f7dLiDAR\u548c\u8fd0\u52a8\u6062\u590d\u7ed3\u6784\uff09\u4e2d\u8fdb\u884c\u7ed3\u6784\u53163D\u5efa\u7b51\u91cd\u5efa\u662f\u4e00\u4e2a\u9ad8\u5ea6\u6b20\u7ea6\u675f\u7684\u95ee\u9898\uff0c\u9700\u8981\u6062\u590d\u827a\u672f\u5bb6\u521b\u5efa\u7684\u5efa\u7b51\u62bd\u8c61\u8868\u793a\u3002", "method": "\u8bbe\u8ba1\u4e86\u677e\u6563\u7ea7\u8054\u6269\u6563\u53d8\u6362\u5668(Loca-DiT)\uff1a1\uff09\u9996\u5148\u5c06\u5206\u5e03\u6062\u590d\u4f5c\u4e3a\u6761\u4ef6\u751f\u6210\u4efb\u52a1\uff0c\u8bad\u7ec3\u4ee5\u8f93\u5165\u70b9\u4e91\u4e3a\u6761\u4ef6\u7684\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff1b2\uff09\u7136\u540e\u5b9a\u5236\u4ec5\u89e3\u7801\u5668\u53d8\u6362\u5668\uff0c\u57fa\u4e8e\u6062\u590d\u7684\u70b9\u4e91\u8fdb\u884c\u6761\u4ef6\u81ea\u56de\u5f52\u7f51\u683c\u751f\u6210\u3002", "result": "\u5728\u5efa\u7b51\u62bd\u8c61\u65b9\u6cd5\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u5b9a\u6027\u548c\u5b9a\u91cf\u6539\u8fdb\uff0c\u5176\u6062\u590d\u7684\u70b9\u4e91\u5728\u5efa\u7b51\u70b9\u4e91\u8865\u5168\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u6539\u8fdb\u7684\u8868\u9762\u7cbe\u5ea6\u548c\u5206\u5e03\u5747\u5300\u6027\u3002", "conclusion": "BuildAnyPoint\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u663e\u5f0f3D\u751f\u6210\u5148\u9a8c\u548c\u81ea\u56de\u5f52\u7f51\u683c\u751f\u6210\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4ece\u591a\u6837\u5316\u70b9\u4e91\u5206\u5e03\u4e2d\u8fdb\u884c\u7ed3\u6784\u53163D\u5efa\u7b51\u91cd\u5efa\u7684\u6311\u6218\u3002"}}
{"id": "2602.23777", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23777", "abs": "https://arxiv.org/abs/2602.23777", "authors": ["Zhipeng Xu", "Zilong Wang", "Xinyang Jiang", "Dongsheng Li", "De Cheng", "Nannan Wang"], "title": "Reasoning-Driven Multimodal LLM for Domain Generalization", "comment": "Accepted at ICLR 2026 (Poster)", "summary": "This paper addresses the domain generalization (DG) problem in deep learning. While most DG methods focus on enforcing visual feature invariance, we leverage the reasoning capability of multimodal large language models (MLLMs) and explore the potential of constructing reasoning chains that derives image categories to achieve more robust predictions under domain shift. To this end, we systematically study the role of reasoning in DG using DomainBed-Reasoning, a newly constructed extension of DomainBed dataset, in which each sample is paired with class-relevant reasoning chains. Our analysis reveals two key challenges: (i) fine-tuning MLLMs with reasoning chains for classification is more challenging than direct label supervision, since the model must optimize complex reasoning sequences before label prediction; and (ii) mismatches in reasoning patterns between supervision signals and fine-tuned MLLMs lead to a trade-off between semantic richness (informative but harder to optimize) and optimization efficiency (easier to optimize but less informative). To address these issues, we propose RD-MLDG (Reasoning-Driven Multimodal LLM for Domain Generalization), a framework with two components: (i) MTCT (Multi-Task Cross-Training), which introduces an additional direct classification pathway to guide reasoning supervision; and (ii) SARR (Self-Aligned Reasoning Regularization), which preserves the semantic richness of reasoning chains while mitigating reasoning-pattern mismatches via iterative self-labeling. Experiments on standard DomainBed datasets (PACS, VLCS, OfficeHome, TerraInc) demonstrate that RD-MLDG achieves state-of-the-art performances, highlighting reasoning as a promising complementary signal for robust out-of-domain generalization.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faRD-MLDG\u6846\u67b6\uff0c\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u89e3\u51b3\u9886\u57df\u6cdb\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u63a8\u7406\u94fe\u6784\u5efa\u5b9e\u73b0\u66f4\u9c81\u68d2\u7684\u8de8\u57df\u9884\u6d4b\u3002", "motivation": "\u73b0\u6709\u9886\u57df\u6cdb\u5316\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u89c6\u89c9\u7279\u5f81\u4e0d\u53d8\u6027\uff0c\u800c\u672c\u6587\u63a2\u7d22\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u901a\u8fc7\u6784\u5efa\u63a8\u7406\u94fe\u6765\u83b7\u5f97\u66f4\u9c81\u68d2\u7684\u8de8\u57df\u9884\u6d4b\u6027\u80fd\u3002", "method": "\u63d0\u51faRD-MLDG\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) MTCT\uff08\u591a\u4efb\u52a1\u4ea4\u53c9\u8bad\u7ec3\uff09\uff0c\u5f15\u5165\u76f4\u63a5\u5206\u7c7b\u8def\u5f84\u6765\u6307\u5bfc\u63a8\u7406\u76d1\u7763\uff1b2) SARR\uff08\u81ea\u5bf9\u9f50\u63a8\u7406\u6b63\u5219\u5316\uff09\uff0c\u901a\u8fc7\u8fed\u4ee3\u81ea\u6807\u6ce8\u4fdd\u6301\u63a8\u7406\u94fe\u7684\u8bed\u4e49\u4e30\u5bcc\u6027\u540c\u65f6\u7f13\u89e3\u63a8\u7406\u6a21\u5f0f\u4e0d\u5339\u914d\u95ee\u9898\u3002", "result": "\u5728\u6807\u51c6DomainBed\u6570\u636e\u96c6\uff08PACS\u3001VLCS\u3001OfficeHome\u3001TerraInc\uff09\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cRD-MLDG\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u63a8\u7406\u4f5c\u4e3a\u9c81\u68d2\u8de8\u57df\u6cdb\u5316\u7684\u6709\u6548\u8865\u5145\u4fe1\u53f7\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u63a8\u7406\u94fe\u53ef\u4ee5\u6210\u4e3a\u9886\u57df\u6cdb\u5316\u7684\u6709\u6548\u8865\u5145\u4fe1\u53f7\uff0cRD-MLDG\u6846\u67b6\u901a\u8fc7\u5e73\u8861\u8bed\u4e49\u4e30\u5bcc\u6027\u548c\u4f18\u5316\u6548\u7387\uff0c\u5728\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u8de8\u57df\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2602.23928", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.23928", "abs": "https://arxiv.org/abs/2602.23928", "authors": ["Gary Lupyan", "Senyi Yang"], "title": "The Astonishing Ability of Large Language Models to Parse Jabberwockified Language", "comment": "Submitted to the 2026 Annual Meeting of the Cognitive Science Society", "summary": "We show that large language models (LLMs) have an astonishing ability to recover meaning from severely degraded English texts. Texts in which content words have been randomly substituted by nonsense strings, e.g., \"At the ghybe of the swuint, we are haiveed to Wourge Phrear-gwurr, who sproles into an ghitch flount with his crurp\", can be translated to conventional English that is, in many cases, close to the original text, e.g., \"At the start of the story, we meet a man, Chow, who moves into an apartment building with his wife.\" These results show that structural cues (e.g., morphosyntax, closed-class words) constrain lexical meaning to a much larger degree than imagined. Although the abilities of LLMs to make sense of \"Jabberwockified\" English are clearly superhuman, they are highly relevant to understanding linguistic structure and suggest that efficient language processing either in biological or artificial systems likely benefits from very tight integration between syntax, lexical semantics, and general world knowledge.", "AI": {"tldr": "\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u4ece\u4e25\u91cd\u9000\u5316\u7684\u82f1\u8bed\u6587\u672c\u4e2d\u6062\u590d\u8bed\u4e49\uff0c\u5373\u4f7f\u5185\u5bb9\u8bcd\u88ab\u968f\u673a\u66ff\u6362\u4e3a\u65e0\u610f\u4e49\u5b57\u7b26\u4e32\uff0c\u6a21\u578b\u4ecd\u80fd\u5c06\u5176\u7ffb\u8bd1\u56de\u63a5\u8fd1\u539f\u6587\u7684\u5e38\u89c4\u82f1\u8bed\u3002", "motivation": "\u63a2\u7d22\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7406\u89e3\u8bed\u8a00\u7ed3\u6784\u65b9\u9762\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u5b83\u4eec\u5982\u4f55\u4ec5\u51ed\u7ed3\u6784\u7ebf\u7d22\uff08\u5982\u5f62\u6001\u53e5\u6cd5\u3001\u5c01\u95ed\u7c7b\u8bcd\u6c47\uff09\u6765\u6062\u590d\u88ab\u7834\u574f\u6587\u672c\u7684\u8bed\u4e49\u3002", "method": "\u901a\u8fc7\u521b\u5efa\"Jabberwockified\"\u82f1\u8bed\u6587\u672c\uff08\u5c06\u5185\u5bb9\u8bcd\u968f\u673a\u66ff\u6362\u4e3a\u65e0\u610f\u4e49\u5b57\u7b26\u4e32\uff09\uff0c\u6d4b\u8bd5\u5927\u8bed\u8a00\u6a21\u578b\u5c06\u8fd9\u4e9b\u9000\u5316\u6587\u672c\u7ffb\u8bd1\u56de\u5e38\u89c4\u82f1\u8bed\u7684\u80fd\u529b\u3002", "result": "\u5927\u8bed\u8a00\u6a21\u578b\u5c55\u73b0\u51fa\u60ca\u4eba\u7684\u80fd\u529b\uff0c\u80fd\u591f\u4ece\u4e25\u91cd\u9000\u5316\u7684\u6587\u672c\u4e2d\u6062\u590d\u8bed\u4e49\uff0c\u5728\u8bb8\u591a\u60c5\u51b5\u4e0b\u80fd\u751f\u6210\u63a5\u8fd1\u539f\u6587\u7684\u7ffb\u8bd1\uff0c\u8868\u660e\u7ed3\u6784\u7ebf\u7d22\u5bf9\u8bcd\u6c47\u610f\u4e49\u7684\u7ea6\u675f\u7a0b\u5ea6\u8fdc\u8d85\u60f3\u8c61\u3002", "conclusion": "\u5927\u8bed\u8a00\u6a21\u578b\u7406\u89e3\"Jabberwockified\"\u82f1\u8bed\u7684\u80fd\u529b\u8fdc\u8d85\u4eba\u7c7b\uff0c\u8fd9\u5bf9\u7406\u89e3\u8bed\u8a00\u7ed3\u6784\u6709\u91cd\u8981\u610f\u4e49\uff0c\u8868\u660e\u9ad8\u6548\u7684\u8bed\u8a00\u5904\u7406\uff08\u65e0\u8bba\u662f\u751f\u7269\u8fd8\u662f\u4eba\u5de5\u7cfb\u7edf\uff09\u90fd\u9700\u8981\u8bed\u6cd5\u3001\u8bcd\u6c47\u8bed\u4e49\u548c\u4e16\u754c\u77e5\u8bc6\u7684\u7d27\u5bc6\u6574\u5408\u3002"}}
{"id": "2602.23719", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23719", "abs": "https://arxiv.org/abs/2602.23719", "authors": ["Wenzhe Zhao", "Yang Zhao", "Ganchao Liu", "Zhiyu Jiang", "Dandan Ma", "Zihao Li", "Xuelong Li"], "title": "SAGE-LLM: Towards Safe and Generalizable LLM Controller with Fuzzy-CBF Verification and Graph-Structured Knowledge Retrieval for UAV Decision", "comment": null, "summary": "In UAV dynamic decision, complex and variable hazardous factors pose severe challenges to the generalization capability of algorithms. Despite offering semantic understanding and scene generalization, Large Language Models (LLM) lack domain-specific UAV control knowledge and formal safety assurances, restricting their direct applicability. To bridge this gap, this paper proposes a train-free two-layer decision architecture based on LLMs, integrating high-level safety planning with low-level precise control. The framework introduces three key contributions: 1) A fuzzy Control Barrier Function verification mechanism for semantically-augmented actions, providing provable safety certification for LLM outputs. 2) A star-hierarchical graph-based retrieval-augmented generation system, enabling efficient, elastic, and interpretable scene adaptation. 3) Systematic experimental validation in pursuit-evasion scenarios with unknown obstacles and emergent threats, demonstrating that our SAGE-LLM maintains performance while significantly enhancing safety and generalization without online training. The proposed framework demonstrates strong extensibility, suggesting its potential for generalization to broader embodied intelligence systems and safety-critical control domains.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65e0\u8bad\u7ec3\u53cc\u5c42\u51b3\u7b56\u67b6\u6784\uff0c\u901a\u8fc7\u6a21\u7cca\u63a7\u5236\u5c4f\u969c\u51fd\u6570\u9a8c\u8bc1\u548c\u661f\u578b\u5c42\u6b21\u56fe\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff0c\u63d0\u5347\u65e0\u4eba\u673a\u52a8\u6001\u51b3\u7b56\u7684\u5b89\u5168\u6027\u548c\u6cdb\u5316\u80fd\u529b", "motivation": "\u65e0\u4eba\u673a\u52a8\u6001\u51b3\u7b56\u4e2d\u590d\u6742\u591a\u53d8\u7684\u5371\u9669\u56e0\u7d20\u5bf9\u7b97\u6cd5\u6cdb\u5316\u80fd\u529b\u6784\u6210\u4e25\u5cfb\u6311\u6218\uff0c\u800c\u5927\u8bed\u8a00\u6a21\u578b\u867d\u7136\u5177\u5907\u8bed\u4e49\u7406\u89e3\u548c\u573a\u666f\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u7f3a\u4e4f\u7279\u5b9a\u9886\u57df\u63a7\u5236\u77e5\u8bc6\u548c\u5f62\u5f0f\u5316\u5b89\u5168\u4fdd\u969c\uff0c\u9650\u5236\u4e86\u5176\u76f4\u63a5\u5e94\u7528", "method": "\u63d0\u51fa\u65e0\u8bad\u7ec3\u53cc\u5c42\u51b3\u7b56\u67b6\u6784\uff1a\u9ad8\u5c42\u5b89\u5168\u89c4\u5212\u4e0e\u5e95\u5c42\u7cbe\u786e\u63a7\u5236\u76f8\u7ed3\u5408\uff1b\u5f15\u5165\u6a21\u7cca\u63a7\u5236\u5c4f\u969c\u51fd\u6570\u9a8c\u8bc1\u673a\u5236\u4e3aLLM\u8f93\u51fa\u63d0\u4f9b\u53ef\u8bc1\u660e\u7684\u5b89\u5168\u8ba4\u8bc1\uff1b\u91c7\u7528\u661f\u578b\u5c42\u6b21\u56fe\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7cfb\u7edf\u5b9e\u73b0\u9ad8\u6548\u3001\u5f39\u6027\u3001\u53ef\u89e3\u91ca\u7684\u573a\u666f\u9002\u5e94", "result": "\u5728\u5177\u6709\u672a\u77e5\u969c\u788d\u548c\u7a81\u53d1\u5a01\u80c1\u7684\u8ffd\u9003\u573a\u666f\u4e2d\u8fdb\u884c\u7cfb\u7edf\u5b9e\u9a8c\u9a8c\u8bc1\uff0cSAGE-LLM\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u5b89\u5168\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u65e0\u9700\u5728\u7ebf\u8bad\u7ec3", "conclusion": "\u8be5\u6846\u67b6\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u53ef\u6269\u5c55\u6027\uff0c\u6709\u671b\u63a8\u5e7f\u5230\u66f4\u5e7f\u6cdb\u7684\u5177\u8eab\u667a\u80fd\u7cfb\u7edf\u548c\u5b89\u5168\u5173\u952e\u63a7\u5236\u9886\u57df"}}
{"id": "2602.23652", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23652", "abs": "https://arxiv.org/abs/2602.23652", "authors": ["Haowen Zhu", "Ning Yin", "Xiaogen Zhou"], "title": "3D Modality-Aware Pre-training for Vision-Language Model in MRI Multi-organ Abnormality Detection", "comment": null, "summary": "Vision-language models (VLMs) show strong potential for complex diagnostic tasks in medical imaging. However, applying VLMs to multi-organ medical imaging introduces two principal challenges: (1) modality-specific vision-language alignment and (2) cross-modal feature fusion. In this work, we propose MedMAP, a Medical Modality-Aware Pretraining framework that enhances vision-language representation learning in 3D MRI. MedMAP comprises a modality-aware vision-language alignment stage and a fine-tuning stage for multi-organ abnormality detection. During the pre-training stage, the modality-aware encoders implicitly capture the joint modality distribution and improve alignment between visual and textual representations. We then fine-tune the pre-trained vision encoders (while keeping the text encoder frozen) for downstream tasks. To this end, we curated MedMoM-MRI3D, comprising 7,392 3D MRI volume-report pairs spanning twelve MRI modalities and nine abnormalities tailored for various 3D medical analysis tasks. Extensive experiments on MedMoM-MRI3D demonstrate that MedMAP significantly outperforms existing VLMs in 3D MRI-based multi-organ abnormality detection. Our code is available at https://github.com/RomantiDr/MedMAP.", "AI": {"tldr": "MedMAP\u662f\u4e00\u4e2a\u533b\u5b66\u6a21\u6001\u611f\u77e5\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u6001\u611f\u77e5\u7684\u89c6\u89c9-\u8bed\u8a00\u5bf9\u9f50\u548c\u5fae\u8c03\u9636\u6bb5\uff0c\u63d0\u53473D MRI\u591a\u5668\u5b98\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd", "motivation": "\u5c06\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5e94\u7528\u4e8e\u591a\u5668\u5b98\u533b\u5b66\u5f71\u50cf\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1a\u6a21\u6001\u7279\u5b9a\u7684\u89c6\u89c9-\u8bed\u8a00\u5bf9\u9f50\u548c\u8de8\u6a21\u6001\u7279\u5f81\u878d\u5408", "method": "\u63d0\u51faMedMAP\u6846\u67b6\uff0c\u5305\u542b\u6a21\u6001\u611f\u77e5\u89c6\u89c9-\u8bed\u8a00\u5bf9\u9f50\u9884\u8bad\u7ec3\u9636\u6bb5\u548c\u4e0b\u6e38\u4efb\u52a1\u5fae\u8c03\u9636\u6bb5\uff1b\u521b\u5efa\u4e86MedMoM-MRI3D\u6570\u636e\u96c6\uff0c\u5305\u542b7,392\u4e2a3D MRI\u4f53\u79ef-\u62a5\u544a\u5bf9", "result": "\u5728MedMoM-MRI3D\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMedMAP\u57283D MRI\u591a\u5668\u5b98\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b", "conclusion": "MedMAP\u901a\u8fc7\u6a21\u6001\u611f\u77e5\u7684\u9884\u8bad\u7ec3\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u591a\u5668\u5b98\u533b\u5b66\u5f71\u50cf\u4e2d\u7684\u89c6\u89c9-\u8bed\u8a00\u5bf9\u9f50\u6311\u6218\uff0c\u63d0\u5347\u4e863D MRI\u8bca\u65ad\u6027\u80fd"}}
{"id": "2602.23802", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23802", "abs": "https://arxiv.org/abs/2602.23802", "authors": ["Yiyang Fang", "Wenke Huang", "Pei Fu", "Yihao Yang", "Kehua Su", "Zhenbo Luo", "Jian Luan", "Mang Ye"], "title": "EMO-R3: Reflective Reinforcement Learning for Emotional Reasoning in Multimodal Large Language Models", "comment": "Accepted by CVPR 2026", "summary": "Multimodal Large Language Models (MLLMs) have shown remarkable progress in visual reasoning and understanding tasks but still struggle to capture the complexity and subjectivity of human emotions. Existing approaches based on supervised fine-tuning often suffer from limited generalization and poor interpretability, while reinforcement learning methods such as Group Relative Policy Optimization fail to align with the intrinsic characteristics of emotional cognition. To address these challenges, we propose Reflective Reinforcement Learning for Emotional Reasoning (EMO-R3), a framework designed to enhance the emotional reasoning ability of MLLMs. Specifically, we introduce Structured Emotional Thinking to guide the model to perform step-by-step emotional reasoning in a structured and interpretable manner, and design a Reflective Emotional Reward that enables the model to re-evaluate its reasoning based on visual-text consistency and emotional coherence. Extensive experiments demonstrate that EMO-R3 significantly improves both the interpretability and emotional intelligence of MLLMs, achieving superior performance across multiple visual emotional understanding benchmarks.", "AI": {"tldr": "EMO-R3\u6846\u67b6\u901a\u8fc7\u7ed3\u6784\u5316\u60c5\u611f\u601d\u7ef4\u548c\u53cd\u601d\u6027\u60c5\u611f\u5956\u52b1\uff0c\u589e\u5f3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u60c5\u611f\u63a8\u7406\u80fd\u529b\uff0c\u63d0\u5347\u89e3\u91ca\u6027\u548c\u60c5\u611f\u667a\u80fd\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u63a8\u7406\u65b9\u9762\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5728\u6355\u6349\u4eba\u7c7b\u60c5\u611f\u7684\u590d\u6742\u6027\u548c\u4e3b\u89c2\u6027\u65b9\u9762\u4ecd\u5b58\u5728\u56f0\u96be\u3002\u73b0\u6709\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\u6cdb\u5316\u80fd\u529b\u6709\u9650\u4e14\u89e3\u91ca\u6027\u5dee\uff0c\u800c\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5982GRPO\u672a\u80fd\u4e0e\u60c5\u611f\u8ba4\u77e5\u7684\u5185\u5728\u7279\u6027\u5bf9\u9f50\u3002", "method": "\u63d0\u51faEMO-R3\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) \u7ed3\u6784\u5316\u60c5\u611f\u601d\u7ef4\uff0c\u5f15\u5bfc\u6a21\u578b\u4ee5\u7ed3\u6784\u5316\u3001\u53ef\u89e3\u91ca\u7684\u65b9\u5f0f\u8fdb\u884c\u9010\u6b65\u60c5\u611f\u63a8\u7406\uff1b2) \u53cd\u601d\u6027\u60c5\u611f\u5956\u52b1\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u57fa\u4e8e\u89c6\u89c9-\u6587\u672c\u4e00\u81f4\u6027\u548c\u60c5\u611f\u8fde\u8d2f\u6027\u91cd\u65b0\u8bc4\u4f30\u5176\u63a8\u7406\u8fc7\u7a0b\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cEMO-R3\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u89e3\u91ca\u6027\u548c\u60c5\u611f\u667a\u80fd\uff0c\u5728\u591a\u4e2a\u89c6\u89c9\u60c5\u611f\u7406\u89e3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "EMO-R3\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u60c5\u611f\u63a8\u7406\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u601d\u7ef4\u548c\u53cd\u601d\u6027\u5956\u52b1\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u60c5\u611f\u7406\u89e3\u548c\u89e3\u91ca\u80fd\u529b\u3002"}}
{"id": "2602.23940", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23940", "abs": "https://arxiv.org/abs/2602.23940", "authors": ["Nischal Karki", "Bipesh Subedi", "Prakash Poudyal", "Rupak Raj Ghimire", "Bal Krishna Bal"], "title": "Benchmarking BERT-based Models for Sentence-level Topic Classification in Nepali Language", "comment": "5 pages, 2 figures. Accepted and presented at the Regional International Conference on Natural Language Processing (RegICON 2025), Gauhati University, Guwahati, India, November 27-29, 2025. To appear in the conference proceedings. Accepted papers list available at: https://www.regicon2025.in/accepted-papers", "summary": "Transformer-based models such as BERT have significantly advanced Natural Language Processing (NLP) across many languages. However, Nepali, a low-resource language written in Devanagari script, remains relatively underexplored. This study benchmarks multilingual, Indic, Hindi, and Nepali BERT variants to evaluate their effectiveness in Nepali topic classification. Ten pre-trained models, including mBERT, XLM-R, MuRIL, DevBERT, HindiBERT, IndicBERT, and NepBERTa, were fine-tuned and tested on the balanced Nepali dataset containing 25,006 sentences across five conceptual domains and the performance was evaluated using accuracy, weighted precision, recall, F1-score, and AUROC metrics. The results reveal that Indic models, particularly MuRIL-large, achieved the highest F1-score of 90.60%, outperforming multilingual and monolingual models. NepBERTa also performed competitively with an F1-score of 88.26%. Overall, these findings establish a robust baseline for future document-level classification and broader Nepali NLP applications.", "AI": {"tldr": "\u672c\u7814\u7a76\u5bf9\u591a\u79cdBERT\u53d8\u4f53\u5728\u5c3c\u6cca\u5c14\u8bed\u4e3b\u9898\u5206\u7c7b\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u53d1\u73b0\u5370\u5ea6\u8bed\u7cfb\u6a21\u578b\uff08\u7279\u522b\u662fMuRIL-large\uff09\u8868\u73b0\u6700\u4f73\uff0cF1\u5206\u6570\u8fbe90.60%\uff0c\u4e3a\u5c3c\u6cca\u5c14\u8bedNLP\u5e94\u7528\u5efa\u7acb\u4e86\u53ef\u9760\u57fa\u7ebf\u3002", "motivation": "\u5c3d\u7ba1\u57fa\u4e8eTransformer\u7684\u6a21\u578b\u5982BERT\u663e\u8457\u63a8\u52a8\u4e86\u591a\u8bed\u8a00\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7684\u53d1\u5c55\uff0c\u4f46\u4f7f\u7528\u5929\u57ce\u4f53\u6587\u5b57\u4e66\u5199\u7684\u4f4e\u8d44\u6e90\u8bed\u8a00\u5c3c\u6cca\u5c14\u8bed\u4ecd\u76f8\u5bf9\u7f3a\u4e4f\u7814\u7a76\u3002\u672c\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u4e0d\u540cBERT\u53d8\u4f53\u5728\u5c3c\u6cca\u5c14\u8bed\u4e3b\u9898\u5206\u7c7b\u4efb\u52a1\u4e0a\u7684\u6709\u6548\u6027\u3002", "method": "\u7814\u7a76\u5bf910\u4e2a\u9884\u8bad\u7ec3\u6a21\u578b\uff08\u5305\u62ecmBERT\u3001XLM-R\u3001MuRIL\u3001DevBERT\u3001HindiBERT\u3001IndicBERT\u548cNepBERTa\uff09\u5728\u5e73\u8861\u7684\u5c3c\u6cca\u5c14\u8bed\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5fae\u8c03\u548c\u6d4b\u8bd5\u3002\u8be5\u6570\u636e\u96c6\u5305\u542b25,006\u4e2a\u53e5\u5b50\uff0c\u6db5\u76d6\u4e94\u4e2a\u6982\u5ff5\u9886\u57df\u3002\u4f7f\u7528\u51c6\u786e\u7387\u3001\u52a0\u6743\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u3001F1\u5206\u6570\u548cAUROC\u7b49\u6307\u6807\u8bc4\u4f30\u6027\u80fd\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u5370\u5ea6\u8bed\u7cfb\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u7279\u522b\u662fMuRIL-large\u6a21\u578b\u53d6\u5f97\u4e86\u6700\u9ad8\u7684F1\u5206\u657090.60%\uff0c\u4f18\u4e8e\u591a\u8bed\u8a00\u548c\u5355\u8bed\u8a00\u6a21\u578b\u3002NepBERTa\u4e5f\u8868\u73b0\u51fa\u7ade\u4e89\u529b\uff0cF1\u5206\u6570\u4e3a88.26%\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u672a\u6765\u6587\u6863\u7ea7\u5206\u7c7b\u548c\u66f4\u5e7f\u6cdb\u7684\u5c3c\u6cca\u5c14\u8bed\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5e94\u7528\u5efa\u7acb\u4e86\u7a33\u5065\u7684\u57fa\u51c6\uff0c\u8868\u660e\u9488\u5bf9\u7279\u5b9a\u8bed\u8a00\u5bb6\u65cf\u4f18\u5316\u7684\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4efb\u52a1\u4e0a\u5177\u6709\u4f18\u52bf\u3002"}}
{"id": "2602.23653", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23653", "abs": "https://arxiv.org/abs/2602.23653", "authors": ["Wei Luo", "Yangfan Ou", "Jin Deng", "Zeshuai Deng", "Xiquan Yan", "Zhiquan Wen", "Mingkui Tan"], "title": "ProtoDCS: Towards Robust and Efficient Open-Set Test-Time Adaptation for Vision-Language Models", "comment": "13 pages, under review", "summary": "Large-scale Vision-Language Models (VLMs) exhibit strong zero-shot recognition, yet their real-world deployment is challenged by distribution shifts. While Test-Time Adaptation (TTA) can mitigate this, existing VLM-based TTA methods operate under a closed-set assumption, failing in open-set scenarios where test streams contain both covariate-shifted in-distribution (csID) and out-of-distribution (csOOD) data. This leads to a critical difficulty: the model must discriminate unknown csOOD samples to avoid interference while simultaneously adapting to known csID classes for accuracy. Current open-set TTA (OSTTA) methods rely on hard thresholds for separation and entropy minimization for adaptation. These strategies are brittle, often misclassifying ambiguous csOOD samples and inducing overconfident predictions, and their parameter-update mechanism is computationally prohibitive for VLMs. To address these limitations, we propose Prototype-based Double-Check Separation (ProtoDCS), a robust framework for OSTTA that effectively separates csID and csOOD samples, enabling safe and efficient adaptation of VLMs to csID data. Our main contributions are: (1) a novel double-check separation mechanism employing probabilistic Gaussian Mixture Model (GMM) verification to replace brittle thresholding; and (2) an evidence-driven adaptation strategy utilizing uncertainty-aware loss and efficient prototype-level updates, mitigating overconfidence and reducing computational overhead. Extensive experiments on CIFAR-10/100-C and Tiny-ImageNet-C demonstrate that ProtoDCS achieves state-of-the-art performance, significantly boosting both known-class accuracy and OOD detection metrics. Code will be available at https://github.com/O-YangF/ProtoDCS.", "AI": {"tldr": "ProtoDCS\uff1a\u4e00\u79cd\u7528\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5f00\u96c6\u6d4b\u8bd5\u65f6\u9002\u5e94\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u91cd\u68c0\u67e5\u5206\u79bb\u673a\u5236\u548c\u8bc1\u636e\u9a71\u52a8\u9002\u5e94\u7b56\u7565\uff0c\u6709\u6548\u5904\u7406\u534f\u53d8\u91cf\u504f\u79fb\u7684\u5206\u5e03\u5185\u548c\u5206\u5e03\u5916\u6570\u636e\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6d4b\u8bd5\u65f6\u9002\u5e94\u65b9\u6cd5\u5728\u5f00\u96c6\u573a\u666f\u4e0b\u5b58\u5728\u5c40\u9650\uff0c\u65e0\u6cd5\u6709\u6548\u533a\u5206\u534f\u53d8\u91cf\u504f\u79fb\u7684\u5206\u5e03\u5185\u548c\u5206\u5e03\u5916\u6570\u636e\uff0c\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u4e0b\u964d\u548c\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\u3002", "method": "\u63d0\u51fa\u539f\u578b\u53cc\u91cd\u68c0\u67e5\u5206\u79bb\u6846\u67b6\uff0c\u4f7f\u7528\u6982\u7387\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u9a8c\u8bc1\u66ff\u4ee3\u8106\u5f31\u7684\u9608\u503c\u65b9\u6cd5\uff0c\u5e76\u7ed3\u5408\u8bc1\u636e\u9a71\u52a8\u9002\u5e94\u7b56\u7565\uff0c\u91c7\u7528\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u635f\u5931\u548c\u9ad8\u6548\u539f\u578b\u7ea7\u66f4\u65b0\u3002", "result": "\u5728CIFAR-10/100-C\u548cTiny-ImageNet-C\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cProtoDCS\u5728\u5df2\u77e5\u7c7b\u522b\u51c6\u786e\u7387\u548cOOD\u68c0\u6d4b\u6307\u6807\u4e0a\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "ProtoDCS\u4e3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9c81\u68d2\u3001\u9ad8\u6548\u7684\u5f00\u96c6\u6d4b\u8bd5\u65f6\u9002\u5e94\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u5206\u5e03\u504f\u79fb\u573a\u666f\u4e0b\u7684\u90e8\u7f72\u80fd\u529b\u3002"}}
{"id": "2602.23581", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23581", "abs": "https://arxiv.org/abs/2602.23581", "authors": ["Xiang Ao"], "title": "SDMixer: Sparse Dual-Mixer for Time Series Forecasting", "comment": "12pages,2 figures", "summary": "Multivariate time series forecasting is widely applied in fields such as transportation, energy, and finance. However, the data commonly suffers from issues of multi-scale characteristics, weak correlations, and noise interference, which limit the predictive performance of existing models. This paper proposes a dual-stream sparse Mixer prediction framework that extracts global trends and local dynamic features from sequences in both the frequency and time domains, respectively. It employs a sparsity mechanism to filter out invalid information, thereby enhancing the accuracy of cross-variable dependency modeling. Experimental results demonstrate that this method achieves leading performance on multiple real-world scenario datasets, validating its effectiveness and generality. The code is available at https://github.com/SDMixer/SDMixer", "AI": {"tldr": "\u63d0\u51fa\u53cc\u6d41\u7a00\u758fMixer\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u9891\u57df\u548c\u65f6\u57df\u5206\u522b\u63d0\u53d6\u5e8f\u5217\u7684\u5168\u5c40\u8d8b\u52bf\u548c\u5c40\u90e8\u52a8\u6001\u7279\u5f81\uff0c\u4f7f\u7528\u7a00\u758f\u673a\u5236\u8fc7\u6ee4\u65e0\u6548\u4fe1\u606f\uff0c\u63d0\u5347\u8de8\u53d8\u91cf\u4f9d\u8d56\u5efa\u6a21\u7cbe\u5ea6", "motivation": "\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u5728\u4ea4\u901a\u3001\u80fd\u6e90\u3001\u91d1\u878d\u7b49\u9886\u57df\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u6570\u636e\u5e38\u5b58\u5728\u591a\u5c3a\u5ea6\u7279\u5f81\u3001\u5f31\u76f8\u5173\u6027\u548c\u566a\u58f0\u5e72\u6270\u7b49\u95ee\u9898\uff0c\u9650\u5236\u4e86\u73b0\u6709\u6a21\u578b\u7684\u9884\u6d4b\u6027\u80fd", "method": "\u53cc\u6d41\u7a00\u758fMixer\u9884\u6d4b\u6846\u67b6\uff0c\u5206\u522b\u4ece\u9891\u57df\u548c\u65f6\u57df\u63d0\u53d6\u5e8f\u5217\u7684\u5168\u5c40\u8d8b\u52bf\u548c\u5c40\u90e8\u52a8\u6001\u7279\u5f81\uff0c\u91c7\u7528\u7a00\u758f\u673a\u5236\u8fc7\u6ee4\u65e0\u6548\u4fe1\u606f\uff0c\u589e\u5f3a\u8de8\u53d8\u91cf\u4f9d\u8d56\u5efa\u6a21\u80fd\u529b", "result": "\u5728\u591a\u4e2a\u771f\u5b9e\u573a\u666f\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u9886\u5148\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u901a\u7528\u6027", "conclusion": "\u63d0\u51fa\u7684\u53cc\u6d41\u7a00\u758fMixer\u6846\u67b6\u80fd\u6709\u6548\u5904\u7406\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u7684\u591a\u5c3a\u5ea6\u7279\u5f81\u3001\u5f31\u76f8\u5173\u6027\u548c\u566a\u58f0\u95ee\u9898\uff0c\u63d0\u5347\u9884\u6d4b\u6027\u80fd\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c"}}
{"id": "2602.23941", "categories": ["cs.CL", "cs.DL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.23941", "abs": "https://arxiv.org/abs/2602.23941", "authors": ["Ludovic Moncla", "Pierre Nugues", "Thierry Joliveau", "Katherine McDonough"], "title": "EDDA-Coordinata: An Annotated Dataset of Historical Geographic Coordinates", "comment": "Accepted at LREC 2026", "summary": "This paper introduces a dataset of enriched geographic coordinates retrieved from Diderot and d'Alembert's eighteenth-century Encyclopedie. Automatically recovering geographic coordinates from historical texts is a complex task, as they are expressed in a variety of ways and with varying levels of precision. To improve retrieval of coordinates from similar digitized early modern texts, we have created a gold standard dataset, trained models, published the resulting inferred and normalized coordinate data, and experimented applying these models to new texts. From 74,000 total articles in each of the digitized versions of the Encyclopedie from ARTFL and ENCCRE, we examined 15,278 geographical entries, manually identifying 4,798 containing coordinates, and 10,480 with descriptive but non-numerical references. Leveraging our gold standard annotations, we trained transformer-based models to retrieve and normalize coordinates. The pipeline presented here combines a classifier to identify coordinate-bearing entries and a second model for retrieval, tested across encoder-decoder and decoder architectures. Cross-validation yielded an 86% EM score. On an out-of-domain eighteenth-century Trevoux dictionary (also in French), our fine-tuned model had a 61% EM score, while for the nineteenth-century, 7th edition of the Encyclopaedia Britannica in English, the EM was 77%. These findings highlight the gold standard dataset's usefulness as training data, and our two-step method's cross-lingual, cross-domain generalizability.", "AI": {"tldr": "\u4ece18\u4e16\u7eaa\u300a\u767e\u79d1\u5168\u4e66\u300b\u4e2d\u6784\u5efa\u5730\u7406\u5750\u6807\u6570\u636e\u96c6\uff0c\u8bad\u7ec3\u6a21\u578b\u81ea\u52a8\u63d0\u53d6\u548c\u6807\u51c6\u5316\u5386\u53f2\u6587\u672c\u4e2d\u7684\u5750\u6807\u4fe1\u606f", "motivation": "\u5386\u53f2\u6587\u672c\u4e2d\u7684\u5730\u7406\u5750\u6807\u8868\u8fbe\u65b9\u5f0f\u591a\u6837\u4e14\u7cbe\u5ea6\u4e0d\u4e00\uff0c\u81ea\u52a8\u63d0\u53d6\u56f0\u96be\uff0c\u9700\u8981\u6784\u5efa\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u6765\u6539\u8fdb\u4ece\u65e9\u671f\u73b0\u4ee3\u6570\u5b57\u5316\u6587\u672c\u4e2d\u63d0\u53d6\u5750\u6807\u7684\u80fd\u529b", "method": "\u4ece\u300a\u767e\u79d1\u5168\u4e66\u300b74,000\u7bc7\u6587\u7ae0\u4e2d\u7b5b\u900915,278\u4e2a\u5730\u7406\u6761\u76ee\uff0c\u4eba\u5de5\u6807\u6ce84,798\u4e2a\u542b\u5750\u6807\u6761\u76ee\uff1b\u8bad\u7ec3\u57fa\u4e8etransformer\u7684\u6a21\u578b\uff0c\u91c7\u7528\u4e24\u6b65\u6d41\u7a0b\uff1a\u5206\u7c7b\u5668\u8bc6\u522b\u542b\u5750\u6807\u6761\u76ee\uff0c\u7b2c\u4e8c\u4e2a\u6a21\u578b\u8fdb\u884c\u5750\u6807\u63d0\u53d6\u548c\u6807\u51c6\u5316", "result": "\u4ea4\u53c9\u9a8c\u8bc1\u83b7\u5f9786%\u7684\u7cbe\u786e\u5339\u914d\u5206\u6570\uff1b\u5728\u8de8\u57df\u6d4b\u8bd5\u4e2d\uff0c\u5bf918\u4e16\u7eaa\u6cd5\u8bedTrevoux\u8bcd\u5178\u8fbe\u523061% EM\uff0c\u5bf919\u4e16\u7eaa\u82f1\u6587\u300a\u5927\u82f1\u767e\u79d1\u5168\u4e66\u300b\u7b2c\u4e03\u7248\u8fbe\u523077% EM", "conclusion": "\u6784\u5efa\u7684\u6570\u636e\u96c6\u4f5c\u4e3a\u8bad\u7ec3\u6570\u636e\u5177\u6709\u5b9e\u7528\u4ef7\u503c\uff0c\u4e24\u6b65\u65b9\u6cd5\u5728\u8de8\u8bed\u8a00\u3001\u8de8\u9886\u57df\u573a\u666f\u4e2d\u5c55\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b"}}
{"id": "2602.23821", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.23821", "abs": "https://arxiv.org/abs/2602.23821", "authors": ["Jixiang Wang", "Siyuan Yang", "Ziyi Wu", "Siqi Wei", "Ashay Wakode", "Agata Barcis", "Hung Nguyen", "Shaoming He"], "title": "Acceleration-Based Control of Fixed-Wing UAVs for Guidance Applications", "comment": null, "summary": "Acceleration-commanded guidance laws (e.g., proportional navigation) are attractive for high-level decision making, but their direct deployment on fixed-wing UAVs is challenging because accelerations are not directly actuated and must be realized through attitude and thrust under flight-envelope constraints. This paper presents an acceleration-level outer-loop control framework that converts commanded tangential and normal accelerations into executable body-rate and normalized thrust commands compatible with mainstream autopilots (e.g., PX4/APM). For the normal channel, we derive an engineering mapping from the desired normal acceleration to roll- and pitch-rate commands that regulate the direction and magnitude of the lift vector under small-angle assumptions. For the tangential channel, we introduce an energy-based formulation inspired by total energy control and identify an empirical thrust-energy acceleration relationship directly from flight data, avoiding explicit propulsion modeling or thrust bench calibration. We further discuss priority handling between normal and tangential accelerations under saturation and non-level maneuvers. Extensive real-flight experiments on a VTOL fixed-wing platform demonstrate accurate acceleration tracking and enable practical implementation of proportional navigation using only body-rate and normalized thrust interfaces.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a0\u901f\u5ea6\u6307\u4ee4\u5916\u73af\u63a7\u5236\u6846\u67b6\uff0c\u5c06\u5207\u5411\u548c\u6cd5\u5411\u52a0\u901f\u5ea6\u547d\u4ee4\u8f6c\u6362\u4e3a\u53ef\u6267\u884c\u7684\u673a\u4f53\u89d2\u901f\u7387\u548c\u5f52\u4e00\u5316\u63a8\u529b\u547d\u4ee4\uff0c\u4f7f\u52a0\u901f\u5ea6\u6307\u4ee4\u5236\u5bfc\u5f8b\uff08\u5982\u6bd4\u4f8b\u5bfc\u5f15\uff09\u80fd\u76f4\u63a5\u90e8\u7f72\u5728\u56fa\u5b9a\u7ffc\u65e0\u4eba\u673a\u4e0a\u3002", "motivation": "\u52a0\u901f\u5ea6\u6307\u4ee4\u5236\u5bfc\u5f8b\uff08\u5982\u6bd4\u4f8b\u5bfc\u5f15\uff09\u5bf9\u4e8e\u9ad8\u5c42\u51b3\u7b56\u5f88\u6709\u5438\u5f15\u529b\uff0c\u4f46\u76f4\u63a5\u90e8\u7f72\u5728\u56fa\u5b9a\u7ffc\u65e0\u4eba\u673a\u4e0a\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u52a0\u901f\u5ea6\u4e0d\u662f\u76f4\u63a5\u9a71\u52a8\u7684\uff0c\u5fc5\u987b\u5728\u98de\u884c\u5305\u7ebf\u7ea6\u675f\u4e0b\u901a\u8fc7\u59ff\u6001\u548c\u63a8\u529b\u5b9e\u73b0\u3002\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u590d\u6742\u7684\u63a8\u8fdb\u7cfb\u7edf\u5efa\u6a21\u6216\u63a8\u529b\u53f0\u6821\u51c6\u3002", "method": "1. \u6cd5\u5411\u901a\u9053\uff1a\u5728\u5047\u8bbe\u5c0f\u89d2\u5ea6\u6761\u4ef6\u4e0b\uff0c\u63a8\u5bfc\u4ece\u671f\u671b\u6cd5\u5411\u52a0\u901f\u5ea6\u5230\u6eda\u8f6c\u548c\u4fef\u4ef0\u901f\u7387\u547d\u4ee4\u7684\u5de5\u7a0b\u6620\u5c04\uff0c\u8c03\u8282\u5347\u529b\u77e2\u91cf\u7684\u65b9\u5411\u548c\u5927\u5c0f\u30022. \u5207\u5411\u901a\u9053\uff1a\u5f15\u5165\u57fa\u4e8e\u80fd\u91cf\u7684\u516c\u5f0f\uff08\u53d7\u603b\u80fd\u91cf\u63a7\u5236\u542f\u53d1\uff09\uff0c\u76f4\u63a5\u4ece\u98de\u884c\u6570\u636e\u4e2d\u8bc6\u522b\u7ecf\u9a8c\u63a8\u529b-\u80fd\u91cf\u52a0\u901f\u5ea6\u5173\u7cfb\uff0c\u907f\u514d\u663e\u5f0f\u63a8\u8fdb\u5efa\u6a21\u6216\u63a8\u529b\u53f0\u6821\u51c6\u30023. \u8ba8\u8bba\u9971\u548c\u548c\u975e\u6c34\u5e73\u673a\u52a8\u4e0b\u6cd5\u5411\u4e0e\u5207\u5411\u52a0\u901f\u5ea6\u4e4b\u95f4\u7684\u4f18\u5148\u7ea7\u5904\u7406\u3002", "result": "\u5728VTOL\u56fa\u5b9a\u7ffc\u5e73\u53f0\u4e0a\u8fdb\u884c\u4e86\u5927\u91cf\u5b9e\u9645\u98de\u884c\u5b9e\u9a8c\uff0c\u5c55\u793a\u4e86\u7cbe\u786e\u7684\u52a0\u901f\u5ea6\u8ddf\u8e2a\u80fd\u529b\uff0c\u5e76\u5b9e\u73b0\u4e86\u4ec5\u4f7f\u7528\u673a\u4f53\u89d2\u901f\u7387\u548c\u5f52\u4e00\u5316\u63a8\u529b\u63a5\u53e3\u7684\u6bd4\u4f8b\u5bfc\u5f15\u5b9e\u9645\u5e94\u7528\u3002", "conclusion": "\u8be5\u52a0\u901f\u5ea6\u7ea7\u5916\u73af\u63a7\u5236\u6846\u67b6\u6210\u529f\u5730\u5c06\u52a0\u901f\u5ea6\u547d\u4ee4\u8f6c\u6362\u4e3a\u4e0e\u4e3b\u6d41\u81ea\u52a8\u9a7e\u9a76\u4eea\uff08\u5982PX4/APM\uff09\u517c\u5bb9\u7684\u53ef\u6267\u884c\u547d\u4ee4\uff0c\u4f7f\u52a0\u901f\u5ea6\u6307\u4ee4\u5236\u5bfc\u5f8b\u80fd\u591f\u76f4\u63a5\u90e8\u7f72\u5728\u56fa\u5b9a\u7ffc\u65e0\u4eba\u673a\u4e0a\uff0c\u65e0\u9700\u590d\u6742\u7684\u63a8\u8fdb\u7cfb\u7edf\u5efa\u6a21\u3002"}}
{"id": "2602.23676", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23676", "abs": "https://arxiv.org/abs/2602.23676", "authors": ["Ao Li", "Rui Liu", "Mingjie Li", "Sheng Liu", "Lei Wang", "Xiaodan Liang", "Lina Yao", "Xiaojun Chang", "Lei Xing"], "title": "Suppressing Prior-Comparison Hallucinations in Radiology Report Generation via Semantically Decoupled Latent Steering", "comment": "15 pages, 5 figures", "summary": "Automated radiology report generation using vision-language models (VLMs) is limited by the risk of prior-comparison hallucination, where the model generates historical findings unsupported by the current study. We address this challenge with a training-free, inference-time control framework termed Semantically Decoupled Latent Steering (SDLS). Unlike generic activation steering, which often suffers from semantic entanglement, our approach constructs a semantic-free intervention vector via large language model (LLM)-driven semantic decomposition followed by $QR$-based orthogonalization. This orthogonalization step is critical. It leverages geometric constraints to filter out the clinical semantics often entangled in standard principal component analysis (PCA) directions, ensuring that the steering vector targets only the ``historical comparison\" axis. We validate our method on the BiomedGPT foundation model, demonstrating that it overcomes the trade-off between hallucination suppression and clinical accuracy. Extensive experiments on MIMIC-CXR, and zero-shot transfer evaluation on CheXpert Plus and IU-Xray, demonstrate the robustness of our approach. Quantitative evaluations on MIMIC-CXR show that our approach significantly reduces the probability of historical hallucinations (FilBERT score decreases from 0.2373 to 0.1889) and improves clinical label fidelity (CheXpert macro-F1 increases from 0.2242 to 0.3208). Supplementary evaluations confirm that the structural integrity of the clinical narrative is maintained.", "AI": {"tldr": "\u63d0\u51faSDLS\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u89e3\u8026\u7684\u6f5c\u5728\u7a7a\u95f4\u64cd\u63a7\uff0c\u5728\u63a8\u7406\u65f6\u65e0\u8bad\u7ec3\u5730\u51cf\u5c11\u653e\u5c04\u5b66\u62a5\u544a\u751f\u6210\u4e2d\u7684\u5386\u53f2\u6bd4\u8f83\u5e7b\u89c9\uff0c\u540c\u65f6\u4fdd\u6301\u4e34\u5e8a\u51c6\u786e\u6027\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u52a8\u653e\u5c04\u5b66\u62a5\u544a\u751f\u6210\u5b58\u5728\u5386\u53f2\u6bd4\u8f83\u5e7b\u89c9\u95ee\u9898\uff0c\u5373\u6a21\u578b\u4f1a\u751f\u6210\u5f53\u524d\u7814\u7a76\u4e2d\u4e0d\u5b58\u5728\u7684\u5386\u53f2\u53d1\u73b0\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u6291\u5236\u5e7b\u89c9\u548c\u4fdd\u6301\u4e34\u5e8a\u51c6\u786e\u6027\u4e4b\u95f4\u5b58\u5728\u6743\u8861\u3002", "method": "\u63d0\u51fa\u8bed\u4e49\u89e3\u8026\u6f5c\u5728\u64cd\u63a7\uff08SDLS\uff09\u6846\u67b6\uff1a1\uff09\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u8bed\u4e49\u5206\u89e3\uff1b2\uff09\u901a\u8fc7QR\u6b63\u4ea4\u5316\u6784\u5efa\u8bed\u4e49\u65e0\u5173\u7684\u5e72\u9884\u5411\u91cf\uff1b3\uff09\u5229\u7528\u51e0\u4f55\u7ea6\u675f\u8fc7\u6ee4\u4e34\u5e8a\u8bed\u4e49\u7ea0\u7f20\uff0c\u4e13\u95e8\u9488\u5bf9\"\u5386\u53f2\u6bd4\u8f83\"\u8f74\u8fdb\u884c\u64cd\u63a7\u3002", "result": "\u5728BiomedGPT\u6a21\u578b\u4e0a\u9a8c\u8bc1\uff1a1\uff09\u5386\u53f2\u5e7b\u89c9\u6982\u7387\u663e\u8457\u964d\u4f4e\uff08FilBERT\u5206\u6570\u4ece0.2373\u964d\u81f30.1889\uff09\uff1b2\uff09\u4e34\u5e8a\u6807\u7b7e\u4fdd\u771f\u5ea6\u63d0\u9ad8\uff08CheXpert\u5b8fF1\u4ece0.2242\u63d0\u5347\u81f30.3208\uff09\uff1b3\uff09\u5728MIMIC-CXR\u3001CheXpert Plus\u548cIU-Xray\u6570\u636e\u96c6\u4e0a\u5747\u8868\u73b0\u7a33\u5065\u3002", "conclusion": "SDLS\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u653e\u5c04\u5b66\u62a5\u544a\u751f\u6210\u4e2d\u7684\u5386\u53f2\u6bd4\u8f83\u5e7b\u89c9\u95ee\u9898\uff0c\u6253\u7834\u4e86\u5e7b\u89c9\u6291\u5236\u4e0e\u4e34\u5e8a\u51c6\u786e\u6027\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e34\u5e8a\u53d9\u8ff0\u7684\u7ed3\u6784\u5b8c\u6574\u6027\u3002"}}
{"id": "2602.23876", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23876", "abs": "https://arxiv.org/abs/2602.23876", "authors": ["Ning Gao", "Xiuhui Zhang", "Xingyu Jiang", "Mukang You", "Mohan Zhang", "Yue Deng"], "title": "RF-Agent: Automated Reward Function Design via Language Agent Tree Search", "comment": "39 pages, 9 tables, 11 figures, Project page see https://github.com/deng-ai-lab/RF-Agent", "summary": "Designing efficient reward functions for low-level control tasks is a challenging problem. Recent research aims to reduce reliance on expert experience by using Large Language Models (LLMs) with task information to generate dense reward functions. These methods typically rely on training results as feedback, iteratively generating new reward functions with greedy or evolutionary algorithms. However, they suffer from poor utilization of historical feedback and inefficient search, resulting in limited improvements in complex control tasks. To address this challenge, we propose RF-Agent, a framework that treats LLMs as language agents and frames reward function design as a sequential decision-making process, enhancing optimization through better contextual reasoning. RF-Agent integrates Monte Carlo Tree Search (MCTS) to manage the reward design and optimization process, leveraging the multi-stage contextual reasoning ability of LLMs. This approach better utilizes historical information and improves search efficiency to identify promising reward functions. Outstanding experimental results in 17 diverse low-level control tasks demonstrate the effectiveness of our method. The source code is available at https://github.com/deng-ai-lab/RF-Agent.", "AI": {"tldr": "RF-Agent\uff1a\u5c06LLM\u4f5c\u4e3a\u8bed\u8a00\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u6846\u67b6\u8bbe\u8ba1\u5f3a\u5316\u5b66\u4e60\u5956\u52b1\u51fd\u6570\uff0c\u572817\u4e2a\u4f4e\u5c42\u63a7\u5236\u4efb\u52a1\u4e2d\u53d6\u5f97\u4f18\u5f02\u8868\u73b0", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u5956\u52b1\u51fd\u6570\u8bbe\u8ba1\u65b9\u6cd5\u5b58\u5728\u5386\u53f2\u53cd\u9988\u5229\u7528\u4e0d\u8db3\u548c\u641c\u7d22\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u5728\u590d\u6742\u63a7\u5236\u4efb\u52a1\u4e2d\u6539\u8fdb\u6709\u9650", "method": "\u63d0\u51faRF-Agent\u6846\u67b6\uff0c\u5c06LLM\u89c6\u4e3a\u8bed\u8a00\u667a\u80fd\u4f53\uff0c\u5c06\u5956\u52b1\u51fd\u6570\u8bbe\u8ba1\u5efa\u6a21\u4e3a\u5e8f\u5217\u51b3\u7b56\u8fc7\u7a0b\uff0c\u96c6\u6210\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u6765\u7ba1\u7406\u5956\u52b1\u8bbe\u8ba1\u548c\u4f18\u5316\u8fc7\u7a0b", "result": "\u572817\u4e2a\u591a\u6837\u5316\u7684\u4f4e\u5c42\u63a7\u5236\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u51fa\u8272\u7684\u5b9e\u9a8c\u6548\u679c", "conclusion": "RF-Agent\u901a\u8fc7\u66f4\u597d\u7684\u4e0a\u4e0b\u6587\u63a8\u7406\u80fd\u529b\uff0c\u63d0\u9ad8\u4e86\u5386\u53f2\u4fe1\u606f\u5229\u7528\u7387\u548c\u641c\u7d22\u6548\u7387\uff0c\u80fd\u53d1\u73b0\u66f4\u6709\u524d\u666f\u7684\u5956\u52b1\u51fd\u6570"}}
{"id": "2602.23944", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.23944", "abs": "https://arxiv.org/abs/2602.23944", "authors": ["Peng Liu", "Zhen Tao", "Jihao Zhao", "Ding Chen", "Yansong Zhang", "Cuiping Li", "Zhiyu Li", "Hong Chen"], "title": "MemEmo: Evaluating Emotion in Memory Systems of Agents", "comment": null, "summary": "Memory systems address the challenge of context loss in Large Language Model during prolonged interactions. However, compared to human cognition, the efficacy of these systems in processing emotion-related information remains inconclusive. To address this gap, we propose an emotion-enhanced memory evaluation benchmark to assess the performance of mainstream and state-of-the-art memory systems in handling affective information. We developed the \\textbf{H}uman-\\textbf{L}ike \\textbf{M}emory \\textbf{E}motion (\\textbf{HLME}) dataset, which evaluates memory systems across three dimensions: emotional information extraction, emotional memory updating, and emotional memory question answering. Experimental results indicate that none of the evaluated systems achieve robust performance across all three tasks. Our findings provide an objective perspective on the current deficiencies of memory systems in processing emotional memories and suggest a new trajectory for future research and system optimization.", "AI": {"tldr": "\u63d0\u51fa\u60c5\u611f\u589e\u5f3a\u8bb0\u5fc6\u8bc4\u4f30\u57fa\u51c6HLME\uff0c\u8bc4\u4f30\u4e3b\u6d41\u8bb0\u5fc6\u7cfb\u7edf\u5904\u7406\u60c5\u611f\u4fe1\u606f\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u73b0\u6709\u7cfb\u7edf\u5728\u60c5\u611f\u4fe1\u606f\u63d0\u53d6\u3001\u66f4\u65b0\u548c\u95ee\u7b54\u65b9\u9762\u5747\u4e0d\u5b8c\u5584\u3002", "motivation": "\u5f53\u524dLLM\u8bb0\u5fc6\u7cfb\u7edf\u5728\u957f\u65f6\u95f4\u4ea4\u4e92\u4e2d\u5b58\u5728\u4e0a\u4e0b\u6587\u4e22\u5931\u95ee\u9898\uff0c\u4f46\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u76f8\u6bd4\uff0c\u8fd9\u4e9b\u7cfb\u7edf\u5904\u7406\u60c5\u611f\u76f8\u5173\u4fe1\u606f\u7684\u6709\u6548\u6027\u5c1a\u4e0d\u660e\u786e\uff0c\u9700\u8981\u4e13\u95e8\u8bc4\u4f30\u57fa\u51c6\u6765\u586b\u8865\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u60c5\u611f\u589e\u5f3a\u8bb0\u5fc6\u8bc4\u4f30\u57fa\u51c6\uff0c\u5f00\u53d1HLME\u6570\u636e\u96c6\uff0c\u4ece\u4e09\u4e2a\u7ef4\u5ea6\u8bc4\u4f30\u8bb0\u5fc6\u7cfb\u7edf\uff1a\u60c5\u611f\u4fe1\u606f\u63d0\u53d6\u3001\u60c5\u611f\u8bb0\u5fc6\u66f4\u65b0\u3001\u60c5\u611f\u8bb0\u5fc6\u95ee\u7b54\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u6709\u8bc4\u4f30\u7684\u8bb0\u5fc6\u7cfb\u7edf\u5728\u4e09\u4e2a\u4efb\u52a1\u4e2d\u5747\u672a\u8fbe\u5230\u7a33\u5065\u6027\u80fd\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u8bb0\u5fc6\u7cfb\u7edf\u5728\u5904\u7406\u60c5\u611f\u8bb0\u5fc6\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "conclusion": "\u7814\u7a76\u4e3a\u8bb0\u5fc6\u7cfb\u7edf\u5904\u7406\u60c5\u611f\u8bb0\u5fc6\u7684\u5f53\u524d\u7f3a\u9677\u63d0\u4f9b\u4e86\u5ba2\u89c2\u89c6\u89d2\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u548c\u7cfb\u7edf\u4f18\u5316\u6307\u51fa\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2602.23832", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.23832", "abs": "https://arxiv.org/abs/2602.23832", "authors": ["Yuhan Li", "Peiyuan Zhi", "Yunshen Wang", "Tengyu Liu", "Sixu Yan", "Wenyu Liu", "Xinggang Wang", "Baoxiong Jia", "Siyuan Huang"], "title": "OmniTrack: General Motion Tracking via Physics-Consistent Reference", "comment": "website: https://omnitrack-humanoid.github.io/", "summary": "Learning motion tracking from rich human motion data is a foundational task for achieving general control in humanoid robots, enabling them to perform diverse behaviors. However, discrepancies in morphology and dynamics between humans and robots, combined with data noise, introduce physically infeasible artifacts in reference motions, such as floating and penetration. During both training and execution, these artifacts create a conflict between following inaccurate reference motions and maintaining the robot's stability, hindering the development of a generalizable motion tracking policy. To address these challenges, we introduce OmniTrack, a general tracking framework that explicitly decouples physical feasibility from general motion tracking. In the first stage, a privileged generalist policy generates physically plausible motions that strictly adhere to the robot's dynamics via trajectory rollout in simulation. In the second stage, the general control policy is trained to track these physically feasible motions, ensuring stable and coherent control transfer to the real robot. Experiments show that OmniTrack improves tracking accuracy and demonstrates strong generalization to unseen motions. In real-world tests, OmniTrack achieves hour-long, consistent, and stable tracking, including complex acrobatic motions such as flips and cartwheels. Additionally, we show that OmniTrack supports human-style stable and dynamic online teleoperation, highlighting its robustness and adaptability to varying user inputs.", "AI": {"tldr": "OmniTrack\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u7269\u7406\u53ef\u884c\u6027\u548c\u8fd0\u52a8\u8ddf\u8e2a\uff0c\u89e3\u51b3\u4eba\u5f62\u673a\u5668\u4eba\u8fd0\u52a8\u8ddf\u8e2a\u4e2d\u56e0\u5f62\u6001\u5dee\u5f02\u548c\u6570\u636e\u566a\u58f0\u5bfc\u81f4\u7684\u7269\u7406\u4e0d\u53ef\u884c\u95ee\u9898\uff0c\u5b9e\u73b0\u7a33\u5b9a\u3001\u901a\u7528\u7684\u8fd0\u52a8\u63a7\u5236\u3002", "motivation": "\u4eba\u7c7b\u4e0e\u673a\u5668\u4eba\u4e4b\u95f4\u7684\u5f62\u6001\u548c\u52a8\u529b\u5b66\u5dee\u5f02\uff0c\u52a0\u4e0a\u6570\u636e\u566a\u58f0\uff0c\u4f1a\u5bfc\u81f4\u53c2\u8003\u8fd0\u52a8\u4e2d\u51fa\u73b0\u7269\u7406\u4e0d\u53ef\u884c\u7684\u4f2a\u5f71\uff08\u5982\u6f02\u6d6e\u548c\u7a7f\u900f\uff09\u3002\u8fd9\u4e9b\u4f2a\u5f71\u5728\u8bad\u7ec3\u548c\u6267\u884c\u8fc7\u7a0b\u4e2d\u9020\u6210\u8ddf\u8e2a\u4e0d\u51c6\u786e\u53c2\u8003\u8fd0\u52a8\u4e0e\u4fdd\u6301\u673a\u5668\u4eba\u7a33\u5b9a\u6027\u4e4b\u95f4\u7684\u51b2\u7a81\uff0c\u963b\u788d\u4e86\u901a\u7528\u8fd0\u52a8\u8ddf\u8e2a\u7b56\u7565\u7684\u53d1\u5c55\u3002", "method": "OmniTrack\u91c7\u7528\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\uff0c\u7279\u6743\u901a\u7528\u7b56\u7565\u901a\u8fc7\u4eff\u771f\u4e2d\u7684\u8f68\u8ff9\u5c55\u5f00\u751f\u6210\u4e25\u683c\u9075\u5faa\u673a\u5668\u4eba\u52a8\u529b\u5b66\u7684\u7269\u7406\u53ef\u884c\u8fd0\u52a8\uff1b\u7b2c\u4e8c\u9636\u6bb5\uff0c\u901a\u7528\u63a7\u5236\u7b56\u7565\u88ab\u8bad\u7ec3\u6765\u8ddf\u8e2a\u8fd9\u4e9b\u7269\u7406\u53ef\u884c\u7684\u8fd0\u52a8\uff0c\u786e\u4fdd\u7a33\u5b9a\u548c\u8fde\u8d2f\u7684\u63a7\u5236\u8fc1\u79fb\u5230\u771f\u5b9e\u673a\u5668\u4eba\u3002", "result": "\u5b9e\u9a8c\u8868\u660eOmniTrack\u63d0\u9ad8\u4e86\u8ddf\u8e2a\u7cbe\u5ea6\uff0c\u5e76\u5bf9\u672a\u89c1\u8fd0\u52a8\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002\u5728\u771f\u5b9e\u4e16\u754c\u6d4b\u8bd5\u4e2d\uff0cOmniTrack\u5b9e\u73b0\u4e86\u957f\u8fbe\u6570\u5c0f\u65f6\u7684\u4e00\u81f4\u7a33\u5b9a\u8ddf\u8e2a\uff0c\u5305\u62ec\u590d\u6742\u7684\u6742\u6280\u52a8\u4f5c\u5982\u7a7a\u7ffb\u548c\u4fa7\u624b\u7ffb\u3002\u6b64\u5916\uff0cOmniTrack\u652f\u6301\u4eba\u7c7b\u98ce\u683c\u7684\u7a33\u5b9a\u52a8\u6001\u5728\u7ebf\u9065\u64cd\u4f5c\uff0c\u7a81\u663e\u5176\u5bf9\u4e0d\u540c\u7528\u6237\u8f93\u5165\u7684\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u3002", "conclusion": "OmniTrack\u901a\u8fc7\u660e\u786e\u89e3\u8026\u7269\u7406\u53ef\u884c\u6027\u548c\u901a\u7528\u8fd0\u52a8\u8ddf\u8e2a\uff0c\u89e3\u51b3\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u8fd0\u52a8\u63a7\u5236\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u7a33\u5b9a\u3001\u901a\u7528\u4e14\u53ef\u8fc1\u79fb\u5230\u771f\u5b9e\u673a\u5668\u4eba\u7684\u8fd0\u52a8\u8ddf\u8e2a\u6846\u67b6\u3002"}}
{"id": "2602.23677", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23677", "abs": "https://arxiv.org/abs/2602.23677", "authors": ["Nazia Hossain", "Xintong Jiang", "Yu Tian", "Philippe Seguin", "O. Grant Clark", "Shangpeng Sun"], "title": "Vision-Language Semantic Grounding for Multi-Domain Crop-Weed Segmentation", "comment": null, "summary": "Fine-grained crop-weed segmentation is essential for enabling targeted herbicide application in precision agriculture. However, existing deep learning models struggle to generalize across heterogeneous agricultural environments due to reliance on dataset-specific visual features. We propose Vision-Language Weed Segmentation (VL-WS), a novel framework that addresses this limitation by grounding pixel-level segmentation in semantically aligned, domain-invariant representations. Our architecture employs a dual-encoder design, where frozen Contrastive Language-Image Pretraining (CLIP) embeddings and task-specific spatial features are fused and modulated via Feature-wise Linear Modulation (FiLM) layers conditioned on natural language captions. This design enables image level textual descriptions to guide channel-wise feature refinement while preserving fine-grained spatial localization. Unlike prior works restricted to training and evaluation on single-source datasets, VL-WS is trained on a unified corpus that includes close-range ground imagery (robotic platforms) and high-altitude UAV imagery, covering diverse crop types, weed species, growth stages, and sensing conditions. Experimental results across four benchmark datasets demonstrate the effectiveness of our framework, with VL-WS achieving a mean Dice score of 91.64% and outperforming the CNN baseline by 4.98%. The largest gains occur on the most challenging weed class, where VL-WS attains 80.45% Dice score compared to 65.03% for the best baseline, representing a 15.42% improvement. VL-WS further maintains stable weed segmentation performance under limited target-domain supervision, indicating improved generalization and data efficiency. These findings highlight the potential of vision-language alignment to enable scalable, label-efficient segmentation models deployable across diverse real-world agricultural domains.", "AI": {"tldr": "VL-WS\uff1a\u57fa\u4e8e\u89c6\u89c9-\u8bed\u8a00\u5bf9\u9f50\u7684\u7ec6\u7c92\u5ea6\u6742\u8349\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7CLIP\u5d4c\u5165\u548c\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u5f15\u5bfc\u7684\u7279\u5f81\u8c03\u5236\uff0c\u5728\u5f02\u6784\u519c\u4e1a\u73af\u5883\u4e2d\u5b9e\u73b0\u66f4\u597d\u7684\u6cdb\u5316\u6027\u80fd", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4f9d\u8d56\u6570\u636e\u96c6\u7279\u5b9a\u7684\u89c6\u89c9\u7279\u5f81\uff0c\u96be\u4ee5\u5728\u5f02\u6784\u519c\u4e1a\u73af\u5883\u4e2d\u6cdb\u5316\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5229\u7528\u8bed\u4e49\u5bf9\u9f50\u3001\u9886\u57df\u4e0d\u53d8\u8868\u793a\u7684\u65b9\u6cd5\u6765\u6539\u8fdb\u7ec6\u7c92\u5ea6\u4f5c\u7269-\u6742\u8349\u5206\u5272", "method": "\u63d0\u51faVL-WS\u6846\u67b6\uff0c\u91c7\u7528\u53cc\u7f16\u7801\u5668\u8bbe\u8ba1\uff1a\u51bb\u7ed3\u7684CLIP\u5d4c\u5165\u4e0e\u4efb\u52a1\u7279\u5b9a\u7a7a\u95f4\u7279\u5f81\u901a\u8fc7FiLM\u5c42\u878d\u5408\uff0c\u8be5\u5c42\u7531\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u6761\u4ef6\u5316\u3002\u5728\u5305\u542b\u5730\u9762\u56fe\u50cf\u548c\u65e0\u4eba\u673a\u56fe\u50cf\u7684\u7edf\u4e00\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5e73\u5747Dice\u5206\u6570\u8fbe91.64%\uff0c\u6bd4CNN\u57fa\u7ebf\u63d0\u53474.98%\u3002\u6700\u5177\u6311\u6218\u6027\u7684\u6742\u8349\u7c7b\u522b\u63d0\u534715.42%\uff0880.45% vs 65.03%\uff09\u3002\u5728\u6709\u9650\u76ee\u6807\u57df\u76d1\u7763\u4e0b\u4fdd\u6301\u7a33\u5b9a\u6027\u80fd", "conclusion": "\u89c6\u89c9-\u8bed\u8a00\u5bf9\u9f50\u80fd\u591f\u5b9e\u73b0\u53ef\u6269\u5c55\u3001\u6807\u7b7e\u9ad8\u6548\u7684\u8de8\u9886\u57df\u5206\u5272\u6a21\u578b\uff0c\u4e3a\u5b9e\u9645\u519c\u4e1a\u5e94\u7528\u63d0\u4f9b\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6570\u636e\u6548\u7387"}}
{"id": "2602.23614", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23614", "abs": "https://arxiv.org/abs/2602.23614", "authors": ["Kejing Yin", "Haizhou Xu", "Wenfang Yao", "Chen Liu", "Zijie Chen", "Yui Haang Cheung", "William K. Cheung", "Jing Qin"], "title": "When Does Multimodal Learning Help in Healthcare? A Benchmark on EHR and Chest X-Ray Fusion", "comment": null, "summary": "Machine learning holds promise for advancing clinical decision support, yet it remains unclear when multimodal learning truly helps in practice, particularly under modality missingness and fairness constraints. In this work, we conduct a systematic benchmark of multimodal fusion between Electronic Health Records (EHR) and chest X-rays (CXR) on standardized cohorts from MIMIC-IV and MIMIC-CXR, aiming to answer four fundamental questions: when multimodal fusion improves clinical prediction, how different fusion strategies compare, how robust existing methods are to missing modalities, and whether multimodal models achieve algorithmic fairness. Our study reveals several key insights. Multimodal fusion improves performance when modalities are complete, with gains concentrating in diseases that require complementary information from both EHR and CXR. While cross-modal learning mechanisms capture clinically meaningful dependencies beyond simple concatenation, the rich temporal structure of EHR introduces strong modality imbalance that architectural complexity alone cannot overcome. Under realistic missingness, multimodal benefits rapidly degrade unless models are explicitly designed to handle incomplete inputs. Moreover, multimodal fusion does not inherently improve fairness, with subgroup disparities mainly arising from unequal sensitivity across demographic groups. To support reproducible and extensible evaluation, we further release a flexible benchmarking toolkit that enables plug-and-play integration of new models and datasets. Together, this work provides actionable guidance on when multimodal learning helps, when it fails, and why, laying the foundation for developing clinically deployable multimodal systems that are both effective and reliable. The open-source toolkit can be found at https://github.com/jakeykj/CareBench.", "AI": {"tldr": "\u8be5\u7814\u7a76\u7cfb\u7edf\u8bc4\u4f30\u4e86\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u548c\u80f8\u90e8X\u5149\uff08CXR\uff09\u7684\u591a\u6a21\u6001\u878d\u5408\u5728\u4e34\u5e8a\u9884\u6d4b\u4e2d\u7684\u6548\u679c\uff0c\u5206\u6790\u4e86\u878d\u5408\u7b56\u7565\u3001\u6a21\u6001\u7f3a\u5931\u9c81\u68d2\u6027\u548c\u7b97\u6cd5\u516c\u5e73\u6027\uff0c\u5e76\u53d1\u5e03\u4e86\u5f00\u6e90\u57fa\u51c6\u5de5\u5177\u5305\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u5728\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u591a\u6a21\u6001\u5b66\u4e60\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u4f55\u65f6\u771f\u6b63\u6709\u6548\u5c1a\u4e0d\u660e\u786e\uff0c\u7279\u522b\u662f\u5728\u6a21\u6001\u7f3a\u5931\u548c\u516c\u5e73\u6027\u7ea6\u675f\u4e0b\u3002\u672c\u7814\u7a76\u65e8\u5728\u56de\u7b54\u56db\u4e2a\u57fa\u672c\u95ee\u9898\uff1a\u591a\u6a21\u6001\u878d\u5408\u4f55\u65f6\u80fd\u6539\u5584\u4e34\u5e8a\u9884\u6d4b\u3001\u4e0d\u540c\u878d\u5408\u7b56\u7565\u7684\u6bd4\u8f83\u3001\u73b0\u6709\u65b9\u6cd5\u5bf9\u7f3a\u5931\u6a21\u6001\u7684\u9c81\u68d2\u6027\uff0c\u4ee5\u53ca\u591a\u6a21\u6001\u6a21\u578b\u662f\u5426\u80fd\u5b9e\u73b0\u7b97\u6cd5\u516c\u5e73\u3002", "method": "\u4f7f\u7528MIMIC-IV\u548cMIMIC-CXR\u6807\u51c6\u5316\u961f\u5217\uff0c\u7cfb\u7edf\u6027\u5730\u5bf9EHR\u548cCXR\u7684\u591a\u6a21\u6001\u878d\u5408\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002\u7814\u7a76\u6bd4\u8f83\u4e86\u4e0d\u540c\u7684\u878d\u5408\u7b56\u7565\uff0c\u8bc4\u4f30\u4e86\u6a21\u578b\u5bf9\u6a21\u6001\u7f3a\u5931\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u5206\u6790\u4e86\u7b97\u6cd5\u516c\u5e73\u6027\u3002\u540c\u65f6\u5f00\u53d1\u5e76\u53d1\u5e03\u4e86\u652f\u6301\u53ef\u63d2\u62d4\u6a21\u578b\u548c\u6570\u636e\u96c6\u96c6\u6210\u7684\u7075\u6d3b\u57fa\u51c6\u6d4b\u8bd5\u5de5\u5177\u5305\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a1\uff09\u6a21\u6001\u5b8c\u6574\u65f6\u591a\u6a21\u6001\u878d\u5408\u80fd\u63d0\u5347\u6027\u80fd\uff0c\u589e\u76ca\u4e3b\u8981\u96c6\u4e2d\u5728\u9700\u8981EHR\u548cCXR\u4e92\u8865\u4fe1\u606f\u7684\u75be\u75c5\u4e0a\uff1b2\uff09\u8de8\u6a21\u6001\u5b66\u4e60\u673a\u5236\u80fd\u6355\u83b7\u8d85\u8d8a\u7b80\u5355\u62fc\u63a5\u7684\u4e34\u5e8a\u76f8\u5173\u4f9d\u8d56\u5173\u7cfb\uff0c\u4f46EHR\u7684\u4e30\u5bcc\u65f6\u95f4\u7ed3\u6784\u5bfc\u81f4\u5f3a\u70c8\u7684\u6a21\u6001\u4e0d\u5e73\u8861\uff1b3\uff09\u5728\u5b9e\u9645\u7f3a\u5931\u60c5\u51b5\u4e0b\uff0c\u591a\u6a21\u6001\u4f18\u52bf\u8fc5\u901f\u4e0b\u964d\uff0c\u9664\u975e\u6a21\u578b\u4e13\u95e8\u8bbe\u8ba1\u5904\u7406\u4e0d\u5b8c\u6574\u8f93\u5165\uff1b4\uff09\u591a\u6a21\u6001\u878d\u5408\u5e76\u4e0d\u56fa\u6709\u5730\u6539\u5584\u516c\u5e73\u6027\uff0c\u4e9a\u7ec4\u5dee\u5f02\u4e3b\u8981\u6e90\u4e8e\u4e0d\u540c\u4eba\u53e3\u7fa4\u4f53\u95f4\u7684\u654f\u611f\u6027\u4e0d\u5e73\u7b49\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u591a\u6a21\u6001\u5b66\u4e60\u4f55\u65f6\u6709\u6548\u3001\u4f55\u65f6\u5931\u8d25\u4ee5\u53ca\u539f\u56e0\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u6307\u5bfc\uff0c\u4e3a\u5f00\u53d1\u65e2\u6709\u6548\u53c8\u53ef\u9760\u7684\u4e34\u5e8a\u53ef\u90e8\u7f72\u591a\u6a21\u6001\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002\u5f00\u6e90\u5de5\u5177\u5305\u652f\u6301\u53ef\u91cd\u73b0\u548c\u53ef\u6269\u5c55\u7684\u8bc4\u4f30\u3002"}}
{"id": "2602.23974", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23974", "abs": "https://arxiv.org/abs/2602.23974", "authors": ["Fan Zhang", "Baoru Huang", "Xin Zhang"], "title": "Pessimistic Auxiliary Policy for Offline Reinforcement Learning", "comment": null, "summary": "Offline reinforcement learning aims to learn an agent from pre-collected datasets, avoiding unsafe and inefficient real-time interaction. However, inevitable access to out-ofdistribution actions during the learning process introduces approximation errors, causing the error accumulation and considerable overestimation. In this paper, we construct a new pessimistic auxiliary policy for sampling reliable actions. Specifically, we develop a pessimistic auxiliary strategy by maximizing the lower confidence bound of the Q-function. The pessimistic auxiliary strategy exhibits a relatively high value and low uncertainty in the vicinity of the learned policy, avoiding the learned policy sampling high-value actions with potentially high errors during the learning process. Less approximation error introduced by sampled action from pessimistic auxiliary strategy leads to the alleviation of error accumulation. Extensive experiments on offline reinforcement learning benchmarks reveal that utilizing the pessimistic auxiliary strategy can effectively improve the efficacy of other offline RL approaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u60b2\u89c2\u8f85\u52a9\u7b56\u7565\uff0c\u901a\u8fc7\u6700\u5927\u5316Q\u51fd\u6570\u7684\u4e0b\u7f6e\u4fe1\u754c\u6765\u91c7\u6837\u53ef\u9760\u52a8\u4f5c\uff0c\u7f13\u89e3\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u5206\u5e03\u5916\u52a8\u4f5c\u5bfc\u81f4\u7684\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898\u3002", "motivation": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4ece\u9884\u6536\u96c6\u7684\u6570\u636e\u96c6\u4e2d\u5b66\u4e60\u667a\u80fd\u4f53\uff0c\u907f\u514d\u4e86\u5b9e\u65f6\u4ea4\u4e92\u7684\u4e0d\u5b89\u5168\u6027\u548c\u4f4e\u6548\u7387\u3002\u7136\u800c\uff0c\u5728\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u4e0d\u53ef\u907f\u514d\u5730\u8bbf\u95ee\u5206\u5e03\u5916\u52a8\u4f5c\u4f1a\u5f15\u5165\u8fd1\u4f3c\u8bef\u5dee\uff0c\u5bfc\u81f4\u8bef\u5dee\u7d2f\u79ef\u548c\u4e25\u91cd\u7684\u9ad8\u4f30\u95ee\u9898\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684\u60b2\u89c2\u8f85\u52a9\u7b56\u7565\u6765\u91c7\u6837\u53ef\u9760\u52a8\u4f5c\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u901a\u8fc7\u6700\u5927\u5316Q\u51fd\u6570\u7684\u4e0b\u7f6e\u4fe1\u754c\u6765\u5f00\u53d1\u60b2\u89c2\u8f85\u52a9\u7b56\u7565\u3002\u8be5\u7b56\u7565\u5728\u5b66\u4e60\u7b56\u7565\u9644\u8fd1\u8868\u73b0\u51fa\u76f8\u5bf9\u8f83\u9ad8\u7684\u4ef7\u503c\u548c\u8f83\u4f4e\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u907f\u514d\u4e86\u5b66\u4e60\u7b56\u7565\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u91c7\u6837\u5177\u6709\u6f5c\u5728\u9ad8\u8bef\u5dee\u7684\u9ad8\u4ef7\u503c\u52a8\u4f5c\u3002", "result": "\u60b2\u89c2\u8f85\u52a9\u7b56\u7565\u5f15\u5165\u7684\u8fd1\u4f3c\u8bef\u5dee\u8f83\u5c11\uff0c\u4ece\u800c\u7f13\u89e3\u4e86\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898\u3002\u5728\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u5229\u7528\u60b2\u89c2\u8f85\u52a9\u7b56\u7565\u53ef\u4ee5\u6709\u6548\u63d0\u9ad8\u5176\u4ed6\u79bb\u7ebfRL\u65b9\u6cd5\u7684\u6548\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u60b2\u89c2\u8f85\u52a9\u7b56\u7565\u901a\u8fc7\u91c7\u6837\u53ef\u9760\u52a8\u4f5c\u6765\u51cf\u5c11\u5206\u5e03\u5916\u52a8\u4f5c\u5f15\u5165\u7684\u8fd1\u4f3c\u8bef\u5dee\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002"}}
{"id": "2602.23993", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.23993", "abs": "https://arxiv.org/abs/2602.23993", "authors": ["Jonathan Drechsel", "Steffen Herbold"], "title": "The GRADIEND Python Package: An End-to-End System for Gradient-Based Feature Learning", "comment": null, "summary": "We present gradiend, an open-source Python package that operationalizes the GRADIEND method for learning feature directions from factual-counterfactual MLM and CLM gradients in language models. The package provides a unified workflow for feature-related data creation, training, evaluation, visualization, persistent model rewriting via controlled weight updates, and multi-feature comparison. We demonstrate GRADIEND on an English pronoun paradigm and on a large-scale feature comparison that reproduces prior use cases.", "AI": {"tldr": "gradiend\u662f\u4e00\u4e2a\u5f00\u6e90Python\u5305\uff0c\u5b9e\u73b0\u4e86GRADIEND\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u8bed\u8a00\u6a21\u578b\u7684\u4e8b\u5b9e-\u53cd\u4e8b\u5b9eMLM\u548cCLM\u68af\u5ea6\u4e2d\u5b66\u4e60\u7279\u5f81\u65b9\u5411\uff0c\u63d0\u4f9b\u4ece\u6570\u636e\u521b\u5efa\u5230\u6a21\u578b\u91cd\u5199\u7684\u5b8c\u6574\u5de5\u4f5c\u6d41\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u7edf\u4e00\u7684\u5de5\u4f5c\u6d41\u7a0b\u6765\u64cd\u4f5c\u5316GRADIEND\u65b9\u6cd5\uff0c\u4f7f\u7814\u7a76\u4eba\u5458\u80fd\u591f\u7cfb\u7edf\u5730\u5b66\u4e60\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u7279\u5f81\u65b9\u5411\uff0c\u5e76\u8fdb\u884c\u6a21\u578b\u91cd\u5199\u548c\u7279\u5f81\u6bd4\u8f83\u3002", "method": "\u57fa\u4e8e\u4e8b\u5b9e-\u53cd\u4e8b\u5b9e\u7684MLM\uff08\u63a9\u7801\u8bed\u8a00\u5efa\u6a21\uff09\u548cCLM\uff08\u56e0\u679c\u8bed\u8a00\u5efa\u6a21\uff09\u68af\u5ea6\uff0c\u5b66\u4e60\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u7279\u5f81\u65b9\u5411\uff0c\u901a\u8fc7\u53d7\u63a7\u7684\u6743\u91cd\u66f4\u65b0\u5b9e\u73b0\u6301\u4e45\u6027\u6a21\u578b\u91cd\u5199\u3002", "result": "\u6210\u529f\u5f00\u53d1\u4e86gradiend\u5f00\u6e90\u5305\uff0c\u5e76\u5728\u82f1\u8bed\u4ee3\u8bcd\u8303\u4f8b\u548c\u5927\u89c4\u6a21\u7279\u5f81\u6bd4\u8f83\u4e2d\u5c55\u793a\u4e86GRADIEND\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u590d\u73b0\u4e86\u5148\u524d\u7684\u7814\u7a76\u7528\u4f8b\u3002", "conclusion": "gradiend\u5305\u4e3a\u8bed\u8a00\u6a21\u578b\u7279\u5f81\u65b9\u5411\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u7684\u5de5\u5177\uff0c\u652f\u6301\u4ece\u6570\u636e\u521b\u5efa\u5230\u6a21\u578b\u91cd\u5199\u7684\u5b8c\u6574\u5de5\u4f5c\u6d41\u7a0b\uff0c\u6709\u52a9\u4e8e\u7cfb\u7edf\u6027\u5730\u7814\u7a76\u548c\u4fee\u6539\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u7279\u5f81\u8868\u793a\u3002"}}
{"id": "2602.23843", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.23843", "abs": "https://arxiv.org/abs/2602.23843", "authors": ["Yunshen Wang", "Shaohang Zhu", "Peiyuan Zhi", "Yuhan Li", "Jiaxin Li", "Yong-Lu Li", "Yuchen Xiao", "Xingxing Wang", "Baoxiong Jia", "Siyuan Huang"], "title": "OmniXtreme: Breaking the Generality Barrier in High-Dynamic Humanoid Control", "comment": null, "summary": "High-fidelity motion tracking serves as the ultimate litmus test for generalizable, human-level motor skills. However, current policies often hit a \"generality barrier\": as motion libraries scale in diversity, tracking fidelity inevitably collapses - especially for real-world deployment of high-dynamic motions. We identify this failure as the result of two compounding factors: the learning bottleneck in scaling multi-motion optimization and the physical executability constraints that arise in real-world actuation. To overcome these challenges, we introduce OmniXtreme, a scalable framework that decouples general motor skill learning from sim-to-real physical skill refinement. Our approach uses a flow-matching policy with high-capacity architectures to scale representation capacity without interference-intensive multi-motion RL optimization, followed by an actuation-aware refinement phase that ensures robust performance on physical hardware. Extensive experiments demonstrate that OmniXtreme maintains high-fidelity tracking across diverse, high-difficulty datasets. On real robots, the unified policy successfully executes multiple extreme motions, effectively breaking the long-standing fidelity-scalability trade-off in high-dynamic humanoid control.", "AI": {"tldr": "OmniXtreme\u6846\u67b6\u901a\u8fc7\u89e3\u8026\u901a\u7528\u8fd0\u52a8\u6280\u80fd\u5b66\u4e60\u4e0e\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u7269\u7406\u6280\u80fd\u7cbe\u70bc\uff0c\u89e3\u51b3\u4e86\u9ad8\u52a8\u6001\u4eba\u5f62\u63a7\u5236\u4e2d\u4fdd\u771f\u5ea6\u4e0e\u53ef\u6269\u5c55\u6027\u7684\u957f\u671f\u6743\u8861\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u7b56\u7565\u5728\u8fd0\u52a8\u5e93\u591a\u6837\u6027\u6269\u5c55\u65f6\u9762\u4e34\"\u901a\u7528\u6027\u969c\u788d\"\uff1a\u968f\u7740\u8fd0\u52a8\u5e93\u89c4\u6a21\u6269\u5927\uff0c\u8ddf\u8e2a\u4fdd\u771f\u5ea6\u4e0d\u53ef\u907f\u514d\u5730\u5d29\u6e83\uff0c\u7279\u522b\u662f\u5728\u9ad8\u52a8\u6001\u8fd0\u52a8\u7684\u73b0\u5b9e\u90e8\u7f72\u4e2d\u3002\u8fd9\u6e90\u4e8e\u4e24\u4e2a\u590d\u5408\u56e0\u7d20\uff1a\u591a\u8fd0\u52a8\u4f18\u5316\u7684\u5b66\u4e60\u74f6\u9888\u548c\u73b0\u5b9e\u6267\u884c\u4e2d\u7684\u7269\u7406\u53ef\u6267\u884c\u6027\u7ea6\u675f\u3002", "method": "\u5f15\u5165OmniXtreme\u6846\u67b6\uff0c\u5c06\u901a\u7528\u8fd0\u52a8\u6280\u80fd\u5b66\u4e60\u4e0e\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u7269\u7406\u6280\u80fd\u7cbe\u70bc\u89e3\u8026\u3002\u4f7f\u7528\u5177\u6709\u9ad8\u5bb9\u91cf\u67b6\u6784\u7684\u6d41\u5339\u914d\u7b56\u7565\u6269\u5c55\u8868\u793a\u80fd\u529b\uff0c\u907f\u514d\u5e72\u6270\u5bc6\u96c6\u7684\u591a\u8fd0\u52a8RL\u4f18\u5316\uff0c\u7136\u540e\u8fdb\u884c\u6267\u884c\u5668\u611f\u77e5\u7684\u7cbe\u70bc\u9636\u6bb5\uff0c\u786e\u4fdd\u5728\u7269\u7406\u786c\u4ef6\u4e0a\u7684\u9c81\u68d2\u6027\u80fd\u3002", "result": "\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cOmniXtreme\u5728\u591a\u6837\u9ad8\u96be\u5ea6\u6570\u636e\u96c6\u4e0a\u4fdd\u6301\u9ad8\u4fdd\u771f\u5ea6\u8ddf\u8e2a\u3002\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\uff0c\u7edf\u4e00\u7b56\u7565\u6210\u529f\u6267\u884c\u591a\u4e2a\u6781\u7aef\u8fd0\u52a8\uff0c\u6709\u6548\u6253\u7834\u4e86\u9ad8\u52a8\u6001\u4eba\u5f62\u63a7\u5236\u4e2d\u957f\u671f\u5b58\u5728\u7684\u4fdd\u771f\u5ea6-\u53ef\u6269\u5c55\u6027\u6743\u8861\u3002", "conclusion": "OmniXtreme\u901a\u8fc7\u89e3\u8026\u5b66\u4e60\u4e0e\u7cbe\u70bc\u7684\u65b9\u6cd5\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u9ad8\u52a8\u6001\u4eba\u5f62\u63a7\u5236\u4e2d\u7684\u4fdd\u771f\u5ea6-\u53ef\u6269\u5c55\u6027\u6743\u8861\u95ee\u9898\uff0c\u4e3a\u901a\u7528\u3001\u4eba\u7c7b\u7ea7\u522b\u8fd0\u52a8\u6280\u80fd\u7684\u89c4\u6a21\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2602.23678", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23678", "abs": "https://arxiv.org/abs/2602.23678", "authors": ["Dingqi Ye", "Daniel Kiv", "Wei Hu", "Jimeng Shi", "Shaowen Wang"], "title": "Any Model, Any Place, Any Time: Get Remote Sensing Foundation Model Embeddings On Demand", "comment": null, "summary": "The remote sensing community is witnessing a rapid growth of foundation models, which provide powerful embeddings for a wide range of downstream tasks. However, practical adoption and fair comparison remain challenging due to substantial heterogeneity in model release formats, platforms and interfaces, and input data specifications. These inconsistencies significantly increase the cost of obtaining, using, and benchmarking embeddings across models. To address this issue, we propose rs-embed, a Python library that offers a unified, region of interst (ROI) centric interface: with a single line of code, users can retrieve embeddings from any supported model for any location and any time range. The library also provides efficient batch processing to enable large-scale embedding generation and evaluation. The code is available at: https://github.com/cybergis/rs-embed", "AI": {"tldr": "rs-embed\u662f\u4e00\u4e2aPython\u5e93\uff0c\u4e3a\u9065\u611f\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u7edf\u4e00\u7684ROI\u4e2d\u5fc3\u63a5\u53e3\uff0c\u7b80\u5316\u5d4c\u5165\u83b7\u53d6\u548c\u6bd4\u8f83\u8fc7\u7a0b", "motivation": "\u9065\u611f\u57fa\u7840\u6a21\u578b\u5feb\u901f\u589e\u957f\uff0c\u4f46\u6a21\u578b\u53d1\u5e03\u683c\u5f0f\u3001\u5e73\u53f0\u63a5\u53e3\u548c\u6570\u636e\u89c4\u8303\u7684\u5f02\u6784\u6027\u5bfc\u81f4\u5b9e\u9645\u5e94\u7528\u548c\u516c\u5e73\u6bd4\u8f83\u56f0\u96be\uff0c\u589e\u52a0\u4e86\u83b7\u53d6\u3001\u4f7f\u7528\u548c\u57fa\u51c6\u6d4b\u8bd5\u7684\u6210\u672c", "method": "\u5f00\u53d1rs-embed Python\u5e93\uff0c\u63d0\u4f9b\u7edf\u4e00\u7684ROI\u4e2d\u5fc3\u63a5\u53e3\uff0c\u652f\u6301\u5355\u884c\u4ee3\u7801\u4ece\u4efb\u4f55\u652f\u6301\u6a21\u578b\u83b7\u53d6\u4efb\u4f55\u4f4d\u7f6e\u548c\u65f6\u95f4\u8303\u56f4\u7684\u5d4c\u5165\uff0c\u5e76\u63d0\u4f9b\u9ad8\u6548\u7684\u6279\u91cf\u5904\u7406\u529f\u80fd", "result": "\u4ee3\u7801\u5df2\u5728GitHub\u5f00\u6e90\uff0c\u5b9e\u73b0\u4e86\u7b80\u5316\u6a21\u578b\u5d4c\u5165\u83b7\u53d6\u3001\u4f7f\u7528\u548c\u6bd4\u8f83\u7684\u76ee\u6807", "conclusion": "rs-embed\u89e3\u51b3\u4e86\u9065\u611f\u57fa\u7840\u6a21\u578b\u751f\u6001\u4e2d\u7684\u4e92\u64cd\u4f5c\u6027\u95ee\u9898\uff0c\u964d\u4f4e\u4e86\u4f7f\u7528\u95e8\u69db\uff0c\u4fc3\u8fdb\u4e86\u516c\u5e73\u6bd4\u8f83\u548c\u5927\u89c4\u6a21\u5d4c\u5165\u751f\u6210\u4e0e\u8bc4\u4f30"}}
{"id": "2602.24002", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.24002", "abs": "https://arxiv.org/abs/2602.24002", "authors": ["Iris Dania Jimenez", "Christoph Kern"], "title": "Dialect and Gender Bias in YouTube's Spanish Captioning System", "comment": "21 pages, 4 tables", "summary": "Spanish is the official language of twenty-one countries and is spoken by over 441 million people. Naturally, there are many variations in how Spanish is spoken across these countries. Media platforms such as YouTube rely on automatic speech recognition systems to make their content accessible to different groups of users. However, YouTube offers only one option for automatically generating captions in Spanish. This raises the question: could this captioning system be biased against certain Spanish dialects? This study examines the potential biases in YouTube's automatic captioning system by analyzing its performance across various Spanish dialects. By comparing the quality of captions for female and male speakers from different regions, we identify systematic disparities which can be attributed to specific dialects. Our study provides further evidence that algorithmic technologies deployed on digital platforms need to be calibrated to the diverse needs and experiences of their user populations.", "AI": {"tldr": "\u7814\u7a76YouTube\u897f\u73ed\u7259\u8bed\u81ea\u52a8\u5b57\u5e55\u7cfb\u7edf\u5bf9\u4e0d\u540c\u897f\u73ed\u7259\u8bed\u65b9\u8a00\u7684\u504f\u89c1\u95ee\u9898\uff0c\u53d1\u73b0\u7cfb\u7edf\u5728\u4e0d\u540c\u65b9\u8a00\u95f4\u5b58\u5728\u7cfb\u7edf\u6027\u5dee\u5f02", "motivation": "\u897f\u73ed\u7259\u8bed\u662f21\u4e2a\u56fd\u5bb6\u7684\u5b98\u65b9\u8bed\u8a00\uff0c\u6709\u8d85\u8fc74.41\u4ebf\u4f7f\u7528\u8005\uff0c\u4f46YouTube\u53ea\u63d0\u4f9b\u4e00\u79cd\u897f\u73ed\u7259\u8bed\u81ea\u52a8\u5b57\u5e55\u9009\u9879\uff0c\u8fd9\u5f15\u53d1\u4e86\u5bf9\u8be5\u7cfb\u7edf\u662f\u5426\u5bf9\u4e0d\u540c\u897f\u73ed\u7259\u8bed\u65b9\u8a00\u5b58\u5728\u504f\u89c1\u7684\u7591\u95ee", "method": "\u901a\u8fc7\u5206\u6790YouTube\u81ea\u52a8\u5b57\u5e55\u7cfb\u7edf\u5728\u4e0d\u540c\u897f\u73ed\u7259\u8bed\u65b9\u8a00\u4e0a\u7684\u8868\u73b0\uff0c\u6bd4\u8f83\u6765\u81ea\u4e0d\u540c\u5730\u533a\u7684\u7537\u5973\u8bf4\u8bdd\u8005\u7684\u5b57\u5e55\u8d28\u91cf", "result": "\u7814\u7a76\u53d1\u73b0YouTube\u81ea\u52a8\u5b57\u5e55\u7cfb\u7edf\u5728\u4e0d\u540c\u897f\u73ed\u7259\u8bed\u65b9\u8a00\u95f4\u5b58\u5728\u7cfb\u7edf\u6027\u5dee\u5f02\uff0c\u8fd9\u4e9b\u5dee\u5f02\u53ef\u5f52\u56e0\u4e8e\u7279\u5b9a\u65b9\u8a00", "conclusion": "\u6570\u5b57\u5e73\u53f0\u4e0a\u90e8\u7f72\u7684\u7b97\u6cd5\u6280\u672f\u9700\u8981\u6839\u636e\u7528\u6237\u7fa4\u4f53\u7684\u591a\u6837\u5316\u9700\u6c42\u548c\u7ecf\u9a8c\u8fdb\u884c\u6821\u51c6\uff0c\u4ee5\u786e\u4fdd\u516c\u5e73\u6027\u548c\u5305\u5bb9\u6027"}}
{"id": "2602.23870", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.23870", "abs": "https://arxiv.org/abs/2602.23870", "authors": ["Edoardo Fazzari", "Omar Mohamed", "Khalfan Hableel", "Hamdan Alhadhrami", "Cesare Stefanini"], "title": "Hybrid Offline-Online Reinforcement Learning for Sensorless, High-Precision Force Regulation in Surgical Robotic Grasping", "comment": null, "summary": "Precise grasp force regulation in tendon-driven surgical instruments is fundamentally limited by nonlinear coupling between motor dynamics, transmission compliance, friction, and distal mechanics. Existing solutions typically rely on distal force sensing or analytical compensation, increasing hardware complexity or degrading performance under dynamic motion. We present a sensorless control framework that combines physics-consistent modeling and hybrid reinforcement learning to achieve high-precision distal force regulation in a proximally actuated surgical end-effector. We develop a first-principles digital twin of the da Vinci Xi grasping mechanism that captures coupled electrical, transmission, and jaw dynamics within a unified differential-algebraic formulation. To safely learn control policies in this stiff and highly nonlinear system, we introduce a three-stage pipeline:(i)a receding-horizon CMA-ES oracle that generates dynamically feasible expert trajectories,(ii)fully offline policy learning via Implicit Q-Learning to ensure stable initialization without unsafe exploration, and (iii)online refinement using TD3 for adaptation to on-policy dynamics. The resulting policy directly maps proximal measurements to motor voltages and requires no distal sensing. In simulation, the controller maintains grasp force within 1% of the desired reference during multi-harmonic jaw motion. Hardware experiments demonstrate average force errors below 4% across diverse trajectories, validating sim-to-real transfer. The learned policy contains approximately 71k param and executes at kH rates, enabling real-time deployment. These results demonstrate that high-fidelity modeling combined with structured offline-online RL can recover precise distal force behavior without additional sensing, offering a scalable and mechanically compatible solution for surgical robotic manipulation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u4f20\u611f\u5668\u63a7\u5236\u6846\u67b6\uff0c\u7ed3\u5408\u7269\u7406\u4e00\u81f4\u6027\u5efa\u6a21\u548c\u6df7\u5408\u5f3a\u5316\u5b66\u4e60\uff0c\u5b9e\u73b0\u4e86\u624b\u672f\u5668\u68b0\u672b\u7aef\u6293\u63e1\u529b\u7684\u9ad8\u7cbe\u5ea6\u8c03\u8282\uff0c\u65e0\u9700\u8fdc\u7aef\u529b\u4f20\u611f\u3002", "motivation": "\u808c\u8171\u9a71\u52a8\u624b\u672f\u5668\u68b0\u7684\u7cbe\u786e\u6293\u63e1\u529b\u8c03\u8282\u53d7\u5230\u7535\u673a\u52a8\u529b\u5b66\u3001\u4f20\u52a8\u67d4\u987a\u6027\u3001\u6469\u64e6\u548c\u8fdc\u7aef\u529b\u5b66\u4e4b\u95f4\u975e\u7ebf\u6027\u8026\u5408\u7684\u6839\u672c\u9650\u5236\u3002\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u901a\u5e38\u4f9d\u8d56\u8fdc\u7aef\u529b\u4f20\u611f\u6216\u5206\u6790\u8865\u507f\uff0c\u589e\u52a0\u4e86\u786c\u4ef6\u590d\u6742\u6027\u6216\u5728\u52a8\u6001\u8fd0\u52a8\u4e0b\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u65e0\u4f20\u611f\u5668\u63a7\u5236\u6846\u67b6\uff0c\u7ed3\u5408\u7269\u7406\u4e00\u81f4\u6027\u5efa\u6a21\u548c\u6df7\u5408\u5f3a\u5316\u5b66\u4e60\u3002\u5f00\u53d1\u8fbe\u82ac\u5947Xi\u6293\u63e1\u673a\u6784\u7684\u7b2c\u4e00\u539f\u7406\u6570\u5b57\u5b6a\u751f\u6a21\u578b\uff0c\u6355\u6349\u8026\u5408\u7684\u7535\u6c14\u3001\u4f20\u52a8\u548c\u94b3\u53e3\u52a8\u529b\u5b66\u3002\u91c7\u7528\u4e09\u9636\u6bb5\u7ba1\u9053\uff1a(i)\u4f7f\u7528\u540e\u9000\u89c6\u91ceCMA-ES\u751f\u6210\u52a8\u6001\u53ef\u884c\u4e13\u5bb6\u8f68\u8ff9\uff0c(ii)\u901a\u8fc7\u9690\u5f0fQ\u5b66\u4e60\u8fdb\u884c\u5b8c\u5168\u79bb\u7ebf\u7b56\u7565\u5b66\u4e60\u786e\u4fdd\u7a33\u5b9a\u521d\u59cb\u5316\uff0c(iii)\u4f7f\u7528TD3\u8fdb\u884c\u5728\u7ebf\u7ec6\u5316\u4ee5\u9002\u5e94\u5728\u7ebf\u7b56\u7565\u52a8\u6001\u3002", "result": "\u5728\u4eff\u771f\u4e2d\uff0c\u63a7\u5236\u5668\u5728\u591a\u8c10\u6ce2\u94b3\u53e3\u8fd0\u52a8\u671f\u95f4\u5c06\u6293\u63e1\u529b\u4fdd\u6301\u5728\u671f\u671b\u53c2\u8003\u503c\u76841%\u4ee5\u5185\u3002\u786c\u4ef6\u5b9e\u9a8c\u663e\u793a\uff0c\u5728\u4e0d\u540c\u8f68\u8ff9\u4e0a\u7684\u5e73\u5747\u529b\u8bef\u5dee\u4f4e\u4e8e4%\uff0c\u9a8c\u8bc1\u4e86\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\u3002\u5b66\u4e60\u7b56\u7565\u5305\u542b\u7ea671k\u53c2\u6570\uff0c\u4ee5kHz\u901f\u7387\u6267\u884c\uff0c\u652f\u6301\u5b9e\u65f6\u90e8\u7f72\u3002", "conclusion": "\u9ad8\u4fdd\u771f\u5efa\u6a21\u7ed3\u5408\u7ed3\u6784\u5316\u79bb\u7ebf-\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u53ef\u4ee5\u5728\u65e0\u9700\u989d\u5916\u4f20\u611f\u7684\u60c5\u51b5\u4e0b\u6062\u590d\u7cbe\u786e\u7684\u8fdc\u7aef\u529b\u884c\u4e3a\uff0c\u4e3a\u624b\u672f\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u673a\u68b0\u517c\u5bb9\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.23697", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23697", "abs": "https://arxiv.org/abs/2602.23697", "authors": ["Jiahui Zhan", "Xianbing Sun", "Xiangnan Zhu", "Yikun Ji", "Ruitong Liu", "Liqing Zhang", "Jianfu Zhang"], "title": "Towards Source-Aware Object Swapping with Initial Noise Perturbation", "comment": "This paper is accepted by CVPR 2026 Findings", "summary": "Object swapping aims to replace a source object in a scene with a reference object while preserving object fidelity, scene fidelity, and object-scene harmony. Existing methods either require per-object finetuning and slow inference or rely on extra paired data that mostly depict the same object across contexts, forcing models to rely on background cues rather than learning cross-object alignment. We propose SourceSwap, a self-supervised and source-aware framework that learns cross-object alignment. Our key insight is to synthesize high-quality pseudo pairs from any image via a frequency-separated perturbation in the initial-noise space, which alters appearance while preserving pose, coarse shape, and scene layout, requiring no videos, multi-view data, or additional images. We then train a dual U-Net with full-source conditioning and a noise-free reference encoder, enabling direct inter-object alignment, zero-shot inference without per-object finetuning, and lightweight iterative refinement. We further introduce SourceBench, a high-quality benchmark with higher resolution, more categories, and richer interactions. Experiments demonstrate that SourceSwap achieves superior fidelity, stronger scene preservation, and more natural harmony, and it transfers well to edits such as subject-driven refinement and face swapping.", "AI": {"tldr": "SourceSwap\u662f\u4e00\u4e2a\u81ea\u76d1\u7763\u7684\u6e90\u611f\u77e5\u6846\u67b6\uff0c\u901a\u8fc7\u9891\u7387\u5206\u79bb\u6270\u52a8\u5728\u521d\u59cb\u566a\u58f0\u7a7a\u95f4\u5408\u6210\u9ad8\u8d28\u91cf\u4f2a\u5bf9\uff0c\u5b9e\u73b0\u8de8\u5bf9\u8c61\u5bf9\u9f50\uff0c\u65e0\u9700\u9010\u5bf9\u8c61\u5fae\u8c03\u5373\u53ef\u8fdb\u884c\u96f6\u6837\u672c\u63a8\u7406", "motivation": "\u73b0\u6709\u5bf9\u8c61\u4ea4\u6362\u65b9\u6cd5\u8981\u4e48\u9700\u8981\u9010\u5bf9\u8c61\u5fae\u8c03\u548c\u6162\u901f\u63a8\u7406\uff0c\u8981\u4e48\u4f9d\u8d56\u989d\u5916\u914d\u5bf9\u6570\u636e\uff08\u901a\u5e38\u63cf\u7ed8\u76f8\u540c\u5bf9\u8c61\u5728\u4e0d\u540c\u4e0a\u4e0b\u6587\u4e2d\u7684\u60c5\u51b5\uff09\uff0c\u8feb\u4f7f\u6a21\u578b\u4f9d\u8d56\u80cc\u666f\u7ebf\u7d22\u800c\u975e\u5b66\u4e60\u8de8\u5bf9\u8c61\u5bf9\u9f50", "method": "\u63d0\u51faSourceSwap\u6846\u67b6\uff1a1\uff09\u901a\u8fc7\u9891\u7387\u5206\u79bb\u6270\u52a8\u5728\u521d\u59cb\u566a\u58f0\u7a7a\u95f4\u5408\u6210\u9ad8\u8d28\u91cf\u4f2a\u5bf9\uff0c\u6539\u53d8\u5916\u89c2\u540c\u65f6\u4fdd\u6301\u59ff\u6001\u3001\u7c97\u5f62\u72b6\u548c\u573a\u666f\u5e03\u5c40\uff1b2\uff09\u8bad\u7ec3\u5177\u6709\u5168\u6e90\u6761\u4ef6\u5316\u548c\u65e0\u566a\u58f0\u53c2\u8003\u7f16\u7801\u5668\u7684\u53ccU-Net\uff1b3\uff09\u5f15\u5165SourceBench\u9ad8\u8d28\u91cf\u57fa\u51c6\u6d4b\u8bd5\u96c6", "result": "SourceSwap\u5728\u4fdd\u771f\u5ea6\u3001\u573a\u666f\u4fdd\u6301\u548c\u81ea\u7136\u548c\u8c10\u5ea6\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u80fd\u591f\u826f\u597d\u5730\u8fc1\u79fb\u5230\u4e3b\u9898\u9a71\u52a8\u7ec6\u5316\u548c\u4eba\u8138\u4ea4\u6362\u7b49\u7f16\u8f91\u4efb\u52a1\uff0c\u652f\u6301\u96f6\u6837\u672c\u63a8\u7406\u65e0\u9700\u9010\u5bf9\u8c61\u5fae\u8c03", "conclusion": "SourceSwap\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u6e90\u611f\u77e5\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u5bf9\u8c61\u4ea4\u6362\u4e2d\u7684\u8de8\u5bf9\u8c61\u5bf9\u9f50\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u5bf9\u8c61\u4ea4\u6362\u6548\u679c\uff0c\u5e76\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5"}}
{"id": "2602.23633", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23633", "abs": "https://arxiv.org/abs/2602.23633", "authors": ["Yubo Zhou", "Luo Luo", "Guang Dai", "Haishan Ye"], "title": "On the Convergence of Single-Loop Stochastic Bilevel Optimization with Approximate Implicit Differentiation", "comment": null, "summary": "Stochastic Bilevel Optimization has emerged as a fundamental framework for meta-learning and hyperparameter optimization. Despite the practical prevalence of single-loop algorithms--which update lower and upper variables concurrently--their theoretical understanding, particularly in the stochastic regime, remains significantly underdeveloped compared to their multi-loop counterparts. Existing analyses often yield suboptimal convergence rates or obscure the critical dependence on the lower-level condition number $\u03ba$, frequently burying it within generic Lipschitz constants. In this paper, we bridge this gap by providing a refined convergence analysis of the Single-loop Stochastic Approximate Implicit Differentiation (SSAID) algorithm. We prove that SSAID achieves an $\u03b5$-stationary point with an oracle complexity of $\\mathcal{O}(\u03ba^7 \u03b5^{-2})$. Our result is noteworthy in two aspects: (i) it matches the optimal $\\mathcal{O}(\u03b5^{-2})$ rate of state-of-the-art multi-loop methods (e.g., stocBiO) while maintaining the computational efficiency of a single-loop update; and (ii) it provides the first explicit, fine-grained characterization of the $\u03ba$-dependence for stochastic AID-based single-loop methods. This work demonstrates that SSAID is not merely a heuristic approach, but admits a rigorous theoretical foundation with convergence guarantees competitive with mainstream multi-loop frameworks.", "AI": {"tldr": "\u672c\u6587\u5bf9\u5355\u5c42\u968f\u673a\u8fd1\u4f3c\u9690\u5f0f\u5fae\u5206\u7b97\u6cd5\uff08SSAID\uff09\u8fdb\u884c\u4e86\u6536\u655b\u6027\u5206\u6790\uff0c\u8bc1\u660e\u4e86\u8be5\u7b97\u6cd5\u8fbe\u5230\u03b5-\u5e73\u7a33\u70b9\u7684Oracle\u590d\u6742\u5ea6\u4e3aO(\u03ba\u2077\u03b5\u207b\u00b2)\uff0c\u5339\u914d\u4e86\u591a\u5c42\u65b9\u6cd5\u7684\u6700\u4f18\u6536\u655b\u7387\u3002", "motivation": "\u968f\u673a\u53cc\u5c42\u4f18\u5316\u5728\u5143\u5b66\u4e60\u548c\u8d85\u53c2\u6570\u4f18\u5316\u4e2d\u5177\u6709\u91cd\u8981\u5e94\u7528\u3002\u5355\u5c42\u7b97\u6cd5\u5728\u5b9e\u8df5\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u7406\u8bba\u5206\u6790\uff08\u7279\u522b\u662f\u5728\u968f\u673a\u73af\u5883\u4e0b\uff09\u8fdc\u843d\u540e\u4e8e\u591a\u5c42\u7b97\u6cd5\u3002\u73b0\u6709\u5206\u6790\u901a\u5e38\u6536\u655b\u7387\u6b21\u4f18\uff0c\u4e14\u672a\u80fd\u6e05\u6670\u63ed\u793a\u5bf9\u4e0b\u5c42\u6761\u4ef6\u6570\u03ba\u7684\u4f9d\u8d56\u5173\u7cfb\u3002", "method": "\u672c\u6587\u5bf9\u5355\u5c42\u968f\u673a\u8fd1\u4f3c\u9690\u5f0f\u5fae\u5206\u7b97\u6cd5\uff08SSAID\uff09\u8fdb\u884c\u4e86\u7cbe\u7ec6\u5316\u7684\u6536\u655b\u6027\u5206\u6790\u3002\u8be5\u65b9\u6cd5\u91c7\u7528\u5355\u5c42\u66f4\u65b0\u7b56\u7565\uff0c\u540c\u65f6\u66f4\u65b0\u4e0a\u5c42\u548c\u4e0b\u5c42\u53d8\u91cf\uff0c\u907f\u514d\u4e86\u591a\u5c42\u7b97\u6cd5\u4e2d\u4ea4\u66ff\u4f18\u5316\u7684\u8ba1\u7b97\u5f00\u9500\u3002", "result": "\u8bc1\u660e\u4e86SSAID\u7b97\u6cd5\u8fbe\u5230\u03b5-\u5e73\u7a33\u70b9\u7684Oracle\u590d\u6742\u5ea6\u4e3aO(\u03ba\u2077\u03b5\u207b\u00b2)\u3002\u8fd9\u4e00\u7ed3\u679c\u5728\u4e24\u4e2a\u65b9\u9762\u5177\u6709\u91cd\u8981\u610f\u4e49\uff1a1\uff09\u5339\u914d\u4e86\u6700\u5148\u8fdb\u591a\u5c42\u65b9\u6cd5\uff08\u5982stocBiO\uff09\u7684\u6700\u4f18O(\u03b5\u207b\u00b2)\u6536\u655b\u7387\uff1b2\uff09\u9996\u6b21\u4e3a\u57fa\u4e8e\u968f\u673aAID\u7684\u5355\u5c42\u65b9\u6cd5\u63d0\u4f9b\u4e86\u660e\u786e\u7684\u3001\u7ec6\u7c92\u5ea6\u7684\u03ba\u4f9d\u8d56\u5173\u7cfb\u5206\u6790\u3002", "conclusion": "SSAID\u4e0d\u4ec5\u662f\u4e00\u79cd\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u800c\u4e14\u5177\u6709\u4e25\u683c\u7684\u7406\u8bba\u57fa\u7840\uff0c\u5176\u6536\u655b\u4fdd\u8bc1\u4e0e\u4e3b\u6d41\u591a\u5c42\u6846\u67b6\u5177\u6709\u7ade\u4e89\u529b\u3002\u8be5\u5de5\u4f5c\u586b\u8865\u4e86\u5355\u5c42\u968f\u673a\u53cc\u5c42\u4f18\u5316\u7b97\u6cd5\u7406\u8bba\u5206\u6790\u7684\u7a7a\u767d\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u3002"}}
{"id": "2602.24055", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.24055", "abs": "https://arxiv.org/abs/2602.24055", "authors": ["Reva Schwartz", "Carina Westling", "Morgan Briggs", "Marzieh Fadaee", "Isar Nejadgholi", "Matthew Holmes", "Fariza Rashid", "Maya Carlyle", "Afaf Ta\u00efk", "Kyra Wilson", "Peter Douglas", "Theodora Skeadas", "Gabriella Waters", "Rumman Chowdhury", "Thiago Lacerda"], "title": "CIRCLE: A Framework for Evaluating AI from a Real-World Lens", "comment": "Accepted at Intelligent Systems Conference (IntelliSys) 2026", "summary": "This paper proposes CIRCLE, a six-stage, lifecycle-based framework to bridge the reality gap between model-centric performance metrics and AI's materialized outcomes in deployment. While existing frameworks like MLOps focus on system stability and benchmarks measure abstract capabilities, decision-makers outside the AI stack lack systematic evidence about the behavior of AI technologies under real-world user variability and constraints. CIRCLE operationalizes the Validation phase of TEVV (Test, Evaluation, Verification, and Validation) by formalizing the translation of stakeholder concerns outside the stack into measurable signals. Unlike participatory design, which often remains localized, or algorithmic audits, which are often retrospective, CIRCLE provides a structured, prospective protocol for linking context-sensitive qualitative insights to scalable quantitative metrics. By integrating methods such as field testing, red teaming, and longitudinal studies into a coordinated pipeline, CIRCLE produces systematic knowledge: evidence that is comparable across sites yet sensitive to local context. This can enable governance based on materialized downstream effects rather than theoretical capabilities.", "AI": {"tldr": "CIRCLE\u662f\u4e00\u4e2a\u516d\u9636\u6bb5\u751f\u547d\u5468\u671f\u6846\u67b6\uff0c\u65e8\u5728\u5f25\u5408\u6a21\u578b\u4e2d\u5fc3\u6027\u80fd\u6307\u6807\u4e0eAI\u5b9e\u9645\u90e8\u7f72\u7ed3\u679c\u4e4b\u95f4\u7684\u73b0\u5b9e\u5dee\u8ddd\uff0c\u901a\u8fc7\u5c06\u5229\u76ca\u76f8\u5173\u8005\u5173\u6ce8\u8f6c\u5316\u4e3a\u53ef\u6d4b\u91cf\u4fe1\u53f7\uff0c\u5b9e\u73b0\u57fa\u4e8e\u5b9e\u9645\u4e0b\u6e38\u6548\u5e94\u7684\u6cbb\u7406\u3002", "motivation": "\u73b0\u6709MLOps\u6846\u67b6\u5173\u6ce8\u7cfb\u7edf\u7a33\u5b9a\u6027\uff0c\u57fa\u51c6\u6d4b\u8bd5\u8861\u91cf\u62bd\u8c61\u80fd\u529b\uff0c\u4f46AI\u5806\u6808\u5916\u7684\u51b3\u7b56\u8005\u7f3a\u4e4f\u5173\u4e8eAI\u6280\u672f\u5728\u771f\u5b9e\u4e16\u754c\u7528\u6237\u53d8\u5f02\u6027\u548c\u7ea6\u675f\u4e0b\u884c\u4e3a\u7684\u7cfb\u7edf\u6027\u8bc1\u636e\uff0c\u9700\u8981\u8fde\u63a5\u4e0a\u4e0b\u6587\u654f\u611f\u7684\u5b9a\u6027\u6d1e\u5bdf\u4e0e\u53ef\u6269\u5c55\u7684\u5b9a\u91cf\u6307\u6807\u3002", "method": "CIRCLE\u662f\u4e00\u4e2a\u516d\u9636\u6bb5\u751f\u547d\u5468\u671f\u6846\u67b6\uff0c\u5c06TEVV\uff08\u6d4b\u8bd5\u3001\u8bc4\u4f30\u3001\u9a8c\u8bc1\u548c\u9a8c\u8bc1\uff09\u4e2d\u7684\u9a8c\u8bc1\u9636\u6bb5\u64cd\u4f5c\u5316\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u524d\u77bb\u6027\u534f\u8bae\uff0c\u6574\u5408\u5b9e\u5730\u6d4b\u8bd5\u3001\u7ea2\u961f\u6d4b\u8bd5\u548c\u7eb5\u5411\u7814\u7a76\u7b49\u65b9\u6cd5\uff0c\u5c06\u5229\u76ca\u76f8\u5173\u8005\u5173\u6ce8\u8f6c\u5316\u4e3a\u53ef\u6d4b\u91cf\u4fe1\u53f7\u3002", "result": "CIRCLE\u4ea7\u751f\u7cfb\u7edf\u6027\u77e5\u8bc6\uff1a\u5728\u4e0d\u540c\u7ad9\u70b9\u95f4\u53ef\u6bd4\u4f46\u5bf9\u672c\u5730\u4e0a\u4e0b\u6587\u654f\u611f\u7684\u8bc1\u636e\uff0c\u652f\u6301\u57fa\u4e8e\u5b9e\u9645\u4e0b\u6e38\u6548\u5e94\u800c\u975e\u7406\u8bba\u80fd\u529b\u7684\u6cbb\u7406\uff0c\u63d0\u4f9b\u8fde\u63a5\u5b9a\u6027\u6d1e\u5bdf\u4e0e\u5b9a\u91cf\u6307\u6807\u7684\u7ed3\u6784\u5316\u65b9\u6cd5\u3002", "conclusion": "CIRCLE\u6846\u67b6\u901a\u8fc7\u5f25\u5408\u6a21\u578b\u4e2d\u5fc3\u6307\u6807\u4e0eAI\u5b9e\u9645\u90e8\u7f72\u7ed3\u679c\u4e4b\u95f4\u7684\u73b0\u5b9e\u5dee\u8ddd\uff0c\u4f7f\u6cbb\u7406\u80fd\u591f\u57fa\u4e8eAI\u6280\u672f\u7684\u5b9e\u9645\u4e0b\u6e38\u6548\u5e94\uff0c\u4e3a\u51b3\u7b56\u8005\u63d0\u4f9b\u5173\u4e8eAI\u5728\u771f\u5b9e\u4e16\u754c\u7ea6\u675f\u4e0b\u884c\u4e3a\u7684\u7cfb\u7edf\u6027\u8bc1\u636e\u3002"}}
{"id": "2602.24060", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.24060", "abs": "https://arxiv.org/abs/2602.24060", "authors": ["Donghao Huang", "Zhaoxia Wang"], "title": "Task Complexity Matters: An Empirical Study of Reasoning in LLMs for Sentiment Analysis", "comment": "12 pages, 1 figure, 3 tables. Accepted at PAKDD 2026", "summary": "Large language models (LLMs) with reasoning capabilities have fueled a compelling narrative that reasoning universally improves performance across language tasks. We test this claim through a comprehensive evaluation of 504 configurations across seven model families--including adaptive, conditional, and reinforcement learning-based reasoning architectures--on sentiment analysis datasets of varying granularity (binary, five-class, and 27-class emotion). Our findings reveal that reasoning effectiveness is strongly task-dependent, challenging prevailing assumptions: (1) Reasoning shows task-complexity dependence--binary classification degrades up to -19.9 F1 percentage points (pp), while 27-class emotion recognition gains up to +16.0pp; (2) Distilled reasoning variants underperform base models by 3-18 pp on simpler tasks, though few-shot prompting enables partial recovery; (3) Few-shot learning improves over zero-shot in most cases regardless of model type, with gains varying by architecture and task complexity; (4) Pareto frontier analysis shows base models dominate efficiency-performance trade-offs, with reasoning justified only for complex emotion recognition despite 2.1x-54x computational overhead. We complement these quantitative findings with qualitative error analysis revealing that reasoning degrades simpler tasks through systematic over-deliberation, offering mechanistic insight beyond the high-level overthinking hypothesis.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7504\u79cd\u914d\u7f6e\u7684\u5168\u9762\u8bc4\u4f30\u53d1\u73b0\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u5e76\u975e\u666e\u904d\u63d0\u5347\u6240\u6709\u8bed\u8a00\u4efb\u52a1\u6027\u80fd\uff0c\u800c\u662f\u5f3a\u70c8\u4f9d\u8d56\u4e8e\u4efb\u52a1\u590d\u6742\u6027\u3002\u63a8\u7406\u5bf9\u7b80\u5355\u4efb\u52a1\uff08\u5982\u4e8c\u5206\u7c7b\uff09\u53ef\u80fd\u9020\u6210\u6027\u80fd\u4e0b\u964d\uff0c\u4f46\u5bf9\u590d\u6742\u4efb\u52a1\uff08\u598227\u7c7b\u60c5\u611f\u8bc6\u522b\uff09\u6709\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u5f53\u524d\u666e\u904d\u8ba4\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u80fd\u591f\u666e\u904d\u63d0\u5347\u5404\u79cd\u8bed\u8a00\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u4f46\u8fd9\u4e00\u5047\u8bbe\u7f3a\u4e4f\u7cfb\u7edf\u6027\u9a8c\u8bc1\u3002\u7814\u7a76\u8005\u65e8\u5728\u901a\u8fc7\u5168\u9762\u8bc4\u4f30\u6765\u68c0\u9a8c\u63a8\u7406\u80fd\u529b\u662f\u5426\u771f\u7684\u5bf9\u6240\u6709\u4efb\u52a1\u90fd\u6709\u5e2e\u52a9\uff0c\u4ee5\u53ca\u8fd9\u79cd\u5e2e\u52a9\u662f\u5426\u4f9d\u8d56\u4e8e\u4efb\u52a1\u7279\u6027\u3002", "method": "\u7814\u7a76\u91c7\u75287\u4e2a\u6a21\u578b\u5bb6\u65cf\u7684504\u79cd\u914d\u7f6e\uff0c\u5305\u62ec\u81ea\u9002\u5e94\u3001\u6761\u4ef6\u5f0f\u548c\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u63a8\u7406\u67b6\u6784\u3002\u5728\u60c5\u611f\u5206\u6790\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u6db5\u76d6\u4e0d\u540c\u7c92\u5ea6\u7684\u4efb\u52a1\uff1a\u4e8c\u5206\u7c7b\u3001\u4e94\u5206\u7c7b\u548c27\u7c7b\u60c5\u611f\u8bc6\u522b\u3002\u4f7f\u7528\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u63d0\u793a\u7b56\u7565\uff0c\u5e76\u8fdb\u884c\u5e15\u7d2f\u6258\u524d\u6cbf\u5206\u6790\u6765\u8bc4\u4f30\u6548\u7387-\u6027\u80fd\u6743\u8861\u3002", "result": "1. \u63a8\u7406\u6548\u679c\u5f3a\u70c8\u4f9d\u8d56\u4e8e\u4efb\u52a1\u590d\u6742\u6027\uff1a\u4e8c\u5206\u7c7b\u4efb\u52a1\u6027\u80fd\u4e0b\u964d\u9ad8\u8fbe19.9\u4e2aF1\u767e\u5206\u70b9\uff0c\u800c27\u7c7b\u60c5\u611f\u8bc6\u522b\u4efb\u52a1\u63d0\u5347\u9ad8\u8fbe16.0\u4e2a\u767e\u5206\u70b9\uff1b2. \u84b8\u998f\u63a8\u7406\u53d8\u4f53\u5728\u7b80\u5355\u4efb\u52a1\u4e0a\u6bd4\u57fa\u7840\u6a21\u578b\u5dee3-18\u4e2a\u767e\u5206\u70b9\uff1b3. \u5c11\u6837\u672c\u5b66\u4e60\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u4f18\u4e8e\u96f6\u6837\u672c\uff1b4. \u57fa\u7840\u6a21\u578b\u5728\u6548\u7387-\u6027\u80fd\u6743\u8861\u4e2d\u5360\u4e3b\u5bfc\u5730\u4f4d\uff0c\u63a8\u7406\u4ec5\u5728\u590d\u6742\u60c5\u611f\u8bc6\u522b\u4efb\u52a1\u4e2d\u5408\u7406\uff0c\u5c3d\u7ba1\u5e26\u67652.1-54\u500d\u7684\u8ba1\u7b97\u5f00\u9500\u3002", "conclusion": "\u63a8\u7406\u80fd\u529b\u5e76\u975e\u666e\u904d\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u6027\u80fd\uff0c\u800c\u662f\u5f3a\u70c8\u4f9d\u8d56\u4e8e\u4efb\u52a1\u590d\u6742\u6027\u3002\u5bf9\u4e8e\u7b80\u5355\u4efb\u52a1\uff0c\u63a8\u7406\u53ef\u80fd\u5bfc\u81f4\"\u8fc7\u5ea6\u601d\u8003\"\u800c\u964d\u4f4e\u6027\u80fd\uff1b\u5bf9\u4e8e\u590d\u6742\u4efb\u52a1\uff0c\u63a8\u7406\u80fd\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002\u7814\u7a76\u6311\u6218\u4e86\u5f53\u524d\u5173\u4e8e\u63a8\u7406\u666e\u904d\u6709\u76ca\u7684\u5047\u8bbe\uff0c\u4e3a\u6a21\u578b\u9009\u62e9\u548c\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2602.23896", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.23896", "abs": "https://arxiv.org/abs/2602.23896", "authors": ["Xiaotong Zhang", "Gang Xiong", "Yuanjing Wang", "Siyu Teng", "Alois Knoll", "Long Chen"], "title": "TSC: Topology-Conditioned Stackelberg Coordination for Multi-Agent Reinforcement Learning in Interactive Driving", "comment": "12 pages, 8 figures", "summary": "Safe and efficient autonomous driving in dense traffic is fundamentally a decentralized multi-agent coordination problem, where interactions at conflict points such as merging and weaving must be resolved reliably under partial observability. With only local and incomplete cues, interaction patterns can change rapidly, often causing unstable behaviors such as oscillatory yielding or unsafe commitments. Existing multi-agent reinforcement learning (MARL) approaches either adopt synchronous decision-making, which exacerbate non-stationarity, or depend on centralized sequencing mechanisms that scale poorly as traffic density increases. To address these limitations, we propose Topology-conditioned Stackelberg Coordination (TSC), a learning framework for decentralized interactive driving under communication-free execution, which extracts a time-varying directed priority graph from braid-inspired weaving relations between trajectories, thereby defining local leader-follower dependencies without constructing a global order of play. Conditioned on this graph, TSC endogenously factorizes dense interactions into graph-local Stackelberg subgames and, under centralized training and decentralized execution (CTDE), learns a sequential coordination policy that anticipates leaders via action prediction and trains followers through action-conditioned value learning to approximate local best responses, improving training stability and safety in dense traffic. Experiments across four dense traffic scenarios show that TSC achieves superior performance over representative MARL baselines across key metrics, most notably reducing collisions while maintaining competitive traffic efficiency and control smoothness.", "AI": {"tldr": "TSC\u6846\u67b6\u901a\u8fc7\u63d0\u53d6\u8f68\u8ff9\u95f4\u7684\u7f16\u7ec7\u5173\u7cfb\u6784\u5efa\u65f6\u53d8\u6709\u5411\u4f18\u5148\u7ea7\u56fe\uff0c\u5c06\u5bc6\u96c6\u4ea4\u4e92\u5206\u89e3\u4e3a\u5c40\u90e8Stackelberg\u5b50\u535a\u5f08\uff0c\u5728CTDE\u8303\u5f0f\u4e0b\u5b66\u4e60\u987a\u5e8f\u534f\u8c03\u7b56\u7565\uff0c\u63d0\u5347\u5bc6\u96c6\u4ea4\u901a\u4e2d\u7684\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u89e3\u51b3\u5bc6\u96c6\u4ea4\u901a\u4e2d\u81ea\u52a8\u9a7e\u9a76\u7684\u5206\u6563\u5f0f\u591a\u667a\u80fd\u4f53\u534f\u8c03\u95ee\u9898\u3002\u73b0\u6709MARL\u65b9\u6cd5\u8981\u4e48\u91c7\u7528\u540c\u6b65\u51b3\u7b56\u52a0\u5267\u975e\u5e73\u7a33\u6027\uff0c\u8981\u4e48\u4f9d\u8d56\u96c6\u4e2d\u5f0f\u6392\u5e8f\u673a\u5236\u5728\u4ea4\u901a\u5bc6\u5ea6\u589e\u52a0\u65f6\u6269\u5c55\u6027\u5dee\u3002", "method": "\u63d0\u51fa\u62d3\u6251\u6761\u4ef6Stackelberg\u534f\u8c03(TSC)\u6846\u67b6\uff1a1)\u4ece\u8f68\u8ff9\u7f16\u7ec7\u5173\u7cfb\u4e2d\u63d0\u53d6\u65f6\u53d8\u6709\u5411\u4f18\u5148\u7ea7\u56fe\uff0c\u5b9a\u4e49\u5c40\u90e8\u9886\u5bfc\u8005-\u8ddf\u968f\u8005\u4f9d\u8d56\u5173\u7cfb\uff1b2)\u57fa\u4e8e\u8be5\u56fe\u5c06\u5bc6\u96c6\u4ea4\u4e92\u5206\u89e3\u4e3a\u56fe\u5c40\u90e8Stackelberg\u5b50\u535a\u5f08\uff1b3)\u5728CTDE\u8303\u5f0f\u4e0b\uff0c\u901a\u8fc7\u52a8\u4f5c\u9884\u6d4b\u5b66\u4e60\u9886\u5bfc\u8005\uff0c\u901a\u8fc7\u52a8\u4f5c\u6761\u4ef6\u4ef7\u503c\u5b66\u4e60\u8bad\u7ec3\u8ddf\u968f\u8005\u8fd1\u4f3c\u5c40\u90e8\u6700\u4f18\u54cd\u5e94\u3002", "result": "\u5728\u56db\u4e2a\u5bc6\u96c6\u4ea4\u901a\u573a\u666f\u4e2d\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTSC\u5728\u5173\u952e\u6307\u6807\u4e0a\u4f18\u4e8e\u4ee3\u8868\u6027MARL\u57fa\u7ebf\uff0c\u6700\u663e\u8457\u7684\u662f\u51cf\u5c11\u78b0\u649e\uff0c\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u4ea4\u901a\u6548\u7387\u548c\u63a7\u5236\u5e73\u6ed1\u6027\u3002", "conclusion": "TSC\u901a\u8fc7\u63d0\u53d6\u5c40\u90e8\u4f18\u5148\u7ea7\u56fe\u5e76\u5206\u89e3\u4e3aStackelberg\u5b50\u535a\u5f08\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5bc6\u96c6\u4ea4\u901a\u4e2d\u5206\u6563\u5f0f\u4ea4\u4e92\u9a7e\u9a76\u7684\u534f\u8c03\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u5b89\u5168\u6027\uff0c\u65e0\u9700\u5168\u5c40\u6392\u5e8f\u673a\u5236\u3002"}}
{"id": "2602.23699", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.23699", "abs": "https://arxiv.org/abs/2602.23699", "authors": ["Hao Wu", "Yingqi Fan", "Jinyang Dai", "Junlong Tong", "Yunpu Ma", "Xiaoyu Shen"], "title": "HiDrop: Hierarchical Vision Token Reduction in MLLMs via Late Injection, Concave Pyramid Pruning, and Early Exit", "comment": "Accepted to ICLR 2026", "summary": "The quadratic computational cost of processing vision tokens in Multimodal Large Language Models (MLLMs) hinders their widespread adoption. While progressive vision token pruning offers a promising solution, current methods misinterpret shallow layer functions and use rigid schedules, which fail to unlock the full efficiency potential. To address these issues, we propose HiDrop, a framework that aligns token pruning with the true hierarchical function of MLLM layers. HiDrop features two key innovations: (1) Late Injection, which bypasses passive shallow layers to introduce visual tokens exactly where active fusion begins; and (2) Concave Pyramid Pruning with an Early Exit mechanism to dynamically adjust pruning rates across middle and deep layers. This process is optimized via an inter-layer similarity measure and a differentiable top-k operator. To ensure practical efficiency, HiDrop further incorporates persistent positional encoding, FlashAttention-compatible token selection, and parallel decoupling of vision computation to eliminate hidden overhead associated with dynamic token reduction. Extensive experiments show that HiDrop compresses about 90% visual tokens while matching the original performance and accelerating training by 1.72 times. Our work not only sets a new state-of-the-art for efficient MLLM training and inference but also provides valuable insights into the hierarchical nature of multimodal fusion. The code is released at https://github.com/EIT-NLP/HiDrop.", "AI": {"tldr": "HiDrop\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u5c42\u6b21\u5316\u89c6\u89c9token\u526a\u679d\uff0c\u538b\u7f29\u7ea690%\u89c6\u89c9token\uff0c\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u52a0\u901f\u8bad\u7ec31.72\u500d\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u89c6\u89c9token\u7684\u4e8c\u6b21\u8ba1\u7b97\u6210\u672c\u963b\u788d\u4e86\u5176\u5e7f\u6cdb\u5e94\u7528\u3002\u73b0\u6709\u7684\u6e10\u8fdb\u5f0f\u89c6\u89c9token\u526a\u679d\u65b9\u6cd5\u8bef\u89e3\u4e86\u6d45\u5c42\u529f\u80fd\u5e76\u4f7f\u7528\u521a\u6027\u8c03\u5ea6\uff0c\u672a\u80fd\u5145\u5206\u53d1\u6325\u6548\u7387\u6f5c\u529b\u3002", "method": "\u63d0\u51faHiDrop\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u521b\u65b0\uff1a1) \u5ef6\u8fdf\u6ce8\u5165\uff1a\u7ed5\u8fc7\u88ab\u52a8\u7684\u6d45\u5c42\uff0c\u5728\u4e3b\u52a8\u878d\u5408\u5f00\u59cb\u5904\u5f15\u5165\u89c6\u89c9token\uff1b2) \u51f9\u91d1\u5b57\u5854\u526a\u679d\u4e0e\u65e9\u671f\u9000\u51fa\u673a\u5236\uff1a\u901a\u8fc7\u5c42\u95f4\u76f8\u4f3c\u6027\u5ea6\u91cf\u548c\u53ef\u5fae\u5206top-k\u7b97\u5b50\u52a8\u6001\u8c03\u6574\u4e2d\u95f4\u5c42\u548c\u6df1\u5c42\u7684\u526a\u679d\u7387\u3002\u8fd8\u5305\u542b\u6301\u4e45\u4f4d\u7f6e\u7f16\u7801\u3001FlashAttention\u517c\u5bb9\u7684token\u9009\u62e9\u548c\u5e76\u884c\u89e3\u8026\u89c6\u89c9\u8ba1\u7b97\u7b49\u6280\u672f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cHiDrop\u538b\u7f29\u7ea690%\u89c6\u89c9token\uff0c\u540c\u65f6\u5339\u914d\u539f\u59cb\u6027\u80fd\uff0c\u8bad\u7ec3\u52a0\u901f1.72\u500d\uff0c\u5728\u9ad8\u6548MLLM\u8bad\u7ec3\u548c\u63a8\u7406\u65b9\u9762\u8fbe\u5230\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "HiDrop\u4e0d\u4ec5\u4e3a\u9ad8\u6548MLLM\u8bad\u7ec3\u548c\u63a8\u7406\u8bbe\u5b9a\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u8fd8\u63d0\u4f9b\u4e86\u5bf9\u591a\u6a21\u6001\u878d\u5408\u5c42\u6b21\u6027\u8d28\u7684\u6709\u4ef7\u503c\u89c1\u89e3\u3002"}}
{"id": "2602.23636", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23636", "abs": "https://arxiv.org/abs/2602.23636", "authors": ["Zhihao Ding", "Jinming Li", "Ze Lu", "Jieming Shi"], "title": "FlexGuard: Continuous Risk Scoring for Strictness-Adaptive LLM Content Moderation", "comment": null, "summary": "Ensuring the safety of LLM-generated content is essential for real-world deployment. Most existing guardrail models formulate moderation as a fixed binary classification task, implicitly assuming a fixed definition of harmfulness. In practice, enforcement strictness - how conservatively harmfulness is defined and enforced - varies across platforms and evolves over time, making binary moderators brittle under shifting requirements. We first introduce FlexBench, a strictness-adaptive LLM moderation benchmark that enables controlled evaluation under multiple strictness regimes. Experiments on FlexBench reveal substantial cross-strictness inconsistency in existing moderators: models that perform well under one regime can degrade substantially under others, limiting their practical usability. To address this, we propose FlexGuard, an LLM-based moderator that outputs a calibrated continuous risk score reflecting risk severity and supports strictness-specific decisions via thresholding. We train FlexGuard via risk-alignment optimization to improve score-severity consistency and provide practical threshold selection strategies to adapt to target strictness at deployment. Experiments on FlexBench and public benchmarks demonstrate that FlexGuard achieves higher moderation accuracy and substantially improved robustness under varying strictness. We release the source code and data to support reproducibility.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faFlexGuard\u65b9\u6cd5\u89e3\u51b3LLM\u5185\u5bb9\u5ba1\u6838\u4e2d\u4e25\u683c\u5ea6\u53d8\u5316\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u8fde\u7eed\u98ce\u9669\u8bc4\u5206\u548c\u9608\u503c\u8c03\u6574\u6765\u9002\u5e94\u4e0d\u540c\u5e73\u53f0\u548c\u65f6\u95f4\u7684\u5ba1\u6838\u9700\u6c42\u3002", "motivation": "\u73b0\u6709LLM\u5185\u5bb9\u5b89\u5168\u5ba1\u6838\u6a21\u578b\u901a\u5e38\u91c7\u7528\u56fa\u5b9a\u7684\u4e8c\u5143\u5206\u7c7b\u65b9\u6cd5\uff0c\u5047\u8bbe\u6709\u5bb3\u5185\u5bb9\u7684\u5b9a\u4e49\u662f\u56fa\u5b9a\u7684\u3002\u4f46\u5b9e\u9645\u4e0a\uff0c\u4e0d\u540c\u5e73\u53f0\u7684\u6267\u884c\u4e25\u683c\u5ea6\uff08\u5982\u4f55\u5b9a\u4e49\u548c\u5f3a\u5236\u6267\u884c\u6709\u5bb3\u5185\u5bb9\uff09\u5404\u4e0d\u76f8\u540c\u4e14\u968f\u65f6\u95f4\u6f14\u53d8\uff0c\u5bfc\u81f4\u4e8c\u5143\u5ba1\u6838\u5668\u5728\u9700\u6c42\u53d8\u5316\u65f6\u53d8\u5f97\u8106\u5f31\u3002", "method": "\u9996\u5148\u521b\u5efaFlexBench\u57fa\u51c6\uff0c\u652f\u6301\u591a\u4e25\u683c\u5ea6\u5236\u5ea6\u4e0b\u7684\u53d7\u63a7\u8bc4\u4f30\u3002\u7136\u540e\u63d0\u51faFlexGuard\u65b9\u6cd5\uff1a\u57fa\u4e8eLLM\u7684\u5ba1\u6838\u5668\u8f93\u51fa\u6821\u51c6\u7684\u8fde\u7eed\u98ce\u9669\u8bc4\u5206\u53cd\u6620\u98ce\u9669\u4e25\u91cd\u7a0b\u5ea6\uff0c\u901a\u8fc7\u9608\u503c\u8c03\u6574\u652f\u6301\u7279\u5b9a\u4e25\u683c\u5ea6\u51b3\u7b56\u3002\u901a\u8fc7\u98ce\u9669\u5bf9\u9f50\u4f18\u5316\u8bad\u7ec3\u63d0\u9ad8\u8bc4\u5206-\u4e25\u91cd\u5ea6\u4e00\u81f4\u6027\uff0c\u5e76\u63d0\u4f9b\u5b9e\u7528\u7684\u9608\u503c\u9009\u62e9\u7b56\u7565\u4ee5\u9002\u5e94\u90e8\u7f72\u65f6\u7684\u76ee\u6807\u4e25\u683c\u5ea6\u3002", "result": "\u5728FlexBench\u548c\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFlexGuard\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u5ba1\u6838\u51c6\u786e\u6027\uff0c\u5e76\u5728\u4e0d\u540c\u4e25\u683c\u5ea6\u4e0b\u663e\u8457\u63d0\u9ad8\u4e86\u9c81\u68d2\u6027\u3002\u73b0\u6709\u5ba1\u6838\u5668\u5728\u4e0d\u540c\u4e25\u683c\u5ea6\u5236\u5ea6\u4e0b\u5b58\u5728\u663e\u8457\u7684\u8de8\u4e25\u683c\u5ea6\u4e0d\u4e00\u81f4\u6027\uff0c\u800cFlexGuard\u6709\u6548\u89e3\u51b3\u4e86\u8fd9\u4e00\u95ee\u9898\u3002", "conclusion": "FlexGuard\u901a\u8fc7\u8fde\u7eed\u98ce\u9669\u8bc4\u5206\u548c\u9608\u503c\u8c03\u6574\u673a\u5236\uff0c\u4e3aLLM\u5185\u5bb9\u5ba1\u6838\u63d0\u4f9b\u4e86\u7075\u6d3b\u9002\u5e94\u4e0d\u540c\u4e25\u683c\u5ea6\u8981\u6c42\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63d0\u9ad8\u4e86\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u53ef\u7528\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2602.24080", "categories": ["cs.AI", "cs.SD"], "pdf": "https://arxiv.org/pdf/2602.24080", "abs": "https://arxiv.org/abs/2602.24080", "authors": ["Xiang Li", "Jiabao Gao", "Sipei Lin", "Xuan Zhou", "Chi Zhang", "Bo Cheng", "Jiale Han", "Benyou Wang"], "title": "Human or Machine? A Preliminary Turing Test for Speech-to-Speech Interaction", "comment": "Accepted by ICLR 2026 Conference", "summary": "The pursuit of human-like conversational agents has long been guided by the Turing test. For modern speech-to-speech (S2S) systems, a critical yet unanswered question is whether they can converse like humans. To tackle this, we conduct the first Turing test for S2S systems, collecting 2,968 human judgments on dialogues between 9 state-of-the-art S2S systems and 28 human participants. Our results deliver a clear finding: no existing evaluated S2S system passes the test, revealing a significant gap in human-likeness. To diagnose this failure, we develop a fine-grained taxonomy of 18 human-likeness dimensions and crowd-annotate our collected dialogues accordingly. Our analysis shows that the bottleneck is not semantic understanding but stems from paralinguistic features, emotional expressivity, and conversational persona. Furthermore, we find that off-the-shelf AI models perform unreliably as Turing test judges. In response, we propose an interpretable model that leverages the fine-grained human-likeness ratings and delivers accurate and transparent human-vs-machine discrimination, offering a powerful tool for automatic human-likeness evaluation. Our work establishes the first human-likeness evaluation for S2S systems and moves beyond binary outcomes to enable detailed diagnostic insights, paving the way for human-like improvements in conversational AI systems.", "AI": {"tldr": "\u9996\u4e2a\u8bed\u97f3\u5230\u8bed\u97f3\u7cfb\u7edf\u7684\u56fe\u7075\u6d4b\u8bd5\u663e\u793a\uff0c\u73b0\u6709\u7cfb\u7edf\u5747\u672a\u901a\u8fc7\uff0c\u4e3b\u8981\u74f6\u9888\u5728\u4e8e\u526f\u8bed\u8a00\u7279\u5f81\u3001\u60c5\u611f\u8868\u8fbe\u548c\u5bf9\u8bdd\u4e2a\u6027\uff0c\u800c\u975e\u8bed\u4e49\u7406\u89e3\u3002", "motivation": "\u73b0\u4ee3\u8bed\u97f3\u5230\u8bed\u97f3\u7cfb\u7edf\u80fd\u5426\u50cf\u4eba\u7c7b\u4e00\u6837\u5bf9\u8bdd\u662f\u4e00\u4e2a\u5173\u952e\u4f46\u672a\u89e3\u51b3\u7684\u95ee\u9898\uff0c\u9700\u8981\u5efa\u7acb\u9996\u4e2a\u9488\u5bf9S2S\u7cfb\u7edf\u7684\u4eba\u7c7b\u76f8\u4f3c\u6027\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u8fdb\u884c\u4e86\u9996\u4e2aS2S\u7cfb\u7edf\u56fe\u7075\u6d4b\u8bd5\uff0c\u6536\u96c6\u4e862,968\u4e2a\u4eba\u7c7b\u5224\u65ad\uff0c\u6d89\u53ca9\u4e2a\u6700\u5148\u8fdbS2S\u7cfb\u7edf\u4e0e28\u540d\u4eba\u7c7b\u53c2\u4e0e\u8005\u7684\u5bf9\u8bdd\uff1b\u5f00\u53d1\u4e86\u5305\u542b18\u4e2a\u4eba\u7c7b\u76f8\u4f3c\u6027\u7ef4\u5ea6\u7684\u7ec6\u7c92\u5ea6\u5206\u7c7b\u6cd5\uff0c\u5e76\u5bf9\u6536\u96c6\u7684\u5bf9\u8bdd\u8fdb\u884c\u4f17\u5305\u6807\u6ce8\u3002", "result": "\u6240\u6709\u8bc4\u4f30\u7684S2S\u7cfb\u7edf\u5747\u672a\u901a\u8fc7\u56fe\u7075\u6d4b\u8bd5\uff1b\u74f6\u9888\u4e3b\u8981\u5728\u4e8e\u526f\u8bed\u8a00\u7279\u5f81\u3001\u60c5\u611f\u8868\u8fbe\u548c\u5bf9\u8bdd\u4e2a\u6027\uff0c\u800c\u975e\u8bed\u4e49\u7406\u89e3\uff1b\u73b0\u6210\u7684AI\u6a21\u578b\u4f5c\u4e3a\u56fe\u7075\u6d4b\u8bd5\u8bc4\u5224\u8005\u8868\u73b0\u4e0d\u53ef\u9760\u3002", "conclusion": "\u5efa\u7acb\u4e86\u9996\u4e2aS2S\u7cfb\u7edf\u4eba\u7c7b\u76f8\u4f3c\u6027\u8bc4\u4f30\u6846\u67b6\uff0c\u8d85\u8d8a\u4e86\u4e8c\u5143\u7ed3\u679c\uff0c\u63d0\u4f9b\u4e86\u8be6\u7ec6\u7684\u8bca\u65ad\u6d1e\u5bdf\uff1b\u63d0\u51fa\u4e86\u4e00\u4e2a\u5229\u7528\u7ec6\u7c92\u5ea6\u4eba\u7c7b\u76f8\u4f3c\u6027\u8bc4\u5206\u7684\u53ef\u89e3\u91ca\u6a21\u578b\uff0c\u4e3a\u81ea\u52a8\u4eba\u7c7b\u76f8\u4f3c\u6027\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5f3a\u5927\u5de5\u5177\u3002"}}
{"id": "2602.24082", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.24082", "abs": "https://arxiv.org/abs/2602.24082", "authors": ["Jaekyung Cho"], "title": "Preference Packing: Efficient Preference Optimization for Large Language Models", "comment": null, "summary": "Resource-efficient training optimization techniques are becoming increasingly important as the size of large language models (LLMs) continues to grow. In particular, batch packing is commonly used in pre-training and supervised fine-tuning to achieve resource-efficient training. We propose preference packing, a method to enhance resource efficiency in training techniques that use data with different responses for the same input prompt, such as reward models or Direct Preference Optimization (DPO). Preference packing improves resource efficiency by reducing the attention operations for duplicate input prompts and decreasing KV cache memory usage. We conducted experiments on text-only datasets and image-included datasets and achieved at least 37% reduction in training time. Notably, this method can be applied alongside existing optimization techniques such as batch sorting, resulting in a 3.22x speedup.", "AI": {"tldr": "\u63d0\u51fapreference packing\u65b9\u6cd5\uff0c\u901a\u8fc7\u51cf\u5c11\u91cd\u590d\u8f93\u5165\u63d0\u793a\u7684\u6ce8\u610f\u529b\u64cd\u4f5c\u548c\u964d\u4f4eKV\u7f13\u5b58\u5185\u5b58\u4f7f\u7528\uff0c\u63d0\u5347\u5956\u52b1\u6a21\u578b\u548cDPO\u7b49\u8bad\u7ec3\u7684\u8d44\u6e90\u6548\u7387\uff0c\u5b9e\u9a8c\u663e\u793a\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c11\u81f3\u5c1137%\uff0c\u7ed3\u5408\u73b0\u6709\u4f18\u5316\u6280\u672f\u53ef\u5b9e\u73b03.22\u500d\u52a0\u901f\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u89c4\u6a21\u6301\u7eed\u589e\u957f\uff0c\u8d44\u6e90\u9ad8\u6548\u7684\u8bad\u7ec3\u4f18\u5316\u6280\u672f\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\u3002\u7279\u522b\u662f\u5728\u4f7f\u7528\u76f8\u540c\u8f93\u5165\u63d0\u793a\u5bf9\u5e94\u4e0d\u540c\u54cd\u5e94\u7684\u6570\u636e\u8bad\u7ec3\u65f6\uff08\u5982\u5956\u52b1\u6a21\u578b\u6216DPO\uff09\uff0c\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u8d44\u6e90\u6548\u7387\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fapreference packing\u65b9\u6cd5\uff0c\u901a\u8fc7\u6253\u5305\u5904\u7406\u76f8\u540c\u8f93\u5165\u63d0\u793a\u5bf9\u5e94\u7684\u4e0d\u540c\u54cd\u5e94\u6570\u636e\uff0c\u51cf\u5c11\u91cd\u590d\u8f93\u5165\u63d0\u793a\u7684\u6ce8\u610f\u529b\u8ba1\u7b97\u64cd\u4f5c\uff0c\u964d\u4f4eKV\u7f13\u5b58\u5185\u5b58\u4f7f\u7528\uff0c\u4ece\u800c\u63d0\u5347\u8bad\u7ec3\u8d44\u6e90\u6548\u7387\u3002", "result": "\u5728\u7eaf\u6587\u672c\u6570\u636e\u96c6\u548c\u5305\u542b\u56fe\u50cf\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u5b9e\u73b0\u4e86\u81f3\u5c1137%\u7684\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c11\u3002\u8be5\u65b9\u6cd5\u53ef\u4e0e\u73b0\u6709\u4f18\u5316\u6280\u672f\uff08\u5982\u6279\u6b21\u6392\u5e8f\uff09\u7ed3\u5408\u4f7f\u7528\uff0c\u83b7\u5f973.22\u500d\u7684\u52a0\u901f\u6548\u679c\u3002", "conclusion": "preference packing\u662f\u4e00\u79cd\u6709\u6548\u7684\u8d44\u6e90\u6548\u7387\u4f18\u5316\u65b9\u6cd5\uff0c\u7279\u522b\u9002\u7528\u4e8e\u4f7f\u7528\u76f8\u540c\u8f93\u5165\u63d0\u793a\u5bf9\u5e94\u4e0d\u540c\u54cd\u5e94\u6570\u636e\u7684\u8bad\u7ec3\u573a\u666f\uff0c\u80fd\u591f\u663e\u8457\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\u548c\u8d44\u6e90\u6d88\u8017\u3002"}}
{"id": "2602.23901", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23901", "abs": "https://arxiv.org/abs/2602.23901", "authors": ["Fan Yang", "Peiguang Jing", "Kaihua Qu", "Ningyuan Zhao", "Yuting Su"], "title": "ABPolicy: Asynchronous B-Spline Flow Policy for Real-Time and Smooth Robotic Manipulation", "comment": null, "summary": "Robotic manipulation requires policies that are smooth and responsive to evolving observations. However, synchronous inference in the raw action space introduces several challenges, including intra-chunk jitter, inter-chunk discontinuities, and stop-and-go execution. These issues undermine a policy's smoothness and its responsiveness to environmental changes. We propose ABPolicy, an asynchronous flow-matching policy that operates in a B-spline control-point action space. First, the B-spline representation ensures intra-chunk smoothness. Second, we introduce bidirectional action prediction coupled with refitting optimization to enforce inter-chunk continuity. Finally, by leveraging asynchronous inference, ABPolicy delivers real-time, continuous updates. We evaluate ABPolicy across seven tasks encompassing both static settings and dynamic settings with moving objects. Empirical results indicate that ABPolicy reduces trajectory jerk, leading to smoother motion and improved performance. Project website: https://teee000.github.io/ABPolicy/.", "AI": {"tldr": "ABPolicy\u662f\u4e00\u79cd\u5f02\u6b65\u6d41\u5339\u914d\u7b56\u7565\uff0c\u901a\u8fc7\u5728B\u6837\u6761\u63a7\u5236\u70b9\u52a8\u4f5c\u7a7a\u95f4\u4e2d\u64cd\u4f5c\u6765\u89e3\u51b3\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u5e73\u6ed1\u6027\u548c\u54cd\u5e94\u6027\u95ee\u9898\uff0c\u51cf\u5c11\u8f68\u8ff9\u6296\u52a8\u5e76\u63d0\u9ad8\u6027\u80fd\u3002", "motivation": "\u673a\u5668\u4eba\u64cd\u4f5c\u9700\u8981\u5e73\u6ed1\u4e14\u80fd\u54cd\u5e94\u73af\u5883\u53d8\u5316\u7684\u7b56\u7565\uff0c\u4f46\u539f\u59cb\u52a8\u4f5c\u7a7a\u95f4\u4e2d\u7684\u540c\u6b65\u63a8\u7406\u4f1a\u5bfc\u81f4\u5757\u5185\u6296\u52a8\u3001\u5757\u95f4\u4e0d\u8fde\u7eed\u548c\u542f\u505c\u6267\u884c\u7b49\u95ee\u9898\uff0c\u5f71\u54cd\u7b56\u7565\u7684\u5e73\u6ed1\u6027\u548c\u54cd\u5e94\u6027\u3002", "method": "\u63d0\u51faABPolicy\u5f02\u6b65\u6d41\u5339\u914d\u7b56\u7565\uff0c\u5728B\u6837\u6761\u63a7\u5236\u70b9\u52a8\u4f5c\u7a7a\u95f4\u4e2d\u64cd\u4f5c\uff1a1\uff09B\u6837\u6761\u8868\u793a\u786e\u4fdd\u5757\u5185\u5e73\u6ed1\u6027\uff1b2\uff09\u5f15\u5165\u53cc\u5411\u52a8\u4f5c\u9884\u6d4b\u7ed3\u5408\u91cd\u65b0\u62df\u5408\u4f18\u5316\u6765\u5f3a\u5236\u5757\u95f4\u8fde\u7eed\u6027\uff1b3\uff09\u5229\u7528\u5f02\u6b65\u63a8\u7406\u5b9e\u73b0\u5b9e\u65f6\u8fde\u7eed\u66f4\u65b0\u3002", "result": "\u5728\u5305\u542b\u9759\u6001\u548c\u52a8\u6001\u73af\u5883\u7684\u4e03\u4e2a\u4efb\u52a1\u4e2d\u8bc4\u4f30ABPolicy\uff0c\u7ecf\u9a8c\u7ed3\u679c\u8868\u660eABPolicy\u51cf\u5c11\u4e86\u8f68\u8ff9\u6296\u52a8\uff0c\u5b9e\u73b0\u4e86\u66f4\u5e73\u6ed1\u7684\u8fd0\u52a8\u5e76\u63d0\u9ad8\u4e86\u6027\u80fd\u3002", "conclusion": "ABPolicy\u901a\u8fc7B\u6837\u6761\u8868\u793a\u3001\u53cc\u5411\u9884\u6d4b\u548c\u5f02\u6b65\u63a8\u7406\u7684\u7ec4\u5408\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u4e2d\u7684\u5e73\u6ed1\u6027\u548c\u54cd\u5e94\u6027\u95ee\u9898\uff0c\u4e3a\u5b9e\u65f6\u8fde\u7eed\u63a7\u5236\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.23709", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23709", "abs": "https://arxiv.org/abs/2602.23709", "authors": ["Shitong Sun", "Ke Han", "Yukai Huang", "Weitong Cai", "Jifei Song"], "title": "EgoGraph: Temporal Knowledge Graph for Egocentric Video Understanding", "comment": "Under review", "summary": "Ultra-long egocentric videos spanning multiple days present significant challenges for video understanding. Existing approaches still rely on fragmented local processing and limited temporal modeling, restricting their ability to reason over such extended sequences. To address these limitations, we introduce EgoGraph, a training-free and dynamic knowledge-graph construction framework that explicitly encodes long-term, cross-entity dependencies in egocentric video streams. EgoGraph employs a novel egocentric schema that unifies the extraction and abstraction of core entities, such as people, objects, locations, and events, and structurally reasons about their attributes and interactions, yielding a significantly richer and more coherent semantic representation than traditional clip-based video models. Crucially, we develop a temporal relational modeling strategy that captures temporal dependencies across entities and accumulates stable long-term memory over multiple days, enabling complex temporal reasoning. Extensive experiments on the EgoLifeQA and EgoR1-bench benchmarks demonstrate that EgoGraph achieves state-of-the-art performance on long-term video question answering, validating its effectiveness as a new paradigm for ultra-long egocentric video understanding.", "AI": {"tldr": "EgoGraph\uff1a\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u52a8\u6001\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u6846\u67b6\uff0c\u7528\u4e8e\u8d85\u957f\u7b2c\u4e00\u4eba\u79f0\u89c6\u9891\u7406\u89e3\uff0c\u901a\u8fc7\u7f16\u7801\u8de8\u5b9e\u4f53\u957f\u671f\u4f9d\u8d56\u5173\u7cfb\u5b9e\u73b0\u66f4\u597d\u7684\u65f6\u5e8f\u63a8\u7406", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u8de8\u8d8a\u591a\u5929\u7684\u8d85\u957f\u7b2c\u4e00\u4eba\u79f0\u89c6\u9891\u65f6\uff0c\u4ecd\u7136\u4f9d\u8d56\u788e\u7247\u5316\u7684\u5c40\u90e8\u5904\u7406\u548c\u6709\u9650\u7684\u65f6\u5e8f\u5efa\u6a21\uff0c\u65e0\u6cd5\u6709\u6548\u63a8\u7406\u8fd9\u79cd\u8d85\u957f\u5e8f\u5217\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7f16\u7801\u957f\u671f\u8de8\u5b9e\u4f53\u4f9d\u8d56\u5173\u7cfb\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u63d0\u51faEgoGraph\u6846\u67b6\uff1a1\uff09\u91c7\u7528\u65b0\u9896\u7684\u7b2c\u4e00\u4eba\u79f0\u6a21\u5f0f\u7edf\u4e00\u63d0\u53d6\u548c\u62bd\u8c61\u6838\u5fc3\u5b9e\u4f53\uff08\u4eba\u7269\u3001\u7269\u4f53\u3001\u4f4d\u7f6e\u3001\u4e8b\u4ef6\uff09\uff1b2\uff09\u7ed3\u6784\u5316\u63a8\u7406\u5b9e\u4f53\u5c5e\u6027\u548c\u4ea4\u4e92\uff1b3\uff09\u5f00\u53d1\u65f6\u5e8f\u5173\u7cfb\u5efa\u6a21\u7b56\u7565\u6355\u6349\u8de8\u5b9e\u4f53\u65f6\u5e8f\u4f9d\u8d56\uff1b4\uff09\u79ef\u7d2f\u591a\u5929\u7684\u7a33\u5b9a\u957f\u671f\u8bb0\u5fc6\u3002", "result": "\u5728EgoLifeQA\u548cEgoR1-bench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cEgoGraph\u5728\u957f\u671f\u89c6\u9891\u95ee\u7b54\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5176\u4f5c\u4e3a\u8d85\u957f\u7b2c\u4e00\u4eba\u79f0\u89c6\u9891\u7406\u89e3\u65b0\u8303\u5f0f\u7684\u6709\u6548\u6027\u3002", "conclusion": "EgoGraph\u901a\u8fc7\u6784\u5efa\u52a8\u6001\u77e5\u8bc6\u56fe\u8c31\uff0c\u663e\u5f0f\u7f16\u7801\u957f\u671f\u8de8\u5b9e\u4f53\u4f9d\u8d56\u5173\u7cfb\uff0c\u4e3a\u8d85\u957f\u7b2c\u4e00\u4eba\u79f0\u89c6\u9891\u7406\u89e3\u63d0\u4f9b\u4e86\u66f4\u4e30\u5bcc\u3001\u66f4\u8fde\u8d2f\u7684\u8bed\u4e49\u8868\u793a\uff0c\u5b9e\u73b0\u4e86\u590d\u6742\u7684\u65f6\u5e8f\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2602.23638", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23638", "abs": "https://arxiv.org/abs/2602.23638", "authors": ["Haoran Zhang", "Dongjun Kim", "Seohyeon Cha", "Haris Vikalo"], "title": "FedRot-LoRA: Mitigating Rotational Misalignment in Federated LoRA", "comment": "preprint", "summary": "Federated LoRA provides a communication-efficient mechanism for fine-tuning large language models on decentralized data. In practice, however, a discrepancy between the factor-wise averaging used to preserve low rank and the mathematically correct aggregation of local updates can cause significant aggregation error and unstable training. We argue that a major source of this problem is rotational misalignment, arising from the rotational invariance of low-rank factorizations -- semantically equivalent updates can be represented in different latent subspaces across clients since $(B_i R_i)(R_i^\\top A_i) = B_i A_i$. When such misaligned factors are averaged directly, they interfere destructively and degrade the global update. To address this issue, we propose FedRot-LoRA, a federated LoRA framework that aligns client updates via orthogonal transformations prior to aggregation. This alignment preserves the semantic update while reducing cross-client subspace mismatch, without increasing communication cost or restricting model expressivity. We provide a convergence analysis that examines the aggregation error induced by factor-wise averaging and shows how rotational alignment yields a tighter upper bound on this error. Extensive experiments on natural language understanding and generative tasks demonstrate that FedRot-LoRA consistently outperforms existing federated LoRA baselines across a range of heterogeneity levels and LoRA ranks.", "AI": {"tldr": "FedRot-LoRA\uff1a\u901a\u8fc7\u6b63\u4ea4\u53d8\u6362\u5bf9\u9f50\u5ba2\u6237\u7aef\u66f4\u65b0\u7684\u8054\u90a6LoRA\u6846\u67b6\uff0c\u89e3\u51b3\u56e0\u5b50\u5e73\u5747\u5bfc\u81f4\u7684\u65cb\u8f6c\u4e0d\u5bf9\u9f50\u95ee\u9898\uff0c\u63d0\u5347\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6027\u80fd", "motivation": "\u8054\u90a6LoRA\u5728\u53bb\u4e2d\u5fc3\u5316\u6570\u636e\u4e0a\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\u65f6\uff0c\u56e0\u5b50\u5e73\u5747\u4e0e\u6570\u5b66\u6b63\u786e\u805a\u5408\u4e4b\u95f4\u7684\u5dee\u5f02\u4f1a\u5bfc\u81f4\u663e\u8457\u7684\u805a\u5408\u8bef\u5dee\u548c\u4e0d\u7a33\u5b9a\u8bad\u7ec3\u3002\u4e3b\u8981\u95ee\u9898\u662f\u65cb\u8f6c\u4e0d\u5bf9\u9f50\u2014\u2014\u7531\u4e8e\u4f4e\u79e9\u5206\u89e3\u7684\u65cb\u8f6c\u4e0d\u53d8\u6027\uff0c\u8bed\u4e49\u7b49\u6548\u7684\u66f4\u65b0\u5728\u4e0d\u540c\u5ba2\u6237\u7aef\u53ef\u80fd\u8868\u793a\u5728\u4e0d\u540c\u7684\u6f5c\u5728\u5b50\u7a7a\u95f4\u4e2d\uff0c\u76f4\u63a5\u5e73\u5747\u8fd9\u4e9b\u4e0d\u5bf9\u9f50\u7684\u56e0\u5b50\u4f1a\u4ea7\u751f\u7834\u574f\u6027\u5e72\u6270\u3002", "method": "\u63d0\u51faFedRot-LoRA\u6846\u67b6\uff0c\u5728\u805a\u5408\u524d\u901a\u8fc7\u6b63\u4ea4\u53d8\u6362\u5bf9\u9f50\u5ba2\u6237\u7aef\u66f4\u65b0\u3002\u8fd9\u79cd\u5bf9\u9f50\u5728\u4fdd\u6301\u8bed\u4e49\u66f4\u65b0\u7684\u540c\u65f6\u51cf\u5c11\u8de8\u5ba2\u6237\u7aef\u5b50\u7a7a\u95f4\u4e0d\u5339\u914d\uff0c\u4e0d\u589e\u52a0\u901a\u4fe1\u6210\u672c\u6216\u9650\u5236\u6a21\u578b\u8868\u8fbe\u80fd\u529b\u3002\u63d0\u4f9b\u4e86\u6536\u655b\u5206\u6790\uff0c\u68c0\u67e5\u56e0\u5b50\u5e73\u5747\u5f15\u8d77\u7684\u805a\u5408\u8bef\u5dee\uff0c\u5e76\u5c55\u793a\u65cb\u8f6c\u5bf9\u9f50\u5982\u4f55\u4ea7\u751f\u66f4\u7d27\u7684\u8bef\u5dee\u4e0a\u754c\u3002", "result": "\u5728\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cFedRot-LoRA\u5728\u5404\u79cd\u5f02\u6784\u7a0b\u5ea6\u548cLoRA\u79e9\u7684\u8bbe\u7f6e\u4e0b\uff0c\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u7684\u8054\u90a6LoRA\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "FedRot-LoRA\u901a\u8fc7\u89e3\u51b3\u8054\u90a6LoRA\u4e2d\u7684\u65cb\u8f6c\u4e0d\u5bf9\u9f50\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u4fe1\u9ad8\u6548\u4e14\u7a33\u5b9a\u7684\u5fae\u8c03\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8054\u90a6\u5b66\u4e60\u573a\u666f\u4e0b\u7684\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2602.24109", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.24109", "abs": "https://arxiv.org/abs/2602.24109", "authors": ["Sara Nabhani", "Federico Pianzola", "Khalid Al-Khatib", "Malvina Nissim"], "title": "ARGUS: Seeing the Influence of Narrative Features on Persuasion in Argumentative Texts", "comment": "22 pages, 8 figures, submitted to ACM Transactions on Intelligent Systems and Technology", "summary": "Can narratives make arguments more persuasive? And to this end, which narrative features matter most? Although stories are often seen as powerful tools for persuasion, their specific role in online, unstructured argumentation remains underexplored. To address this gap, we present ARGUS, a framework for studying the impact of narration on persuasion in argumentative discourse. ARGUS introduces a new ChangeMyView corpus annotated for story presence and six key narrative features, integrating insights from two established theoretical frameworks that capture both textual narrative features and their effects on recipients. Leveraging both encoder-based classifiers and zero-shot large language models (LLMs), ARGUS identifies stories and narrative features and applies them at scale to examine how different narrative dimensions influence persuasion success in online argumentation.", "AI": {"tldr": "ARGUS\u6846\u67b6\u7814\u7a76\u53d9\u4e8b\u5728\u5728\u7ebf\u8bba\u8bc1\u4e2d\u7684\u8bf4\u670d\u529b\u5f71\u54cd\uff0c\u901a\u8fc7\u6807\u6ce8\u6545\u4e8b\u5b58\u5728\u548c\u516d\u4e2a\u5173\u952e\u53d9\u4e8b\u7279\u5f81\uff0c\u7ed3\u5408\u7406\u8bba\u6846\u67b6\u548cLLM\u6280\u672f\u5206\u6790\u4e0d\u540c\u53d9\u4e8b\u7ef4\u5ea6\u5982\u4f55\u5f71\u54cd\u8bf4\u670d\u6210\u529f", "motivation": "\u867d\u7136\u6545\u4e8b\u5e38\u88ab\u89c6\u4e3a\u5f3a\u5927\u7684\u8bf4\u670d\u5de5\u5177\uff0c\u4f46\u5176\u5728\u5728\u7ebf\u975e\u7ed3\u6784\u5316\u8bba\u8bc1\u4e2d\u7684\u5177\u4f53\u4f5c\u7528\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u7814\u7a76\u53d9\u4e8b\u5982\u4f55\u5f71\u54cd\u5728\u7ebf\u8bba\u8bc1\u4e2d\u7684\u8bf4\u670d\u529b", "method": "\u63d0\u51faARGUS\u6846\u67b6\uff0c\u521b\u5efa\u65b0\u7684ChangeMyView\u8bed\u6599\u5e93\uff0c\u6807\u6ce8\u6545\u4e8b\u5b58\u5728\u548c\u516d\u4e2a\u5173\u952e\u53d9\u4e8b\u7279\u5f81\u3002\u7ed3\u5408\u4e24\u4e2a\u6210\u719f\u7684\u7406\u8bba\u6846\u67b6\uff0c\u4f7f\u7528\u7f16\u7801\u5668\u5206\u7c7b\u5668\u548c\u96f6\u6837\u672c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8bc6\u522b\u6545\u4e8b\u548c\u53d9\u4e8b\u7279\u5f81\uff0c\u5927\u89c4\u6a21\u5206\u6790\u53d9\u4e8b\u7ef4\u5ea6\u5bf9\u8bf4\u670d\u6210\u529f\u7684\u5f71\u54cd", "result": "ARGUS\u6846\u67b6\u80fd\u591f\u8bc6\u522b\u6545\u4e8b\u548c\u53d9\u4e8b\u7279\u5f81\uff0c\u5e76\u5e94\u7528\u4e8e\u5927\u89c4\u6a21\u5206\u6790\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u53d9\u4e8b\u7ef4\u5ea6\u5982\u4f55\u5f71\u54cd\u5728\u7ebf\u8bba\u8bc1\u4e2d\u7684\u8bf4\u670d\u6210\u529f", "conclusion": "\u53d9\u4e8b\u786e\u5b9e\u80fd\u589e\u5f3a\u8bba\u8bc1\u7684\u8bf4\u670d\u529b\uff0cARGUS\u6846\u67b6\u4e3a\u7814\u7a76\u53d9\u4e8b\u5728\u5728\u7ebf\u8bba\u8bc1\u4e2d\u7684\u4f5c\u7528\u63d0\u4f9b\u4e86\u7cfb\u7edf\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u5173\u952e\u53d9\u4e8b\u7279\u5f81\u5bf9\u8bf4\u670d\u6548\u679c\u7684\u5f71\u54cd"}}
{"id": "2602.23923", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.23923", "abs": "https://arxiv.org/abs/2602.23923", "authors": ["Rolif Lima", "Somdeb Saha", "Nijil George", "Vismay Vakharia", "Shubham Parab", "Sahil Gaonkar", "Vighnesh Vatsal", "Kaushik Das"], "title": "Teleoperated Omni-directional Dual Arm Mobile Manipulation Robotic System with Shared Control for Retail Store", "comment": "This work has been accepted for publication in the Proceedings of the IEEE International Conference on Systems, Man, and Cybernetics (SMC 2024). $\u00a9$ IEEE. The final version is available via IEEE Xplore", "summary": "The swiftly expanding retail sector is increasingly adopting autonomous mobile robots empowered by artificial intelligence and machine learning algorithms to gain an edge in the competitive market. However, these autonomous robots encounter challenges in adapting to the dynamic nature of retail products, often struggling to operate autonomously in novel situations. In this study, we introduce an omni-directional dual-arm mobile robot specifically tailored for use in retail environments. Additionally, we propose a tele-operation method that enables shared control between the robot and a human operator. This approach utilizes a Virtual Reality (VR) motion capture system to capture the operator's commands, which are then transmitted to the robot located remotely in a retail setting. Furthermore, the robot is equipped with heterogeneous grippers on both manipulators, facilitating the handling of a wide range of items. We validate the efficacy of the proposed system through testing in a mockup of retail environment, demonstrating its ability to manipulate various commonly encountered retail items using both single and dual-arm coordinated manipulation techniques.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u96f6\u552e\u73af\u5883\u7684\u5168\u5411\u53cc\u81c2\u79fb\u52a8\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u7ed3\u5408VR\u8fdc\u7a0b\u64cd\u4f5c\u548c\u5f02\u6784\u5939\u722a\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u96f6\u552e\u7269\u54c1\u7684\u7075\u6d3b\u6293\u53d6", "motivation": "\u96f6\u552e\u4e1a\u5feb\u901f\u53d1\u5c55\uff0cAI\u9a71\u52a8\u7684\u81ea\u4e3b\u79fb\u52a8\u673a\u5668\u4eba\u9762\u4e34\u52a8\u6001\u96f6\u552e\u4ea7\u54c1\u73af\u5883\u7684\u9002\u5e94\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u65b0\u60c5\u5883\u4e0b\u7684\u81ea\u4e3b\u64cd\u4f5c\u56f0\u96be", "method": "\u5f00\u53d1\u5168\u5411\u53cc\u81c2\u79fb\u52a8\u673a\u5668\u4eba\uff0c\u91c7\u7528VR\u8fd0\u52a8\u6355\u6349\u7cfb\u7edf\u5b9e\u73b0\u4eba\u673a\u5171\u4eab\u63a7\u5236\u8fdc\u7a0b\u64cd\u4f5c\uff0c\u53cc\u81c2\u914d\u5907\u5f02\u6784\u5939\u722a\u4ee5\u5904\u7406\u591a\u6837\u5316\u7269\u54c1", "result": "\u5728\u6a21\u62df\u96f6\u552e\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u6709\u6548\u6027\uff0c\u80fd\u591f\u4f7f\u7528\u5355\u81c2\u548c\u53cc\u81c2\u534f\u8c03\u64cd\u4f5c\u6280\u672f\u5904\u7406\u5404\u79cd\u5e38\u89c1\u96f6\u552e\u7269\u54c1", "conclusion": "\u63d0\u51fa\u7684\u673a\u5668\u4eba\u7cfb\u7edf\u548c\u8fdc\u7a0b\u64cd\u4f5c\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u96f6\u552e\u73af\u5883\u4e2d\u673a\u5668\u4eba\u81ea\u4e3b\u64cd\u4f5c\u7684\u9002\u5e94\u6027\u95ee\u9898\uff0c\u63d0\u5347\u96f6\u552e\u81ea\u52a8\u5316\u7684\u7075\u6d3b\u6027"}}
{"id": "2602.23711", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23711", "abs": "https://arxiv.org/abs/2602.23711", "authors": ["Hongbo Jiang", "Jie Li", "Yunhang Shen", "Pingyang Dai", "Xing Sun", "Haoyu Cao", "Liujuan Cao"], "title": "Can Unified Generation and Understanding Models Maintain Semantic Equivalence Across Different Output Modalities?", "comment": "Equal contribution by Jie Li", "summary": "Unified Multimodal Large Language Models (U-MLLMs) integrate understanding and generation within a single architecture. However, existing evaluations typically assess these capabilities separately, overlooking semantic equivalence, i.e., the ability to manifest consistent reasoning results regardless of the output modality. In this work, we investigate whether current U-MLLMs satisfy this premise. We observe that while models demonstrate robust textual reasoning, they fail to maintain semantic equivalence when required to render the same results in the image modality. To rigorously diagnose this discrepancy, we introduce VGUBench, a framework to decouple reasoning logic from generation fidelity. VGUBench comprises three diagnostic tasks: (1)Textual Generative Understanding, establishing a baseline for reasoning accuracy in textual response; (2)Visual Generative Understanding, evaluating the ability to generate visual responses that represent the correct answer; and (3)a Visual Rendering control task, which assesses the ability to directly render explicit visual descriptions into images without complex reasoning. Our evaluation reveals a significant disparity: despite strong performance in textual understanding and visual rendering, U-MLLMs exhibit a marked performance collapse when required to generate visual answers to questions. Furthermore, we find a negligible correlation between visual answering performance and basic rendering quality. These results suggest that the failure stems not from insufficient generation fidelity, but from a breakdown in cross-modal semantic alignment. We provide diagnostic insights to address this challenge in future Unified Generation and Understanding Models.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u7edf\u4e00\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6587\u672c\u63a8\u7406\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u9700\u8981\u5c06\u76f8\u540c\u63a8\u7406\u7ed3\u679c\u4ee5\u56fe\u50cf\u5f62\u5f0f\u5448\u73b0\u65f6\uff0c\u65e0\u6cd5\u4fdd\u6301\u8bed\u4e49\u4e00\u81f4\u6027\uff0c\u5b58\u5728\u8de8\u6a21\u6001\u8bed\u4e49\u5bf9\u9f50\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7edf\u4e00\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u901a\u5e38\u5c06\u7406\u89e3\u548c\u751f\u6210\u80fd\u529b\u5206\u5f00\u8bc4\u4f30\uff0c\u5ffd\u7565\u4e86\u8bed\u4e49\u7b49\u4ef7\u6027\u2014\u2014\u5373\u65e0\u8bba\u8f93\u51fa\u6a21\u6001\u5982\u4f55\u90fd\u80fd\u8868\u73b0\u4e00\u81f4\u63a8\u7406\u7ed3\u679c\u7684\u80fd\u529b\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7a76\u5f53\u524dU-MLLMs\u662f\u5426\u6ee1\u8db3\u8fd9\u4e00\u524d\u63d0\u3002", "method": "\u5f15\u5165VGUBench\u6846\u67b6\uff0c\u5c06\u63a8\u7406\u903b\u8f91\u4e0e\u751f\u6210\u4fdd\u771f\u5ea6\u89e3\u8026\u3002\u5305\u542b\u4e09\u4e2a\u8bca\u65ad\u4efb\u52a1\uff1a1)\u6587\u672c\u751f\u6210\u7406\u89e3\uff0c\u5efa\u7acb\u6587\u672c\u54cd\u5e94\u7684\u63a8\u7406\u51c6\u786e\u6027\u57fa\u7ebf\uff1b2)\u89c6\u89c9\u751f\u6210\u7406\u89e3\uff0c\u8bc4\u4f30\u751f\u6210\u6b63\u786e\u89c6\u89c9\u7b54\u6848\u7684\u80fd\u529b\uff1b3)\u89c6\u89c9\u6e32\u67d3\u63a7\u5236\u4efb\u52a1\uff0c\u8bc4\u4f30\u76f4\u63a5\u5c06\u660e\u786e\u89c6\u89c9\u63cf\u8ff0\u6e32\u67d3\u4e3a\u56fe\u50cf\u7684\u80fd\u529b\u3002", "result": "\u8bc4\u4f30\u663e\u793a\u663e\u8457\u5dee\u5f02\uff1a\u5c3d\u7ba1\u5728\u6587\u672c\u7406\u89e3\u548c\u89c6\u89c9\u6e32\u67d3\u65b9\u9762\u8868\u73b0\u5f3a\u52b2\uff0c\u4f46U-MLLMs\u5728\u9700\u8981\u751f\u6210\u89c6\u89c9\u7b54\u6848\u65f6\u51fa\u73b0\u660e\u663e\u6027\u80fd\u5d29\u6e83\u3002\u89c6\u89c9\u56de\u7b54\u6027\u80fd\u4e0e\u57fa\u672c\u6e32\u67d3\u8d28\u91cf\u4e4b\u95f4\u51e0\u4e4e\u6ca1\u6709\u76f8\u5173\u6027\u3002", "conclusion": "\u5931\u8d25\u539f\u56e0\u4e0d\u662f\u751f\u6210\u4fdd\u771f\u5ea6\u4e0d\u8db3\uff0c\u800c\u662f\u8de8\u6a21\u6001\u8bed\u4e49\u5bf9\u9f50\u7684\u5d29\u6e83\u3002\u7814\u7a76\u63d0\u4f9b\u4e86\u8bca\u65ad\u89c1\u89e3\uff0c\u4ee5\u89e3\u51b3\u672a\u6765\u7edf\u4e00\u751f\u6210\u548c\u7406\u89e3\u6a21\u578b\u4e2d\u7684\u8fd9\u4e00\u6311\u6218\u3002"}}
{"id": "2602.23662", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23662", "abs": "https://arxiv.org/abs/2602.23662", "authors": ["Kohei Obata", "Zheng Chen", "Yasuko Matsubara", "Lingwei Zhu", "Yasushi Sakurai"], "title": "Selective Denoising Diffusion Model for Time Series Anomaly Detection", "comment": null, "summary": "Time series anomaly detection (TSAD) has been an important area of research for decades, with reconstruction-based methods, mostly based on generative models, gaining popularity and demonstrating success. Diffusion models have recently attracted attention due to their advanced generative capabilities. Existing diffusion-based methods for TSAD rely on a conditional strategy, which reconstructs input instances from white noise with the aid of the conditioner. However, this poses challenges in accurately reconstructing the normal parts, resulting in suboptimal detection performance. In response, we propose a novel diffusion-based method, named AnomalyFilter, which acts as a selective filter that only denoises anomaly parts in the instance while retaining normal parts. To build such a filter, we mask Gaussian noise during the training phase and conduct the denoising process without adding noise to the instances. The synergy of the two simple components greatly enhances the performance of naive diffusion models. Extensive experiments on five datasets demonstrate that AnomalyFilter achieves notably low reconstruction error on normal parts, providing empirical support for its effectiveness in anomaly detection. AnomalyFilter represents a pioneering approach that focuses on the noise design of diffusion models specifically tailored for TSAD.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAnomalyFilter\u7684\u65b0\u578b\u6269\u6563\u6a21\u578b\u65b9\u6cd5\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u8fc7\u6ee4\u4ec5\u5bf9\u65f6\u95f4\u5e8f\u5217\u4e2d\u7684\u5f02\u5e38\u90e8\u5206\u8fdb\u884c\u53bb\u566a\uff0c\u540c\u65f6\u4fdd\u7559\u6b63\u5e38\u90e8\u5206\uff0c\u4ece\u800c\u63d0\u5347\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u91c7\u7528\u6761\u4ef6\u7b56\u7565\uff0c\u4ece\u767d\u566a\u58f0\u4e2d\u91cd\u5efa\u8f93\u5165\u5b9e\u4f8b\uff0c\u4f46\u96be\u4ee5\u51c6\u786e\u91cd\u5efa\u6b63\u5e38\u90e8\u5206\uff0c\u5bfc\u81f4\u68c0\u6d4b\u6027\u80fd\u4e0d\u7406\u60f3\u3002\u9700\u8981\u4e00\u79cd\u80fd\u9009\u62e9\u6027\u5904\u7406\u5f02\u5e38\u90e8\u5206\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faAnomalyFilter\u65b9\u6cd5\uff0c\u5728\u8bad\u7ec3\u9636\u6bb5\u5bf9\u9ad8\u65af\u566a\u58f0\u8fdb\u884c\u63a9\u7801\u5904\u7406\uff0c\u5728\u53bb\u566a\u8fc7\u7a0b\u4e2d\u4e0d\u5411\u5b9e\u4f8b\u6dfb\u52a0\u566a\u58f0\uff0c\u6784\u5efa\u4e00\u4e2a\u9009\u62e9\u6027\u8fc7\u6ee4\u5668\uff0c\u4ec5\u5bf9\u5f02\u5e38\u90e8\u5206\u8fdb\u884c\u53bb\u566a\u800c\u4fdd\u7559\u6b63\u5e38\u90e8\u5206\u3002", "result": "\u5728\u4e94\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cAnomalyFilter\u5728\u6b63\u5e38\u90e8\u5206\u5b9e\u73b0\u4e86\u663e\u8457\u8f83\u4f4e\u7684\u91cd\u5efa\u8bef\u5dee\uff0c\u4e3a\u5f02\u5e38\u68c0\u6d4b\u7684\u6709\u6548\u6027\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u652f\u6301\u3002", "conclusion": "AnomalyFilter\u4ee3\u8868\u4e86\u4e00\u79cd\u4e13\u6ce8\u4e8e\u4e3a\u65f6\u95f4\u5e8f\u5217\u5f02\u5e38\u68c0\u6d4b\u91cf\u8eab\u5b9a\u5236\u7684\u6269\u6563\u6a21\u578b\u566a\u58f0\u8bbe\u8ba1\u7684\u5f00\u521b\u6027\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e24\u4e2a\u7b80\u5355\u7ec4\u4ef6\u7684\u534f\u540c\u4f5c\u7528\u5927\u5927\u63d0\u5347\u4e86\u6734\u7d20\u6269\u6563\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2602.24119", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.24119", "abs": "https://arxiv.org/abs/2602.24119", "authors": ["James L. Zainaldin", "Cameron Pattison", "Manuela Marai", "Jacob Wu", "Mark J. Schiefsky"], "title": "Terminology Rarity Predicts Catastrophic Failure in LLM Translation of Low-Resource Ancient Languages: Evidence from Ancient Greek", "comment": "Article + supplementary information", "summary": "This study presents the first systematic, reference-free human evaluation of large language model (LLM) machine translation (MT) for Ancient Greek (AG) technical prose. We evaluate translations by three commercial LLMs (Claude, Gemini, ChatGPT) of twenty paragraph-length passages from two works by the Greek physician Galen of Pergamum (ca. 129-216 CE): On Mixtures, which has two published English translations, and On the Composition of Drugs according to Kinds, which has never been fully translated into English. We assess translation quality using both standard automated evaluation metrics (BLEU, chrF++, METEOR, ROUGE-L, BERTScore, COMET, BLEURT) and expert human evaluation via a modified Multidimensional Quality Metrics (MQM) framework applied to all 60 translations by a team of domain specialists. On the previously translated expository text, LLMs achieved high translation quality (mean MQM score 95.2/100), with performance approaching expert level. On the untranslated pharmacological text, aggregate quality was lower (79.9/100) but with high variance driven by two passages presenting extreme terminological density; excluding these, scores converged to within 4 points of the translated text. Terminology rarity, operationalized via corpus frequency in the literary Diorisis Ancient Greek Corpus, emerged as a strong predictor of translation failure (r = -.97 for passage-level quality on the untranslated text). Automated metrics showed moderate correlation with human judgment overall on the text with a wide quality spread (Composition), but no metric discriminated among high-quality translations. We discuss implications for the use of LLMs in Classical scholarship and for the design of automated evaluation pipelines for low-resource ancient languages.", "AI": {"tldr": "\u672c\u7814\u7a76\u9996\u6b21\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u53e4\u5e0c\u814a\u6280\u672f\u6563\u6587\u7ffb\u8bd1\u65b9\u9762\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u3001\u65e0\u53c2\u8003\u7684\u4eba\u7c7b\u8bc4\u4f30\uff0c\u8bc4\u4f30\u4e86\u4e09\u79cd\u5546\u4e1aLLM\u5bf9\u76d6\u4f26\u533b\u5b66\u8457\u4f5c\u7684\u7ffb\u8bd1\u8d28\u91cf\uff0c\u53d1\u73b0\u7ffb\u8bd1\u8d28\u91cf\u4e0e\u672f\u8bed\u7a00\u6709\u5ea6\u9ad8\u5ea6\u76f8\u5173\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u586b\u8865\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u53e4\u4ee3\u8bed\u8a00\u7ffb\u8bd1\u65b9\u9762\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u7279\u522b\u662f\u9488\u5bf9\u53e4\u5e0c\u814a\u6280\u672f\u6563\u6587\u8fd9\u79cd\u4e13\u4e1a\u9886\u57df\uff0c\u4e3a\u53e4\u5178\u5b66\u672f\u7814\u7a76\u548c\u81ea\u52a8\u5316\u8bc4\u4f30\u6d41\u7a0b\u8bbe\u8ba1\u63d0\u4f9b\u4f9d\u636e\u3002", "method": "\u4f7f\u7528\u4e09\u79cd\u5546\u4e1aLLM\uff08Claude\u3001Gemini\u3001ChatGPT\uff09\u7ffb\u8bd1\u76d6\u4f26\u7684\u4e24\u90e8\u533b\u5b66\u8457\u4f5c\u6bb5\u843d\uff0c\u5176\u4e2d\u4e00\u90e8\u5df2\u6709\u82f1\u6587\u8bd1\u672c\uff0c\u53e6\u4e00\u90e8\u4ece\u672a\u5b8c\u5168\u7ffb\u8bd1\u3002\u91c7\u7528\u6807\u51c6\u81ea\u52a8\u5316\u8bc4\u4f30\u6307\u6807\uff08BLEU\u3001chrF++\u7b49\uff09\u548c\u4e13\u5bb6\u4eba\u7c7b\u8bc4\u4f30\uff08\u4fee\u6539\u7248MQM\u6846\u67b6\uff09\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\u3002", "result": "\u5728\u5df2\u6709\u7ffb\u8bd1\u7684\u8bf4\u660e\u6027\u6587\u672c\u4e0a\uff0cLLM\u8fbe\u5230\u9ad8\u8d28\u91cf\u7ffb\u8bd1\uff08\u5e73\u5747MQM\u5f97\u520695.2/100\uff09\uff0c\u63a5\u8fd1\u4e13\u5bb6\u6c34\u5e73\u3002\u5728\u672a\u7ffb\u8bd1\u7684\u836f\u7406\u5b66\u6587\u672c\u4e0a\uff0c\u603b\u4f53\u8d28\u91cf\u8f83\u4f4e\uff0879.9/100\uff09\uff0c\u4f46\u6392\u9664\u4e24\u4e2a\u672f\u8bed\u5bc6\u5ea6\u6781\u9ad8\u7684\u6bb5\u843d\u540e\uff0c\u5f97\u5206\u63a5\u8fd1\u5df2\u6709\u7ffb\u8bd1\u6587\u672c\u3002\u672f\u8bed\u7a00\u6709\u5ea6\uff08\u901a\u8fc7\u8bed\u6599\u5e93\u9891\u7387\u8861\u91cf\uff09\u662f\u7ffb\u8bd1\u5931\u8d25\u7684\u91cd\u8981\u9884\u6d4b\u56e0\u5b50\uff08r=-0.97\uff09\u3002\u81ea\u52a8\u5316\u6307\u6807\u5728\u8d28\u91cf\u5dee\u5f02\u5927\u7684\u6587\u672c\u4e0a\u4e0e\u4eba\u7c7b\u5224\u65ad\u6709\u4e2d\u7b49\u76f8\u5173\u6027\uff0c\u4f46\u65e0\u6cd5\u533a\u5206\u9ad8\u8d28\u91cf\u7ffb\u8bd1\u3002", "conclusion": "LLM\u5728\u53e4\u5e0c\u814a\u6280\u672f\u6563\u6587\u7ffb\u8bd1\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u7279\u522b\u662f\u5728\u5df2\u6709\u53c2\u8003\u7ffb\u8bd1\u7684\u6587\u672c\u4e0a\u63a5\u8fd1\u4e13\u5bb6\u6c34\u5e73\u3002\u672f\u8bed\u7a00\u6709\u5ea6\u662f\u7ffb\u8bd1\u8d28\u91cf\u7684\u5173\u952e\u5f71\u54cd\u56e0\u7d20\u3002\u81ea\u52a8\u5316\u8bc4\u4f30\u6307\u6807\u5728\u4f4e\u8d44\u6e90\u53e4\u4ee3\u8bed\u8a00\u7ffb\u8bd1\u8bc4\u4f30\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u7ed3\u5408\u4eba\u7c7b\u4e13\u5bb6\u8bc4\u4f30\u3002\u7814\u7a76\u4e3a\u53e4\u5178\u5b66\u672f\u4e2dLLM\u7684\u5e94\u7528\u548c\u53e4\u4ee3\u8bed\u8a00\u81ea\u52a8\u5316\u8bc4\u4f30\u6d41\u7a0b\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2602.23934", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23934", "abs": "https://arxiv.org/abs/2602.23934", "authors": ["Jingwen Wang", "Johannes Kirschner", "Paul Rolland", "Luis Salamanca", "Stefana Parascho"], "title": "Learning to Build: Autonomous Robotic Assembly of Stable Structures Without Predefined Plans", "comment": null, "summary": "This paper presents a novel autonomous robotic assembly framework for constructing stable structures without relying on predefined architectural blueprints. Instead of following fixed plans, construction tasks are defined through targets and obstacles, allowing the system to adapt more flexibly to environmental uncertainty and variations during the building process. A reinforcement learning (RL) policy, trained using deep Q-learning with successor features, serves as the decision-making component. As a proof of concept, we evaluate the approach on a benchmark of 15 2D robotic assembly tasks of discrete block construction. Experiments using a real-world closed-loop robotic setup demonstrate the feasibility of the method and its ability to handle construction noise. The results suggest that our framework offers a promising direction for more adaptable and robust robotic construction in real-world environments.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u81ea\u4e3b\u673a\u5668\u4eba\u88c5\u914d\u6846\u67b6\uff0c\u65e0\u9700\u9884\u5b9a\u4e49\u84dd\u56fe\uff0c\u901a\u8fc7\u76ee\u6807\u548c\u969c\u788d\u5b9a\u4e49\u4efb\u52a1\uff0c\u572815\u4e2a2D\u79bb\u6563\u5757\u88c5\u914d\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u53ef\u884c\u6027", "motivation": "\u4f20\u7edf\u673a\u5668\u4eba\u88c5\u914d\u4f9d\u8d56\u9884\u5b9a\u4e49\u84dd\u56fe\uff0c\u7f3a\u4e4f\u5bf9\u65bd\u5de5\u8fc7\u7a0b\u4e2d\u73af\u5883\u4e0d\u786e\u5b9a\u6027\u548c\u53d8\u5316\u7684\u9002\u5e94\u6027\u3002\u9700\u8981\u66f4\u7075\u6d3b\u3001\u9c81\u68d2\u7684\u81ea\u4e3b\u88c5\u914d\u65b9\u6cd5", "method": "\u4f7f\u7528\u57fa\u4e8e\u6df1\u5ea6Q\u5b66\u4e60\u548c\u540e\u7ee7\u7279\u5f81\u7684\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u4f5c\u4e3a\u51b3\u7b56\u7ec4\u4ef6\uff0c\u901a\u8fc7\u76ee\u6807\u548c\u969c\u788d\u5b9a\u4e49\u88c5\u914d\u4efb\u52a1\u800c\u975e\u56fa\u5b9a\u8ba1\u5212\uff0c\u5b9e\u73b0\u95ed\u73af\u673a\u5668\u4eba\u7cfb\u7edf", "result": "\u572815\u4e2a2D\u673a\u5668\u4eba\u88c5\u914d\u4efb\u52a1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u53ef\u884c\u6027\uff0c\u771f\u5b9e\u4e16\u754c\u95ed\u73af\u673a\u5668\u4eba\u5b9e\u9a8c\u8868\u660e\u80fd\u591f\u5904\u7406\u65bd\u5de5\u566a\u58f0\uff0c\u5c55\u793a\u4e86\u826f\u597d\u7684\u9002\u5e94\u6027", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u73b0\u5b9e\u73af\u5883\u4e2d\u66f4\u9002\u5e94\u6027\u5f3a\u3001\u9c81\u68d2\u7684\u673a\u5668\u4eba\u65bd\u5de5\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u9884\u5b9a\u4e49\u84dd\u56fe\u7684\u81ea\u4e3b\u88c5\u914d"}}
{"id": "2602.23732", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23732", "abs": "https://arxiv.org/abs/2602.23732", "authors": ["Xinyi Qi", "Kai Ye", "Chengchun Shi", "Ying Yang", "Hongyi Zhou", "Jin Zhu"], "title": "A Difference-in-Difference Approach to Detecting AI-Generated Images", "comment": null, "summary": "Diffusion models are able to produce AI-generated images that are almost indistinguishable from real ones. This raises concerns about their potential misuse and poses substantial challenges for detecting them. Many existing detectors rely on reconstruction error -- the difference between the input image and its reconstructed version -- as the basis for distinguishing real from fake images. However, these detectors become less effective as modern AI-generated images become increasingly similar to real ones. To address this challenge, we propose a novel difference-in-difference method. Instead of directly using the reconstruction error (a first-order difference), we compute the difference in reconstruction error -- a second-order difference -- for variance reduction and improving detection accuracy. Extensive experiments demonstrate that our method achieves strong generalization performance, enabling reliable detection of AI-generated images in the era of generative AI.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u4e8c\u9636\u5dee\u5206\uff08\u91cd\u5efa\u8bef\u5dee\u4e4b\u5dee\uff09\u7684\u65b9\u6cd5\u6765\u68c0\u6d4bAI\u751f\u6210\u56fe\u50cf\uff0c\u76f8\u6bd4\u4f20\u7edf\u7684\u4e00\u9636\u5dee\u5206\uff08\u91cd\u5efa\u8bef\u5dee\uff09\u65b9\u6cd5\u5728\u751f\u6210\u56fe\u50cf\u8d8a\u6765\u8d8a\u903c\u771f\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u66f4\u597d\u3002", "motivation": "\u968f\u7740\u6269\u6563\u6a21\u578b\u751f\u6210\u7684\u56fe\u50cf\u8d8a\u6765\u8d8a\u903c\u771f\uff0c\u4f20\u7edf\u7684\u57fa\u4e8e\u91cd\u5efa\u8bef\u5dee\u7684\u68c0\u6d4b\u65b9\u6cd5\u6548\u679c\u4e0b\u964d\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u68c0\u6d4b\u6280\u672f\u6765\u5e94\u5bf9AI\u751f\u6210\u56fe\u50cf\u7684\u6f5c\u5728\u6ee5\u7528\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u5dee\u5206\u4e2d\u7684\u5dee\u5206\u65b9\u6cd5\uff0c\u4e0d\u76f4\u63a5\u4f7f\u7528\u91cd\u5efa\u8bef\u5dee\uff08\u4e00\u9636\u5dee\u5206\uff09\uff0c\u800c\u662f\u8ba1\u7b97\u91cd\u5efa\u8bef\u5dee\u7684\u5dee\u5f02\uff08\u4e8c\u9636\u5dee\u5206\uff09\uff0c\u901a\u8fc7\u65b9\u5dee\u51cf\u5c11\u6765\u63d0\u9ad8\u68c0\u6d4b\u51c6\u786e\u7387\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5177\u6709\u5f3a\u5927\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u80fd\u591f\u5728\u751f\u6210\u5f0fAI\u65f6\u4ee3\u53ef\u9760\u5730\u68c0\u6d4bAI\u751f\u6210\u7684\u56fe\u50cf\u3002", "conclusion": "\u4e8c\u9636\u5dee\u5206\u65b9\u6cd5\u76f8\u6bd4\u4f20\u7edf\u7684\u4e00\u9636\u5dee\u5206\u65b9\u6cd5\u80fd\u66f4\u6709\u6548\u5730\u68c0\u6d4b\u8d8a\u6765\u8d8a\u903c\u771f\u7684AI\u751f\u6210\u56fe\u50cf\uff0c\u4e3a\u89e3\u51b3\u751f\u6210\u5f0fAI\u5e26\u6765\u7684\u68c0\u6d4b\u6311\u6218\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2602.23663", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23663", "abs": "https://arxiv.org/abs/2602.23663", "authors": ["Kohei Obata", "Taichi Murayama", "Zheng Chen", "Yasuko Matsubara", "Yasushi Sakurai"], "title": "Disentangled Mode-Specific Representations for Tensor Time Series via Contrastive Learning", "comment": null, "summary": "Multi-mode tensor time series (TTS) can be found in many domains, such as search engines and environmental monitoring systems. Learning representations of a TTS benefits various applications, but it is also challenging since the complexities inherent in the tensor hinder the realization of rich representations. In this paper, we propose a novel representation learning method designed specifically for TTS, namely MoST. Specifically, MoST uses a tensor slicing approach to reduce the complexity of the TTS structure and learns representations that can be disentangled into individual non-temporal modes. Each representation captures mode-specific features, which are the relationship between variables within the same mode, and mode-invariant features, which are in common in representations of different modes. We employ a contrastive learning framework to learn parameters; the loss function comprises two parts intended to learn representation in a mode-specific way and mode-invariant way, effectively exploiting disentangled representations as augmentations. Extensive experiments on real-world datasets show that MoST consistently outperforms the state-of-the-art methods in terms of classification and forecasting accuracy. Code is available at https://github.com/KoheiObata/MoST.", "AI": {"tldr": "MoST\uff1a\u4e00\u79cd\u9488\u5bf9\u591a\u6a21\u6001\u5f20\u91cf\u65f6\u95f4\u5e8f\u5217\u7684\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f20\u91cf\u5207\u7247\u964d\u4f4e\u590d\u6742\u5ea6\uff0c\u5b66\u4e60\u53ef\u89e3\u8026\u4e3a\u5404\u975e\u65f6\u5e8f\u6a21\u6001\u7684\u8868\u793a\uff0c\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\u63d0\u5347\u5206\u7c7b\u548c\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u591a\u6a21\u6001\u5f20\u91cf\u65f6\u95f4\u5e8f\u5217\uff08\u5982\u641c\u7d22\u5f15\u64ce\u3001\u73af\u5883\u76d1\u6d4b\u7cfb\u7edf\uff09\u7684\u8868\u793a\u5b66\u4e60\u5bf9\u591a\u79cd\u5e94\u7528\u6709\u76ca\uff0c\u4f46\u7531\u4e8e\u5f20\u91cf\u7ed3\u6784\u7684\u590d\u6742\u6027\uff0c\u96be\u4ee5\u83b7\u5f97\u4e30\u5bcc\u7684\u8868\u793a\u3002", "method": "\u63d0\u51faMoST\u65b9\u6cd5\uff1a1\uff09\u4f7f\u7528\u5f20\u91cf\u5207\u7247\u964d\u4f4eTTS\u7ed3\u6784\u590d\u6742\u5ea6\uff1b2\uff09\u5b66\u4e60\u53ef\u89e3\u8026\u4e3a\u5404\u975e\u65f6\u5e8f\u6a21\u6001\u7684\u8868\u793a\uff1b3\uff09\u6bcf\u4e2a\u8868\u793a\u5305\u542b\u6a21\u6001\u7279\u5b9a\u7279\u5f81\uff08\u540c\u4e00\u6a21\u6001\u5185\u53d8\u91cf\u5173\u7cfb\uff09\u548c\u6a21\u6001\u4e0d\u53d8\u7279\u5f81\uff08\u4e0d\u540c\u6a21\u6001\u5171\u6709\uff09\uff1b4\uff09\u91c7\u7528\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u635f\u5931\u51fd\u6570\u5305\u542b\u6a21\u6001\u7279\u5b9a\u5b66\u4e60\u548c\u6a21\u6001\u4e0d\u53d8\u5b66\u4e60\u4e24\u90e8\u5206\uff0c\u6709\u6548\u5229\u7528\u89e3\u8026\u8868\u793a\u4f5c\u4e3a\u589e\u5f3a\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cMoST\u5728\u5206\u7c7b\u548c\u9884\u6d4b\u51c6\u786e\u7387\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "MoST\u662f\u4e00\u79cd\u4e13\u95e8\u4e3a\u5f20\u91cf\u65f6\u95f4\u5e8f\u5217\u8bbe\u8ba1\u7684\u6709\u6548\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u80fd\u591f\u5904\u7406\u5f20\u91cf\u7ed3\u6784\u7684\u590d\u6742\u6027\uff0c\u901a\u8fc7\u89e3\u8026\u8868\u793a\u5b66\u4e60\u6a21\u6001\u7279\u5b9a\u548c\u6a21\u6001\u4e0d\u53d8\u7279\u5f81\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2602.24110", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.24110", "abs": "https://arxiv.org/abs/2602.24110", "authors": ["Yanwei Ren", "Haotian Zhang", "Likang Xiao", "Xikai Zhang", "Jiaxing Huang", "Jiayan Qiu", "Baosheng Yu", "Quan Chen", "Liu Liu"], "title": "Recycling Failures: Salvaging Exploration in RLVR via Fine-Grained Off-Policy Guidance", "comment": null, "summary": "Reinforcement Learning from Verifiable Rewards (RLVR) has emerged as a powerful paradigm for enhancing the complex reasoning capabilities of Large Reasoning Models. However, standard outcome-based supervision suffers from a critical limitation that penalizes trajectories that are largely correct but fail due to several missteps as heavily as completely erroneous ones. This coarse feedback signal causes the model to discard valuable largely correct rollouts, leading to a degradation in rollout diversity that prematurely narrows the exploration space. Process Reward Models have demonstrated efficacy in providing reliable step-wise verification for test-time scaling, naively integrating these signals into RLVR as dense rewards proves ineffective.Prior methods attempt to introduce off-policy guided whole-trajectory replacement that often outside the policy model's distribution, but still fail to utilize the largely correct rollouts generated by the model itself and thus do not effectively mitigate the narrowing of the exploration space. To address these issues, we propose SCOPE (Step-wise Correction for On-Policy Exploration), a novel framework that utilizes Process Reward Models to pinpoint the first erroneous step in suboptimal rollouts and applies fine-grained, step-wise off-policy rectification. By applying precise refinement on partially correct rollout, our method effectively salvages partially correct trajectories and increases diversity score by 13.5%, thereby sustaining a broad exploration space. Extensive experiments demonstrate that our approach establishes new state-of-the-art results, achieving an average accuracy of 46.6% on math reasoning and exhibiting robust generalization with 53.4% accuracy on out-of-distribution reasoning tasks.", "AI": {"tldr": "SCOPE\u6846\u67b6\u901a\u8fc7\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u5b9a\u4f4d\u63a8\u7406\u8f68\u8ff9\u4e2d\u7684\u9996\u4e2a\u9519\u8bef\u6b65\u9aa4\uff0c\u8fdb\u884c\u7ec6\u7c92\u5ea6\u4fee\u6b63\uff0c\u6709\u6548\u5229\u7528\u90e8\u5206\u6b63\u786e\u7684\u63a8\u7406\u8f68\u8ff9\uff0c\u63d0\u5347\u63a2\u7d22\u591a\u6837\u6027\uff0c\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u8fbe\u523046.6%\u7684\u5e73\u5747\u51c6\u786e\u7387\u3002", "motivation": "\u4f20\u7edf\u7684\u57fa\u4e8e\u7ed3\u679c\u7684\u5f3a\u5316\u5b66\u4e60\u76d1\u7763\u5b58\u5728\u5173\u952e\u5c40\u9650\uff1a\u5bf9\u5927\u90e8\u5206\u6b63\u786e\u4f46\u6709\u51e0\u4e2a\u9519\u8bef\u6b65\u9aa4\u7684\u8f68\u8ff9\u4e0e\u5b8c\u5168\u9519\u8bef\u7684\u8f68\u8ff9\u7ed9\u4e88\u540c\u6837\u60e9\u7f5a\uff0c\u5bfc\u81f4\u6a21\u578b\u4e22\u5f03\u6709\u4ef7\u503c\u7684\u90e8\u5206\u6b63\u786e\u63a8\u7406\u8f68\u8ff9\uff0c\u964d\u4f4e\u4e86\u63a2\u7d22\u591a\u6837\u6027\u5e76\u8fc7\u65e9\u7f29\u5c0f\u4e86\u63a2\u7d22\u7a7a\u95f4\u3002", "method": "\u63d0\u51faSCOPE\u6846\u67b6\uff0c\u5229\u7528\u8fc7\u7a0b\u5956\u52b1\u6a21\u578b\u7cbe\u786e\u5b9a\u4f4d\u6b21\u4f18\u63a8\u7406\u8f68\u8ff9\u4e2d\u7684\u7b2c\u4e00\u4e2a\u9519\u8bef\u6b65\u9aa4\uff0c\u5e94\u7528\u7ec6\u7c92\u5ea6\u7684\u3001\u6b65\u9aa4\u7ea7\u522b\u7684\u79bb\u7b56\u7565\u4fee\u6b63\u3002\u901a\u8fc7\u5bf9\u90e8\u5206\u6b63\u786e\u8f68\u8ff9\u8fdb\u884c\u7cbe\u786e\u4fee\u6b63\uff0c\u6709\u6548\u633d\u6551\u8fd9\u4e9b\u8f68\u8ff9\u5e76\u4fdd\u6301\u5e7f\u9614\u7684\u63a2\u7d22\u7a7a\u95f4\u3002", "result": "SCOPE\u5c06\u591a\u6837\u6027\u5206\u6570\u63d0\u5347\u4e8613.5%\uff0c\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u8fbe\u523046.6%\u7684\u5e73\u5747\u51c6\u786e\u7387\uff0c\u5728\u5206\u5e03\u5916\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa53.4%\u7684\u51c6\u786e\u7387\uff0c\u5c55\u73b0\u4e86\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "SCOPE\u6846\u67b6\u901a\u8fc7\u6b65\u9aa4\u7ea7\u522b\u7684\u4fee\u6b63\u6709\u6548\u5229\u7528\u4e86\u90e8\u5206\u6b63\u786e\u7684\u63a8\u7406\u8f68\u8ff9\uff0c\u7ef4\u6301\u4e86\u63a2\u7d22\u591a\u6837\u6027\uff0c\u5728\u5f3a\u5316\u5b66\u4e60\u4ece\u53ef\u9a8c\u8bc1\u5956\u52b1\u8303\u5f0f\u4e0b\u53d6\u5f97\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u7ed3\u679c\uff0c\u5e76\u5c55\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2602.23937", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23937", "abs": "https://arxiv.org/abs/2602.23937", "authors": ["Haoxuan Xu", "Tianfu Li", "Wenbo Chen", "Yi Liu", "Xingxing Zuo", "Yaoxian Song", "Haoang Li"], "title": "Enhancing Vision-Language Navigation with Multimodal Event Knowledge from Real-World Indoor Tour Videos", "comment": null, "summary": "Vision-Language Navigation (VLN) agents often struggle with long-horizon reasoning in unseen environments, particularly when facing ambiguous, coarse-grained instructions. While recent advances use knowledge graph to enhance reasoning, the potential of multimodal event knowledge inspired by human episodic memory remains underexplored. In this work, we propose an event-centric knowledge enhancement strategy for automated process knowledge mining and feature fusion to solve coarse-grained instruction and long-horizon reasoning in VLN task. First, we construct YE-KG, the first large-scale multimodal spatiotemporal knowledge graph, with over 86k nodes and 83k edges, derived from real-world indoor videos. By leveraging multimodal large language models (i.e., LLaVa, GPT4), we extract unstructured video streams into structured semantic-action-effect events to serve as explicit episodic memory. Second, we introduce STE-VLN, which integrates the above graph into VLN models via a Coarse-to-Fine Hierarchical Retrieval mechanism. This allows agents to retrieve causal event sequences and dynamically fuse them with egocentric visual observations. Experiments on REVERIE, R2R, and R2R-CE benchmarks demonstrate the efficiency of our event-centric strategy, outperforming state-of-the-art approaches across diverse action spaces. Our data and code are available on the project website https://sites.google.com/view/y-event-kg/.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u4e8b\u4ef6\u4e2d\u5fc3\u77e5\u8bc6\u589e\u5f3a\u7684\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u591a\u6a21\u6001\u65f6\u7a7a\u77e5\u8bc6\u56fe\u8c31\u548c\u5206\u5c42\u68c0\u7d22\u673a\u5236\uff0c\u89e3\u51b3\u7c97\u7c92\u5ea6\u6307\u4ee4\u548c\u957f\u65f6\u7a0b\u63a8\u7406\u95ee\u9898", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\uff08VLN\uff09\u667a\u80fd\u4f53\u5728\u672a\u89c1\u73af\u5883\u4e2d\u5904\u7406\u957f\u65f6\u7a0b\u63a8\u7406\u548c\u6a21\u7cca\u7c97\u7c92\u5ea6\u6307\u4ee4\u65f6\u5b58\u5728\u56f0\u96be\u3002\u867d\u7136\u5df2\u6709\u7814\u7a76\u4f7f\u7528\u77e5\u8bc6\u56fe\u8c31\u589e\u5f3a\u63a8\u7406\uff0c\u4f46\u53d7\u4eba\u7c7b\u60c5\u666f\u8bb0\u5fc6\u542f\u53d1\u7684\u591a\u6a21\u6001\u4e8b\u4ef6\u77e5\u8bc6\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u6316\u6398\u3002", "method": "1. \u6784\u5efaYE-KG\uff1a\u9996\u4e2a\u5927\u89c4\u6a21\u591a\u6a21\u6001\u65f6\u7a7a\u77e5\u8bc6\u56fe\u8c31\uff0c\u5305\u542b\u8d85\u8fc78.6\u4e07\u4e2a\u8282\u70b9\u548c8.3\u4e07\u6761\u8fb9\uff0c\u4ece\u771f\u5b9e\u5ba4\u5185\u89c6\u9891\u4e2d\u63d0\u53d6\u7ed3\u6784\u5316\u8bed\u4e49-\u52a8\u4f5c-\u6548\u679c\u4e8b\u4ef6\u4f5c\u4e3a\u663e\u5f0f\u60c5\u666f\u8bb0\u5fc6\u30022. \u63d0\u51faSTE-VLN\uff1a\u901a\u8fc7\u7c97\u5230\u7ec6\u5206\u5c42\u68c0\u7d22\u673a\u5236\u5c06\u77e5\u8bc6\u56fe\u8c31\u96c6\u6210\u5230VLN\u6a21\u578b\u4e2d\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u68c0\u7d22\u56e0\u679c\u4e8b\u4ef6\u5e8f\u5217\u5e76\u52a8\u6001\u878d\u5408\u81ea\u6211\u4e2d\u5fc3\u89c6\u89c9\u89c2\u5bdf\u3002", "result": "\u5728REVERIE\u3001R2R\u548cR2R-CE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u4e8b\u4ef6\u4e2d\u5fc3\u7b56\u7565\u5728\u4e0d\u540c\u52a8\u4f5c\u7a7a\u95f4\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8e\u4e8b\u4ef6\u4e2d\u5fc3\u77e5\u8bc6\u589e\u5f3a\u7b56\u7565\u901a\u8fc7\u6784\u5efa\u591a\u6a21\u6001\u65f6\u7a7a\u77e5\u8bc6\u56fe\u8c31\u548c\u5206\u5c42\u68c0\u7d22\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86VLN\u4efb\u52a1\u4e2d\u7684\u7c97\u7c92\u5ea6\u6307\u4ee4\u7406\u89e3\u548c\u957f\u65f6\u7a0b\u63a8\u7406\u95ee\u9898\uff0c\u4e3a\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u7c7b\u4f3c\u4eba\u7c7b\u60c5\u666f\u8bb0\u5fc6\u7684\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2602.23734", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.23734", "abs": "https://arxiv.org/abs/2602.23734", "authors": ["Hao Wu", "Xudong Wang", "Jialiang Zhang", "Junlong Tong", "Xinghao Chen", "Junyan Lin", "Yunpu Ma", "Xiaoyu Shen"], "title": "UTPTrack: Towards Simple and Unified Token Pruning for Visual Tracking", "comment": "Accepted to CVPR 2026", "summary": "One-stream Transformer-based trackers achieve advanced performance in visual object tracking but suffer from significant computational overhead that hinders real-time deployment. While token pruning offers a path to efficiency, existing methods are fragmented. They typically prune the search region, dynamic template, and static template in isolation, overlooking critical inter-component dependencies, which yields suboptimal pruning and degraded accuracy. To address this, we introduce UTPTrack, a simple and Unified Token Pruning framework that, for the first time, jointly compresses all three components. UTPTrack employs an attention-guided, token type-aware strategy to holistically model redundancy, a design that seamlessly supports unified tracking across multimodal and language-guided tasks within a single model. Extensive evaluations on 10 benchmarks demonstrate that UTPTrack achieves a new state-of-the-art in the accuracy-efficiency trade-off for pruning-based trackers, pruning 65.4% of vision tokens in RGB-based tracking and 67.5% in unified tracking while preserving 99.7% and 100.5% of baseline performance, respectively. This strong performance across both RGB and multimodal scenarios underlines its potential as a robust foundation for future research in efficient visual tracking. Code will be released at https://github.com/EIT-NLP/UTPTrack.", "AI": {"tldr": "UTPTrack\u662f\u4e00\u4e2a\u7edf\u4e00\u7684Transformer\u4ee4\u724c\u526a\u679d\u6846\u67b6\uff0c\u9996\u6b21\u8054\u5408\u538b\u7f29\u641c\u7d22\u533a\u57df\u3001\u52a8\u6001\u6a21\u677f\u548c\u9759\u6001\u6a21\u677f\u4e09\u4e2a\u7ec4\u4ef6\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500", "motivation": "\u73b0\u6709\u7684\u5355\u6d41Transformer\u8ddf\u8e2a\u5668\u867d\u7136\u6027\u80fd\u5148\u8fdb\uff0c\u4f46\u8ba1\u7b97\u5f00\u9500\u5927\uff0c\u96be\u4ee5\u5b9e\u65f6\u90e8\u7f72\u3002\u73b0\u6709\u7684\u4ee4\u724c\u526a\u679d\u65b9\u6cd5\u901a\u5e38\u662f\u5b64\u7acb\u5730\u526a\u679d\u5404\u4e2a\u7ec4\u4ef6\uff0c\u5ffd\u7565\u4e86\u7ec4\u4ef6\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u5bfc\u81f4\u6b21\u4f18\u526a\u679d\u548c\u7cbe\u5ea6\u4e0b\u964d", "method": "\u63d0\u51faUTPTrack\u6846\u67b6\uff0c\u91c7\u7528\u6ce8\u610f\u529b\u5f15\u5bfc\u3001\u4ee4\u724c\u7c7b\u578b\u611f\u77e5\u7684\u7b56\u7565\uff0c\u7edf\u4e00\u5efa\u6a21\u6240\u6709\u4e09\u4e2a\u7ec4\u4ef6\u7684\u5197\u4f59\u6027\uff0c\u652f\u6301\u591a\u6a21\u6001\u548c\u8bed\u8a00\u5f15\u5bfc\u7684\u7edf\u4e00\u8ddf\u8e2a\u4efb\u52a1", "result": "\u572810\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cUTPTrack\u5728\u526a\u679d\u7c7b\u8ddf\u8e2a\u5668\u4e2d\u5b9e\u73b0\u4e86\u7cbe\u5ea6-\u6548\u7387\u6743\u8861\u7684\u65b0SOTA\uff1a\u5728RGB\u8ddf\u8e2a\u4e2d\u526a\u679d65.4%\u89c6\u89c9\u4ee4\u724c\uff0c\u4fdd\u630199.7%\u57fa\u7ebf\u6027\u80fd\uff1b\u5728\u7edf\u4e00\u8ddf\u8e2a\u4e2d\u526a\u679d67.5%\u4ee4\u724c\uff0c\u4fdd\u6301100.5%\u57fa\u7ebf\u6027\u80fd", "conclusion": "UTPTrack\u901a\u8fc7\u7edf\u4e00\u7684\u4ee4\u724c\u526a\u679d\u6846\u67b6\uff0c\u5728RGB\u548c\u591a\u6a21\u6001\u573a\u666f\u4e0b\u90fd\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u9ad8\u6548\u89c6\u89c9\u8ddf\u8e2a\u7684\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840"}}
{"id": "2602.23696", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23696", "abs": "https://arxiv.org/abs/2602.23696", "authors": ["Yongzhong Xu"], "title": "Optimizer-Induced Low-Dimensional Drift and Transverse Dynamics in Transformer Training", "comment": "18 pages, 4 figures", "summary": "We study the geometry of training trajectories in small transformer models and find that parameter updates organize into a dominant drift direction with transverse residual dynamics. Using uncentered, row-normalized trajectory PCA, we show that a single direction captures a large fraction of cumulative parameter movement early in training, while remaining components encode oscillatory behavior in auxiliary probe performance. Instantaneous gradients exhibit little alignment with this dominant direction, indicating that it arises from accumulated optimizer updates rather than per-batch gradient structure. Comparing AdamW with SGD variants at matched loss levels reveals substantial differences in trajectory geometry: AdamW develops multi-dimensional drift structure, whereas SGD-family optimizers produce nearly colinear parameter evolution and weaker probe dynamics. Reheating selectively perturbs transverse components with minimal effect on the dominant drift coordinate. These findings suggest that optimizer choice shapes the effective dimensionality and structure of learning trajectories beyond what is apparent from loss values alone.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5c0fTransformer\u6a21\u578b\u7684\u8bad\u7ec3\u8f68\u8ff9\u5177\u6709\u4e3b\u5bfc\u6f02\u79fb\u65b9\u5411\u4e0e\u6a2a\u5411\u6b8b\u4f59\u52a8\u529b\u5b66\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u4e0d\u540c\u4f18\u5316\u5668\uff08AdamW vs SGD\uff09\u5728\u8f68\u8ff9\u51e0\u4f55\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "motivation": "\u7814\u7a76\u8bad\u7ec3\u8f68\u8ff9\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u63a2\u7d22\u4f18\u5316\u5668\u9009\u62e9\u5982\u4f55\u5f71\u54cd\u5b66\u4e60\u8f68\u8ff9\u7684\u6709\u6548\u7ef4\u5ea6\u548c\u7ed3\u6784\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u635f\u5931\u503c\u7684\u53d8\u5316\u3002", "method": "\u4f7f\u7528\u672a\u4e2d\u5fc3\u5316\u3001\u884c\u5f52\u4e00\u5316\u7684\u8f68\u8ff9PCA\u5206\u6790\u5c0fTransformer\u6a21\u578b\u7684\u53c2\u6570\u66f4\u65b0\u8f68\u8ff9\uff0c\u6bd4\u8f83AdamW\u4e0eSGD\u53d8\u4f53\u5728\u5339\u914d\u635f\u5931\u6c34\u5e73\u4e0b\u7684\u8f68\u8ff9\u51e0\u4f55\u5dee\u5f02\uff0c\u5e76\u901a\u8fc7\u91cd\u65b0\u52a0\u70ed\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u53c2\u6570\u66f4\u65b0\u7ec4\u7ec7\u6210\u4e3b\u5bfc\u6f02\u79fb\u65b9\u5411\u4e0e\u6a2a\u5411\u6b8b\u4f59\u52a8\u529b\u5b66\uff1b\u5355\u4e00\u65b9\u5411\u5728\u8bad\u7ec3\u65e9\u671f\u6355\u83b7\u5927\u90e8\u5206\u7d2f\u79ef\u53c2\u6570\u79fb\u52a8\uff1b\u77ac\u65f6\u68af\u5ea6\u4e0e\u4e3b\u5bfc\u65b9\u5411\u5bf9\u9f50\u5ea6\u4f4e\uff1bAdamW\u4ea7\u751f\u591a\u7ef4\u6f02\u79fb\u7ed3\u6784\uff0c\u800cSGD\u4ea7\u751f\u8fd1\u4e4e\u5171\u7ebf\u7684\u53c2\u6570\u6f14\u5316\uff1b\u91cd\u65b0\u52a0\u70ed\u9009\u62e9\u6027\u5730\u6270\u52a8\u6a2a\u5411\u5206\u91cf\u3002", "conclusion": "\u4f18\u5316\u5668\u9009\u62e9\u5851\u9020\u4e86\u5b66\u4e60\u8f68\u8ff9\u7684\u6709\u6548\u7ef4\u5ea6\u548c\u7ed3\u6784\uff0c\u8d85\u8d8a\u4e86\u4ec5\u4ece\u635f\u5931\u503c\u89c2\u5bdf\u5230\u7684\u6548\u679c\uff0cAdamW\u548cSGD\u5728\u8bad\u7ec3\u8f68\u8ff9\u51e0\u4f55\u4e0a\u5b58\u5728\u672c\u8d28\u5dee\u5f02\u3002"}}
{"id": "2602.24173", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.24173", "abs": "https://arxiv.org/abs/2602.24173", "authors": ["Antoine Peyronnet", "Fabian Gloeckle", "Amaury Hayat"], "title": "LemmaBench: A Live, Research-Level Benchmark to Evaluate LLM Capabilities in Mathematics", "comment": "15 pages, 3 figures, 5 Tables", "summary": "We present a new approach for benchmarking Large Language Model (LLM) capabilities on research-level mathematics. Existing benchmarks largely rely on static, hand-curated sets of contest or textbook-style problems as proxies for mathematical research. Instead, we establish an updatable benchmark evaluating models directly on the latest research results in mathematics. This consists of an automatic pipeline that extracts lemmas from arXiv and rewrites them into self-contained statements by making all assumptions and required definitions explicit. It results in a benchmark that can be updated regularly with new problems taken directly from human mathematical research, while previous instances can be used for training without compromising future evaluations. We benchmark current state-of-the-art LLMs, which obtain around 10-15$\\%$ accuracy in theorem proving (pass@1) depending on the model, showing that there is currently a large margin of progression for LLMs to reach human-level proving capabilities in a research context.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8earXiv\u6700\u65b0\u6570\u5b66\u7814\u7a76\u7ed3\u679c\u7684\u52a8\u6001\u53ef\u66f4\u65b0LLM\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u66ff\u4ee3\u4f20\u7edf\u7684\u9759\u6001\u7ade\u8d5b\u9898\u57fa\u51c6", "motivation": "\u73b0\u6709LLM\u6570\u5b66\u80fd\u529b\u57fa\u51c6\u4e3b\u8981\u4f9d\u8d56\u9759\u6001\u7684\u624b\u5de5\u6574\u7406\u7ade\u8d5b\u9898\u6216\u6559\u79d1\u4e66\u4e60\u9898\uff0c\u65e0\u6cd5\u53cd\u6620\u771f\u5b9e\u6570\u5b66\u7814\u7a76\u6c34\u5e73\uff0c\u9700\u8981\u5efa\u7acb\u76f4\u63a5\u8bc4\u4f30\u6a21\u578b\u5728\u6700\u65b0\u6570\u5b66\u7814\u7a76\u6210\u679c\u4e0a\u8868\u73b0\u7684\u52a8\u6001\u57fa\u51c6", "method": "\u5f00\u53d1\u81ea\u52a8\u6d41\u6c34\u7ebf\u4ecearXiv\u63d0\u53d6\u5f15\u7406\uff0c\u901a\u8fc7\u663e\u5f0f\u5316\u6240\u6709\u5047\u8bbe\u548c\u5b9a\u4e49\u5c06\u5176\u91cd\u5199\u4e3a\u81ea\u5305\u542b\u7684\u9648\u8ff0\uff0c\u521b\u5efa\u53ef\u5b9a\u671f\u66f4\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\u96c6", "result": "\u5f53\u524d\u6700\u5148\u8fdbLLM\u5728\u5b9a\u7406\u8bc1\u660e\u4efb\u52a1\u4e0a\u51c6\u786e\u7387\u7ea6\u4e3a10-15%\uff08pass@1\uff09\uff0c\u8868\u660eLLM\u8ddd\u79bb\u4eba\u7c7b\u7814\u7a76\u6c34\u5e73\u7684\u8bc1\u660e\u80fd\u529b\u4ecd\u6709\u8f83\u5927\u5dee\u8ddd", "conclusion": "\u8be5\u52a8\u6001\u57fa\u51c6\u80fd\u591f\u6301\u7eed\u8bc4\u4f30LLM\u5728\u771f\u5b9e\u6570\u5b66\u7814\u7a76\u73af\u5883\u4e2d\u7684\u80fd\u529b\uff0c\u4e3a\u6a21\u578b\u53d1\u5c55\u63d0\u4f9b\u66f4\u6709\u610f\u4e49\u7684\u8bc4\u4f30\u6807\u51c6\uff0c\u540c\u65f6\u65e7\u5b9e\u4f8b\u53ef\u7528\u4e8e\u8bad\u7ec3\u800c\u4e0d\u5f71\u54cd\u672a\u6765\u8bc4\u4f30"}}
{"id": "2602.23972", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.23972", "abs": "https://arxiv.org/abs/2602.23972", "authors": ["Yuanlin Yang", "Lin Hong", "Fumin Zhang"], "title": "Learning Robust Control Policies for Inverted Pose on Miniature Blimp Robots", "comment": null, "summary": "The ability to achieve and maintain inverted poses is essential for unlocking the full agility of miniature blimp robots (MBRs). However, developing reliable control methods for MBRs remains challenging due to their complex and underactuated dynamics. To address this challenge, we propose a novel framework that enables robust control policy learning for inverted pose on MBRs. The proposed framework operates through three core stages: First, a high-fidelity three-dimensional (3D) simulation environment was constructed, which was calibrated against real-world MBR motion data to ensure accurate replication of inverted-state dynamics. Second, a robust policy for MBR inverted control was trained within the simulation environment via a domain randomization strategy and a modified Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm. Third, a mapping layer was designed to bridge the sim-to-real gap for the learned policy deployment. Comprehensive evaluations in the simulation environment demonstrate that the learned policy achieves a higher success rate compared to the energy-shaping controller. Furthermore, experimental results confirm that the learned policy with a mapping layer enables an MBR to achieve and maintain a fully upside-down pose in real-world settings.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u4e09\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u9ad8\u4fdd\u771f\u4eff\u771f\u3001\u5f3a\u5316\u5b66\u4e60\u548c\u6620\u5c04\u5c42\u5b9e\u73b0\u5fae\u578b\u98de\u8247\u673a\u5668\u4eba\u7684\u5012\u7acb\u59ff\u6001\u63a7\u5236", "motivation": "\u5fae\u578b\u98de\u8247\u673a\u5668\u4eba(MBRs)\u7684\u5012\u7acb\u59ff\u6001\u63a7\u5236\u5bf9\u4e8e\u53d1\u6325\u5176\u5168\u90e8\u654f\u6377\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u5176\u590d\u6742\u548c\u6b20\u9a71\u52a8\u7684\u52a8\u529b\u5b66\u7279\u6027\uff0c\u5f00\u53d1\u53ef\u9760\u7684\u63a7\u5236\u65b9\u6cd5\u5177\u6709\u6311\u6218\u6027", "method": "\u4e09\u9636\u6bb5\u6846\u67b6\uff1a1)\u6784\u5efa\u57fa\u4e8e\u771f\u5b9e\u6570\u636e\u6821\u51c6\u7684\u9ad8\u4fdd\u771f3D\u4eff\u771f\u73af\u5883\uff1b2)\u901a\u8fc7\u57df\u968f\u673a\u5316\u548c\u6539\u8fdb\u7684TD3\u7b97\u6cd5\u5728\u4eff\u771f\u4e2d\u8bad\u7ec3\u9c81\u68d2\u63a7\u5236\u7b56\u7565\uff1b3)\u8bbe\u8ba1\u6620\u5c04\u5c42\u6765\u5f25\u5408\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u5dee\u8ddd", "result": "\u4eff\u771f\u8bc4\u4f30\u663e\u793a\u5b66\u4e60\u7b56\u7565\u6bd4\u80fd\u91cf\u6574\u5f62\u63a7\u5236\u5668\u6709\u66f4\u9ad8\u7684\u6210\u529f\u7387\uff1b\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u5e26\u6620\u5c04\u5c42\u7684\u5b66\u4e60\u7b56\u7565\u80fd\u4f7fMBR\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u5b9e\u73b0\u5e76\u4fdd\u6301\u5b8c\u5168\u5012\u7acb\u59ff\u6001", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u5fae\u578b\u98de\u8247\u673a\u5668\u4eba\u7684\u5012\u7acb\u59ff\u6001\u63a7\u5236\uff0c\u901a\u8fc7\u4eff\u771f\u8bad\u7ec3\u548c\u6620\u5c04\u5c42\u8bbe\u8ba1\u6709\u6548\u89e3\u51b3\u4e86\u590d\u6742\u52a8\u529b\u5b66\u548c\u4eff\u771f\u5230\u73b0\u5b9e\u8f6c\u79fb\u7684\u6311\u6218"}}
{"id": "2602.23737", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23737", "abs": "https://arxiv.org/abs/2602.23737", "authors": ["Hanping Zhang", "Yuhong Guo"], "title": "Bridging Dynamics Gaps via Diffusion Schr\u00f6dinger Bridge for Cross-Domain Reinforcement Learning", "comment": null, "summary": "Cross-domain reinforcement learning (RL) aims to learn transferable policies under dynamics shifts between source and target domains. A key challenge lies in the lack of target-domain environment interaction and reward supervision, which prevents direct policy learning. To address this challenge, we propose Bridging Dynamics Gaps for Cross-Domain Reinforcement Learning (BDGxRL), a novel framework that leverages Diffusion Schr\u00f6dinger Bridge (DSB) to align source transitions with target-domain dynamics encoded in offline demonstrations. Moreover, we introduce a reward modulation mechanism that estimates rewards based on state transitions, applying to DSB-aligned samples to ensure consistency between rewards and target-domain dynamics. BDGxRL performs target-oriented policy learning entirely within the source domain, without access to the target environment or its rewards. Experiments on MuJoCo cross-domain benchmarks demonstrate that BDGxRL outperforms state-of-the-art baselines and shows strong adaptability under transition dynamics shifts.", "AI": {"tldr": "\u63d0\u51faBDGxRL\u6846\u67b6\uff0c\u5229\u7528\u6269\u6563\u859b\u5b9a\u8c14\u6865\u5bf9\u9f50\u6e90\u57df\u548c\u76ee\u6807\u57df\u52a8\u6001\uff0c\u901a\u8fc7\u5956\u52b1\u8c03\u5236\u673a\u5236\u4f30\u8ba1\u5956\u52b1\uff0c\u5b9e\u73b0\u5728\u6e90\u57df\u5185\u5b66\u4e60\u9002\u7528\u4e8e\u76ee\u6807\u57df\u7684\u7b56\u7565\uff0c\u65e0\u9700\u76ee\u6807\u57df\u73af\u5883\u4ea4\u4e92\u6216\u5956\u52b1\u76d1\u7763\u3002", "motivation": "\u8de8\u57df\u5f3a\u5316\u5b66\u4e60\u9762\u4e34\u7684\u4e3b\u8981\u6311\u6218\u662f\u7f3a\u4e4f\u76ee\u6807\u57df\u73af\u5883\u4ea4\u4e92\u548c\u5956\u52b1\u76d1\u7763\uff0c\u8fd9\u963b\u788d\u4e86\u76f4\u63a5\u7b56\u7565\u5b66\u4e60\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u80fd\u591f\u5728\u6e90\u57df\u5185\u5b66\u4e60\u9002\u7528\u4e8e\u76ee\u6807\u57df\u7684\u7b56\u7565\u3002", "method": "\u63d0\u51faBDGxRL\u6846\u67b6\uff1a1\uff09\u4f7f\u7528\u6269\u6563\u859b\u5b9a\u8c14\u6865\u5c06\u6e90\u57df\u72b6\u6001\u8f6c\u79fb\u4e0e\u76ee\u6807\u57df\u79bb\u7ebf\u6f14\u793a\u7f16\u7801\u7684\u52a8\u6001\u5bf9\u9f50\uff1b2\uff09\u5f15\u5165\u5956\u52b1\u8c03\u5236\u673a\u5236\uff0c\u57fa\u4e8e\u72b6\u6001\u8f6c\u79fb\u4f30\u8ba1\u5956\u52b1\uff0c\u786e\u4fdd\u5956\u52b1\u4e0e\u76ee\u6807\u57df\u52a8\u6001\u7684\u4e00\u81f4\u6027\u3002", "result": "\u5728MuJoCo\u8de8\u57df\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cBDGxRL\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5728\u8f6c\u79fb\u52a8\u6001\u53d8\u5316\u4e0b\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u9002\u5e94\u6027\u3002", "conclusion": "BDGxRL\u901a\u8fc7\u52a8\u6001\u5bf9\u9f50\u548c\u5956\u52b1\u8c03\u5236\uff0c\u5b9e\u73b0\u4e86\u5728\u6e90\u57df\u5185\u8fdb\u884c\u76ee\u6807\u5bfc\u5411\u7684\u7b56\u7565\u5b66\u4e60\uff0c\u65e0\u9700\u8bbf\u95ee\u76ee\u6807\u73af\u5883\u6216\u5176\u5956\u52b1\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8de8\u57df\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u52a8\u6001\u5dee\u5f02\u95ee\u9898\u3002"}}
{"id": "2602.24174", "categories": ["cs.CL", "cs.AI", "cs.IT"], "pdf": "https://arxiv.org/pdf/2602.24174", "abs": "https://arxiv.org/abs/2602.24174", "authors": ["Dor Tsur", "Sharon Adar", "Ran Levy"], "title": "Task-Centric Acceleration of Small-Language Models", "comment": null, "summary": "Small language models (SLMs) have emerged as efficient alternatives to large language models for task-specific applications. However, they are often employed in high-volume, low-latency settings, where efficiency is crucial. We propose TASC, Task-Adaptive Sequence Compression, a framework for SLM acceleration comprising two use-cases: When performing SLM fine-tuning, we propose TASC-ft, which iteratively enriches the tokenizer vocabulary with high-frequency output n-grams and then fine-tunes the model to utilize the expanded vocabulary. Next, we propose an inference-time method, termed TASC-spec. TASC-spec is a lightweight, training-free speculative decoding method that constructs an n-gram draft model from the task's output corpus, mixing task and context n-gram information.TASC-spec avoids any additional training, while bypassing draft-target vocabulary alignment constraints. We demonstrate the effectiveness of both methods across multiple low output-variability generation tasks. Our methods show consistent improvements in inference efficiency while maintaining task performance.", "AI": {"tldr": "TASC\u6846\u67b6\u5305\u542bTASC-ft\u548cTASC-spec\u4e24\u79cd\u65b9\u6cd5\uff0c\u7528\u4e8e\u52a0\u901f\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7279\u5b9a\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u901a\u8fc7\u8bcd\u6c47\u6269\u5c55\u548c\u63a8\u6d4b\u89e3\u7801\u63d0\u5347\u6548\u7387\u3002", "motivation": "\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u5728\u9700\u8981\u9ad8\u541e\u5410\u91cf\u548c\u4f4e\u5ef6\u8fdf\u7684\u5e94\u7528\u573a\u666f\u4e2d\u6548\u7387\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u4efb\u52a1\u7279\u5b9a\u4f18\u5316\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u52a0\u901f\u65b9\u6848\u3002", "method": "\u63d0\u51faTASC\u6846\u67b6\uff1a1) TASC-ft\u5728\u5fae\u8c03\u65f6\u5411\u5206\u8bcd\u5668\u8bcd\u6c47\u8868\u4e2d\u6dfb\u52a0\u9ad8\u9891\u8f93\u51fan-gram\uff0c\u7136\u540e\u5fae\u8c03\u6a21\u578b\u4f7f\u7528\u6269\u5c55\u8bcd\u6c47\uff1b2) TASC-spec\u5728\u63a8\u7406\u65f6\u6784\u5efan-gram\u8349\u7a3f\u6a21\u578b\uff0c\u6df7\u5408\u4efb\u52a1\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "result": "\u5728\u591a\u4e2a\u4f4e\u8f93\u51fa\u53d8\u5f02\u6027\u751f\u6210\u4efb\u52a1\u4e2d\uff0c\u4e24\u79cd\u65b9\u6cd5\u90fd\u663e\u793a\u51fa\u63a8\u7406\u6548\u7387\u7684\u6301\u7eed\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "TASC\u6846\u67b6\u4e3a\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u4efb\u52a1\u81ea\u9002\u5e94\u52a0\u901f\u65b9\u6848\uff0c\u901a\u8fc7\u8bcd\u6c47\u6269\u5c55\u548c\u8bad\u7ec3\u81ea\u7531\u7684\u63a8\u6d4b\u89e3\u7801\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u63a8\u7406\u6548\u7387\u3002"}}
{"id": "2602.23759", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23759", "abs": "https://arxiv.org/abs/2602.23759", "authors": ["Zuyao You", "Zuxuan Wu", "Yu-Gang Jiang"], "title": "Learning Accurate Segmentation Purely from Self-Supervision", "comment": null, "summary": "Accurately segmenting objects without any manual annotations remains one of the core challenges in computer vision. In this work, we introduce Selfment, a fully self-supervised framework that segments foreground objects directly from raw images without human labels, pretrained segmentation models, or any post-processing. Selfment first constructs patch-level affinity graphs from self-supervised features and applies NCut to obtain an initial coarse foreground--background separation. We then introduce Iterative Patch Optimization (IPO), a feature-space refinement procedure that progressively enforces spatial coherence and semantic consistency through iterative patch clustering. The refined masks are subsequently used as supervisory signals to train a lightweight segmentation head with contrastive and region-consistency objectives, allowing the model to learn stable and transferable object representations. Despite its simplicity and complete absence of manual supervision, Selfment sets new state-of-the-art (SoTA) results across multiple benchmarks. It achieves substantial improvements on $F_{\\max}$ over previous unsupervised saliency detection methods on ECSSD ($+4.0\\%$), HKUIS ($+4.6\\%$), and PASCAL-S ($+5.7\\%$). Moreover, without any additional fine-tuning, Selfment demonstrates remarkable zero-shot generalization to camouflaged object detection tasks (e.g., $0.910$ $S_m$ on CHAMELEON and $0.792$ $F_\u03b2^\u03c9$ on CAMO), outperforming all existing unsupervised approaches and even rivaling the SoTA fully supervised methods.", "AI": {"tldr": "Selfment\u662f\u4e00\u4e2a\u5b8c\u5168\u81ea\u76d1\u7763\u7684\u6846\u67b6\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u3001\u9884\u8bad\u7ec3\u5206\u5272\u6a21\u578b\u6216\u540e\u5904\u7406\uff0c\u76f4\u63a5\u4ece\u539f\u59cb\u56fe\u50cf\u5206\u5272\u524d\u666f\u5bf9\u8c61\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u65b0\u7684SOTA\u6027\u80fd\u3002", "motivation": "\u5728\u6ca1\u6709\u4eba\u5de5\u6807\u6ce8\u7684\u60c5\u51b5\u4e0b\u51c6\u786e\u5206\u5272\u5bf9\u8c61\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u7684\u6838\u5fc3\u6311\u6218\u4e4b\u4e00\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u3001\u9884\u8bad\u7ec3\u6a21\u578b\u6216\u590d\u6742\u7684\u540e\u5904\u7406\uff0c\u800cSelfment\u65e8\u5728\u5b9e\u73b0\u5b8c\u5168\u81ea\u76d1\u7763\u7684\u524d\u666f\u5bf9\u8c61\u5206\u5272\u3002", "method": "1. \u4ece\u81ea\u76d1\u7763\u7279\u5f81\u6784\u5efa\u8865\u4e01\u7ea7\u4eb2\u548c\u529b\u56fe\uff0c\u5e94\u7528NCut\u83b7\u5f97\u521d\u59cb\u7684\u7c97\u7c92\u5ea6\u524d\u666f-\u80cc\u666f\u5206\u79bb\uff1b2. \u5f15\u5165\u8fed\u4ee3\u8865\u4e01\u4f18\u5316(IPO)\uff0c\u901a\u8fc7\u8fed\u4ee3\u8865\u4e01\u805a\u7c7b\u5728\u7279\u5f81\u7a7a\u95f4\u4e2d\u8fdb\u884c\u7ec6\u5316\uff0c\u589e\u5f3a\u7a7a\u95f4\u4e00\u81f4\u6027\u548c\u8bed\u4e49\u4e00\u81f4\u6027\uff1b3. \u4f7f\u7528\u7ec6\u5316\u7684\u63a9\u7801\u4f5c\u4e3a\u76d1\u7763\u4fe1\u53f7\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u548c\u533a\u57df\u4e00\u81f4\u6027\u76ee\u6807\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u5206\u5272\u5934\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u65b0\u7684SOTA\uff1aECSSD\u4e0aF_max\u63d0\u53474.0%\uff0cHKUIS\u63d0\u53474.6%\uff0cPASCAL-S\u63d0\u53475.7%\u3002\u5728\u4f2a\u88c5\u5bf9\u8c61\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff1aCHAMELEON\u4e0aSm\u8fbe\u52300.910\uff0cCAMO\u4e0aF\u03b2^\u03c9\u8fbe\u52300.792\uff0c\u8d85\u8d8a\u6240\u6709\u73b0\u6709\u65e0\u76d1\u7763\u65b9\u6cd5\uff0c\u751a\u81f3\u5ab2\u7f8e\u5168\u76d1\u7763SOTA\u65b9\u6cd5\u3002", "conclusion": "Selfment\u8bc1\u660e\u4e86\u5b8c\u5168\u81ea\u76d1\u7763\u5bf9\u8c61\u5206\u5272\u7684\u53ef\u884c\u6027\uff0c\u901a\u8fc7\u7b80\u5355\u7684\u6846\u67b6\u5b9e\u73b0\u4e86\u5353\u8d8a\u6027\u80fd\uff0c\u5728\u65e0\u76d1\u7763\u5206\u5272\u548c\u96f6\u6837\u672c\u6cdb\u5316\u65b9\u9762\u53d6\u5f97\u4e86\u7a81\u7834\u6027\u8fdb\u5c55\uff0c\u4e3a\u65e0\u6807\u6ce8\u89c6\u89c9\u7406\u89e3\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2602.23761", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23761", "abs": "https://arxiv.org/abs/2602.23761", "authors": ["Yuyu Geng", "Lei Sun", "Yao Gao", "Xinxin Hu", "Zhonghua Yi", "Xiaolong Qian", "Weijian Hu", "Jian Bai", "Kaiwei Wang"], "title": "OPTIAGENT: A Physics-Driven Agentic Framework for Automated Optical Design", "comment": null, "summary": "Optical design is the process of configuring optical elements to precisely manipulate light for high-fidelity imaging. It is inherently a highly non-convex optimization problem that relies heavily on human heuristic expertise and domain-specific knowledge. While Large Language Models (LLMs) possess extensive optical knowledge, their capabilities in leveraging the knowledge in designing lens system remain significantly constrained. This work represents the first attempt to employ LLMs in the field of optical design. We bridge the expertise gap by enabling users without formal optical training to successfully develop functional lens systems. Concretely, we curate a comprehensive dataset, named OptiDesignQA, which encompasses both classical lens systems sourced from standard optical textbooks and novel configurations generated by automated design algorithms for training and evaluation. Furthermore, we inject domain-specific optical expertise into the LLM through a hybrid objective of full-system synthesis and lens completion. To align the model with optical principles, we employ Group Relative Policy Optimization Done Right (DrGRPO) guided by Optical Lexicographic Reward for physics-driven policy alignment. This reward system incorporates structural format rewards, physical feasibility rewards, light-manipulation accuracy, and LLM-based heuristics. Finally, our model integrates with specialized optical optimization routines for end-to-end fine-tuning and precision refinement. We benchmark our proposed method against both traditional optimization-based automated design algorithms and LLM counterparts, and experimental results show the superiority of our method.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u5e94\u7528\u4e8e\u5149\u5b66\u8bbe\u8ba1\u9886\u57df\uff0c\u901a\u8fc7\u6784\u5efaOptiDesignQA\u6570\u636e\u96c6\u548c\u6df7\u5408\u8bad\u7ec3\u76ee\u6807\uff0c\u7ed3\u5408\u7269\u7406\u9a71\u52a8\u7684\u5956\u52b1\u673a\u5236\uff0c\u4f7f\u975e\u4e13\u4e1a\u7528\u6237\u4e5f\u80fd\u8bbe\u8ba1\u529f\u80fd\u6027\u900f\u955c\u7cfb\u7edf\u3002", "motivation": "\u5149\u5b66\u8bbe\u8ba1\u662f\u9ad8\u5ea6\u975e\u51f8\u7684\u4f18\u5316\u95ee\u9898\uff0c\u4e25\u91cd\u4f9d\u8d56\u4e13\u5bb6\u7ecf\u9a8c\u548c\u9886\u57df\u77e5\u8bc6\u3002\u867d\u7136\u5927\u8bed\u8a00\u6a21\u578b\u62e5\u6709\u4e30\u5bcc\u7684\u5149\u5b66\u77e5\u8bc6\uff0c\u4f46\u5728\u900f\u955c\u7cfb\u7edf\u8bbe\u8ba1\u65b9\u9762\u7684\u80fd\u529b\u4ecd\u53d7\u9650\u5236\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u8ba9\u6ca1\u6709\u6b63\u5f0f\u5149\u5b66\u8bad\u7ec3\u7684\u7528\u6237\u4e5f\u80fd\u6210\u529f\u5f00\u53d1\u529f\u80fd\u6027\u900f\u955c\u7cfb\u7edf\u3002", "method": "1) \u6784\u5efaOptiDesignQA\u6570\u636e\u96c6\uff0c\u5305\u542b\u7ecf\u5178\u900f\u955c\u7cfb\u7edf\u548c\u81ea\u52a8\u8bbe\u8ba1\u7b97\u6cd5\u751f\u6210\u7684\u65b0\u914d\u7f6e\uff1b2) \u901a\u8fc7\u5168\u7cfb\u7edf\u5408\u6210\u548c\u900f\u955c\u8865\u5168\u7684\u6df7\u5408\u76ee\u6807\u5411LLM\u6ce8\u5165\u9886\u57df\u77e5\u8bc6\uff1b3) \u4f7f\u7528DrGRPO\u7b97\u6cd5\u548c\u5149\u5b66\u8bcd\u5178\u5956\u52b1\u8fdb\u884c\u7269\u7406\u9a71\u52a8\u7684\u7b56\u7565\u5bf9\u9f50\uff1b4) \u96c6\u6210\u4e13\u4e1a\u5149\u5b66\u4f18\u5316\u6d41\u7a0b\u8fdb\u884c\u7aef\u5230\u7aef\u5fae\u8c03\u548c\u7cbe\u5ea6\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4f20\u7edf\u57fa\u4e8e\u4f18\u5316\u7684\u81ea\u52a8\u8bbe\u8ba1\u7b97\u6cd5\u548cLLM\u5bf9\u6bd4\u65b9\u6cd5\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\uff0c\u80fd\u591f\u6709\u6548\u8bbe\u8ba1\u529f\u80fd\u6027\u900f\u955c\u7cfb\u7edf\u3002", "conclusion": "\u672c\u6587\u6210\u529f\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u5f15\u5165\u5149\u5b66\u8bbe\u8ba1\u9886\u57df\uff0c\u901a\u8fc7\u6570\u636e\u3001\u8bad\u7ec3\u76ee\u6807\u548c\u7269\u7406\u5bf9\u9f50\u673a\u5236\u7684\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u5149\u5b66\u8bbe\u8ba1\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\uff0c\u4e3a\u975e\u4e13\u4e1a\u7528\u6237\u53c2\u4e0e\u5149\u5b66\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u53ef\u884c\u9014\u5f84\u3002"}}
{"id": "2602.24195", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.24195", "abs": "https://arxiv.org/abs/2602.24195", "authors": ["Gregory Kang Ruey Lau", "Hieu Dao", "Nicole Kan Hui Lin", "Bryan Kian Hsiang Low"], "title": "Uncertainty Quantification for Multimodal Large Language Models with Incoherence-adjusted Semantic Volume", "comment": "Earlier versions presented at ICLR 2025 QUESTION workshop and ICML 2025 R2-FM workshop", "summary": "Despite their capabilities, Multimodal Large Language Models (MLLMs) may produce plausible but erroneous outputs, hindering reliable deployment. Accurate uncertainty metrics could enable escalation of unreliable queries to human experts or larger models for improved performance. However, existing uncertainty metrics have practical constraints, such as being designed only for specific modalities, reliant on external tools, or computationally expensive. We introduce UMPIRE, a training-free uncertainty quantification framework for MLLMs that works efficiently across various input and output modalities without external tools, relying only on the models' own internal modality features. UMPIRE computes the incoherence-adjusted semantic volume of sampled MLLM responses for a given task instance, effectively capturing both the global semantic diversity of samples and the local incoherence of responses based on internal model confidence. We propose uncertainty desiderata for MLLMs and provide theoretical analysis motivating UMPIRE's design. Extensive experiments show that UMPIRE consistently outperforms baseline metrics in error detection and uncertainty calibration across image, audio, and video-text benchmarks, including adversarial and out-of-distribution settings. We also demonstrate UMPIRE's generalization to non-text output tasks, including image and audio generation.", "AI": {"tldr": "UMPIRE\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u8ba1\u7b97\u91c7\u6837\u54cd\u5e94\u7684\u8bed\u4e49\u4f53\u79ef\u6765\u8bc4\u4f30\u4e0d\u786e\u5b9a\u6027\uff0c\u5728\u591a\u79cd\u6a21\u6001\u548c\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u53ef\u80fd\u4ea7\u751f\u770b\u4f3c\u5408\u7406\u4f46\u9519\u8bef\u7684\u8f93\u51fa\uff0c\u963b\u788d\u53ef\u9760\u90e8\u7f72\u3002\u73b0\u6709\u4e0d\u786e\u5b9a\u6027\u5ea6\u91cf\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u4ec5\u9002\u7528\u4e8e\u7279\u5b9a\u6a21\u6001\u3001\u4f9d\u8d56\u5916\u90e8\u5de5\u5177\u6216\u8ba1\u7b97\u6210\u672c\u9ad8\u3002", "method": "UMPIRE\u6846\u67b6\u5229\u7528\u6a21\u578b\u5185\u90e8\u6a21\u6001\u7279\u5f81\uff0c\u8ba1\u7b97\u7ed9\u5b9a\u4efb\u52a1\u5b9e\u4f8b\u91c7\u6837\u54cd\u5e94\u7684\u4e0d\u8fde\u8d2f\u8c03\u6574\u8bed\u4e49\u4f53\u79ef\uff0c\u6355\u6349\u6837\u672c\u7684\u5168\u5c40\u8bed\u4e49\u591a\u6837\u6027\u548c\u57fa\u4e8e\u5185\u90e8\u6a21\u578b\u7f6e\u4fe1\u5ea6\u7684\u5c40\u90e8\u4e0d\u8fde\u8d2f\u6027\u3002", "result": "\u5728\u56fe\u50cf\u3001\u97f3\u9891\u548c\u89c6\u9891-\u6587\u672c\u57fa\u51c6\u6d4b\u8bd5\uff08\u5305\u62ec\u5bf9\u6297\u6027\u548c\u5206\u5e03\u5916\u8bbe\u7f6e\uff09\u4e2d\uff0cUMPIRE\u5728\u9519\u8bef\u68c0\u6d4b\u548c\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u57fa\u7ebf\u6307\u6807\uff0c\u5e76\u80fd\u63a8\u5e7f\u5230\u975e\u6587\u672c\u8f93\u51fa\u4efb\u52a1\u5982\u56fe\u50cf\u548c\u97f3\u9891\u751f\u6210\u3002", "conclusion": "UMPIRE\u662f\u4e00\u4e2a\u9ad8\u6548\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u8f93\u5165\u8f93\u51fa\u6a21\u6001\uff0c\u65e0\u9700\u5916\u90e8\u5de5\u5177\uff0c\u4e3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u9760\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.24188", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.24188", "abs": "https://arxiv.org/abs/2602.24188", "authors": ["Jacob Eisenstein", "Fantine Huot", "Adam Fisch", "Jonathan Berant", "Mirella Lapata"], "title": "MT-PingEval: Evaluating Multi-Turn Collaboration with Private Information Games", "comment": null, "summary": "We present a scalable methodology for evaluating language models in multi-turn interactions, using a suite of collaborative games that require effective communication about private information. This enables an interactive scaling analysis, in which a fixed token budget is divided over a variable number of turns. We find that in many cases, language models are unable to use interactive collaboration to improve over the non-interactive baseline scenario in which one agent attempts to summarize its information and the other agent immediately acts -- despite substantial headroom. This suggests that state-of-the-art models still suffer from significant weaknesses in planning and executing multi-turn collaborative conversations. We analyze the linguistic features of these dialogues, assessing the roles of sycophancy, information density, and discourse coherence. While there is no single linguistic explanation for the collaborative weaknesses of contemporary language models, we note that humans achieve comparable task success at superior token efficiency by producing dialogues that are more coherent than those produced by most language models. The proactive management of private information is a defining feature of real-world communication, and we hope that MT-PingEval will drive further work towards improving this capability.", "AI": {"tldr": "\u63d0\u51faMT-PingEval\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u8f6e\u534f\u4f5c\u6e38\u620f\u6d4b\u8bd5\u8bed\u8a00\u6a21\u578b\u5728\u79c1\u5bc6\u4fe1\u606f\u4ea4\u6d41\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u591a\u8f6e\u534f\u4f5c\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u65e0\u6cd5\u6709\u6548\u5229\u7528\u4ea4\u4e92\u63d0\u5347\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u8bed\u8a00\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u901a\u4fe1\u4e2d\u9700\u8981\u5904\u7406\u79c1\u5bc6\u4fe1\u606f\uff0c\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u8bc4\u4f30\u5176\u5728\u591a\u8f6e\u4ea4\u4e92\u534f\u4f5c\u4e2d\u80fd\u529b\u7684\u6846\u67b6\u3002\u79c1\u5bc6\u4fe1\u606f\u7ba1\u7406\u662f\u73b0\u5b9e\u901a\u4fe1\u7684\u5173\u952e\u7279\u5f81\uff0c\u9700\u8981\u4e13\u95e8\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1MT-PingEval\u8bc4\u4f30\u6846\u67b6\uff0c\u4f7f\u7528\u9700\u8981\u79c1\u5bc6\u4fe1\u606f\u4ea4\u6d41\u7684\u534f\u4f5c\u6e38\u620f\u5957\u4ef6\u3002\u91c7\u7528\u4ea4\u4e92\u5f0f\u7f29\u653e\u5206\u6790\uff0c\u5c06\u56fa\u5b9atoken\u9884\u7b97\u5206\u914d\u5230\u53ef\u53d8\u8f6e\u6b21\u4e2d\uff0c\u6bd4\u8f83\u6a21\u578b\u5728\u591a\u8f6e\u4ea4\u4e92\u4e0e\u5355\u8f6e\u603b\u7ed3\u573a\u666f\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u5728\u8bb8\u591a\u60c5\u51b5\u4e0b\uff0c\u8bed\u8a00\u6a21\u578b\u65e0\u6cd5\u901a\u8fc7\u591a\u8f6e\u4ea4\u4e92\u534f\u4f5c\u8d85\u8d8a\u975e\u4ea4\u4e92\u57fa\u7ebf\uff08\u5355\u667a\u80fd\u4f53\u603b\u7ed3\u4fe1\u606f\u540e\u7acb\u5373\u884c\u52a8\uff09\uff0c\u5c3d\u7ba1\u5b58\u5728\u663e\u8457\u63d0\u5347\u7a7a\u95f4\u3002\u4eba\u7c7b\u5728\u76f8\u540c\u4efb\u52a1\u4e2d\u80fd\u4ee5\u66f4\u9ad8token\u6548\u7387\u5b9e\u73b0\u53ef\u6bd4\u6210\u529f\u7387\uff0c\u4e14\u5bf9\u8bdd\u66f4\u8fde\u8d2f\u3002", "conclusion": "\u6700\u5148\u8fdb\u7684\u8bed\u8a00\u6a21\u578b\u5728\u591a\u8f6e\u534f\u4f5c\u5bf9\u8bdd\u7684\u89c4\u5212\u548c\u6267\u884c\u65b9\u9762\u4ecd\u5b58\u5728\u663e\u8457\u5f31\u70b9\u3002\u867d\u7136\u65e0\u6cd5\u7528\u5355\u4e00\u8bed\u8a00\u7279\u5f81\u89e3\u91ca\u8fd9\u4e9b\u534f\u4f5c\u5f31\u70b9\uff0c\u4f46\u4eba\u7c7b\u5bf9\u8bdd\u7684\u8fde\u8d2f\u6027\u4f18\u52bf\u8868\u660e\u6539\u8fdb\u65b9\u5411\u3002MT-PingEval\u6846\u67b6\u6709\u671b\u63a8\u52a8\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u7684\u79c1\u5bc6\u4fe1\u606f\u7ba1\u7406\u80fd\u529b\u3002"}}
{"id": "2602.23770", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23770", "abs": "https://arxiv.org/abs/2602.23770", "authors": ["Chenxing Lin", "Xinhui Gao", "Haipeng Zhang", "Xinran Li", "Haitao Wang", "Songzhu Mei", "Chenglu Wen", "Weiquan Liu", "Siqi Shen", "Cheng Wang"], "title": "MAGE: Multi-scale Autoregressive Generation for Offline Reinforcement Learning", "comment": "ICLR2026", "summary": "Generative models have gained significant traction in offline reinforcement learning (RL) due to their ability to model complex trajectory distributions. However, existing generation-based approaches still struggle with long-horizon tasks characterized by sparse rewards. Some hierarchical generation methods have been developed to mitigate this issue by decomposing the original problem into shorter-horizon subproblems using one policy and generating detailed actions with another. While effective, these methods often overlook the multi-scale temporal structure inherent in trajectories, resulting in suboptimal performance. To overcome these limitations, we propose MAGE, a Multi-scale Autoregressive GEneration-based offline RL method. MAGE incorporates a condition-guided multi-scale autoencoder to learn hierarchical trajectory representations, along with a multi-scale transformer that autoregressively generates trajectory representations from coarse to fine temporal scales. MAGE effectively captures temporal dependencies of trajectories at multiple resolutions. Additionally, a condition-guided decoder is employed to exert precise control over short-term behaviors. Extensive experiments on five offline RL benchmarks against fifteen baseline algorithms show that MAGE successfully integrates multi-scale trajectory modeling with conditional guidance, generating coherent and controllable trajectories in long-horizon sparse-reward settings.", "AI": {"tldr": "MAGE\u662f\u4e00\u79cd\u57fa\u4e8e\u591a\u5c3a\u5ea6\u81ea\u56de\u5f52\u751f\u6210\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u8f68\u8ff9\u5efa\u6a21\u89e3\u51b3\u957f\u65f6\u7a0b\u7a00\u758f\u5956\u52b1\u4efb\u52a1\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u5f0f\u79bb\u7ebfRL\u65b9\u6cd5\u5728\u957f\u65f6\u7a0b\u7a00\u758f\u5956\u52b1\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4f20\u7edf\u5206\u5c42\u751f\u6210\u65b9\u6cd5\u5ffd\u7565\u4e86\u8f68\u8ff9\u56fa\u6709\u7684\u591a\u5c3a\u5ea6\u65f6\u95f4\u7ed3\u6784\uff0c\u5bfc\u81f4\u6b21\u4f18\u6027\u80fd\u3002", "method": "\u63d0\u51faMAGE\u65b9\u6cd5\uff1a1\uff09\u6761\u4ef6\u5f15\u5bfc\u7684\u591a\u5c3a\u5ea6\u81ea\u7f16\u7801\u5668\u5b66\u4e60\u5206\u5c42\u8f68\u8ff9\u8868\u793a\uff1b2\uff09\u591a\u5c3a\u5ea6Transformer\u4ece\u7c97\u5230\u7ec6\u81ea\u56de\u5f52\u751f\u6210\u8f68\u8ff9\u8868\u793a\uff1b3\uff09\u6761\u4ef6\u5f15\u5bfc\u89e3\u7801\u5668\u7cbe\u786e\u63a7\u5236\u77ed\u671f\u884c\u4e3a\u3002", "result": "\u57285\u4e2a\u79bb\u7ebfRL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e0e15\u4e2a\u57fa\u7ebf\u7b97\u6cd5\u5bf9\u6bd4\uff0cMAGE\u6210\u529f\u6574\u5408\u591a\u5c3a\u5ea6\u8f68\u8ff9\u5efa\u6a21\u4e0e\u6761\u4ef6\u5f15\u5bfc\uff0c\u5728\u957f\u65f6\u7a0b\u7a00\u758f\u5956\u52b1\u573a\u666f\u4e2d\u751f\u6210\u8fde\u8d2f\u53ef\u63a7\u7684\u8f68\u8ff9\u3002", "conclusion": "MAGE\u901a\u8fc7\u591a\u5c3a\u5ea6\u81ea\u56de\u5f52\u751f\u6210\u6709\u6548\u6355\u6349\u8f68\u8ff9\u7684\u591a\u5206\u8fa8\u7387\u65f6\u95f4\u4f9d\u8d56\uff0c\u89e3\u51b3\u4e86\u751f\u6210\u5f0f\u79bb\u7ebfRL\u5728\u957f\u65f6\u7a0b\u7a00\u758f\u5956\u52b1\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2602.24273", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.24273", "abs": "https://arxiv.org/abs/2602.24273", "authors": ["Borja Requena Pozo", "Austin Letson", "Krystian Nowakowski", "Izan Beltran Ferreiro", "Leopoldo Sarra"], "title": "A Minimal Agent for Automated Theorem Proving", "comment": null, "summary": "We propose a minimal agentic baseline that enables systematic comparison across different AI-based theorem prover architectures. This design implements the core features shared among state-of-the-art systems: iterative proof refinement, library search and context management. We evaluate our baseline using qualitatively different benchmarks and compare various popular models and design choices, and demonstrate competitive performance compared to state-of-the-art approaches, while using a significantly simpler architecture. Our results demonstrate consistent advantages of an iterative approach over multiple single-shot generations, especially in terms of sample efficiency and cost effectiveness. The implementation is released open-source as a candidate reference for future research and as an accessible prover for the community.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u6700\u5c0f\u5316\u7684\u667a\u80fd\u4f53\u57fa\u51c6\u7cfb\u7edf\uff0c\u7528\u4e8e\u7cfb\u7edf\u6bd4\u8f83\u4e0d\u540cAI\u5b9a\u7406\u8bc1\u660e\u5668\u67b6\u6784\uff0c\u5c55\u793a\u8fed\u4ee3\u65b9\u6cd5\u76f8\u6bd4\u5355\u6b21\u751f\u6210\u7684\u4f18\u8d8a\u6027", "motivation": "\u4e3a\u4e86\u7cfb\u7edf\u6bd4\u8f83\u4e0d\u540cAI\u5b9a\u7406\u8bc1\u660e\u5668\u67b6\u6784\uff0c\u9700\u8981\u4e00\u4e2a\u5171\u4eab\u6838\u5fc3\u529f\u80fd\u7684\u6700\u5c0f\u5316\u57fa\u51c6\u7cfb\u7edf\uff0c\u907f\u514d\u590d\u6742\u67b6\u6784\u5e26\u6765\u7684\u6df7\u6dc6\u56e0\u7d20", "method": "\u8bbe\u8ba1\u5b9e\u73b0\u5305\u542b\u8fed\u4ee3\u8bc1\u660e\u7cbe\u70bc\u3001\u5e93\u641c\u7d22\u548c\u4e0a\u4e0b\u6587\u7ba1\u7406\u7b49\u6838\u5fc3\u529f\u80fd\u7684\u6700\u5c0f\u5316\u667a\u80fd\u4f53\u57fa\u7ebf\uff0c\u4f7f\u7528\u4e0d\u540c\u57fa\u51c6\u8bc4\u4f30\u5404\u79cd\u6d41\u884c\u6a21\u578b\u548c\u8bbe\u8ba1\u9009\u62e9", "result": "\u8be5\u57fa\u7ebf\u7cfb\u7edf\u5728\u4fdd\u6301\u663e\u8457\u7b80\u5316\u67b6\u6784\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u7ade\u4e89\u7684\u6027\u80fd\uff0c\u8fed\u4ee3\u65b9\u6cd5\u5728\u6837\u672c\u6548\u7387\u548c\u6210\u672c\u6548\u76ca\u65b9\u9762\u4f18\u4e8e\u591a\u6b21\u5355\u6b21\u751f\u6210", "conclusion": "\u8fed\u4ee3\u65b9\u6cd5\u76f8\u6bd4\u591a\u6b21\u5355\u6b21\u751f\u6210\u5177\u6709\u660e\u663e\u4f18\u52bf\uff0c\u8be5\u5f00\u6e90\u5b9e\u73b0\u53ef\u4f5c\u4e3a\u672a\u6765\u7814\u7a76\u7684\u53c2\u8003\u57fa\u51c6\u548c\u793e\u533a\u53ef\u8bbf\u95ee\u7684\u8bc1\u660e\u5668"}}
{"id": "2602.24210", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.24210", "abs": "https://arxiv.org/abs/2602.24210", "authors": ["Haritz Puerto", "Haonan Li", "Xudong Han", "Timothy Baldwin", "Iryna Gurevych"], "title": "Controllable Reasoning Models Are Private Thinkers", "comment": null, "summary": "AI agents powered by reasoning models require access to sensitive user data. However, their reasoning traces are difficult to control, which can result in the unintended leakage of private information to external parties. We propose training models to follow instructions not only in the final answer, but also in reasoning traces, potentially under different constraints. We hypothesize that improving their instruction following abilities in the reasoning traces can improve their privacy-preservation skills. To demonstrate this, we fine-tune models on a new instruction-following dataset with explicit restrictions on reasoning traces. We further introduce a generation strategy that decouples reasoning and answer generation using separate LoRA adapters. We evaluate our approach on six models from two model families, ranging from 1.7B to 14B parameters, across two instruction-following benchmarks and two privacy benchmarks. Our method yields substantial improvements, achieving gains of up to 20.9 points in instruction-following performance and up to 51.9 percentage points on privacy benchmarks. These improvements, however, can come at the cost of task utility, due to the trade-off between reasoning performance and instruction-following abilities. Overall, our results show that improving instruction-following behavior in reasoning models can significantly enhance privacy, suggesting a promising direction for the development of future privacy-aware agents. Our code and data are available at https://github.com/UKPLab/arxiv2026-controllable-reasoning-models", "AI": {"tldr": "\u8bad\u7ec3\u63a8\u7406\u6a21\u578b\u5728\u63a8\u7406\u8f68\u8ff9\u4e2d\u9075\u5faa\u6307\u4ee4\u4ee5\u4fdd\u62a4\u9690\u79c1\uff0c\u901a\u8fc7\u5206\u79bb\u63a8\u7406\u548c\u7b54\u6848\u751f\u6210\u7684LoRA\u9002\u914d\u5668\u65b9\u6cd5\uff0c\u5728\u9690\u79c1\u57fa\u51c6\u4e0a\u63d0\u534751.9\u4e2a\u767e\u5206\u70b9\uff0c\u4f46\u53ef\u80fd\u727a\u7272\u4efb\u52a1\u6548\u7528\u3002", "motivation": "AI\u63a8\u7406\u6a21\u578b\u9700\u8981\u8bbf\u95ee\u654f\u611f\u7528\u6237\u6570\u636e\uff0c\u4f46\u5176\u63a8\u7406\u8f68\u8ff9\u96be\u4ee5\u63a7\u5236\uff0c\u53ef\u80fd\u5bfc\u81f4\u9690\u79c1\u4fe1\u606f\u610f\u5916\u6cc4\u9732\u7ed9\u5916\u90e8\u65b9\u3002\u73b0\u6709\u6a21\u578b\u53ea\u5728\u6700\u7ec8\u7b54\u6848\u4e2d\u9075\u5faa\u6307\u4ee4\uff0c\u800c\u63a8\u7406\u8f68\u8ff9\u7f3a\u4e4f\u63a7\u5236\u3002", "method": "1) \u5728\u65b0\u6307\u4ee4\u9075\u5faa\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u6a21\u578b\uff0c\u5bf9\u63a8\u7406\u8f68\u8ff9\u65bd\u52a0\u660e\u786e\u9650\u5236\uff1b2) \u5f15\u5165\u5206\u79bb\u63a8\u7406\u548c\u7b54\u6848\u751f\u6210\u7684\u751f\u6210\u7b56\u7565\uff0c\u4f7f\u7528\u72ec\u7acb\u7684LoRA\u9002\u914d\u5668\uff1b3) \u57286\u4e2a\u6a21\u578b\uff081.7B-14B\u53c2\u6570\uff09\u4e0a\u8bc4\u4f30\uff0c\u6db5\u76d6\u4e24\u4e2a\u6307\u4ee4\u9075\u5faa\u57fa\u51c6\u548c\u4e24\u4e2a\u9690\u79c1\u57fa\u51c6\u3002", "result": "\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u6027\u80fd\uff1a\u6307\u4ee4\u9075\u5faa\u6027\u80fd\u63d0\u5347\u8fbe20.9\u5206\uff0c\u9690\u79c1\u57fa\u51c6\u63d0\u5347\u8fbe51.9\u4e2a\u767e\u5206\u70b9\u3002\u4f46\u6539\u8fdb\u53ef\u80fd\u4ee5\u4efb\u52a1\u6548\u7528\u4e3a\u4ee3\u4ef7\uff0c\u4f53\u73b0\u4e86\u63a8\u7406\u6027\u80fd\u4e0e\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u4e4b\u95f4\u7684\u6743\u8861\u3002", "conclusion": "\u63d0\u5347\u63a8\u7406\u6a21\u578b\u7684\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u53ef\u663e\u8457\u589e\u5f3a\u9690\u79c1\u4fdd\u62a4\uff0c\u4e3a\u672a\u6765\u9690\u79c1\u611f\u77e5\u667a\u80fd\u4f53\u7684\u5f00\u53d1\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002\u9700\u8981\u5728\u9690\u79c1\u4fdd\u62a4\u548c\u4efb\u52a1\u6548\u7528\u4e4b\u95f4\u627e\u5230\u5e73\u8861\u3002"}}
{"id": "2602.24104", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.24104", "abs": "https://arxiv.org/abs/2602.24104", "authors": ["Rui Chen", "Daniele Leonardis", "Domenico Chiaradia", "Antonio Frisoli"], "title": "Geometry-based pneumatic actuators for soft robotics", "comment": null, "summary": "Soft pneumatic actuators enable safe human-machine interaction with lightweight and powerful applied parts. On the other side, they suffer design limitations as regards complex actuation patterns, including minimum bending radii, multi-states capabilities and structural stability. We present geometry-based pneumatic actuators (GPAs), a design and implementation approach that introduces constraint layers with configurable CNC heat-sealed chambers. The approach achieves predictable deformation, near-zero bending radii, multi-states actuation, and enables customizable and repeatable complex actuated geometries. Mathematical modeling reveals predictable linear angle transformations and validates nonlinear torque-angle relationships across diverse configurations. We demonstrate versatility of the GPAs approach through three applications: a 49 g wrist exoskeleton reducing muscle activity by up to 51%, a 30.8 g haptic interface delivering 8 N force feedback with fast response, and a 208 g bipedal robot achieving multi-gait locomotion. GPAs establish a configurable platform for next-generation wearable robotics, haptic systems, and soft locomotion devices.", "AI": {"tldr": "\u63d0\u51fa\u51e0\u4f55\u57fa\u6c14\u52a8\u6267\u884c\u5668(GPAs)\uff0c\u901a\u8fc7\u53ef\u914d\u7f6eCNC\u70ed\u5c01\u8154\u5ba4\u5b9e\u73b0\u590d\u6742\u9a71\u52a8\u6a21\u5f0f\uff0c\u5e94\u7528\u4e8e\u8155\u90e8\u5916\u9aa8\u9abc\u3001\u89e6\u89c9\u63a5\u53e3\u548c\u53cc\u8db3\u673a\u5668\u4eba", "motivation": "\u8f6f\u6c14\u52a8\u6267\u884c\u5668\u5728\u5b89\u5168\u4eba\u673a\u4ea4\u4e92\u65b9\u9762\u6709\u4f18\u52bf\uff0c\u4f46\u5728\u590d\u6742\u9a71\u52a8\u6a21\u5f0f\u65b9\u9762\u5b58\u5728\u8bbe\u8ba1\u9650\u5236\uff0c\u5982\u6700\u5c0f\u5f2f\u66f2\u534a\u5f84\u3001\u591a\u72b6\u6001\u80fd\u529b\u548c\u7ed3\u6784\u7a33\u5b9a\u6027\u4e0d\u8db3", "method": "\u91c7\u7528\u51e0\u4f55\u57fa\u6c14\u52a8\u6267\u884c\u5668(GPAs)\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u5f15\u5165\u5177\u6709\u53ef\u914d\u7f6eCNC\u70ed\u5c01\u8154\u5ba4\u7684\u7ea6\u675f\u5c42\uff0c\u5b9e\u73b0\u53ef\u9884\u6d4b\u53d8\u5f62\u3001\u8fd1\u96f6\u5f2f\u66f2\u534a\u5f84\u548c\u591a\u72b6\u6001\u9a71\u52a8", "result": "\u6570\u5b66\u5efa\u6a21\u9a8c\u8bc1\u4e86\u53ef\u9884\u6d4b\u7684\u7ebf\u6027\u89d2\u5ea6\u53d8\u6362\u548c\u975e\u7ebf\u6027\u626d\u77e9-\u89d2\u5ea6\u5173\u7cfb\uff1b49g\u8155\u90e8\u5916\u9aa8\u9abc\u51cf\u5c11\u808c\u8089\u6d3b\u52a8\u8fbe51%\uff1b30.8g\u89e6\u89c9\u63a5\u53e3\u63d0\u4f9b8N\u529b\u53cd\u9988\uff1b208g\u53cc\u8db3\u673a\u5668\u4eba\u5b9e\u73b0\u591a\u6b65\u6001\u8fd0\u52a8", "conclusion": "GPAs\u4e3a\u4e0b\u4e00\u4ee3\u53ef\u7a7f\u6234\u673a\u5668\u4eba\u3001\u89e6\u89c9\u7cfb\u7edf\u548c\u8f6f\u4f53\u8fd0\u52a8\u8bbe\u5907\u5efa\u7acb\u4e86\u53ef\u914d\u7f6e\u5e73\u53f0\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u8f6f\u6c14\u52a8\u6267\u884c\u5668\u7684\u8bbe\u8ba1\u9650\u5236"}}
{"id": "2602.23790", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23790", "abs": "https://arxiv.org/abs/2602.23790", "authors": ["Changyu Gu", "Linwei Chen", "Lin Gu", "Ying Fu"], "title": "Fourier Angle Alignment for Oriented Object Detection in Remote Sensing", "comment": "Accepted by CVPR 2026", "summary": "In remote sensing rotated object detection, mainstream methods suffer from two bottlenecks, directional incoherence at detector neck and task conflict at detecting head. Ulitising fourier rotation equivariance, we introduce Fourier Angle Alignment, which analyses angle information through frequency spectrum and aligns the main direction to a certain orientation. Then we propose two plug and play modules : FAAFusion and FAA Head. FAAFusion works at the detector neck, aligning the main direction of higher-level features to the lower-level features and then fusing them. FAA Head serves as a new detection head, which pre-aligns RoI features to a canonical angle and adds them to the original features before classification and regression. Experiments on DOTA-v1.0, DOTA-v1.5 and HRSC2016 show that our method can greatly improve previous work. Particularly, our method achieves new state-of-the-art results of 78.72% mAP on DOTA-v1.0 and 72.28% mAP on DOTA-v1.5 datasets with single scale training and testing, validating the efficacy of our approach in remote sensing object detection. The code is made publicly available at https://github.com/gcy0423/Fourier-Angle-Alignment .", "AI": {"tldr": "\u672c\u6587\u63d0\u51faFourier Angle Alignment\u65b9\u6cd5\uff0c\u901a\u8fc7\u5085\u91cc\u53f6\u65cb\u8f6c\u7b49\u53d8\u6027\u5206\u6790\u89d2\u5ea6\u4fe1\u606f\uff0c\u89e3\u51b3\u9065\u611f\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b\u4e2d\u7684\u65b9\u5411\u4e0d\u4e00\u81f4\u548c\u4efb\u52a1\u51b2\u7a81\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97SOTA\u7ed3\u679c\u3002", "motivation": "\u9065\u611f\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b\u4e2d\u7684\u4e24\u4e2a\u74f6\u9888\uff1a\u68c0\u6d4b\u5668\u9888\u90e8\u5b58\u5728\u65b9\u5411\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u68c0\u6d4b\u5934\u90e8\u5b58\u5728\u4efb\u52a1\u51b2\u7a81\u95ee\u9898\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u6709\u6548\u5904\u7406\u65cb\u8f6c\u76ee\u6807\u89d2\u5ea6\u4fe1\u606f\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u5085\u91cc\u53f6\u89d2\u5ea6\u5bf9\u9f50\u65b9\u6cd5\uff0c\u901a\u8fc7\u9891\u7387\u8c31\u5206\u6790\u89d2\u5ea6\u4fe1\u606f\u5e76\u5c06\u4e3b\u65b9\u5411\u5bf9\u9f50\u5230\u7279\u5b9a\u65b9\u5411\u3002\u8bbe\u8ba1\u4e24\u4e2a\u5373\u63d2\u5373\u7528\u6a21\u5757\uff1aFAAFusion\uff08\u5728\u68c0\u6d4b\u5668\u9888\u90e8\u5de5\u4f5c\uff0c\u5c06\u9ad8\u5c42\u7279\u5f81\u4e3b\u65b9\u5411\u4e0e\u4f4e\u5c42\u7279\u5f81\u5bf9\u9f50\u5e76\u878d\u5408\uff09\u548cFAA Head\uff08\u4f5c\u4e3a\u65b0\u68c0\u6d4b\u5934\uff0c\u5c06RoI\u7279\u5f81\u9884\u5bf9\u9f50\u5230\u89c4\u8303\u89d2\u5ea6\u540e\u6dfb\u52a0\u5230\u539f\u59cb\u7279\u5f81\u4e2d\uff09\u3002", "result": "\u5728DOTA-v1.0\u3001DOTA-v1.5\u548cHRSC2016\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u73b0\u6709\u65b9\u6cd5\u6027\u80fd\u3002\u5355\u5c3a\u5ea6\u8bad\u7ec3\u548c\u6d4b\u8bd5\u4e0b\uff0c\u5728DOTA-v1.0\u4e0a\u8fbe\u523078.72% mAP\uff0c\u5728DOTA-v1.5\u4e0a\u8fbe\u523072.28% mAP\uff0c\u5747\u521b\u4e0b\u65b0\u7684SOTA\u8bb0\u5f55\u3002", "conclusion": "\u5085\u91cc\u53f6\u89d2\u5ea6\u5bf9\u9f50\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u9065\u611f\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b\u4e2d\u7684\u65b9\u5411\u4e0d\u4e00\u81f4\u548c\u4efb\u52a1\u51b2\u7a81\u95ee\u9898\uff0c\u901a\u8fc7\u9891\u7387\u57df\u7684\u89d2\u5ea6\u5206\u6790\u548c\u7279\u5f81\u5bf9\u9f50\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2602.24287", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.24287", "abs": "https://arxiv.org/abs/2602.24287", "authors": ["Jenny Y. Huang", "Leshem Choshen", "Ramon Astudillo", "Tamara Broderick", "Jacob Andreas"], "title": "Do LLMs Benefit From Their Own Words?", "comment": null, "summary": "Multi-turn interactions with large language models typically retain the assistant's own past responses in the conversation history. In this work, we revisit this design choice by asking whether large language models benefit from conditioning on their own prior responses. Using in-the-wild, multi-turn conversations, we compare standard (full-context) prompting with a user-turn-only prompting approach that omits all previous assistant responses, across three open reasoning models and one state-of-the-art model. To our surprise, we find that removing prior assistant responses does not affect response quality on a large fraction of turns. Omitting assistant-side history can reduce cumulative context lengths by up to 10x. To explain this result, we find that multi-turn conversations consist of a substantial proportion (36.4%) of self-contained prompts, and that many follow-up prompts provide sufficient instruction to be answered using only the current user turn and prior user turns. When analyzing cases where user-turn-only prompting substantially outperforms full context, we identify instances of context pollution, in which models over-condition on their previous responses, introducing errors, hallucinations, or stylistic artifacts that propagate across turns. Motivated by these findings, we design a context-filtering approach that selectively omits assistant-side context. Our findings suggest that selectively omitting assistant history can improve response quality while reducing memory consumption.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7701\u7565\u8bed\u8a00\u6a21\u578b\u81ea\u5df1\u4e4b\u524d\u7684\u56de\u590d\uff0c\u4e0d\u4ec5\u4e0d\u4f1a\u964d\u4f4e\u56de\u7b54\u8d28\u91cf\uff0c\u53cd\u800c\u80fd\u51cf\u5c11\u4e0a\u4e0b\u6587\u957f\u5ea6\u5e76\u907f\u514d\u4e0a\u4e0b\u6587\u6c61\u67d3\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u591a\u8f6e\u5bf9\u8bdd\u7cfb\u7edf\u901a\u5e38\u4fdd\u7559\u52a9\u624b\u7684\u6240\u6709\u5386\u53f2\u56de\u590d\uff0c\u4f46\u4f5c\u8005\u8d28\u7591\u8fd9\u79cd\u8bbe\u8ba1\u662f\u5426\u771f\u6b63\u6709\u76ca\uff0c\u60f3\u63a2\u7a76\u8bed\u8a00\u6a21\u578b\u662f\u5426\u771f\u7684\u9700\u8981\u4f9d\u8d56\u81ea\u5df1\u4e4b\u524d\u7684\u56de\u7b54\u6765\u751f\u6210\u66f4\u597d\u7684\u56de\u590d\u3002", "method": "\u4f7f\u7528\u771f\u5b9e\u591a\u8f6e\u5bf9\u8bdd\u6570\u636e\uff0c\u6bd4\u8f83\u6807\u51c6\u5168\u4e0a\u4e0b\u6587\u63d0\u793a\u4e0e\u4ec5\u7528\u6237\u8f6e\u6b21\u63d0\u793a\uff08\u7701\u7565\u6240\u6709\u52a9\u624b\u5386\u53f2\u56de\u590d\uff09\u7684\u6548\u679c\uff0c\u6d4b\u8bd5\u4e86\u4e09\u4e2a\u5f00\u653e\u63a8\u7406\u6a21\u578b\u548c\u4e00\u4e2a\u6700\u5148\u8fdb\u6a21\u578b\u3002\u8bbe\u8ba1\u4e86\u4e0a\u4e0b\u6587\u8fc7\u6ee4\u65b9\u6cd5\uff0c\u9009\u62e9\u6027\u7701\u7565\u52a9\u624b\u4fa7\u4e0a\u4e0b\u6587\u3002", "result": "1. \u79fb\u9664\u52a9\u624b\u5386\u53f2\u56de\u590d\u5bf9\u5927\u90e8\u5206\u8f6e\u6b21\u56de\u7b54\u8d28\u91cf\u65e0\u5f71\u54cd\uff1b2. \u7701\u7565\u52a9\u624b\u4fa7\u5386\u53f2\u53ef\u5c06\u7d2f\u8ba1\u4e0a\u4e0b\u6587\u957f\u5ea6\u51cf\u5c11\u9ad8\u8fbe10\u500d\uff1b3. 36.4%\u7684\u63d0\u793a\u662f\u81ea\u5305\u542b\u7684\uff1b4. \u53d1\u73b0\u4e0a\u4e0b\u6587\u6c61\u67d3\u73b0\u8c61\uff0c\u6a21\u578b\u8fc7\u5ea6\u4f9d\u8d56\u81ea\u5df1\u4e4b\u524d\u7684\u56de\u590d\u5bfc\u81f4\u9519\u8bef\u4f20\u64ad\uff1b5. \u9009\u62e9\u6027\u7701\u7565\u52a9\u624b\u5386\u53f2\u53ef\u540c\u65f6\u63d0\u5347\u56de\u7b54\u8d28\u91cf\u548c\u51cf\u5c11\u5185\u5b58\u6d88\u8017\u3002", "conclusion": "\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\uff0c\u9009\u62e9\u6027\u7701\u7565\u8bed\u8a00\u6a21\u578b\u81ea\u5df1\u7684\u5386\u53f2\u56de\u590d\u662f\u66f4\u4f18\u7684\u8bbe\u8ba1\u9009\u62e9\uff0c\u65e2\u80fd\u63d0\u9ad8\u56de\u7b54\u8d28\u91cf\uff0c\u53c8\u80fd\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u3002"}}
{"id": "2602.24121", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.24121", "abs": "https://arxiv.org/abs/2602.24121", "authors": ["Tyler Han", "Siyang Shen", "Rohan Baijal", "Harine Ravichandiran", "Bat Nemekhbold", "Kevin Huang", "Sanghun Jung", "Byron Boots"], "title": "Planning from Observation and Interaction", "comment": null, "summary": "Observational learning requires an agent to learn to perform a task by referencing only observations of the performed task. This work investigates the equivalent setting in real-world robot learning where access to hand-designed rewards and demonstrator actions are not assumed. To address this data-constrained setting, this work presents a planning-based Inverse Reinforcement Learning (IRL) algorithm for world modeling from observation and interaction alone. Experiments conducted entirely in the real-world demonstrate that this paradigm is effective for learning image-based manipulation tasks from scratch in under an hour, without assuming prior knowledge, pre-training, or data of any kind beyond task observations. Moreover, this work demonstrates that the learned world model representation is capable of online transfer learning in the real-world from scratch. In comparison to existing approaches, including IRL, RL, and Behavior Cloning (BC), which have more restrictive assumptions, the proposed approach demonstrates significantly greater sample efficiency and success rates, enabling a practical path forward for online world modeling and planning from observation and interaction. Videos and more at: https://uwrobotlearning.github.io/mpail2/.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u89c4\u5212\u7684\u53cd\u5411\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u4ec5\u901a\u8fc7\u89c2\u5bdf\u548c\u4ea4\u4e92\u5b9e\u73b0\u673a\u5668\u4eba\u4e16\u754c\u5efa\u6a21\uff0c\u65e0\u9700\u4eba\u5de5\u8bbe\u8ba1\u5956\u52b1\u6216\u6f14\u793a\u8005\u52a8\u4f5c\uff0c\u80fd\u57281\u5c0f\u65f6\u5185\u4ece\u96f6\u5b66\u4e60\u56fe\u50cf\u64cd\u4f5c\u4efb\u52a1", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u5b66\u4e60\u4e2d\u7684\u89c2\u5bdf\u5b66\u4e60\u95ee\u9898\uff0c\u5728\u65e0\u6cd5\u83b7\u53d6\u4eba\u5de5\u8bbe\u8ba1\u5956\u52b1\u548c\u6f14\u793a\u8005\u52a8\u4f5c\u7684\u7ea6\u675f\u6761\u4ef6\u4e0b\uff0c\u4ec5\u901a\u8fc7\u89c2\u5bdf\u4efb\u52a1\u6267\u884c\u8fc7\u7a0b\u6765\u5b66\u4e60", "method": "\u63d0\u51fa\u57fa\u4e8e\u89c4\u5212\u7684\u53cd\u5411\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u4ec5\u901a\u8fc7\u89c2\u5bdf\u548c\u4ea4\u4e92\u8fdb\u884c\u4e16\u754c\u5efa\u6a21\uff0c\u4e0d\u4f9d\u8d56\u5148\u9a8c\u77e5\u8bc6\u3001\u9884\u8bad\u7ec3\u6216\u989d\u5916\u6570\u636e", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u57281\u5c0f\u65f6\u5185\u4ece\u96f6\u5b66\u4e60\u56fe\u50cf\u64cd\u4f5c\u4efb\u52a1\uff0c\u6837\u672c\u6548\u7387\u548c\u6210\u529f\u7387\u663e\u8457\u4f18\u4e8e\u73b0\u6709IRL\u3001RL\u548cBC\u65b9\u6cd5\uff0c\u5e76\u80fd\u5b9e\u73b0\u5728\u7ebf\u8fc1\u79fb\u5b66\u4e60", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4ec5\u901a\u8fc7\u89c2\u5bdf\u548c\u4ea4\u4e92\u8fdb\u884c\u5728\u7ebf\u4e16\u754c\u5efa\u6a21\u548c\u89c4\u5212\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84\uff0c\u5728\u6570\u636e\u53d7\u9650\u7684\u673a\u5668\u4eba\u5b66\u4e60\u573a\u666f\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf"}}
{"id": "2602.23785", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23785", "abs": "https://arxiv.org/abs/2602.23785", "authors": ["Zhiwei Han", "Stefan Matthes", "Hao Shen"], "title": "Provable Subspace Identification of Nonlinear Multi-view CCA", "comment": null, "summary": "We investigate the identifiability of nonlinear Canonical Correlation Analysis (CCA) in a multi-view setup, where each view is generated by an unknown nonlinear map applied to a linear mixture of shared latents and view-private noise. Rather than attempting exact unmixing, a problem proven to be ill-posed, we instead reframe multi-view CCA as a basis-invariant subspace identification problem. We prove that, under suitable latent priors and spectral separation conditions, multi-view CCA recovers the pairwise correlated signal subspaces up to view-wise orthogonal ambiguity. For $N \\geq 3$ views, the objective provably isolates the jointly correlated subspaces shared across all views while eliminating view-private variations. We further establish finite-sample consistency guarantees by translating the concentration of empirical cross-covariances into explicit subspace error bounds via spectral perturbation theory. Experiments on synthetic and rendered image datasets validate our theoretical findings and confirm the necessity of the assumed conditions.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u591a\u89c6\u56fe\u975e\u7ebf\u6027\u5178\u578b\u76f8\u5173\u5206\u6790(CCA)\u7684\u53ef\u8bc6\u522b\u6027\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u5728\u591a\u89c6\u56fe\u8bbe\u7f6e\u4e0b\uff0c\u5373\u4f7f\u5b58\u5728\u975e\u7ebf\u6027\u53d8\u6362\uff0cCCA\u4ecd\u80fd\u6062\u590d\u5171\u4eab\u6f5c\u5728\u5b50\u7a7a\u95f4\uff0c\u4f46\u5b58\u5728\u89c6\u56fe\u7279\u5b9a\u7684\u6b63\u4ea4\u6a21\u7cca\u6027\u3002", "motivation": "\u4f20\u7edf\u975e\u7ebf\u6027CCA\u7684\u7cbe\u786e\u89e3\u6df7\u88ab\u8bc1\u660e\u662f\u4e0d\u9002\u5b9a\u95ee\u9898\uff0c\u4f5c\u8005\u5e0c\u671b\u91cd\u65b0\u6784\u5efa\u591a\u89c6\u56feCCA\u4f5c\u4e3a\u4e00\u4e2a\u57fa\u4e0d\u53d8\u5b50\u7a7a\u95f4\u8bc6\u522b\u95ee\u9898\uff0c\u63a2\u7d22\u5728\u4ec0\u4e48\u6761\u4ef6\u4e0b\u53ef\u4ee5\u8bc6\u522b\u5171\u4eab\u7684\u6f5c\u5728\u7ed3\u6784\u3002", "method": "\u5c06\u591a\u89c6\u56feCCA\u91cd\u65b0\u5b9a\u4e49\u4e3a\u57fa\u4e0d\u53d8\u5b50\u7a7a\u95f4\u8bc6\u522b\u95ee\u9898\uff0c\u5728\u5408\u9002\u7684\u6f5c\u5728\u5148\u9a8c\u548c\u8c31\u5206\u79bb\u6761\u4ef6\u4e0b\uff0c\u8bc1\u660e\u591a\u89c6\u56feCCA\u80fd\u591f\u6062\u590d\u6210\u5bf9\u76f8\u5173\u7684\u4fe1\u53f7\u5b50\u7a7a\u95f4\u3002\u5bf9\u4e8eN\u22653\u4e2a\u89c6\u56fe\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5206\u79bb\u51fa\u6240\u6709\u89c6\u56fe\u5171\u4eab\u7684\u8054\u5408\u76f8\u5173\u5b50\u7a7a\u95f4\uff0c\u540c\u65f6\u6d88\u9664\u89c6\u56fe\u79c1\u6709\u53d8\u5f02\u3002", "result": "\u7406\u8bba\u8bc1\u660e\uff1a1) \u591a\u89c6\u56feCCA\u6062\u590d\u6210\u5bf9\u76f8\u5173\u4fe1\u53f7\u5b50\u7a7a\u95f4\uff0c\u4f46\u5b58\u5728\u89c6\u56fe\u7279\u5b9a\u7684\u6b63\u4ea4\u6a21\u7cca\u6027\uff1b2) N\u22653\u65f6\u80fd\u591f\u5206\u79bb\u51fa\u6240\u6709\u89c6\u56fe\u5171\u4eab\u7684\u8054\u5408\u76f8\u5173\u5b50\u7a7a\u95f4\uff1b3) \u5efa\u7acb\u4e86\u6709\u9650\u6837\u672c\u4e00\u81f4\u6027\u4fdd\u8bc1\uff0c\u901a\u8fc7\u8c31\u6270\u52a8\u7406\u8bba\u5c06\u7ecf\u9a8c\u4ea4\u53c9\u534f\u65b9\u5dee\u7684\u96c6\u4e2d\u6027\u8f6c\u5316\u4e3a\u660e\u786e\u7684\u5b50\u7a7a\u95f4\u8bef\u5dee\u754c\u3002", "conclusion": "\u591a\u89c6\u56feCCA\u80fd\u591f\u8bc6\u522b\u5171\u4eab\u6f5c\u5728\u5b50\u7a7a\u95f4\uff0c\u4f46\u5b58\u5728\u89c6\u56fe\u7279\u5b9a\u7684\u6b63\u4ea4\u6a21\u7cca\u6027\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7406\u8bba\u53d1\u73b0\u5e76\u786e\u8ba4\u4e86\u5047\u8bbe\u6761\u4ef6\u7684\u5fc5\u8981\u6027\u3002\u8be5\u65b9\u6cd5\u4e3a\u975e\u7ebf\u6027\u591a\u89c6\u56fe\u5206\u6790\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u7528\u4fdd\u8bc1\u3002"}}
{"id": "2602.24143", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.24143", "abs": "https://arxiv.org/abs/2602.24143", "authors": ["David Emukpere", "Romain Deffayet", "Jean-Michel Renders"], "title": "Robust Skills, Brittle Grounding: Diagnosing Restricted Generalization in Vision-Language Action Policies via Multi-Object Picking", "comment": null, "summary": "Vision-language action (VLA) policies often report strong manipulation benchmark performance with relatively few demonstrations, but it remains unclear whether this reflects robust language-to-object grounding or reliance on object--location correlations that do not transfer beyond the training distribution. We present a controlled multi-object picking study that progressively increases object placement variability up to full workspace randomization and evaluates held-out object--location pairings that break familiar associations without increasing spatial difficulty. Across these stress tests and data scaling, we find that for representative VLA policies, including SmolVLA and $\u03c0_{0.5}$, execution of the manipulation primitive remains substantially more reliable than instruction-conditioned task success in harder regimes, suggesting that manipulation skill acquisition is decoupled from instruction following. We recommend augmenting manipulation benchmarks with task ladders and decomposed metrics that separately measure primitive execution and instruction-conditioned success to better diagnose instruction-grounded generalization.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u63a7\u5236\u6027\u591a\u7269\u4f53\u62fe\u53d6\u5b9e\u9a8c\u53d1\u73b0\uff0c\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u7b56\u7565\u5728\u64cd\u4f5c\u6280\u80fd\u83b7\u53d6\u4e0e\u6307\u4ee4\u8ddf\u968f\u4e4b\u95f4\u5b58\u5728\u8131\u8282\uff0c\u5efa\u8bae\u4f7f\u7528\u4efb\u52a1\u9636\u68af\u548c\u5206\u89e3\u6307\u6807\u6765\u66f4\u597d\u8bc4\u4f30\u6307\u4ee4\u57fa\u7840\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7a76\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u7b56\u7565\u5728\u64cd\u4f5c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u5f3a\u6027\u80fd\u662f\u5426\u771f\u6b63\u53cd\u6620\u4e86\u7a33\u5065\u7684\u8bed\u8a00\u5230\u7269\u4f53\u57fa\u7840\u80fd\u529b\uff0c\u8fd8\u662f\u4ec5\u4ec5\u4f9d\u8d56\u4e8e\u8bad\u7ec3\u5206\u5e03\u4e2d\u7684\u7269\u4f53-\u4f4d\u7f6e\u76f8\u5173\u6027\uff0c\u8fd9\u79cd\u76f8\u5173\u6027\u5728\u8d85\u51fa\u8bad\u7ec3\u5206\u5e03\u65f6\u65e0\u6cd5\u8fc1\u79fb\u3002", "method": "\u91c7\u7528\u63a7\u5236\u6027\u591a\u7269\u4f53\u62fe\u53d6\u7814\u7a76\uff0c\u9010\u6b65\u589e\u52a0\u7269\u4f53\u653e\u7f6e\u53d8\u5f02\u6027\u76f4\u81f3\u5b8c\u5168\u5de5\u4f5c\u7a7a\u95f4\u968f\u673a\u5316\uff0c\u8bc4\u4f30\u6253\u7834\u719f\u6089\u5173\u8054\u7684\u4fdd\u7559\u7269\u4f53-\u4f4d\u7f6e\u914d\u5bf9\uff0c\u540c\u65f6\u4e0d\u589e\u52a0\u7a7a\u95f4\u96be\u5ea6\u3002\u5bf9\u4ee3\u8868\u6027VLA\u7b56\u7565\uff08\u5305\u62ecSmolVLA\u548c\u03c0\u2080.\u2085\uff09\u8fdb\u884c\u538b\u529b\u6d4b\u8bd5\u548c\u6570\u636e\u7f29\u653e\u5206\u6790\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u8f83\u96be\u7684\u4efb\u52a1\u72b6\u6001\u4e0b\uff0c\u4ee3\u8868\u6027VLA\u7b56\u7565\u7684\u64cd\u4f5c\u539f\u8bed\u6267\u884c\u53ef\u9760\u6027\u663e\u8457\u9ad8\u4e8e\u6307\u4ee4\u6761\u4ef6\u5316\u4efb\u52a1\u6210\u529f\u7387\uff0c\u8868\u660e\u64cd\u4f5c\u6280\u80fd\u83b7\u53d6\u4e0e\u6307\u4ee4\u8ddf\u968f\u4e4b\u95f4\u5b58\u5728\u8131\u8282\u3002\u64cd\u4f5c\u539f\u8bed\u6267\u884c\u6bd4\u6307\u4ee4\u6761\u4ef6\u5316\u4efb\u52a1\u6210\u529f\u66f4\u4e3a\u53ef\u9760\u3002", "conclusion": "\u5efa\u8bae\u589e\u5f3a\u64cd\u4f5c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u52a0\u5165\u4efb\u52a1\u9636\u68af\u548c\u5206\u89e3\u6307\u6807\uff0c\u5206\u522b\u6d4b\u91cf\u539f\u8bed\u6267\u884c\u548c\u6307\u4ee4\u6761\u4ef6\u5316\u6210\u529f\u7387\uff0c\u4ee5\u66f4\u597d\u5730\u8bca\u65ad\u6307\u4ee4\u57fa\u7840\u6cdb\u5316\u80fd\u529b\u3002\u8fd9\u6709\u52a9\u4e8e\u533a\u5206\u64cd\u4f5c\u6280\u80fd\u83b7\u53d6\u4e0e\u771f\u6b63\u7684\u8bed\u8a00\u6307\u4ee4\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2602.23814", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23814", "abs": "https://arxiv.org/abs/2602.23814", "authors": ["Chongyang Xu", "Haipeng Li", "Shen Cheng", "Jingyu Hu", "Haoqiang Fan", "Ziliang Feng", "Shuaicheng Liu"], "title": "Action-Geometry Prediction with 3D Geometric Prior for Bimanual Manipulation", "comment": "Accepted by CVPR 2026", "summary": "Bimanual manipulation requires policies that can reason about 3D geometry, anticipate how it evolves under action, and generate smooth, coordinated motions. However, existing methods typically rely on 2D features with limited spatial awareness, or require explicit point clouds that are difficult to obtain reliably in real-world settings. At the same time, recent 3D geometric foundation models show that accurate and diverse 3D structure can be reconstructed directly from RGB images in a fast and robust manner. We leverage this opportunity and propose a framework that builds bimanual manipulation directly on a pre-trained 3D geometric foundation model. Our policy fuses geometry-aware latents, 2D semantic features, and proprioception into a unified state representation, and uses diffusion model to jointly predict a future action chunk and a future 3D latent that decodes into a dense pointmap. By explicitly predicting how the 3D scene will evolve together with the action sequence, the policy gains strong spatial understanding and predictive capability using only RGB observations. We evaluate our method both in simulation on the RoboTwin benchmark and in real-world robot executions. Our approach consistently outperforms 2D-based and point-cloud-based baselines, achieving state-of-the-art performance in manipulation success, inter-arm coordination, and 3D spatial prediction accuracy. Code is available at https://github.com/Chongyang-99/GAP.git.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u9884\u8bad\u7ec33D\u51e0\u4f55\u57fa\u7840\u6a21\u578b\u7684\u53cc\u624b\u64cd\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u878d\u5408\u51e0\u4f55\u611f\u77e5\u6f5c\u5728\u7279\u5f81\u30012D\u8bed\u4e49\u7279\u5f81\u548c\u672c\u4f53\u611f\u77e5\uff0c\u4f7f\u7528\u6269\u6563\u6a21\u578b\u8054\u5408\u9884\u6d4b\u672a\u6765\u52a8\u4f5c\u5757\u548c3D\u6f5c\u5728\u8868\u793a\uff0c\u5b9e\u73b0\u4ec5\u7528RGB\u89c2\u6d4b\u7684\u5f3a\u7a7a\u95f4\u7406\u89e3\u548c\u9884\u6d4b\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u53cc\u624b\u64cd\u4f5c\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u7a7a\u95f4\u611f\u77e5\u6709\u9650\u76842D\u7279\u5f81\uff0c\u6216\u9700\u8981\u96be\u4ee5\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u53ef\u9760\u83b7\u53d6\u7684\u663e\u5f0f\u70b9\u4e91\u3002\u540c\u65f6\uff0c\u6700\u8fd1\u76843D\u51e0\u4f55\u57fa\u7840\u6a21\u578b\u8868\u660e\u53ef\u4ee5\u76f4\u63a5\u4eceRGB\u56fe\u50cf\u5feb\u901f\u7a33\u5065\u5730\u91cd\u5efa\u51c6\u786e\u591a\u6837\u76843D\u7ed3\u6784\u3002", "method": "\u6784\u5efa\u5728\u9884\u8bad\u7ec33D\u51e0\u4f55\u57fa\u7840\u6a21\u578b\u4e0a\u7684\u6846\u67b6\uff0c\u5c06\u51e0\u4f55\u611f\u77e5\u6f5c\u5728\u7279\u5f81\u30012D\u8bed\u4e49\u7279\u5f81\u548c\u672c\u4f53\u611f\u77e5\u878d\u5408\u4e3a\u7edf\u4e00\u72b6\u6001\u8868\u793a\uff0c\u4f7f\u7528\u6269\u6563\u6a21\u578b\u8054\u5408\u9884\u6d4b\u672a\u6765\u52a8\u4f5c\u5757\u548c\u53ef\u89e3\u7801\u4e3a\u5bc6\u96c6\u70b9\u56fe\u7684\u672a\u67653D\u6f5c\u5728\u8868\u793a\u3002", "result": "\u5728RoboTwin\u57fa\u51c6\u7684\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u6267\u884c\u4e2d\u8bc4\u4f30\uff0c\u65b9\u6cd5\u5728\u64cd\u4f5c\u6210\u529f\u7387\u3001\u53cc\u81c2\u534f\u8c03\u6027\u548c3D\u7a7a\u95f4\u9884\u6d4b\u51c6\u786e\u6027\u65b9\u9762\u4e00\u81f4\u4f18\u4e8e\u57fa\u4e8e2D\u548c\u70b9\u4e91\u7684\u57fa\u7ebf\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u5229\u75283D\u51e0\u4f55\u57fa\u7840\u6a21\u578b\u5e76\u8054\u5408\u9884\u6d4b\u52a8\u4f5c\u548c3D\u573a\u666f\u6f14\u5316\uff0c\u4ec5\u4f7f\u7528RGB\u89c2\u6d4b\u5373\u53ef\u83b7\u5f97\u5f3a\u5927\u7684\u7a7a\u95f4\u7406\u89e3\u548c\u9884\u6d4b\u80fd\u529b\uff0c\u4e3a\u53cc\u624b\u64cd\u4f5c\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.23789", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23789", "abs": "https://arxiv.org/abs/2602.23789", "authors": ["Aleksandr Ananikian", "Daniil Drozdov", "Konstantin Yakovlev"], "title": "UPath: Universal Planner Across Topological Heterogeneity For Grid-Based Pathfinding", "comment": null, "summary": "The performance of search algorithms for grid-based pathfinding, e.g. A*, critically depends on the heuristic function that is used to focus the search. Recent studies have shown that informed heuristics that take the positions/shapes of the obstacles into account can be approximated with the deep neural networks. Unfortunately, the existing learning-based approaches mostly rely on the assumption that training and test grid maps are drawn from the same distribution (e.g., city maps, indoor maps, etc.) and perform poorly on out-of-distribution tasks. This naturally limits their application in practice when often a universal solver is needed that is capable of efficiently handling any problem instance. In this work, we close this gap by designing an universal heuristic predictor: a model trained once, but capable of generalizing across a full spectrum of unseen tasks. Our extensive empirical evaluation shows that the suggested approach halves the computational effort of A* by up to a factor of 2.2, while still providing solutions within 3% of the optimal cost on average altogether on the tasks that are completely different from the ones used for training $\\unicode{x2013}$ a milestone reached for the first time by a learnable solver.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u542f\u53d1\u5f0f\u9884\u6d4b\u5668\uff0c\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u8bad\u7ec3\u4e00\u6b21\u5373\u53ef\u6cdb\u5316\u5230\u5404\u79cd\u672a\u89c1\u8fc7\u7684\u7f51\u683c\u8def\u5f84\u89c4\u5212\u4efb\u52a1\uff0c\u5c06A*\u7b97\u6cd5\u7684\u8ba1\u7b97\u91cf\u51cf\u5c11\u6700\u591a2.2\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u89e3\u51b3\u65b9\u6848\u5728\u6700\u4f18\u6210\u672c\u76843%\u4ee5\u5185\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u5b66\u4e60\u7684\u542f\u53d1\u5f0f\u51fd\u6570\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u8bad\u7ec3\u548c\u6d4b\u8bd5\u7f51\u683c\u5730\u56fe\u6765\u81ea\u76f8\u540c\u5206\u5e03\uff08\u5982\u57ce\u5e02\u5730\u56fe\u3001\u5ba4\u5185\u5730\u56fe\u7b49\uff09\u7684\u5047\u8bbe\uff0c\u5728\u5206\u5e03\u5916\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002\u8fd9\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u4e2d\u9700\u8981\u80fd\u591f\u9ad8\u6548\u5904\u7406\u4efb\u4f55\u95ee\u9898\u5b9e\u4f8b\u7684\u901a\u7528\u6c42\u89e3\u5668\u7684\u9700\u6c42\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u901a\u7528\u7684\u542f\u53d1\u5f0f\u9884\u6d4b\u5668\u6a21\u578b\uff0c\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u8bad\u7ec3\u4e00\u6b21\uff0c\u4f46\u80fd\u591f\u6cdb\u5316\u5230\u5b8c\u5168\u672a\u89c1\u8fc7\u7684\u4efb\u52a1\u8c31\u7cfb\u3002\u8be5\u65b9\u6cd5\u4e0d\u4f9d\u8d56\u4e8e\u7279\u5b9a\u5730\u56fe\u5206\u5e03\uff0c\u800c\u662f\u5b66\u4e60\u901a\u7528\u7684\u8def\u5f84\u89c4\u5212\u542f\u53d1\u5f0f\u3002", "result": "\u8be5\u65b9\u6cd5\u5c06A*\u7b97\u6cd5\u7684\u8ba1\u7b97\u5de5\u4f5c\u91cf\u51cf\u5c11\u4e86\u6700\u591a2.2\u500d\uff0c\u540c\u65f6\u5728\u5b8c\u5168\u4e0d\u540c\u4e8e\u8bad\u7ec3\u4efb\u52a1\u7684\u95ee\u9898\u4e0a\uff0c\u5e73\u5747\u89e3\u51b3\u65b9\u6848\u6210\u672c\u4ecd\u4fdd\u6301\u5728\u6700\u4f18\u6210\u672c\u76843%\u4ee5\u5185\u3002\u8fd9\u662f\u53ef\u5b66\u4e60\u6c42\u89e3\u5668\u9996\u6b21\u8fbe\u5230\u8fd9\u4e00\u91cc\u7a0b\u7891\u3002", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86\u73b0\u6709\u5b66\u4e60\u578b\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\u7684\u6cdb\u5316\u80fd\u529b\u7a7a\u767d\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86\u8bad\u7ec3\u4e00\u6b21\u5373\u53ef\u6cdb\u5316\u5230\u5404\u79cd\u672a\u89c1\u4efb\u52a1\u7684\u901a\u7528\u542f\u53d1\u5f0f\u9884\u6d4b\u5668\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u901a\u7528\u6c42\u89e3\u5668\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2602.24156", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.24156", "abs": "https://arxiv.org/abs/2602.24156", "authors": ["Sue Min Cho", "Jan Emily Mangulabnan", "Han Zhang", "Zhekai Mao", "Yufan He", "Pengfei Guo", "Daguang Xu", "Gregory Hager", "Masaru Ishii", "Mathias Unberath"], "title": "Humanoid Robots as First Assistants in Endoscopic Surgery", "comment": null, "summary": "Humanoid robots have become a focal point of technological ambition, with claims of surgical capability within years in mainstream discourse. These projections are aspirational yet lack empirical grounding. To date, no humanoid has assisted a surgeon through an actual procedure, let alone performed one. The work described here breaks this new ground. Here we report a proof of concept in which a teleoperated Unitree G1 provided endoscopic visualization while an attending otolaryngologist performed a cadaveric sphenoidectomy. The procedure was completed successfully, with stable visualization maintained throughout. Teleoperation allowed assessment of whether the humanoid form factor could meet the physical demands of surgical assistance in terms of sustenance and precision; the cognitive demands were satisfied -- for now -- by the operator. Post-procedure analysis identified engineering targets for clinical translation, alongside near-term opportunities such as autonomous diagnostic scoping. This work establishes form-factor feasibility for humanoid surgical assistance while identifying challenges for continued development.", "AI": {"tldr": "\u9996\u4e2a\u6982\u5ff5\u9a8c\u8bc1\uff1a\u4eba\u5f62\u673a\u5668\u4ebaUnitree G1\u5728\u5c38\u4f53\u8776\u7aa6\u5207\u9664\u672f\u4e2d\u6210\u529f\u63d0\u4f9b\u5185\u7aa5\u955c\u53ef\u89c6\u5316\u8f85\u52a9\uff0c\u8bc1\u660e\u4e86\u4eba\u5f62\u5f62\u6001\u5728\u624b\u672f\u8f85\u52a9\u4e2d\u7684\u7269\u7406\u53ef\u884c\u6027", "motivation": "\u5f53\u524d\u5173\u4e8e\u4eba\u5f62\u673a\u5668\u4eba\u624b\u672f\u80fd\u529b\u7684\u9884\u6d4b\u7f3a\u4e4f\u5b9e\u8bc1\u57fa\u7840\uff0c\u5c1a\u65e0\u4efb\u4f55\u4eba\u5f62\u673a\u5668\u4eba\u5b9e\u9645\u8f85\u52a9\u5916\u79d1\u624b\u672f\u3002\u672c\u7814\u7a76\u65e8\u5728\u6253\u7834\u8fd9\u4e00\u7a7a\u767d\uff0c\u9a8c\u8bc1\u4eba\u5f62\u5f62\u6001\u80fd\u5426\u6ee1\u8db3\u624b\u672f\u8f85\u52a9\u7684\u7269\u7406\u9700\u6c42\u3002", "method": "\u91c7\u7528\u8fdc\u7a0b\u64cd\u4f5c\u7684Unitree G1\u4eba\u5f62\u673a\u5668\u4eba\uff0c\u5728\u8033\u9f3b\u5589\u79d1\u533b\u751f\u8fdb\u884c\u5c38\u4f53\u8776\u7aa6\u5207\u9664\u672f\u65f6\u63d0\u4f9b\u5185\u7aa5\u955c\u53ef\u89c6\u5316\u652f\u6301\u3002\u901a\u8fc7\u8fdc\u7a0b\u64cd\u4f5c\u8bc4\u4f30\u4eba\u5f62\u5f62\u6001\u5728\u6301\u4e45\u6027\u548c\u7cbe\u786e\u6027\u65b9\u9762\u80fd\u5426\u6ee1\u8db3\u624b\u672f\u8f85\u52a9\u7684\u7269\u7406\u8981\u6c42\u3002", "result": "\u624b\u672f\u6210\u529f\u5b8c\u6210\uff0c\u6574\u4e2a\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u4e86\u7a33\u5b9a\u7684\u53ef\u89c6\u5316\u3002\u8fdc\u7a0b\u64cd\u4f5c\u8bc1\u5b9e\u4eba\u5f62\u5f62\u6001\u80fd\u591f\u6ee1\u8db3\u624b\u672f\u8f85\u52a9\u7684\u7269\u7406\u9700\u6c42\uff0c\u800c\u8ba4\u77e5\u9700\u6c42\u76ee\u524d\u7531\u64cd\u4f5c\u5458\u6ee1\u8db3\u3002\u672f\u540e\u5206\u6790\u786e\u5b9a\u4e86\u4e34\u5e8a\u8f6c\u5316\u7684\u5de5\u7a0b\u76ee\u6807\u3002", "conclusion": "\u672c\u7814\u7a76\u786e\u7acb\u4e86\u4eba\u5f62\u624b\u672f\u8f85\u52a9\u7684\u5f62\u6001\u53ef\u884c\u6027\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u6301\u7eed\u5f00\u53d1\u9762\u4e34\u7684\u6311\u6218\uff0c\u4e3a\u81ea\u4e3b\u8bca\u65ad\u5185\u7aa5\u955c\u7b49\u8fd1\u671f\u5e94\u7528\u63d0\u4f9b\u4e86\u673a\u4f1a\u3002"}}
{"id": "2602.23817", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23817", "abs": "https://arxiv.org/abs/2602.23817", "authors": ["Pratibha Kumari", "Daniel Reisenb\u00fcchler", "Afshin Bozorgpour", "yousef Sadegheih", "Priyankar Choudhary", "Dorit Merhof"], "title": "Footprint-Guided Exemplar-Free Continual Histopathology Report Generation", "comment": null, "summary": "Rapid progress in vision-language modeling has enabled pathology report generation from gigapixel whole-slide images, but most approaches assume static training with simultaneous access to all data. In clinical deployment, however, new organs, institutions, and reporting conventions emerge over time, and sequential fine-tuning can cause catastrophic forgetting. We introduce an exemplar-free continual learning framework for WSI-to-report generation that avoids storing raw slides or patch exemplars. The core idea is a compact domain footprint built in a frozen patch-embedding space: a small codebook of representative morphology tokens together with slide-level co-occurrence summaries and lightweight patch-count priors. These footprints support generative replay by synthesizing pseudo-WSI representations that reflect domain-specific morphological mixtures, while a teacher snapshot provides pseudo-reports to supervise the updated model without retaining past data. To address shifting reporting conventions, we distill domain-specific linguistic characteristics into a compact style descriptor and use it to steer generation. At inference, the model identifies the most compatible descriptor directly from the slide signal, enabling domain-agnostic setup without requiring explicit domain identifiers. Evaluated across multiple public continual learning benchmarks, our approach outperforms exemplar-free and limited-buffer rehearsal baselines, highlighting footprint-based generative replay as a practical solution for deployment in evolving clinical settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u793a\u4f8b\u7684\u6301\u7eed\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u5168\u5207\u7247\u56fe\u50cf\u751f\u6210\u75c5\u7406\u62a5\u544a\uff0c\u901a\u8fc7\u6784\u5efa\u7d27\u51d1\u7684\u9886\u57df\u8db3\u8ff9\u6765\u907f\u514d\u707e\u96be\u6027\u9057\u5fd8\uff0c\u65e0\u9700\u5b58\u50a8\u539f\u59cb\u6570\u636e\u3002", "motivation": "\u4e34\u5e8a\u90e8\u7f72\u4e2d\uff0c\u65b0\u7684\u5668\u5b98\u3001\u673a\u6784\u548c\u62a5\u544a\u89c4\u8303\u4f1a\u968f\u65f6\u95f4\u51fa\u73b0\uff0c\u4f20\u7edf\u7684\u987a\u5e8f\u5fae\u8c03\u4f1a\u5bfc\u81f4\u707e\u96be\u6027\u9057\u5fd8\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u5047\u8bbe\u9759\u6001\u8bad\u7ec3\u5e76\u540c\u65f6\u8bbf\u95ee\u6240\u6709\u6570\u636e\uff0c\u4e0d\u9002\u7528\u4e8e\u5b9e\u9645\u4e34\u5e8a\u73af\u5883\u3002", "method": "\u6838\u5fc3\u662f\u6784\u5efa\u7d27\u51d1\u7684\u9886\u57df\u8db3\u8ff9\uff1a\u5728\u51bb\u7ed3\u7684\u8865\u4e01\u5d4c\u5165\u7a7a\u95f4\u4e2d\uff0c\u4f7f\u7528\u5c0f\u578b\u4ee3\u8868\u6027\u5f62\u6001\u6807\u8bb0\u7801\u672c\u3001\u5207\u7247\u7ea7\u5171\u73b0\u6458\u8981\u548c\u8f7b\u91cf\u7ea7\u8865\u4e01\u8ba1\u6570\u5148\u9a8c\u3002\u901a\u8fc7\u751f\u6210\u5f0f\u91cd\u653e\u5408\u6210\u4f2aWSI\u8868\u793a\uff0c\u540c\u65f6\u4f7f\u7528\u6559\u5e08\u5feb\u7167\u751f\u6210\u4f2a\u62a5\u544a\u76d1\u7763\u66f4\u65b0\u6a21\u578b\u3002\u4e3a\u5904\u7406\u62a5\u544a\u89c4\u8303\u53d8\u5316\uff0c\u5c06\u9886\u57df\u7279\u5b9a\u8bed\u8a00\u7279\u5f81\u84b8\u998f\u5230\u7d27\u51d1\u7684\u98ce\u683c\u63cf\u8ff0\u7b26\u4e2d\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5f00\u6301\u7eed\u5b66\u4e60\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u65e0\u793a\u4f8b\u548c\u6709\u9650\u7f13\u51b2\u533a\u91cd\u653e\u57fa\u7ebf\uff0c\u5c55\u793a\u4e86\u8db3\u8ff9\u57fa\u751f\u6210\u5f0f\u91cd\u653e\u5728\u4e0d\u65ad\u6f14\u5316\u7684\u4e34\u5e8a\u73af\u5883\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u65e0\u793a\u4f8b\u6301\u7eed\u5b66\u4e60\u6846\u67b6\u901a\u8fc7\u7d27\u51d1\u9886\u57df\u8db3\u8ff9\u548c\u751f\u6210\u5f0f\u91cd\u653e\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u75c5\u7406\u62a5\u544a\u751f\u6210\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u4e3a\u4e34\u5e8a\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.23795", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23795", "abs": "https://arxiv.org/abs/2602.23795", "authors": ["Wenwu Tang", "Dong Wang", "Lothar Thiele", "Olga Saukh"], "title": "GRAIL: Post-hoc Compensation by Linear Reconstruction for Compressed Networks", "comment": "Conference on Parsimony and Learning (CPAL)", "summary": "Structured deep model compression methods are hardware-friendly and substantially reduce memory and inference costs. However, under aggressive compression, the resulting accuracy degradation often necessitates post-compression finetuning, which can be impractical due to missing labeled data or high training cost. We propose post-hoc blockwise compensation, called GRAIL, a simple zero-finetuning step applied after model compression that restores each block's input-output behavior using a small calibration set. The method summarizes hidden activations via a Gram matrix and applies ridge regression to linearly reconstruct the original hidden representation from the reduced one. The resulting reconstruction map is absorbed into the downstream projection weights, while the upstream layer is compressed. The approach is selector-agnostic (Magnitude, Wanda, Gram-based selection, or folding), data-aware (requiring only a few forward passes without gradients or labels), and recovers classic pruning or folding when the Gram matrix is near identity, indicating weak inter-channel correlations. Across ResNets, ViTs, and decoder-only LLMs, GRAIL consistently improves accuracy or perplexity over data-free and data-aware pruning or folding baselines in practical compression regimes, with manageable overhead and no backpropagation. The code is available at https://github.com/TWWinde/GRAIL.", "AI": {"tldr": "GRAIL\u662f\u4e00\u79cd\u65e0\u9700\u5fae\u8c03\u7684\u540e\u5904\u7406\u5757\u8865\u507f\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c0f\u578b\u6821\u51c6\u96c6\u6062\u590d\u538b\u7f29\u540e\u6a21\u578b\u7684\u5757\u7ea7\u8f93\u5165\u8f93\u51fa\u884c\u4e3a\uff0c\u4f7f\u7528Gram\u77e9\u9635\u548c\u5cad\u56de\u5f52\u7ebf\u6027\u91cd\u5efa\u539f\u59cb\u9690\u85cf\u8868\u793a\u3002", "motivation": "\u7ed3\u6784\u5316\u6df1\u5ea6\u6a21\u578b\u538b\u7f29\u65b9\u6cd5\u867d\u7136\u786c\u4ef6\u53cb\u597d\u4e14\u80fd\u663e\u8457\u964d\u4f4e\u5185\u5b58\u548c\u63a8\u7406\u6210\u672c\uff0c\u4f46\u5728\u6fc0\u8fdb\u538b\u7f29\u4e0b\u4f1a\u5bfc\u81f4\u7cbe\u5ea6\u4e0b\u964d\uff0c\u800c\u540e\u7eed\u5fae\u8c03\u53ef\u80fd\u56e0\u7f3a\u5c11\u6807\u8bb0\u6570\u636e\u6216\u8bad\u7ec3\u6210\u672c\u9ad8\u800c\u4e0d\u5207\u5b9e\u9645\u3002", "method": "\u63d0\u51faGRAIL\u65b9\u6cd5\uff1a1) \u4f7f\u7528Gram\u77e9\u9635\u603b\u7ed3\u9690\u85cf\u6fc0\u6d3b\uff1b2) \u5e94\u7528\u5cad\u56de\u5f52\u4ece\u538b\u7f29\u540e\u7684\u8868\u793a\u7ebf\u6027\u91cd\u5efa\u539f\u59cb\u9690\u85cf\u8868\u793a\uff1b3) \u5c06\u91cd\u5efa\u6620\u5c04\u5438\u6536\u5230\u4e0b\u6e38\u6295\u5f71\u6743\u91cd\u4e2d\uff0c\u540c\u65f6\u538b\u7f29\u4e0a\u6e38\u5c42\u3002\u8be5\u65b9\u6cd5\u9009\u62e9\u5668\u65e0\u5173\u3001\u6570\u636e\u611f\u77e5\u4e14\u65e0\u9700\u53cd\u5411\u4f20\u64ad\u3002", "result": "\u5728ResNets\u3001ViTs\u548c\u4ec5\u89e3\u7801\u5668LLMs\u4e0a\uff0cGRAIL\u5728\u5b9e\u7528\u538b\u7f29\u8303\u56f4\u5185\u6301\u7eed\u6539\u8fdb\u6570\u636e\u65e0\u5173\u548c\u6570\u636e\u611f\u77e5\u526a\u679d\u6216\u6298\u53e0\u57fa\u7ebf\u7684\u51c6\u786e\u6027\u6216\u56f0\u60d1\u5ea6\uff0c\u5177\u6709\u53ef\u63a7\u5f00\u9500\u4e14\u65e0\u9700\u53cd\u5411\u4f20\u64ad\u3002", "conclusion": "GRAIL\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u96f6\u5fae\u8c03\u540e\u5904\u7406\u8865\u507f\u65b9\u6848\uff0c\u80fd\u591f\u663e\u8457\u6062\u590d\u538b\u7f29\u6a21\u578b\u7684\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u6a21\u578b\u67b6\u6784\u548c\u538b\u7f29\u65b9\u6cd5\u3002"}}
{"id": "2602.24192", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.24192", "abs": "https://arxiv.org/abs/2602.24192", "authors": ["Moumita Mukherjee", "Magnus Nor\u00e9n", "Anton Koval", "Avijit Banerjee", "George Nikolakopoulos"], "title": "How IMU Drift Influences Multi-Radar Inertial Odometry for Ground Robots in Subterranean Terrains", "comment": "Accepted in IEEE International Conference on Robotics and Automation (ICRA), 2026", "summary": "Reliable radar inertial odometry (RIO) requires mitigating IMU bias drift, a challenge that intensifies in subterranean environments due to extreme temperatures and gravity-induced accelerations. Cost-effective IMUs such as the Pixhawk, when paired with FMCW TI IWR6843AOP EVM radars, suffer from drift-induced degradation compounded by sparse, noisy, and flickering radar returns, making fusion less stable than LiDAR-based odometry. Yet, LiDAR fails under smoke, dust, and aerosols, whereas FMCW radars remain compact, lightweight, cost-effective, and robust in these situations. To address these challenges, we propose a two-stage MRIO framework that combines an IMU bias estimator for resilient localization and mapping in GPS-denied subterranean environments affected by smoke. Radar-based ego-velocity estimation is formulated through a least-squares approach and incorporated into an EKF for online IMU bias correction; the corrected IMU accelerations are fused with heterogeneous measurements from multiple radars and an IMU to refine odometry. The proposed framework further supports radar-only mapping by exploiting the robot's estimated translational and rotational displacements. In subterranean field trials, MRIO delivers robust localization and mapping, outperforming EKF-RIO. It maintains accuracy across cost-efficient FMCW radar setups and different IMUs, showing resilience with Pixhawk and higher-grade units such as VectorNav. The implementation will be provided as an open-source resource to the community (code available at https://github.com/LTU-RAI/MRIO", "AI": {"tldr": "\u63d0\u51faMRIO\u6846\u67b6\uff0c\u7ed3\u5408IMU\u504f\u7f6e\u4f30\u8ba1\u5668\u548c\u96f7\u8fbe\u81ea\u901f\u5ea6\u4f30\u8ba1\uff0c\u7528\u4e8eGPS\u62d2\u6b62\u7684\u5730\u4e0b\u73af\u5883\u4e2d\u5b9e\u73b0\u9c81\u68d2\u7684\u5b9a\u4f4d\u4e0e\u5efa\u56fe\uff0c\u7279\u522b\u662f\u5728\u70df\u96fe\u6761\u4ef6\u4e0b\u4f18\u4e8e\u4f20\u7edfEKF-RIO\u65b9\u6cd5\u3002", "motivation": "\u5728\u5730\u4e0b\u73af\u5883\u4e2d\uff0c\u4f4e\u6210\u672cIMU\uff08\u5982Pixhawk\uff09\u5b58\u5728\u504f\u7f6e\u6f02\u79fb\u95ee\u9898\uff0c\u800c\u96f7\u8fbe\u6570\u636e\u7a00\u758f\u3001\u566a\u58f0\u5927\u4e14\u95ea\u70c1\uff0c\u4f7f\u5f97\u96f7\u8fbe\u60ef\u6027\u91cc\u7a0b\u8ba1\u878d\u5408\u4e0d\u7a33\u5b9a\u3002\u867d\u7136\u6fc0\u5149\u96f7\u8fbe\u5728\u70df\u96fe\u3001\u7070\u5c18\u548c\u6c14\u6eb6\u80f6\u6761\u4ef6\u4e0b\u5931\u6548\uff0c\u4f46FMCW\u96f7\u8fbe\u5728\u8fd9\u4e9b\u60c5\u51b5\u4e0b\u4ecd\u4fdd\u6301\u7d27\u51d1\u3001\u8f7b\u91cf\u3001\u7ecf\u6d4e\u4e14\u9c81\u68d2\u7684\u7279\u6027\uff0c\u56e0\u6b64\u9700\u8981\u89e3\u51b3IMU\u504f\u7f6e\u6f02\u79fb\u548c\u96f7\u8fbe\u6570\u636e\u8d28\u91cf\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5MRIO\u6846\u67b6\uff1a1\uff09\u4f7f\u7528\u6700\u5c0f\u4e8c\u4e58\u6cd5\u8fdb\u884c\u96f7\u8fbe\u81ea\u901f\u5ea6\u4f30\u8ba1\uff0c\u5e76\u96c6\u6210\u5230EKF\u4e2d\u8fdb\u884c\u5728\u7ebfIMU\u504f\u7f6e\u6821\u6b63\uff1b2\uff09\u5c06\u6821\u6b63\u540e\u7684IMU\u52a0\u901f\u5ea6\u4e0e\u591a\u4e2a\u96f7\u8fbe\u548cIMU\u7684\u5f02\u6784\u6d4b\u91cf\u503c\u878d\u5408\u4ee5\u4f18\u5316\u91cc\u7a0b\u8ba1\u3002\u8be5\u6846\u67b6\u8fd8\u652f\u6301\u4ec5\u4f7f\u7528\u96f7\u8fbe\u8fdb\u884c\u5efa\u56fe\uff0c\u5229\u7528\u4f30\u8ba1\u7684\u5e73\u79fb\u548c\u65cb\u8f6c\u4f4d\u79fb\u3002", "result": "\u5728\u5730\u4e0b\u5b9e\u5730\u6d4b\u8bd5\u4e2d\uff0cMRIO\u5b9e\u73b0\u4e86\u9c81\u68d2\u7684\u5b9a\u4f4d\u548c\u5efa\u56fe\uff0c\u6027\u80fd\u4f18\u4e8eEKF-RIO\u3002\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u6210\u672c\u6548\u76ca\u7684FMCW\u96f7\u8fbe\u8bbe\u7f6e\u548c\u4e0d\u540cIMU\uff08\u5305\u62ecPixhawk\u548c\u66f4\u9ad8\u7ea7\u7684VectorNav\uff09\u4e0a\u5747\u4fdd\u6301\u51c6\u786e\u6027\uff0c\u5c55\u73b0\u4e86\u826f\u597d\u7684\u9002\u5e94\u6027\u3002", "conclusion": "MRIO\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5730\u4e0b\u73af\u5883\u4e2dIMU\u504f\u7f6e\u6f02\u79fb\u548c\u96f7\u8fbe\u6570\u636e\u8d28\u91cf\u95ee\u9898\uff0c\u5728GPS\u62d2\u6b62\u4e14\u5b58\u5728\u70df\u96fe\u7684\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e86\u9c81\u68d2\u7684\u5b9a\u4f4d\u4e0e\u5efa\u56fe\u3002\u8be5\u65b9\u6cd5\u5bf9\u4f4e\u6210\u672c\u786c\u4ef6\u5177\u6709\u9002\u5e94\u6027\uff0c\u5e76\u5c06\u4f5c\u4e3a\u5f00\u6e90\u8d44\u6e90\u63d0\u4f9b\u7ed9\u793e\u533a\u3002"}}
{"id": "2602.23820", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23820", "abs": "https://arxiv.org/abs/2602.23820", "authors": ["Xiaojing Zhao", "Shiyang Li", "Zena Chu", "Ying Zhang", "Peinan Hao", "Tianzi Yan", "Jiajia Chen", "Huicong Ning"], "title": "Denoising-Enhanced YOLO for Robust SAR Ship Detection", "comment": null, "summary": "With the rapid advancement of deep learning, synthetic aperture radar (SAR) imagery has become a key modality for ship detection. However, robust performance remains challenging in complex scenes, where clutter and speckle noise can induce false alarms and small targets are easily missed. To address these issues, we propose CPN-YOLO, a high-precision ship detection framework built upon YOLOv8 with three targeted improvements. First, we introduce a learnable large-kernel denoising module for input pre-processing, producing cleaner representations and more discriminative features across diverse ship types. Second, we design a feature extraction enhancement strategy based on the PPA attention mechanism to strengthen multi-scale modeling and improve sensitivity to small ships. Third, we incorporate a Gaussian similarity loss derived from the normalized Wasserstein distance (NWD) to better measure similarity under complex bounding-box distributions and improve generalization. Extensive experiments on HRSID and SSDD demonstrate the effectiveness of our method. On SSDD, CPN-YOLO surpasses the YOLOv8 baseline, achieving 97.0% precision, 95.1% recall, and 98.9% mAP, and consistently outperforms other representative deep-learning detectors in overall performance.", "AI": {"tldr": "CPN-YOLO\uff1a\u57fa\u4e8eYOLOv8\u6539\u8fdb\u7684\u9ad8\u7cbe\u5ea6SAR\u56fe\u50cf\u8239\u8236\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u5927\u6838\u53bb\u566a\u6a21\u5757\u3001PPA\u6ce8\u610f\u529b\u673a\u5236\u548cNWD\u9ad8\u65af\u76f8\u4f3c\u5ea6\u635f\u5931\uff0c\u5728\u590d\u6742\u573a\u666f\u4e0b\u63d0\u5347\u68c0\u6d4b\u6027\u80fd", "motivation": "SAR\u56fe\u50cf\u8239\u8236\u68c0\u6d4b\u5728\u590d\u6742\u573a\u666f\u4e0b\u9762\u4e34\u6311\u6218\uff1a\u6742\u6ce2\u548c\u6591\u70b9\u566a\u58f0\u5bfc\u81f4\u8bef\u62a5\uff0c\u5c0f\u76ee\u6807\u5bb9\u6613\u6f0f\u68c0\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u573a\u666f\u4e0b\u7684\u9c81\u68d2\u6027\u4e0d\u8db3\uff0c\u9700\u8981\u6539\u8fdb\u68c0\u6d4b\u7cbe\u5ea6", "method": "1. \u5f15\u5165\u53ef\u5b66\u4e60\u5927\u6838\u53bb\u566a\u6a21\u5757\u8fdb\u884c\u8f93\u5165\u9884\u5904\u7406\uff0c\u751f\u6210\u66f4\u5e72\u51c0\u7684\u8868\u793a\u548c\u66f4\u5177\u533a\u5206\u6027\u7684\u7279\u5f81\uff1b2. \u57fa\u4e8ePPA\u6ce8\u610f\u529b\u673a\u5236\u8bbe\u8ba1\u7279\u5f81\u63d0\u53d6\u589e\u5f3a\u7b56\u7565\uff0c\u52a0\u5f3a\u591a\u5c3a\u5ea6\u5efa\u6a21\u548c\u5c0f\u8239\u654f\u611f\u6027\uff1b3. \u91c7\u7528\u57fa\u4e8e\u5f52\u4e00\u5316Wasserstein\u8ddd\u79bb\u7684\u9ad8\u65af\u76f8\u4f3c\u5ea6\u635f\u5931\uff0c\u66f4\u597d\u5730\u5904\u7406\u590d\u6742\u8fb9\u754c\u6846\u5206\u5e03", "result": "\u5728HRSID\u548cSSDD\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCPN-YOLO\u663e\u8457\u4f18\u4e8e\u57fa\u7ebfYOLOv8\u3002\u5728SSDD\u6570\u636e\u96c6\u4e0a\u8fbe\u523097.0%\u7cbe\u786e\u7387\u300195.1%\u53ec\u56de\u7387\u548c98.9% mAP\uff0c\u6574\u4f53\u6027\u80fd\u4f18\u4e8e\u5176\u4ed6\u4ee3\u8868\u6027\u6df1\u5ea6\u5b66\u4e60\u68c0\u6d4b\u5668", "conclusion": "CPN-YOLO\u901a\u8fc7\u9488\u5bf9\u6027\u7684\u6539\u8fdb\u6709\u6548\u89e3\u51b3\u4e86SAR\u56fe\u50cf\u8239\u8236\u68c0\u6d4b\u4e2d\u7684\u6311\u6218\uff0c\u5728\u590d\u6742\u573a\u666f\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u68c0\u6d4b\uff0c\u4e3aSAR\u56fe\u50cf\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.23798", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2602.23798", "abs": "https://arxiv.org/abs/2602.23798", "authors": ["Tiantong Wang", "Xinyu Yan", "Tiantong Wu", "Yurong Hao", "Yong Jiang", "Fei Huang", "Wei Yang Bryan Lim"], "title": "MPU: Towards Secure and Privacy-Preserving Knowledge Unlearning for Large Language Models", "comment": null, "summary": "Machine unlearning for large language models often faces a privacy dilemma in which strict constraints prohibit sharing either the server's parameters or the client's forget set. To address this dual non-disclosure constraint, we propose MPU, an algorithm-agnostic privacy-preserving Multiple Perturbed Copies Unlearning framework that primarily introduces two server-side modules: Pre-Process for randomized copy generation and Post-Process for update aggregation. In Pre-Process, the server distributes multiple perturbed and reparameterized model instances, allowing the client to execute unlearning locally on its private forget set without accessing the server's exact original parameters. After local unlearning, the server performs Post-Process by inverting the reparameterization and aggregating updates with a harmonic denoising procedure to alleviate the impact of perturbation. Experiments with seven unlearning algorithms show that MPU achieves comparable unlearning performance to noise-free baselines, with most algorithms' average degradation well below 1% under 10% noise, and can even outperform the noise-free baseline for some algorithms under 1% noise. Code is available at https://github.com/Tristan-SHU/MPU.", "AI": {"tldr": "MPU\u662f\u4e00\u4e2a\u7b97\u6cd5\u65e0\u5173\u7684\u9690\u79c1\u4fdd\u62a4\u591a\u6270\u52a8\u526f\u672c\u9057\u5fd8\u6846\u67b6\uff0c\u901a\u8fc7\u670d\u52a1\u5668\u7aef\u9884\u5904\u7406\u7684\u968f\u673a\u526f\u672c\u751f\u6210\u548c\u540e\u5904\u7406\u7684\u66f4\u65b0\u805a\u5408\uff0c\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u9057\u5fd8\u4e2d\u7684\u53cc\u91cd\u975e\u62ab\u9732\u7ea6\u675f\u95ee\u9898\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u9057\u5fd8\u9762\u4e34\u9690\u79c1\u56f0\u5883\uff1a\u4e25\u683c\u7ea6\u675f\u7981\u6b62\u5171\u4eab\u670d\u52a1\u5668\u53c2\u6570\u6216\u5ba2\u6237\u7aef\u9057\u5fd8\u96c6\u3002\u9700\u8981\u89e3\u51b3\u8fd9\u79cd\u53cc\u91cd\u975e\u62ab\u9732\u7ea6\u675f\uff0c\u540c\u65f6\u4fdd\u6301\u6709\u6548\u7684\u9057\u5fd8\u6027\u80fd\u3002", "method": "\u63d0\u51faMPU\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u670d\u52a1\u5668\u7aef\u6a21\u5757\uff1a\u9884\u5904\u7406\u6a21\u5757\u751f\u6210\u968f\u673a\u6270\u52a8\u548c\u91cd\u53c2\u6570\u5316\u7684\u6a21\u578b\u526f\u672c\uff0c\u5ba2\u6237\u7aef\u5728\u672c\u5730\u79c1\u6709\u9057\u5fd8\u96c6\u4e0a\u6267\u884c\u9057\u5fd8\uff1b\u540e\u5904\u7406\u6a21\u5757\u901a\u8fc7\u53cd\u91cd\u53c2\u6570\u5316\u548c\u8c10\u6ce2\u53bb\u566a\u805a\u5408\u66f4\u65b0\uff0c\u51cf\u8f7b\u6270\u52a8\u5f71\u54cd\u3002", "result": "\u57287\u79cd\u9057\u5fd8\u7b97\u6cd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMPU\u572810%\u566a\u58f0\u4e0b\u6027\u80fd\u9000\u5316\u5927\u591a\u4f4e\u4e8e1%\uff0c\u4e0e\u65e0\u566a\u58f0\u57fa\u7ebf\u76f8\u5f53\uff1b\u57281%\u566a\u58f0\u4e0b\u67d0\u4e9b\u7b97\u6cd5\u751a\u81f3\u4f18\u4e8e\u65e0\u566a\u58f0\u57fa\u7ebf\u3002", "conclusion": "MPU\u6709\u6548\u89e3\u51b3\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u9057\u5fd8\u4e2d\u7684\u53cc\u91cd\u9690\u79c1\u7ea6\u675f\u95ee\u9898\uff0c\u901a\u8fc7\u6270\u52a8\u526f\u672c\u548c\u805a\u5408\u673a\u5236\u5b9e\u73b0\u4e86\u9690\u79c1\u4fdd\u62a4\u4e0e\u9057\u5fd8\u6027\u80fd\u7684\u826f\u597d\u5e73\u8861\u3002"}}
{"id": "2602.24202", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2602.24202", "abs": "https://arxiv.org/abs/2602.24202", "authors": ["Alexis E. Laudenslager", "Antonio Alvarez Valdivia", "Nathaniel Hanson", "Margaret McGuinness"], "title": "Evaluating Accuracy of Vine Robot Shape Sensing with Distributed Inertial Measurement Units", "comment": null, "summary": "Soft, tip-extending vine robots are well suited for navigating tight, debris-filled environments, making them ideal for urban search and rescue. Sensing the full shape of a vine robot's body is helpful both for localizing information from other sensors placed along the robot body and for determining the robot's configuration within the space being explored. Prior approaches have localized vine robot tips using a single inertial measurement unit (IMU) combined with force sensing or length estimation, while one method demonstrated full-body shape sensing using distributed IMUs on a passively steered robot in controlled maze environments. However, the accuracy of distributed IMU-based shape sensing under active steering, varying robot lengths, and different sensor spacings has not been systematically quantified. In this work, we experimentally evaluate the accuracy of vine robot shape sensing using distributed IMUs along the robot body. We quantify IMU drift, measuring an average orientation drift rate of 1.33 degrees/min across 15 sensors. For passive steering, mean tip position error was 11% of robot length. For active steering, mean tip position error increased to 16%. During growth experiments across lengths from 30-175 cm, mean tip error was 8%, with a positive trend with increasing length. We also analyze the influence of sensor spacing and observe that intermediate spacings can minimize error for single-curvature shapes. These results demonstrate the feasibility of distributed IMU-based shape sensing for vine robots while highlighting key limitations and opportunities for improved modeling and algorithmic integration for field deployment.", "AI": {"tldr": "\u672c\u7814\u7a76\u7cfb\u7edf\u8bc4\u4f30\u4e86\u57fa\u4e8e\u5206\u5e03\u5f0fIMU\u7684\u85e4\u8513\u673a\u5668\u4eba\u5f62\u72b6\u611f\u77e5\u7cbe\u5ea6\uff0c\u91cf\u5316\u4e86\u5728\u4e0d\u540c\u64cd\u4f5c\u6761\u4ef6\u4e0b\u7684\u8bef\u5dee\u8868\u73b0", "motivation": "\u85e4\u8513\u673a\u5668\u4eba\u5728\u57ce\u5e02\u641c\u6551\u7b49\u72ed\u7a84\u73af\u5883\u4e2d\u5177\u6709\u4f18\u52bf\uff0c\u4f46\u73b0\u6709\u5f62\u72b6\u611f\u77e5\u65b9\u6cd5\u5728\u4e3b\u52a8\u8f6c\u5411\u3001\u53d8\u957f\u5ea6\u548c\u4e0d\u540c\u4f20\u611f\u5668\u95f4\u8ddd\u4e0b\u7684\u7cbe\u5ea6\u5c1a\u672a\u7cfb\u7edf\u91cf\u5316", "method": "\u4f7f\u7528\u6cbf\u673a\u5668\u4eba\u8eab\u4f53\u5206\u5e03\u7684IMU\u8fdb\u884c\u5f62\u72b6\u611f\u77e5\u5b9e\u9a8c\uff0c\u91cf\u5316IMU\u6f02\u79fb\u7387\uff0c\u8bc4\u4f30\u88ab\u52a8\u8f6c\u5411\u3001\u4e3b\u52a8\u8f6c\u5411\u3001\u4e0d\u540c\u957f\u5ea6\u6761\u4ef6\u4e0b\u7684\u5c16\u7aef\u4f4d\u7f6e\u8bef\u5dee\uff0c\u5e76\u5206\u6790\u4f20\u611f\u5668\u95f4\u8ddd\u7684\u5f71\u54cd", "result": "IMU\u5e73\u5747\u65b9\u5411\u6f02\u79fb\u7387\u4e3a1.33\u5ea6/\u5206\u949f\uff1b\u88ab\u52a8\u8f6c\u5411\u65f6\u5c16\u7aef\u4f4d\u7f6e\u8bef\u5dee\u4e3a\u673a\u5668\u4eba\u957f\u5ea6\u768411%\uff1b\u4e3b\u52a8\u8f6c\u5411\u65f6\u589e\u52a0\u523016%\uff1b30-175cm\u957f\u5ea6\u8303\u56f4\u5185\u5e73\u5747\u8bef\u5dee\u4e3a8%\uff1b\u4e2d\u7b49\u4f20\u611f\u5668\u95f4\u8ddd\u5bf9\u5355\u66f2\u7387\u5f62\u72b6\u8bef\u5dee\u6700\u5c0f", "conclusion": "\u5206\u5e03\u5f0fIMU\u5f62\u72b6\u611f\u77e5\u5bf9\u85e4\u8513\u673a\u5668\u4eba\u5177\u6709\u53ef\u884c\u6027\uff0c\u4f46\u9700\u6539\u8fdb\u5efa\u6a21\u548c\u7b97\u6cd5\u96c6\u6210\u4ee5\u5e94\u5bf9\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u5173\u952e\u9650\u5236"}}
{"id": "2602.23823", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23823", "abs": "https://arxiv.org/abs/2602.23823", "authors": ["Henghui Du", "Chang Zhou", "Xi Chen", "Di Hu"], "title": "APPO: Attention-guided Perception Policy Optimization for Video Reasoning", "comment": null, "summary": "Complex video reasoning, actually, relies excessively on fine-grained perception rather than on expert (e.g., Ph.D, Science)-level reasoning. Through extensive empirical observation, we have recognized the critical impact of perception. In particular, when perception ability is almost fixed, enhancing reasoning from Qwen3-8B to OpenAI-o3 yields only 0.7% performance improvement. Conversely, even minimal change in perception model scale (from 7B to 32B) boosts performance by 1.4%, indicating enhancing perception, rather than reasoning, is more critical to improve performance. Therefore, exploring how to enhance perception ability through reasoning without the need for expensive fine-grained annotation information is worthwhile. To achieve this goal, we specially propose APPO, the Attention-guided Perception Policy Optimization algorithm that leverages token-level dense rewards to improve model's fine-grained perception. The core idea behind APPO is to optimize those tokens from different responses that primarily focus on the same crucial video frame (called intra-group perception tokens). Experimental results on diverse video benchmarks and models with different scales (3/7B) demonstrate APPO consistently outperforms GRPO and DAPO (0.5%~4%). We hope our work provides a promising approach to effectively enhance model's perception abilities through reasoning in a low-cost manner, serving diverse scenarios and demands.", "AI": {"tldr": "\u8be5\u8bba\u6587\u53d1\u73b0\u590d\u6742\u89c6\u9891\u63a8\u7406\u4e2d\u611f\u77e5\u80fd\u529b\u6bd4\u63a8\u7406\u80fd\u529b\u66f4\u91cd\u8981\uff0c\u63d0\u51fa\u4e86APPO\u7b97\u6cd5\u901a\u8fc7\u6ce8\u610f\u529b\u5f15\u5bfc\u4f18\u5316\u611f\u77e5\u4ee4\u724c\u6765\u63d0\u5347\u6a21\u578b\u7ec6\u7c92\u5ea6\u611f\u77e5\u80fd\u529b\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u53d1\u73b0\u590d\u6742\u89c6\u9891\u63a8\u7406\u8fc7\u5ea6\u4f9d\u8d56\u7ec6\u7c92\u5ea6\u611f\u77e5\u800c\u975e\u4e13\u5bb6\u7ea7\u63a8\u7406\u80fd\u529b\u3002\u5f53\u611f\u77e5\u80fd\u529b\u57fa\u672c\u56fa\u5b9a\u65f6\uff0c\u63d0\u5347\u63a8\u7406\u80fd\u529b\uff08\u4eceQwen3-8B\u5230OpenAI-o3\uff09\u4ec5\u5e26\u67650.7%\u6027\u80fd\u63d0\u5347\uff1b\u800c\u611f\u77e5\u6a21\u578b\u89c4\u6a21\u5fae\u5c0f\u53d8\u5316\uff08\u4ece7B\u523032B\uff09\u5374\u80fd\u5e26\u67651.4%\u6027\u80fd\u63d0\u5347\uff0c\u8868\u660e\u589e\u5f3a\u611f\u77e5\u6bd4\u589e\u5f3a\u63a8\u7406\u5bf9\u6027\u80fd\u63d0\u5347\u66f4\u5173\u952e\u3002\u56e0\u6b64\u9700\u8981\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u63a8\u7406\u589e\u5f3a\u611f\u77e5\u80fd\u529b\uff0c\u540c\u65f6\u907f\u514d\u6602\u8d35\u7684\u7ec6\u7c92\u5ea6\u6807\u6ce8\u6210\u672c\u3002", "method": "\u63d0\u51fa\u4e86APPO\uff08Attention-guided Perception Policy Optimization\uff09\u7b97\u6cd5\uff0c\u5229\u7528\u4ee4\u724c\u7ea7\u5bc6\u96c6\u5956\u52b1\u6765\u63d0\u5347\u6a21\u578b\u7684\u7ec6\u7c92\u5ea6\u611f\u77e5\u80fd\u529b\u3002\u6838\u5fc3\u601d\u60f3\u662f\u4f18\u5316\u90a3\u4e9b\u6765\u81ea\u4e0d\u540c\u54cd\u5e94\u4f46\u4e3b\u8981\u5173\u6ce8\u76f8\u540c\u5173\u952e\u89c6\u9891\u5e27\u7684\u4ee4\u724c\uff08\u79f0\u4e3a\u7ec4\u5185\u611f\u77e5\u4ee4\u724c\uff09\u3002\u901a\u8fc7\u6ce8\u610f\u529b\u5f15\u5bfc\u7684\u7b56\u7565\u4f18\u5316\u6765\u589e\u5f3a\u6a21\u578b\u5bf9\u89c6\u9891\u5173\u952e\u5e27\u7684\u611f\u77e5\u80fd\u529b\u3002", "result": "\u5728\u591a\u6837\u5316\u7684\u89c6\u9891\u57fa\u51c6\u6d4b\u8bd5\u548c\u4e0d\u540c\u89c4\u6a21\u6a21\u578b\uff083B/7B\uff09\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cAPPO\u4e00\u81f4\u6027\u5730\u4f18\u4e8eGRPO\u548cDAPO\u65b9\u6cd5\uff0c\u6027\u80fd\u63d0\u5347\u5e45\u5ea6\u57280.5%\u52304%\u4e4b\u95f4\u3002\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u901a\u8fc7\u63a8\u7406\u589e\u5f3a\u611f\u77e5\u80fd\u529b\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u7684\u4f4e\u6210\u672c\u65b9\u6cd5\uff0c\u901a\u8fc7\u63a8\u7406\u6709\u6548\u589e\u5f3a\u6a21\u578b\u7684\u611f\u77e5\u80fd\u529b\uff0c\u80fd\u591f\u670d\u52a1\u4e8e\u591a\u6837\u5316\u7684\u573a\u666f\u548c\u9700\u6c42\u3002\u5f3a\u8c03\u4e86\u5728\u590d\u6742\u89c6\u9891\u63a8\u7406\u4efb\u52a1\u4e2d\uff0c\u63d0\u5347\u611f\u77e5\u80fd\u529b\u6bd4\u5355\u7eaf\u589e\u5f3a\u63a8\u7406\u80fd\u529b\u66f4\u4e3a\u91cd\u8981\u3002"}}
{"id": "2602.23863", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.23863", "abs": "https://arxiv.org/abs/2602.23863", "authors": ["Xiaoyu Guo", "Arkaitz Zubiaga"], "title": "NAU-QMUL: Utilizing BERT and CLIP for Multi-modal AI-Generated Image Detection", "comment": null, "summary": "With the aim of detecting AI-generated images and identifying the specific models responsible for their generation, we propose a multi-modal multi-task model. The model leverages pre-trained BERT and CLIP Vision encoders for text and image feature extraction, respectively, and employs cross-modal feature fusion with a tailored multi-task loss function. Additionally, a pseudo-labeling-based data augmentation strategy was utilized to expand the training dataset with high-confidence samples. The model achieved fifth place in both Tasks A and B of the `CT2: AI-Generated Image Detection' competition, with F1 scores of 83.16\\% and 48.88\\%, respectively. These findings highlight the effectiveness of the proposed architecture and its potential for advancing AI-generated content detection in real-world scenarios. The source code for our method is published on https://github.com/xxxxxxxxy/AIGeneratedImageDetection.", "AI": {"tldr": "\u63d0\u51fa\u591a\u6a21\u6001\u591a\u4efb\u52a1\u6a21\u578b\u68c0\u6d4bAI\u751f\u6210\u56fe\u50cf\u5e76\u8bc6\u522b\u751f\u6210\u6a21\u578b\uff0c\u4f7f\u7528BERT\u548cCLIP\u7f16\u7801\u5668\u63d0\u53d6\u7279\u5f81\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u878d\u5408\u548c\u591a\u4efb\u52a1\u635f\u5931\u51fd\u6570\uff0c\u5728\u7ade\u8d5b\u4e2d\u83b7\u5f97\u7b2c\u4e94\u540d\u3002", "motivation": "\u4e3a\u4e86\u68c0\u6d4bAI\u751f\u6210\u7684\u56fe\u50cf\u5e76\u8bc6\u522b\u751f\u6210\u8fd9\u4e9b\u56fe\u50cf\u7684\u5177\u4f53\u6a21\u578b\uff0c\u9700\u8981\u5f00\u53d1\u6709\u6548\u7684\u591a\u6a21\u6001\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u9884\u8bad\u7ec3\u7684BERT\u548cCLIP Vision\u7f16\u7801\u5668\u5206\u522b\u63d0\u53d6\u6587\u672c\u548c\u56fe\u50cf\u7279\u5f81\uff0c\u91c7\u7528\u8de8\u6a21\u6001\u7279\u5f81\u878d\u5408\u548c\u5b9a\u5236\u7684\u591a\u4efb\u52a1\u635f\u5931\u51fd\u6570\uff0c\u5e76\u5229\u7528\u4f2a\u6807\u7b7e\u6570\u636e\u589e\u5f3a\u7b56\u7565\u6269\u5c55\u8bad\u7ec3\u6570\u636e\u96c6\u3002", "result": "\u5728CT2\u7ade\u8d5b\u7684Task A\u548cTask B\u4e2d\u5747\u83b7\u5f97\u7b2c\u4e94\u540d\uff0cF1\u5206\u6570\u5206\u522b\u4e3a83.16%\u548c48.88%\u3002", "conclusion": "\u63d0\u51fa\u7684\u67b6\u6784\u5728AI\u751f\u6210\u5185\u5bb9\u68c0\u6d4b\u65b9\u9762\u5177\u6709\u6709\u6548\u6027\uff0c\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u6709\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2602.24235", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.24235", "abs": "https://arxiv.org/abs/2602.24235", "authors": ["Jialiang Fan", "Weizhe Xu", "Mengyu Liu", "Oleg Sokolsky", "Insup Lee", "Fangxin Kong"], "title": "SafeGen-LLM: Enhancing Safety Generalization in Task Planning for Robotic Systems", "comment": "12 pages, 6 figures", "summary": "Safety-critical task planning in robotic systems remains challenging: classical planners suffer from poor scalability, Reinforcement Learning (RL)-based methods generalize poorly, and base Large Language Models (LLMs) cannot guarantee safety. To address this gap, we propose safety-generalizable large language models, named SafeGen-LLM. SafeGen-LLM can not only enhance the safety satisfaction of task plans but also generalize well to novel safety properties in various domains. We first construct a multi-domain Planning Domain Definition Language 3 (PDDL3) benchmark with explicit safety constraints. Then, we introduce a two-stage post-training framework: Supervised Fine-Tuning (SFT) on a constraint-compliant planning dataset to learn planning syntax and semantics, and Group Relative Policy Optimization (GRPO) guided by fine-grained reward machines derived from formal verification to enforce safety alignment and by curriculum learning to better handle complex tasks. Extensive experiments show that SafeGen-LLM achieves strong safety generalization and outperforms frontier proprietary baselines across multi-domain planning tasks and multiple input formats (e.g., PDDLs and natural language).", "AI": {"tldr": "SafeGen-LLM\uff1a\u4e00\u79cd\u5b89\u5168\u53ef\u6cdb\u5316\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u540e\u8bad\u7ec3\u6846\u67b6\u89e3\u51b3\u673a\u5668\u4eba\u4efb\u52a1\u89c4\u5212\u4e2d\u7684\u5b89\u5168\u6311\u6218\uff0c\u5728\u591a\u4e2a\u9886\u57df\u5b9e\u73b0\u5b89\u5168\u7ea6\u675f\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u673a\u5668\u4eba\u4efb\u52a1\u89c4\u5212\u9762\u4e34\u6311\u6218\uff1a\u7ecf\u5178\u89c4\u5212\u5668\u53ef\u6269\u5c55\u6027\u5dee\uff0c\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u65b9\u6cd5\u6cdb\u5316\u80fd\u529b\u5f31\uff0c\u57fa\u7840\u5927\u8bed\u8a00\u6a21\u578b\u65e0\u6cd5\u4fdd\u8bc1\u5b89\u5168\u6027\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u8bc1\u5b89\u5168\u53c8\u80fd\u6cdb\u5316\u5230\u65b0\u5b89\u5168\u5c5e\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "1) \u6784\u5efa\u591a\u9886\u57dfPDDL3\u57fa\u51c6\u6d4b\u8bd5\u96c6\uff1b2) \u4e24\u9636\u6bb5\u540e\u8bad\u7ec3\u6846\u67b6\uff1a\u76d1\u7763\u5fae\u8c03\u5b66\u4e60\u89c4\u5212\u8bed\u6cd5\u8bed\u4e49\uff0c\u57fa\u4e8e\u7ec6\u7c92\u5ea6\u5956\u52b1\u673a\u7684\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\u5b9e\u73b0\u5b89\u5168\u5bf9\u9f50\uff0c\u7ed3\u5408\u8bfe\u7a0b\u5b66\u4e60\u5904\u7406\u590d\u6742\u4efb\u52a1\u3002", "result": "SafeGen-LLM\u5728\u591a\u9886\u57df\u89c4\u5212\u4efb\u52a1\u548c\u591a\u79cd\u8f93\u5165\u683c\u5f0f\uff08PDDL\u548c\u81ea\u7136\u8bed\u8a00\uff09\u4e0a\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u5b89\u5168\u6cdb\u5316\u80fd\u529b\uff0c\u8d85\u8d8a\u4e86\u524d\u6cbf\u7684\u4e13\u6709\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "SafeGen-LLM\u901a\u8fc7\u5f62\u5f0f\u5316\u9a8c\u8bc1\u6307\u5bfc\u7684\u5f3a\u5316\u5b66\u4e60\u548c\u8bfe\u7a0b\u5b66\u4e60\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u4efb\u52a1\u89c4\u5212\u4e2d\u7684\u5b89\u5168\u6cdb\u5316\u95ee\u9898\uff0c\u4e3a\u5b89\u5168\u5173\u952e\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.23811", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23811", "abs": "https://arxiv.org/abs/2602.23811", "authors": ["Xiang Li", "Nan Jiang", "Yuheng Zhang"], "title": "Beyond State-Wise Mirror Descent: Offline Policy Optimization with Parameteric Policies", "comment": null, "summary": "We investigate the theoretical aspects of offline reinforcement learning (RL) under general function approximation. While prior works (e.g., Xie et al., 2021) have established the theoretical foundations of learning a good policy from offline data via pessimism, existing algorithms that are computationally tractable (often in an oracle-efficient sense), such as PSPI, only apply to finite and small action spaces. Moreover, these algorithms rely on state-wise mirror descent and require actors to be implicitly induced from the critic functions, failing to accommodate standalone policy parameterization which is ubiquitous in practice. In this work, we address these limitations and extend the theoretical guarantees to parameterized policy classes over large or continuous action spaces. When extending mirror descent to parameterized policies, we identify contextual coupling as the core difficulty, and show how connecting mirror descent to natural policy gradient leads to novel analyses, guarantees, and algorithmic insights, including a surprising unification between offline RL and imitation learning.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u5728\u4e00\u822c\u51fd\u6570\u903c\u8fd1\u4e0b\u7684\u7406\u8bba\u95ee\u9898\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7b97\u6cd5\u53ea\u80fd\u5904\u7406\u6709\u9650\u5c0f\u52a8\u4f5c\u7a7a\u95f4\u3001\u65e0\u6cd5\u652f\u6301\u72ec\u7acb\u7b56\u7565\u53c2\u6570\u5316\u7684\u95ee\u9898\uff0c\u5c06\u7406\u8bba\u4fdd\u8bc1\u6269\u5c55\u5230\u5927\u89c4\u6a21\u6216\u8fde\u7eed\u52a8\u4f5c\u7a7a\u95f4\u7684\u53c2\u6570\u5316\u7b56\u7565\u7c7b\u3002", "motivation": "\u73b0\u6709\u79bb\u7ebfRL\u7b97\u6cd5\uff08\u5982PSPI\uff09\u867d\u7136\u8ba1\u7b97\u53ef\u884c\uff0c\u4f46\u4ec5\u9002\u7528\u4e8e\u6709\u9650\u5c0f\u52a8\u4f5c\u7a7a\u95f4\uff0c\u4e14\u4f9d\u8d56\u72b6\u6001\u7ea7\u955c\u50cf\u4e0b\u964d\uff0c\u8981\u6c42\u4ece\u8bc4\u4ef7\u51fd\u6570\u9690\u5f0f\u63a8\u5bfc\u884c\u52a8\u8005\uff0c\u65e0\u6cd5\u652f\u6301\u5b9e\u8df5\u4e2d\u666e\u904d\u4f7f\u7528\u7684\u72ec\u7acb\u7b56\u7565\u53c2\u6570\u5316\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\u3002", "method": "\u5c06\u955c\u50cf\u4e0b\u964d\u6269\u5c55\u5230\u53c2\u6570\u5316\u7b56\u7565\u65f6\uff0c\u8bc6\u522b\u51fa\u4e0a\u4e0b\u6587\u8026\u5408\u662f\u6838\u5fc3\u96be\u70b9\uff0c\u901a\u8fc7\u5c06\u955c\u50cf\u4e0b\u964d\u4e0e\u81ea\u7136\u7b56\u7565\u68af\u5ea6\u8fde\u63a5\u8d77\u6765\uff0c\u63d0\u51fa\u65b0\u7684\u5206\u6790\u6846\u67b6\u3001\u7406\u8bba\u4fdd\u8bc1\u548c\u7b97\u6cd5\u89c1\u89e3\uff0c\u5305\u62ec\u79bb\u7ebfRL\u4e0e\u6a21\u4eff\u5b66\u4e60\u7684\u7edf\u4e00\u3002", "result": "\u6210\u529f\u5c06\u79bb\u7ebfRL\u7684\u7406\u8bba\u4fdd\u8bc1\u6269\u5c55\u5230\u5927\u89c4\u6a21\u6216\u8fde\u7eed\u52a8\u4f5c\u7a7a\u95f4\u7684\u53c2\u6570\u5316\u7b56\u7565\u7c7b\uff0c\u63d0\u51fa\u4e86\u80fd\u591f\u5904\u7406\u72ec\u7acb\u7b56\u7565\u53c2\u6570\u5316\u7684\u65b0\u7b97\u6cd5\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u79bb\u7ebfRL\u4e0e\u6a21\u4eff\u5b66\u4e60\u4e4b\u95f4\u7684\u7edf\u4e00\u5173\u7cfb\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u8fde\u63a5\u955c\u50cf\u4e0b\u964d\u548c\u81ea\u7136\u7b56\u7565\u68af\u5ea6\uff0c\u89e3\u51b3\u4e86\u79bb\u7ebfRL\u5728\u53c2\u6570\u5316\u7b56\u7565\u4e0b\u7684\u7406\u8bba\u6311\u6218\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u5e7f\u6cdb\u4f7f\u7528\u7684\u72ec\u7acb\u7b56\u7565\u53c2\u6570\u5316\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\uff0c\u5e76\u63ed\u793a\u4e86\u79bb\u7ebfRL\u4e0e\u6a21\u4eff\u5b66\u4e60\u4e4b\u95f4\u7684\u6df1\u523b\u8054\u7cfb\u3002"}}
{"id": "2602.23872", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.23872", "abs": "https://arxiv.org/abs/2602.23872", "authors": ["Xingyu Shao", "Mengfan He", "Chunyu Li", "Liangzheng Sun", "Ziyang Meng"], "title": "Altitude-Aware Visual Place Recognition in Top-Down View", "comment": null, "summary": "To address the challenge of aerial visual place recognition (VPR) problem under significant altitude variations, this study proposes an altitude-adaptive VPR approach that integrates ground feature density analysis with image classification techniques. The proposed method estimates airborne platforms' relative altitude by analyzing the density of ground features in images, then applies relative altitude-based cropping to generate canonical query images, which are subsequently used in a classification-based VPR strategy for localization. Extensive experiments across diverse terrains and altitude conditions demonstrate that the proposed approach achieves high accuracy and robustness in both altitude estimation and VPR under significant altitude changes. Compared to conventional methods relying on barometric altimeters or Time-of-Flight (ToF) sensors, this solution requires no additional hardware and offers a plug-and-play solution for downstream applications, {making it suitable for small- and medium-sized airborne platforms operating in diverse environments, including rural and urban areas.} Under significant altitude variations, incorporating our relative altitude estimation module into the VPR retrieval pipeline boosts average R@1 and R@5 by 29.85\\% and 60.20\\%, respectively, compared with applying VPR retrieval alone. Furthermore, compared to traditional {Monocular Metric Depth Estimation (MMDE) methods}, the proposed method reduces the mean error by 202.1 m, yielding average additional improvements of 31.4\\% in R@1 and 44\\% in R@5. These results demonstrate that our method establishes a robust, vision-only framework for three-dimensional visual place recognition, offering a practical and scalable solution for accurate airborne platforms localization under large altitude variations and limited sensor availability.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5730\u9762\u7279\u5f81\u5bc6\u5ea6\u5206\u6790\u548c\u56fe\u50cf\u5206\u7c7b\u7684\u89c6\u89c9\u5730\u70b9\u8bc6\u522b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u56fe\u50cf\u4e2d\u5730\u9762\u7279\u5f81\u5bc6\u5ea6\u4f30\u8ba1\u76f8\u5bf9\u9ad8\u5ea6\uff0c\u751f\u6210\u89c4\u8303\u5316\u67e5\u8be2\u56fe\u50cf\uff0c\u5b9e\u73b0\u5927\u9ad8\u5ea6\u53d8\u5316\u4e0b\u7684\u7a7a\u4e2d\u89c6\u89c9\u5b9a\u4f4d\u3002", "motivation": "\u89e3\u51b3\u7a7a\u4e2d\u89c6\u89c9\u5730\u70b9\u8bc6\u522b\u5728\u663e\u8457\u9ad8\u5ea6\u53d8\u5316\u4e0b\u7684\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u6c14\u538b\u9ad8\u5ea6\u8ba1\u6216ToF\u4f20\u611f\u5668\uff0c\u9700\u8981\u989d\u5916\u786c\u4ef6\uff0c\u800c\u5355\u76ee\u5ea6\u91cf\u6df1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u7cbe\u5ea6\u6709\u9650\uff0c\u9700\u8981\u4e00\u79cd\u65e0\u9700\u989d\u5916\u786c\u4ef6\u7684\u89c6\u89c9\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u5206\u6790\u56fe\u50cf\u4e2d\u5730\u9762\u7279\u5f81\u5bc6\u5ea6\u4f30\u8ba1\u76f8\u5bf9\u9ad8\u5ea6\uff0c\u57fa\u4e8e\u76f8\u5bf9\u9ad8\u5ea6\u8fdb\u884c\u56fe\u50cf\u88c1\u526a\u751f\u6210\u89c4\u8303\u5316\u67e5\u8be2\u56fe\u50cf\uff0c\u7136\u540e\u91c7\u7528\u57fa\u4e8e\u5206\u7c7b\u7684\u89c6\u89c9\u5730\u70b9\u8bc6\u522b\u7b56\u7565\u8fdb\u884c\u5b9a\u4f4d\u3002", "result": "\u5728\u591a\u79cd\u5730\u5f62\u548c\u9ad8\u5ea6\u6761\u4ef6\u4e0b\uff0c\u8be5\u65b9\u6cd5\u5728\u9ad8\u5ea6\u4f30\u8ba1\u548c\u89c6\u89c9\u5730\u70b9\u8bc6\u522b\u65b9\u9762\u5747\u8868\u73b0\u51fa\u9ad8\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002\u76f8\u6bd4\u4f20\u7edfVPR\u68c0\u7d22\uff0cR@1\u548cR@5\u5206\u522b\u63d0\u534729.85%\u548c60.20%\uff1b\u76f8\u6bd4\u5355\u76ee\u5ea6\u91cf\u6df1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\uff0c\u5e73\u5747\u8bef\u5dee\u51cf\u5c11202.1\u7c73\uff0cR@1\u548cR@5\u5206\u522b\u989d\u5916\u63d0\u534731.4%\u548c44%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5efa\u7acb\u4e86\u4e00\u4e2a\u7a33\u5065\u7684\u7eaf\u89c6\u89c9\u4e09\u7ef4\u89c6\u89c9\u5730\u70b9\u8bc6\u522b\u6846\u67b6\uff0c\u4e3a\u5927\u9ad8\u5ea6\u53d8\u5316\u548c\u4f20\u611f\u5668\u53d7\u9650\u6761\u4ef6\u4e0b\u7684\u7a7a\u4e2d\u5e73\u53f0\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.23869", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23869", "abs": "https://arxiv.org/abs/2602.23869", "authors": ["Mohammadreza Heidarianbaei", "Mareike Dorozynski", "Hubert Kanyamahanga", "Max Mehltretter", "Franz Rottensteiner"], "title": "Open-Vocabulary Semantic Segmentation in Remote Sensing via Hierarchical Attention Masking and Model Composition", "comment": "Published in the proceedings of the British Machine Vision Conference Workshops 2025", "summary": "In this paper, we propose ReSeg-CLIP, a new training-free Open-Vocabulary Semantic Segmentation method for remote sensing data. To compensate for the problems of vision language models, such as CLIP in semantic segmentation caused by inappropriate interactions within the self-attention layers, we introduce a hierarchical scheme utilizing masks generated by SAM to constrain the interactions at multiple scales. We also present a model composition approach that averages the parameters of multiple RS-specific CLIP variants, taking advantage of a new weighting scheme that evaluates representational quality using varying text prompts. Our method achieves state-of-the-art results across three RS benchmarks without additional training.", "AI": {"tldr": "ReSeg-CLIP\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u5f00\u653e\u8bcd\u6c47\u9065\u611f\u8bed\u4e49\u5206\u5272\u65b9\u6cd5\uff0c\u901a\u8fc7SAM\u751f\u6210\u7684\u63a9\u7801\u7ea6\u675f\u81ea\u6ce8\u610f\u529b\u5c42\u4ea4\u4e92\uff0c\u5e76\u7ec4\u5408\u591a\u4e2a\u9065\u611f\u4e13\u7528CLIP\u53d8\u4f53\u53c2\u6570\uff0c\u5728\u4e09\u4e2a\u9065\u611f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u9488\u5bf9CLIP\u7b49\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u4e2d\u56e0\u81ea\u6ce8\u610f\u529b\u5c42\u4e0d\u9002\u5f53\u4ea4\u4e92\u5bfc\u81f4\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u9065\u611f\u6570\u636e\u9886\u57df\uff0c\u9700\u8981\u4e00\u79cd\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5c31\u80fd\u63d0\u5347\u5f00\u653e\u8bcd\u6c47\u8bed\u4e49\u5206\u5272\u6027\u80fd\u7684\u65b9\u6cd5\u3002", "method": "1. \u5f15\u5165\u5206\u5c42\u65b9\u6848\uff1a\u5229\u7528SAM\u751f\u6210\u7684\u63a9\u7801\u5728\u591a\u4e2a\u5c3a\u5ea6\u4e0a\u7ea6\u675f\u81ea\u6ce8\u610f\u529b\u5c42\u7684\u4ea4\u4e92\uff1b2. \u6a21\u578b\u7ec4\u5408\u65b9\u6cd5\uff1a\u5e73\u5747\u591a\u4e2a\u9065\u611f\u4e13\u7528CLIP\u53d8\u4f53\u7684\u53c2\u6570\uff0c\u91c7\u7528\u65b0\u7684\u52a0\u6743\u65b9\u6848\u8bc4\u4f30\u4e0d\u540c\u6587\u672c\u63d0\u793a\u4e0b\u7684\u8868\u793a\u8d28\u91cf\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4e09\u4e2a\u9065\u611f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u4e14\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "conclusion": "ReSeg-CLIP\u901a\u8fc7\u7ea6\u675f\u81ea\u6ce8\u610f\u529b\u4ea4\u4e92\u548c\u7ec4\u5408\u591a\u4e2aCLIP\u53d8\u4f53\uff0c\u6709\u6548\u63d0\u5347\u4e86\u9065\u611f\u5f00\u653e\u8bcd\u6c47\u8bed\u4e49\u5206\u5272\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u8bad\u7ec3\u65e0\u5173\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2602.23816", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23816", "abs": "https://arxiv.org/abs/2602.23816", "authors": ["George Papadopoulos", "George A. Vouros"], "title": "Learning to maintain safety through expert demonstrations in settings with unknown constraints: A Q-learning perspective", "comment": "Accepted for publication at AAMAS 2026", "summary": "Given a set of trajectories demonstrating the execution of a task safely in a constrained MDP with observable rewards but with unknown constraints and non-observable costs, we aim to find a policy that maximizes the likelihood of demonstrated trajectories trading the balance between being conservative and increasing significantly the likelihood of high-rewarding trajectories but with potentially unsafe steps. Having these objectives, we aim towards learning a policy that maximizes the probability of the most $promising$ trajectories with respect to the demonstrations. In so doing, we formulate the ``promise\" of individual state-action pairs in terms of $Q$ values, which depend on task-specific rewards as well as on the assessment of states' safety, mixing expectations in terms of rewards and safety. This entails a safe Q-learning perspective of the inverse learning problem under constraints: The devised Safe $Q$ Inverse Constrained Reinforcement Learning (SafeQIL) algorithm is compared to state-of-the art inverse constraint reinforcement learning algorithms to a set of challenging benchmark tasks, showing its merits.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSafeQIL\u7b97\u6cd5\uff0c\u5728\u7ea6\u675fMDP\u4e2d\u901a\u8fc7\u6f14\u793a\u8f68\u8ff9\u5b66\u4e60\u5b89\u5168\u7b56\u7565\uff0c\u5e73\u8861\u4fdd\u5b88\u6027\u4e0e\u9ad8\u5956\u52b1\u8f68\u8ff9\u7684\u53ef\u80fd\u6027\uff0c\u4f7f\u7528Q\u503c\u8bc4\u4f30\u72b6\u6001-\u52a8\u4f5c\u5bf9\u7684\u5b89\u5168\u6027\u548c\u5956\u52b1\u6f5c\u529b\u3002", "motivation": "\u5728\u7ea6\u675fMDP\u73af\u5883\u4e2d\uff0c\u7ed9\u5b9a\u6f14\u793a\u8f68\u8ff9\u4f46\u7ea6\u675f\u672a\u77e5\u4e14\u6210\u672c\u4e0d\u53ef\u89c2\u6d4b\u7684\u60c5\u51b5\u4e0b\uff0c\u9700\u8981\u5b66\u4e60\u4e00\u4e2a\u7b56\u7565\uff0c\u65e2\u80fd\u6700\u5927\u5316\u6f14\u793a\u8f68\u8ff9\u7684\u53ef\u80fd\u6027\uff0c\u53c8\u80fd\u5e73\u8861\u4fdd\u5b88\u6027\u4e0e\u6f5c\u5728\u4e0d\u5b89\u5168\u4f46\u9ad8\u5956\u52b1\u8f68\u8ff9\u7684\u6743\u8861\u3002", "method": "\u63d0\u51faSafeQIL\u7b97\u6cd5\uff0c\u5c06\u72b6\u6001-\u52a8\u4f5c\u5bf9\u7684\"\u627f\u8bfa\u5ea6\"\u7528Q\u503c\u8868\u793a\uff0c\u7efc\u5408\u8003\u8651\u4efb\u52a1\u7279\u5b9a\u5956\u52b1\u548c\u72b6\u6001\u5b89\u5168\u6027\u8bc4\u4f30\uff0c\u4ece\u5b89\u5168Q\u5b66\u4e60\u7684\u89d2\u5ea6\u89e3\u51b3\u7ea6\u675f\u4e0b\u7684\u9006\u5b66\u4e60\u95ee\u9898\u3002", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u4efb\u52a1\u4e0a\u4e0e\u6700\u5148\u8fdb\u7684\u9006\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u8fdb\u884c\u6bd4\u8f83\uff0c\u5c55\u793a\u4e86SafeQIL\u7b97\u6cd5\u7684\u4f18\u52bf\u3002", "conclusion": "SafeQIL\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u5b66\u4e60\u5b89\u5168\u7b56\u7565\uff0c\u5e73\u8861\u6f14\u793a\u8f68\u8ff9\u7684\u4f3c\u7136\u6027\u4e0e\u9ad8\u5956\u52b1\u8f68\u8ff9\u7684\u53ef\u80fd\u6027\uff0c\u5728\u7ea6\u675f\u672a\u77e5\u7684\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2602.23898", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.23898", "abs": "https://arxiv.org/abs/2602.23898", "authors": ["Qihua Dong", "Kuo Yang", "Lin Ju", "Handong Zhao", "Yitian Zhang", "Yizhou Wang", "Huimin Zeng", "Jianglin Lu", "Yun Fu"], "title": "Ref-Adv: Exploring MLLM Visual Reasoning in Referring Expression Tasks", "comment": "ICLR 2026", "summary": "Referring Expression Comprehension (REC) links language to region level visual perception. Standard benchmarks (RefCOCO, RefCOCO+, RefCOCOg) have progressed rapidly with multimodal LLMs but remain weak tests of visual reasoning and grounding: (i) many expressions are very short, leaving little reasoning demand; (ii) images often contain few distractors, making the target easy to find; and (iii) redundant descriptors enable shortcut solutions that bypass genuine text understanding and visual reasoning. We introduce Ref-Adv, a modern REC benchmark that suppresses shortcuts by pairing linguistically nontrivial expressions with only the information necessary to uniquely identify the target. The dataset contains referring expressions on real images, curated with hard distractors and annotated with reasoning facets including negation. We conduct comprehensive ablations (word order perturbations and descriptor deletion sufficiency) to show that solving Ref-Adv requires reasoning beyond simple cues, and we evaluate a broad suite of contemporary multimodal LLMs on Ref-Adv. Despite strong results on RefCOCO, RefCOCO+, and RefCOCOg, models drop markedly on Ref-Adv, revealing reliance on shortcuts and gaps in visual reasoning and grounding. We provide an in depth failure analysis and aim for Ref-Adv to guide future work on visual reasoning and grounding in MLLMs.", "AI": {"tldr": "\u7814\u7a76\u8005\u63d0\u51fa\u4e86Ref-Adv\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u8bbe\u8ba1\u5177\u6709\u6311\u6218\u6027\u7684\u8bed\u8a00\u8868\u8fbe\u548c\u89c6\u89c9\u5e72\u6270\u7269\u6765\u6291\u5236\u73b0\u6709REC\u57fa\u51c6\u4e2d\u7684\u6377\u5f84\u89e3\u51b3\u65b9\u6848\uff0c\u4ece\u800c\u66f4\u771f\u5b9e\u5730\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u6a21\u578b\u7684\u89c6\u89c9\u63a8\u7406\u548c\u63a5\u5730\u80fd\u529b\u3002", "motivation": "\u73b0\u6709REC\u57fa\u51c6\uff08RefCOCO\u3001RefCOCO+\u3001RefCOCOg\uff09\u867d\u7136\u63a8\u52a8\u4e86\u591a\u6a21\u6001LLMs\u7684\u53d1\u5c55\uff0c\u4f46\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u8bb8\u591a\u8868\u8fbe\u8fc7\u4e8e\u7b80\u77ed\uff0c\u63a8\u7406\u9700\u6c42\u4f4e\uff1b2\uff09\u56fe\u50cf\u4e2d\u5e72\u6270\u7269\u5c11\uff0c\u76ee\u6807\u5bb9\u6613\u8bc6\u522b\uff1b3\uff09\u5197\u4f59\u63cf\u8ff0\u7b26\u4f7f\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7\u6377\u5f84\u7ed5\u8fc7\u771f\u6b63\u7684\u6587\u672c\u7406\u89e3\u548c\u89c6\u89c9\u63a8\u7406\u3002\u8fd9\u4e9b\u57fa\u51c6\u4e0d\u80fd\u5145\u5206\u6d4b\u8bd5\u6a21\u578b\u7684\u89c6\u89c9\u63a8\u7406\u548c\u63a5\u5730\u80fd\u529b\u3002", "method": "\u7814\u7a76\u8005\u521b\u5efa\u4e86Ref-Adv\u57fa\u51c6\uff0c\u5305\u542b\u771f\u5b9e\u56fe\u50cf\u4e0a\u7684\u6307\u4ee3\u8868\u8fbe\uff0c\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u5177\u6709\u6311\u6218\u6027\u7684\u5e72\u6270\u7269\u548c\u5305\u542b\u5426\u5b9a\u7b49\u63a8\u7406\u8981\u7d20\u7684\u6807\u6ce8\u6765\u6291\u5236\u6377\u5f84\u3002\u8fdb\u884c\u4e86\u5168\u9762\u7684\u6d88\u878d\u5b9e\u9a8c\uff08\u8bcd\u5e8f\u6270\u52a8\u548c\u63cf\u8ff0\u7b26\u5220\u9664\u5145\u5206\u6027\u5206\u6790\uff09\u6765\u9a8c\u8bc1\u57fa\u51c6\u7684\u4e25\u8c28\u6027\uff0c\u5e76\u8bc4\u4f30\u4e86\u5f53\u4ee3\u591a\u6a21\u6001LLMs\u5728\u8be5\u57fa\u51c6\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u5c3d\u7ba1\u6a21\u578b\u5728RefCOCO\u3001RefCOCO+\u548cRefCOCOg\u4e0a\u8868\u73b0\u5f3a\u52b2\uff0c\u4f46\u5728Ref-Adv\u4e0a\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5bf9\u6377\u5f84\u7684\u4f9d\u8d56\u4ee5\u53ca\u5728\u89c6\u89c9\u63a8\u7406\u548c\u63a5\u5730\u80fd\u529b\u4e0a\u7684\u4e0d\u8db3\u3002\u7814\u7a76\u8005\u63d0\u4f9b\u4e86\u6df1\u5165\u7684\u5931\u8d25\u5206\u6790\u3002", "conclusion": "Ref-Adv\u57fa\u51c6\u80fd\u591f\u66f4\u771f\u5b9e\u5730\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u6a21\u578b\u7684\u89c6\u89c9\u63a8\u7406\u548c\u63a5\u5730\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u5728\u8fd9\u4e9b\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u65e8\u5728\u6307\u5bfc\u672a\u6765\u591a\u6a21\u6001LLMs\u5728\u89c6\u89c9\u63a8\u7406\u548c\u63a5\u5730\u65b9\u9762\u7684\u7814\u7a76\u5de5\u4f5c\u3002"}}
{"id": "2602.23824", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23824", "abs": "https://arxiv.org/abs/2602.23824", "authors": ["Pavlin G. Poli\u010dar", "Dalibor Stanimirovi\u0107", "Bla\u017e Zupan"], "title": "Inferring Chronic Treatment Onset from ePrescription Data: A Renewal Process Approach", "comment": null, "summary": "Longitudinal electronic health record (EHR) data are often left-censored, making diagnosis records incomplete and unreliable for determining disease onset. In contrast, outpatient prescriptions form renewal-based trajectories that provide a continuous signal of disease management. We propose a probabilistic framework to infer chronic treatment onset by modeling prescription dynamics as a renewal process and detecting transitions from sporadic to sustained therapy via change-point detection between a baseline Poisson (sporadic prescribing) regime and a regime-specific Weibull (sustained therapy) renewal model. Using a nationwide ePrescription dataset of 2.4 million individuals, we show that the approach yields more temporally plausible onset estimates than naive rule-based triggering, substantially reducing implausible early detections under strong left censoring. Detection performance varies across diseases and is strongly associated with prescription density, highlighting both the strengths and limits of treatment-based onset inference.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u6982\u7387\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u5904\u65b9\u52a8\u6001\u5efa\u6a21\u4e3a\u66f4\u65b0\u8fc7\u7a0b\uff0c\u68c0\u6d4b\u4ece\u5076\u53d1\u6027\u6cbb\u7597\u5230\u6301\u7eed\u6027\u6cbb\u7597\u7684\u8f6c\u53d8\uff0c\u6765\u63a8\u65ad\u6162\u6027\u6cbb\u7597\u7684\u5f00\u59cb\u65f6\u95f4", "motivation": "\u7eb5\u5411\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u6570\u636e\u901a\u5e38\u5b58\u5728\u5de6\u622a\u65ad\u95ee\u9898\uff0c\u5bfc\u81f4\u8bca\u65ad\u8bb0\u5f55\u4e0d\u5b8c\u6574\u4e14\u4e0d\u53ef\u9760\uff0c\u65e0\u6cd5\u51c6\u786e\u786e\u5b9a\u75be\u75c5\u53d1\u75c5\u65f6\u95f4\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u95e8\u8bca\u5904\u65b9\u5f62\u6210\u7684\u66f4\u65b0\u8f68\u8ff9\u63d0\u4f9b\u4e86\u75be\u75c5\u7ba1\u7406\u7684\u8fde\u7eed\u4fe1\u53f7\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u6982\u7387\u6846\u67b6\uff0c\u5c06\u5904\u65b9\u52a8\u6001\u5efa\u6a21\u4e3a\u66f4\u65b0\u8fc7\u7a0b\uff0c\u901a\u8fc7\u53d8\u5316\u70b9\u68c0\u6d4b\u6765\u8bc6\u522b\u4ece\u5076\u53d1\u6027\u6cbb\u7597\uff08\u57fa\u7ebf\u6cca\u677e\u6a21\u578b\uff09\u5230\u6301\u7eed\u6027\u6cbb\u7597\uff08\u7279\u5b9a\u673a\u5236\u7684\u5a01\u5e03\u5c14\u66f4\u65b0\u6a21\u578b\uff09\u7684\u8f6c\u53d8\u3002", "result": "\u4f7f\u7528\u5168\u56fd240\u4e07\u4eba\u7684\u7535\u5b50\u5904\u65b9\u6570\u636e\u96c6\uff0c\u8be5\u65b9\u6cd5\u6bd4\u57fa\u4e8e\u89c4\u5219\u7684\u89e6\u53d1\u65b9\u6cd5\u4ea7\u751f\u66f4\u5408\u7406\u7684\u65f6\u95f4\u53d1\u75c5\u4f30\u8ba1\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u5728\u5f3a\u5de6\u622a\u65ad\u60c5\u51b5\u4e0b\u7684\u4e0d\u5408\u7406\u65e9\u671f\u68c0\u6d4b\u3002\u68c0\u6d4b\u6027\u80fd\u56e0\u75be\u75c5\u800c\u5f02\uff0c\u4e0e\u5904\u65b9\u5bc6\u5ea6\u5bc6\u5207\u76f8\u5173\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u57fa\u4e8e\u6cbb\u7597\u7684\u53d1\u75c5\u65f6\u95f4\u63a8\u65ad\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u6846\u67b6\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u5176\u5728\u4e0d\u540c\u75be\u75c5\u548c\u5904\u65b9\u5bc6\u5ea6\u60c5\u51b5\u4e0b\u7684\u4f18\u52bf\u548c\u5c40\u9650\u6027\u3002"}}
{"id": "2602.24040", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.24040", "abs": "https://arxiv.org/abs/2602.24040", "authors": ["Daniel Yang", "Samuel Stante", "Florian Redhardt", "Lena Libon", "Parnian Kassraie", "Ido Hakimi", "Barna P\u00e1sztor", "Andreas Krause"], "title": "RewardUQ: A Unified Framework for Uncertainty-Aware Reward Models", "comment": null, "summary": "Reward models are central to aligning large language models (LLMs) with human preferences. Yet most approaches rely on pointwise reward estimates that overlook the epistemic uncertainty in reward models arising from limited human feedback. Recent work suggests that quantifying this uncertainty can reduce the costs of human annotation via uncertainty-guided active learning and mitigate reward overoptimization in LLM post-training. However, uncertainty-aware reward models have so far been adopted without thorough comparison, leaving them poorly understood. This work introduces a unified framework, RewardUQ, to systematically evaluate uncertainty quantification for reward models. We compare common methods along standard metrics measuring accuracy and calibration, and we propose a new ranking strategy incorporating both dimensions for a simplified comparison. Our experimental results suggest that model size and initialization have the most meaningful impact on performance, and most prior work could have benefited from alternative design choices. To foster the development and evaluation of new methods and aid the deployment in downstream applications, we release our open-source framework as a Python package. Our code is available at https://github.com/lasgroup/rewarduq.", "AI": {"tldr": "RewardUQ\uff1a\u4e00\u4e2a\u7cfb\u7edf\u8bc4\u4f30\u5956\u52b1\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u53d1\u73b0\u6a21\u578b\u89c4\u6a21\u548c\u521d\u59cb\u5316\u5bf9\u6027\u80fd\u5f71\u54cd\u6700\u5927\uff0c\u591a\u6570\u5148\u524d\u5de5\u4f5c\u53ef\u901a\u8fc7\u66ff\u4ee3\u8bbe\u8ba1\u9009\u62e9\u6539\u8fdb\u3002", "motivation": "\u5956\u52b1\u6a21\u578b\u5bf9\u4e8e\u5c06\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u4eba\u7c7b\u504f\u597d\u5bf9\u9f50\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u70b9\u4f30\u8ba1\uff0c\u5ffd\u7565\u4e86\u6709\u9650\u4eba\u7c7b\u53cd\u9988\u5e26\u6765\u7684\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u3002\u867d\u7136\u5df2\u6709\u7814\u7a76\u8868\u660e\u91cf\u5316\u8fd9\u79cd\u4e0d\u786e\u5b9a\u6027\u53ef\u4ee5\u964d\u4f4e\u4eba\u5de5\u6807\u6ce8\u6210\u672c\u5e76\u7f13\u89e3\u5956\u52b1\u8fc7\u4f18\u5316\u95ee\u9898\uff0c\u4f46\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u5956\u52b1\u6a21\u578b\u7f3a\u4e4f\u7cfb\u7edf\u6bd4\u8f83\u548c\u6df1\u5165\u7406\u89e3\u3002", "method": "\u63d0\u51fa\u4e86RewardUQ\u7edf\u4e00\u6846\u67b6\uff0c\u7cfb\u7edf\u8bc4\u4f30\u5956\u52b1\u6a21\u578b\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\u3002\u6bd4\u8f83\u4e86\u5e38\u89c1\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u6821\u51c6\u6027\u65b9\u9762\u7684\u6807\u51c6\u6307\u6807\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8fd9\u4e24\u4e2a\u7ef4\u5ea6\u7684\u65b0\u6392\u540d\u7b56\u7565\u4ee5\u7b80\u5316\u6bd4\u8f83\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6a21\u578b\u89c4\u6a21\u548c\u521d\u59cb\u5316\u5bf9\u6027\u80fd\u5f71\u54cd\u6700\u4e3a\u663e\u8457\uff0c\u5927\u591a\u6570\u5148\u524d\u5de5\u4f5c\u672c\u53ef\u4ee5\u901a\u8fc7\u66ff\u4ee3\u8bbe\u8ba1\u9009\u62e9\u83b7\u5f97\u6539\u8fdb\u3002\u4f5c\u8005\u53d1\u5e03\u4e86\u5f00\u6e90\u6846\u67b6\u4f5c\u4e3aPython\u5305\uff0c\u4ee5\u4fc3\u8fdb\u65b0\u65b9\u6cd5\u7684\u5f00\u53d1\u548c\u8bc4\u4f30\u3002", "conclusion": "RewardUQ\u4e3a\u7cfb\u7edf\u8bc4\u4f30\u5956\u52b1\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u63d0\u4f9b\u4e86\u7edf\u4e00\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u89c4\u6a21\u548c\u521d\u59cb\u5316\u7684\u5173\u952e\u4f5c\u7528\uff0c\u5e76\u901a\u8fc7\u5f00\u6e90\u5de5\u5177\u652f\u6301\u8be5\u9886\u57df\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002"}}
{"id": "2602.23827", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23827", "abs": "https://arxiv.org/abs/2602.23827", "authors": ["Junkang Liu", "Fanhua Shang", "Yuxuan Tian", "Hongying Liu", "Yuanyuan Liu"], "title": "FedNSAM:Consistency of Local and Global Flatness for Federated Learning", "comment": null, "summary": "In federated learning (FL), multi-step local updates and data heterogeneity usually lead to sharper global minima, which degrades the performance of the global model. Popular FL algorithms integrate sharpness-aware minimization (SAM) into local training to address this issue. However, in the high data heterogeneity setting, the flatness in local training does not imply the flatness of the global model. Therefore, minimizing the sharpness of the local loss surfaces on the client data does not enable the effectiveness of SAM in FL to improve the generalization ability of the global model. We define the \\textbf{flatness distance} to explain this phenomenon. By rethinking the SAM in FL and theoretically analyzing the \\textbf{flatness distance}, we propose a novel \\textbf{FedNSAM} algorithm that accelerates the SAM algorithm by introducing global Nesterov momentum into the local update to harmonize the consistency of global and local flatness. \\textbf{FedNSAM} uses the global Nesterov momentum as the direction of local estimation of client global perturbations and extrapolation. Theoretically, we prove a tighter convergence bound than FedSAM by Nesterov extrapolation. Empirically, we conduct comprehensive experiments on CNN and Transformer models to verify the superior performance and efficiency of \\textbf{FedNSAM}. The code is available at https://github.com/junkangLiu0/FedNSAM.", "AI": {"tldr": "FedNSAM\uff1a\u4e00\u79cd\u65b0\u7684\u8054\u90a6\u5b66\u4e60\u7b97\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u5168\u5c40Nesterov\u52a8\u91cf\u6765\u534f\u8c03\u5168\u5c40\u548c\u5c40\u90e8\u5e73\u5766\u5ea6\u7684\u4e00\u81f4\u6027\uff0c\u89e3\u51b3\u6570\u636e\u5f02\u6784\u73af\u5883\u4e0bSAM\u7b97\u6cd5\u5728FL\u4e2d\u7684\u5c40\u9650\u6027", "motivation": "\u8054\u90a6\u5b66\u4e60\u4e2d\u591a\u6b65\u672c\u5730\u66f4\u65b0\u548c\u6570\u636e\u5f02\u6784\u6027\u901a\u5e38\u5bfc\u81f4\u66f4\u5c16\u9510\u7684\u5168\u5c40\u6700\u5c0f\u503c\uff0c\u8fd9\u4f1a\u964d\u4f4e\u5168\u5c40\u6a21\u578b\u7684\u6027\u80fd\u3002\u73b0\u6709\u7684FL\u7b97\u6cd5\u5c06SAM\u96c6\u6210\u5230\u672c\u5730\u8bad\u7ec3\u4e2d\uff0c\u4f46\u5728\u9ad8\u6570\u636e\u5f02\u6784\u6027\u8bbe\u7f6e\u4e0b\uff0c\u672c\u5730\u8bad\u7ec3\u7684\u5e73\u5766\u5ea6\u5e76\u4e0d\u4ee3\u8868\u5168\u5c40\u6a21\u578b\u7684\u5e73\u5766\u5ea6\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6765\u534f\u8c03\u5168\u5c40\u548c\u5c40\u90e8\u5e73\u5766\u5ea6\u7684\u4e00\u81f4\u6027\u3002", "method": "\u63d0\u51faFedNSAM\u7b97\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u5168\u5c40Nesterov\u52a8\u91cf\u5230\u672c\u5730\u66f4\u65b0\u4e2d\uff0c\u5c06\u5168\u5c40Nesterov\u52a8\u91cf\u4f5c\u4e3a\u5ba2\u6237\u7aef\u5168\u5c40\u6270\u52a8\u7684\u672c\u5730\u4f30\u8ba1\u65b9\u5411\u548c\u5916\u63a8\u65b9\u5411\uff0c\u4ece\u800c\u52a0\u901fSAM\u7b97\u6cd5\u5e76\u534f\u8c03\u5168\u5c40\u548c\u5c40\u90e8\u5e73\u5766\u5ea6\u7684\u4e00\u81f4\u6027\u3002", "result": "\u7406\u8bba\u4e0a\u8bc1\u660e\u4e86FedNSAM\u6bd4FedSAM\u5177\u6709\u66f4\u7d27\u7684\u6536\u655b\u754c\uff1b\u5b9e\u8bc1\u4e0a\u5728CNN\u548cTransformer\u6a21\u578b\u4e0a\u8fdb\u884c\u4e86\u5168\u9762\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86FedNSAM\u7684\u4f18\u8d8a\u6027\u80fd\u548c\u6548\u7387\u3002", "conclusion": "FedNSAM\u901a\u8fc7\u5f15\u5165\u5168\u5c40Nesterov\u52a8\u91cf\u6709\u6548\u89e3\u51b3\u4e86\u6570\u636e\u5f02\u6784\u73af\u5883\u4e0bSAM\u5728FL\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u90fd\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\uff0c\u4e3a\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u63d0\u5347\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2602.23852", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2602.23852", "abs": "https://arxiv.org/abs/2602.23852", "authors": ["Zhaowen Wang", "Dongdong Zhou", "Qi Xu", "Fengyu Cong", "Mohammad Al-Sa'd", "Jenni Raitoharju"], "title": "ULW-SleepNet: An Ultra-Lightweight Network for Multimodal Sleep Stage Scoring", "comment": "Accepted to ICASSP 2026", "summary": "Automatic sleep stage scoring is crucial for the diagnosis and treatment of sleep disorders. Although deep learning models have advanced the field, many existing models are computationally demanding and designed for single-channel electroencephalography (EEG), limiting their practicality for multimodal polysomnography (PSG) data. To overcome this, we propose ULW-SleepNet, an ultra-lightweight multimodal sleep stage scoring framework that efficiently integrates information from multiple physiological signals. ULW-SleepNet incorporates a novel Dual-Stream Separable Convolution (DSSC) Block, depthwise separable convolutions, channel-wise parameter sharing, and global average pooling to reduce computational overhead while maintaining competitive accuracy. Evaluated on the Sleep-EDF-20 and Sleep-EDF-78 datasets, ULW-SleepNet achieves accuracies of 86.9% and 81.4%, respectively, with only 13.3K parameters and 7.89M FLOPs. Compared to state-of-the-art methods, our model reduces parameters by up to 98.6% with only marginal performance loss, demonstrating its strong potential for real-time sleep monitoring on wearable and IoT devices. The source code for this study is publicly available at https://github.com/wzw999/ULW-SLEEPNET.", "AI": {"tldr": "\u63d0\u51faULW-SleepNet\uff0c\u4e00\u79cd\u8d85\u8f7b\u91cf\u7ea7\u591a\u6a21\u6001\u7761\u7720\u5206\u671f\u6846\u67b6\uff0c\u901a\u8fc7\u65b0\u9896\u7684\u53cc\u6d41\u53ef\u5206\u79bb\u5377\u79ef\u5757\u7b49\u6280\u672f\uff0c\u5728\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u51c6\u786e\u7387\u4e0b\u5927\u5e45\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\uff0c\u9002\u7528\u4e8e\u53ef\u7a7f\u6234\u548c\u7269\u8054\u7f51\u8bbe\u5907\u7684\u5b9e\u65f6\u7761\u7720\u76d1\u6d4b\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8ba1\u7b97\u9700\u6c42\u5927\u4e14\u591a\u4e3a\u5355\u901a\u9053EEG\u8bbe\u8ba1\uff0c\u9650\u5236\u4e86\u5728\u591a\u6a21\u6001PSG\u6570\u636e\u4e2d\u7684\u5b9e\u7528\u6027\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u8f7b\u91cf\u3001\u9ad8\u6548\u7684\u591a\u6a21\u6001\u7761\u7720\u5206\u671f\u65b9\u6cd5\u3002", "method": "\u63d0\u51faULW-SleepNet\u6846\u67b6\uff0c\u5305\u542b\u65b0\u9896\u7684\u53cc\u6d41\u53ef\u5206\u79bb\u5377\u79ef\u5757\u3001\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\u3001\u901a\u9053\u53c2\u6570\u5171\u4eab\u548c\u5168\u5c40\u5e73\u5747\u6c60\u5316\u7b49\u6280\u672f\uff0c\u4ee5\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u3002", "result": "\u5728Sleep-EDF-20\u548cSleep-EDF-78\u6570\u636e\u96c6\u4e0a\u5206\u522b\u8fbe\u523086.9%\u548c81.4%\u7684\u51c6\u786e\u7387\uff0c\u4ec5\u970013.3K\u53c2\u6570\u548c7.89M FLOPs\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u53c2\u6570\u51cf\u5c11\u9ad8\u8fbe98.6%\u3002", "conclusion": "ULW-SleepNet\u5728\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u51c6\u786e\u7387\u4e0b\u5927\u5e45\u51cf\u5c11\u4e86\u8ba1\u7b97\u9700\u6c42\uff0c\u5c55\u793a\u4e86\u5728\u53ef\u7a7f\u6234\u548c\u7269\u8054\u7f51\u8bbe\u5907\u4e0a\u8fdb\u884c\u5b9e\u65f6\u7761\u7720\u76d1\u6d4b\u7684\u5f3a\u5927\u6f5c\u529b\u3002"}}
{"id": "2602.23880", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23880", "abs": "https://arxiv.org/abs/2602.23880", "authors": ["Zhang Wan", "Tingting Mu", "Samuel Kaski"], "title": "A Theory of Random Graph Shift in Truncated-Spectrum vRKHS", "comment": null, "summary": "This paper develops a theory of graph classification under domain shift through a random-graph generative lens, where we consider intra-class graphs sharing the same random graph model (RGM) and the domain shift induced by changes in RGM components. While classic domain adaptation (DA) theories have well-underpinned existing techniques to handle graph distribution shift, the information of graph samples, which are itself structured objects, is less explored. The non-Euclidean nature of graphs and specialized architectures for graph learning further complicate a fine-grained analysis of graph distribution shifts. In this paper, we propose a theory that assumes RGM as the data generative process, exploiting its connection to hypothesis complexity in function space perspective for such fine-grained analysis. Building on a vector-valued reproducing kernel Hilbert space (vRKHS) formulation, we derive a generalization bound whose shift penalty admits a factorization into (i) a domain discrepancy term, (ii) a spectral-geometry term summarized by the accessible truncated spectrum, and (iii) an amplitude term that aggregates convergence and construction-stability effects. We empirically verify the insights on these terms in both real data and simulations.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u968f\u673a\u56fe\u751f\u6210\u6a21\u578b\u7684\u56fe\u5206\u7c7b\u57df\u9002\u5e94\u7406\u8bba\uff0c\u901a\u8fc7\u5411\u91cf\u503c\u518d\u751f\u6838\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u63a8\u5bfc\u51fa\u5305\u542b\u57df\u5dee\u5f02\u3001\u8c31\u51e0\u4f55\u548c\u632f\u5e45\u4e09\u9879\u56e0\u5b50\u7684\u6cdb\u5316\u8bef\u5dee\u754c\u3002", "motivation": "\u73b0\u6709\u57df\u9002\u5e94\u7406\u8bba\u867d\u7136\u652f\u6301\u5904\u7406\u56fe\u5206\u5e03\u504f\u79fb\uff0c\u4f46\u56fe\u4f5c\u4e3a\u7ed3\u6784\u5316\u5bf9\u8c61\u7684\u4fe1\u606f\u7279\u6027\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u56fe\u7684\u975e\u6b27\u51e0\u91cc\u5f97\u6027\u8d28\u548c\u4e13\u7528\u56fe\u5b66\u4e60\u67b6\u6784\u4f7f\u5f97\u5bf9\u56fe\u5206\u5e03\u504f\u79fb\u7684\u7ec6\u7c92\u5ea6\u5206\u6790\u53d8\u5f97\u590d\u6742\u3002", "method": "\u5047\u8bbe\u968f\u673a\u56fe\u6a21\u578b\u4f5c\u4e3a\u6570\u636e\u751f\u6210\u8fc7\u7a0b\uff0c\u5229\u7528\u5176\u5728\u51fd\u6570\u7a7a\u95f4\u89c6\u89d2\u4e0b\u4e0e\u5047\u8bbe\u590d\u6742\u5ea6\u7684\u8054\u7cfb\u8fdb\u884c\u7ec6\u7c92\u5ea6\u5206\u6790\u3002\u57fa\u4e8e\u5411\u91cf\u503c\u518d\u751f\u6838\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u516c\u5f0f\uff0c\u63a8\u5bfc\u51fa\u5305\u542b\u4e09\u9879\u56e0\u5b50\u7684\u6cdb\u5316\u8bef\u5dee\u754c\u3002", "result": "\u6cdb\u5316\u8bef\u5dee\u754c\u7684\u504f\u79fb\u60e9\u7f5a\u53ef\u5206\u89e3\u4e3a\uff1a(1)\u57df\u5dee\u5f02\u9879\uff0c(2)\u7531\u53ef\u8bbf\u95ee\u622a\u65ad\u8c31\u603b\u7ed3\u7684\u8c31\u51e0\u4f55\u9879\uff0c(3)\u805a\u5408\u6536\u655b\u548c\u6784\u9020\u7a33\u5b9a\u6027\u6548\u5e94\u7684\u632f\u5e45\u9879\u3002\u5728\u771f\u5b9e\u6570\u636e\u548c\u6a21\u62df\u4e2d\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u9879\u7684\u89c1\u89e3\u3002", "conclusion": "\u8be5\u7406\u8bba\u4e3a\u56fe\u5206\u7c7b\u4e2d\u7684\u57df\u504f\u79fb\u63d0\u4f9b\u4e86\u7ec6\u7c92\u5ea6\u5206\u6790\u6846\u67b6\uff0c\u901a\u8fc7\u968f\u673a\u56fe\u751f\u6210\u89c6\u89d2\u548cvRKHS\u516c\u5f0f\uff0c\u63ed\u793a\u4e86\u5f71\u54cd\u6cdb\u5316\u6027\u80fd\u7684\u5173\u952e\u56e0\u7d20\uff0c\u4e3a\u56fe\u57df\u9002\u5e94\u65b9\u6cd5\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7406\u8bba\u6307\u5bfc\u3002"}}
{"id": "2602.24134", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.24134", "abs": "https://arxiv.org/abs/2602.24134", "authors": ["Zhengren Wang", "Dongsheng Ma", "Huaping Zhong", "Jiayu Li", "Wentao Zhang", "Bin Wang", "Conghui He"], "title": "AgenticOCR: Parsing Only What You Need for Efficient Retrieval-Augmented Generation", "comment": null, "summary": "The expansion of retrieval-augmented generation (RAG) into multimodal domains has intensified the challenge for processing complex visual documents, such as financial reports. While page-level chunking and retrieval is a natural starting point, it creates a critical bottleneck: delivering entire pages to the generator introduces excessive extraneous context. This not only overloads the generator's attention mechanism but also dilutes the most salient evidence. Moreover, compressing these information-rich pages into a limited visual token budget further increases the risk of hallucinations. To address this, we introduce AgenticOCR, a dynamic parsing paradigm that transforms optical character recognition (OCR) from a static, full-text process into a query-driven, on-demand extraction system. By autonomously analyzing document layout in a \"thinking with images\" manner, AgenticOCR identifies and selectively recognizes regions of interest. This approach performs on-demand decompression of visual tokens precisely where needed, effectively decoupling retrieval granularity from rigid page-level chunking. AgenticOCR has the potential to serve as the \"third building block\" of the visual document RAG stack, operating alongside and enhancing standard Embedding and Reranking modules. Experimental results demonstrate that AgenticOCR improves both the efficiency and accuracy of visual RAG systems, achieving expert-level performance in long document understanding. Code and models are available at https://github.com/OpenDataLab/AgenticOCR.", "AI": {"tldr": "AgenticOCR\u662f\u4e00\u79cd\u52a8\u6001OCR\u89e3\u6790\u8303\u5f0f\uff0c\u5c06\u4f20\u7edf\u9759\u6001OCR\u8f6c\u53d8\u4e3a\u67e5\u8be2\u9a71\u52a8\u7684\u6309\u9700\u63d0\u53d6\u7cfb\u7edf\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u8bc6\u522b\u611f\u5174\u8da3\u533a\u57df\u6765\u4f18\u5316\u89c6\u89c9\u6587\u6863RAG\u7cfb\u7edf\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u591a\u6a21\u6001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u5728\u89c6\u89c9\u6587\u6863\u5904\u7406\u4e2d\u9762\u4e34\u6311\u6218\uff1a\u9875\u9762\u7ea7\u5206\u5757\u68c0\u7d22\u4f1a\u5411\u751f\u6210\u5668\u5f15\u5165\u8fc7\u591a\u65e0\u5173\u4e0a\u4e0b\u6587\uff0c\u4e0d\u4ec5\u8fc7\u8f7d\u6ce8\u610f\u529b\u673a\u5236\u8fd8\u7a00\u91ca\u5173\u952e\u8bc1\u636e\uff0c\u540c\u65f6\u5c06\u4fe1\u606f\u4e30\u5bcc\u7684\u9875\u9762\u538b\u7f29\u5230\u6709\u9650\u89c6\u89c9token\u9884\u7b97\u4f1a\u589e\u52a0\u5e7b\u89c9\u98ce\u9669\u3002", "method": "\u63d0\u51faAgenticOCR\u52a8\u6001\u89e3\u6790\u8303\u5f0f\uff0c\u901a\u8fc7\"thinking with images\"\u65b9\u5f0f\u81ea\u4e3b\u5206\u6790\u6587\u6863\u5e03\u5c40\uff0c\u8bc6\u522b\u5e76\u9009\u62e9\u6027\u8bc6\u522b\u611f\u5174\u8da3\u533a\u57df\uff0c\u5b9e\u73b0\u6309\u9700\u89c6\u89c9token\u89e3\u538b\u7f29\uff0c\u5c06\u68c0\u7d22\u7c92\u5ea6\u4e0e\u521a\u6027\u9875\u9762\u7ea7\u5206\u5757\u89e3\u8026\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eAgenticOCR\u63d0\u9ad8\u4e86\u89c6\u89c9RAG\u7cfb\u7edf\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u5728\u957f\u6587\u6863\u7406\u89e3\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e13\u5bb6\u7ea7\u6027\u80fd\u3002", "conclusion": "AgenticOCR\u6709\u6f5c\u529b\u6210\u4e3a\u89c6\u89c9\u6587\u6863RAG\u5806\u6808\u7684\"\u7b2c\u4e09\u4e2a\u6784\u5efa\u6a21\u5757\"\uff0c\u4e0e\u6807\u51c6\u5d4c\u5165\u548c\u91cd\u6392\u5e8f\u6a21\u5757\u534f\u540c\u5de5\u4f5c\uff0c\u589e\u5f3a\u591a\u6a21\u6001\u6587\u6863\u5904\u7406\u80fd\u529b\u3002"}}
{"id": "2602.23894", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23894", "abs": "https://arxiv.org/abs/2602.23894", "authors": ["Xavier Timoneda", "Markus Herb", "Fabian Duerr", "Daniel Goehring"], "title": "SelfOccFlow: Towards end-to-end self-supervised 3D Occupancy Flow prediction", "comment": "Accepted version. Final version is published in IEEE Robotics and Automation Letters, DOI: 10.1109/LRA.2026.3665447", "summary": "Estimating 3D occupancy and motion at the vehicle's surroundings is essential for autonomous driving, enabling situational awareness in dynamic environments. Existing approaches jointly learn geometry and motion but rely on expensive 3D occupancy and flow annotations, velocity labels from bounding boxes, or pretrained optical flow models. We propose a self-supervised method for 3D occupancy flow estimation that eliminates the need for human-produced annotations or external flow supervision. Our method disentangles the scene into separate static and dynamic signed distance fields and learns motion implicitly through temporal aggregation. Additionally, we introduce a strong self-supervised flow cue derived from features' cosine similarities. We demonstrate the efficacy of our 3D occupancy flow method on SemanticKITTI, KITTI-MOT, and nuScenes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e3D\u5360\u7528\u6d41\u4f30\u8ba1\u7684\u81ea\u76d1\u7763\u65b9\u6cd5\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u6216\u5916\u90e8\u6d41\u76d1\u7763\uff0c\u901a\u8fc7\u5206\u79bb\u9759\u6001\u548c\u52a8\u6001\u573a\u666f\u5e76\u5229\u7528\u65f6\u95f4\u805a\u5408\u5b66\u4e60\u8fd0\u52a8", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u9700\u8981\u51c6\u786e\u4f30\u8ba1\u8f66\u8f86\u5468\u56f4\u76843D\u5360\u7528\u548c\u8fd0\u52a8\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u76843D\u5360\u7528\u548c\u6d41\u6807\u6ce8\u3001\u901f\u5ea6\u6807\u7b7e\u6216\u9884\u8bad\u7ec3\u5149\u6d41\u6a21\u578b\uff0c\u9700\u8981\u6d88\u9664\u8fd9\u4e9b\u4f9d\u8d56", "method": "\u5c06\u573a\u666f\u89e3\u8026\u4e3a\u9759\u6001\u548c\u52a8\u6001\u7684\u6709\u7b26\u53f7\u8ddd\u79bb\u573a\uff0c\u901a\u8fc7\u65f6\u95f4\u805a\u5408\u9690\u5f0f\u5b66\u4e60\u8fd0\u52a8\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u7279\u5f81\u4f59\u5f26\u76f8\u4f3c\u5ea6\u7684\u81ea\u76d1\u7763\u6d41\u7ebf\u7d22", "result": "\u5728SemanticKITTI\u3001KITTI-MOT\u548cnuScenes\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027", "conclusion": "\u63d0\u51fa\u7684\u81ea\u76d1\u77633D\u5360\u7528\u6d41\u4f30\u8ba1\u65b9\u6cd5\u80fd\u591f\u5728\u4e0d\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u6216\u5916\u90e8\u76d1\u7763\u7684\u60c5\u51b5\u4e0b\u6709\u6548\u5b66\u4e60\u573a\u666f\u51e0\u4f55\u548c\u8fd0\u52a8"}}
{"id": "2602.23899", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23899", "abs": "https://arxiv.org/abs/2602.23899", "authors": ["Pramit Saha", "Mohammad Alsharid", "Joshua Strong", "J. Alison Noble"], "title": "Experience-Guided Self-Adaptive Cascaded Agents for Breast Cancer Screening and Diagnosis with Reduced Biopsy Referrals", "comment": null, "summary": "We propose an experience-guided cascaded multi-agent framework for Breast Ultrasound Screening and Diagnosis, called BUSD-Agent, that aims to reduce diagnostic escalation and unnecessary biopsy referrals. Our framework models screening and diagnosis as a two-stage, selective decision-making process. A lightweight `screening clinic' agent, restricted to classification models as tools, selectively filters out benign and normal cases from further diagnostic escalation when malignancy risk and uncertainty are estimated as low. Cases that have higher risks are escalated to the `diagnostic clinic' agent, which integrates richer perception and radiological description tools to make a secondary decision on biopsy referral. To improve agent performance, past records of pathology-confirmed outcomes along with image embeddings, model predictions, and historical agent actions are stored in a memory bank as structured decision trajectories. For each new case, BUSD-Agent retrieves similar past cases based on image, model response and confidence similarity to condition the agent's current decision policy. This enables retrieval-conditioned in-context adaptation that dynamically adjusts model trust and escalation thresholds from prior experiences without parameter updates. Evaluation across 10 breast ultrasound datasets shows that the proposed experience-guided workflow reduces diagnostic escalation in BUSD-Agent from 84.95% to 58.72% and overall biopsy referrals from 59.50% to 37.08%, compared to the same architecture without trajectory conditioning, while improving average screening specificity by 68.48% and diagnostic specificity by 6.33%.", "AI": {"tldr": "BUSD-Agent\u662f\u4e00\u4e2a\u7ecf\u9a8c\u5f15\u5bfc\u7684\u7ea7\u8054\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u9009\u62e9\u6027\u51b3\u7b56\u6d41\u7a0b\u51cf\u5c11\u4e73\u817a\u8d85\u58f0\u7b5b\u67e5\u4e2d\u7684\u8bca\u65ad\u5347\u7ea7\u548c\u4e0d\u5fc5\u8981\u6d3b\u68c0\u8f6c\u8bca\uff0c\u5229\u7528\u8bb0\u5fc6\u5e93\u4e2d\u7684\u5386\u53f2\u51b3\u7b56\u8f68\u8ff9\u8fdb\u884c\u68c0\u7d22\u6761\u4ef6\u5316\u4e0a\u4e0b\u6587\u9002\u5e94\u3002", "motivation": "\u51cf\u5c11\u4e73\u817a\u8d85\u58f0\u7b5b\u67e5\u4e2d\u7684\u8bca\u65ad\u5347\u7ea7\u548c\u4e0d\u5fc5\u8981\u6d3b\u68c0\u8f6c\u8bca\uff0c\u964d\u4f4e\u533b\u7597\u8d44\u6e90\u6d6a\u8d39\u548c\u60a3\u8005\u5fc3\u7406\u8d1f\u62c5\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u7ea7\u8054\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff1a1) \u7b5b\u67e5\u8bca\u6240\u667a\u80fd\u4f53\u4f7f\u7528\u5206\u7c7b\u6a21\u578b\u7b5b\u9009\u4f4e\u98ce\u9669\u75c5\u4f8b\uff1b2) \u9ad8\u98ce\u9669\u75c5\u4f8b\u5347\u7ea7\u5230\u8bca\u65ad\u8bca\u6240\u667a\u80fd\u4f53\uff0c\u96c6\u6210\u66f4\u4e30\u5bcc\u7684\u611f\u77e5\u548c\u653e\u5c04\u5b66\u63cf\u8ff0\u5de5\u5177\u3002\u5229\u7528\u8bb0\u5fc6\u5e93\u5b58\u50a8\u5386\u53f2\u51b3\u7b56\u8f68\u8ff9\uff0c\u901a\u8fc7\u68c0\u7d22\u76f8\u4f3c\u75c5\u4f8b\u8fdb\u884c\u6761\u4ef6\u5316\u4e0a\u4e0b\u6587\u9002\u5e94\u3002", "result": "\u572810\u4e2a\u4e73\u817a\u8d85\u58f0\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u76f8\u6bd4\u65e0\u8f68\u8ff9\u6761\u4ef6\u5316\u7684\u76f8\u540c\u67b6\u6784\uff0cBUSD-Agent\u5c06\u8bca\u65ad\u5347\u7ea7\u7387\u4ece84.95%\u964d\u81f358.72%\uff0c\u603b\u4f53\u6d3b\u68c0\u8f6c\u8bca\u7387\u4ece59.50%\u964d\u81f337.08%\uff0c\u7b5b\u67e5\u7279\u5f02\u6027\u5e73\u5747\u63d0\u9ad868.48%\uff0c\u8bca\u65ad\u7279\u5f02\u6027\u63d0\u9ad86.33%\u3002", "conclusion": "\u7ecf\u9a8c\u5f15\u5bfc\u7684\u7ea7\u8054\u591a\u667a\u80fd\u4f53\u6846\u67b6\u80fd\u6709\u6548\u51cf\u5c11\u4e73\u817a\u8d85\u58f0\u7b5b\u67e5\u4e2d\u7684\u8fc7\u5ea6\u8bca\u65ad\u548c\u4e0d\u5fc5\u8981\u6d3b\u68c0\uff0c\u901a\u8fc7\u68c0\u7d22\u6761\u4ef6\u5316\u4e0a\u4e0b\u6587\u9002\u5e94\u5b9e\u73b0\u52a8\u6001\u8c03\u6574\u6a21\u578b\u4fe1\u4efb\u548c\u5347\u7ea7\u9608\u503c\uff0c\u63d0\u9ad8\u4e34\u5e8a\u51b3\u7b56\u6548\u7387\u3002"}}
{"id": "2602.23906", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23906", "abs": "https://arxiv.org/abs/2602.23906", "authors": ["Bora Kargi", "Arnas Uselis", "Seong Joon Oh"], "title": "Half-Truths Break Similarity-Based Retrieval", "comment": null, "summary": "When a text description is extended with an additional detail, image-text similarity should drop if that detail is wrong. We show that CLIP-style dual encoders often violate this intuition: appending a plausible but incorrect object or relation to an otherwise correct description can increase the similarity score. We call such cases half-truths. On COCO, CLIP prefers the correct shorter description only 40.6% of the time, and performance drops to 32.9% when the added detail is a relation. We trace this vulnerability to weak supervision on caption parts: contrastive training aligns full sentences but does not explicitly enforce that individual entities and relations are grounded. We propose CS-CLIP (Component-Supervised CLIP), which decomposes captions into entity and relation units, constructs a minimally edited foil for each unit, and fine-tunes the model to score the correct unit above its foil while preserving standard dual-encoder inference. CS-CLIP raises half-truth accuracy to 69.3% and improves average performance on established compositional benchmarks by 5.7 points, suggesting that reducing half-truth errors aligns with broader gains in compositional understanding. Code is publicly available at: https://github.com/kargibora/CS-CLIP", "AI": {"tldr": "CLIP\u6a21\u578b\u5728\u56fe\u50cf-\u6587\u672c\u5339\u914d\u4e2d\u5b58\u5728\"\u534a\u771f\"\u6f0f\u6d1e\uff1a\u7ed9\u6b63\u786e\u63cf\u8ff0\u6dfb\u52a0\u9519\u8bef\u7ec6\u8282\u53cd\u800c\u63d0\u9ad8\u76f8\u4f3c\u5ea6\u5f97\u5206\uff0cCS-CLIP\u901a\u8fc7\u7ec4\u4ef6\u76d1\u7763\u8bad\u7ec3\u89e3\u51b3\u6b64\u95ee\u9898", "motivation": "CLIP\u7b49\u53cc\u7f16\u7801\u5668\u6a21\u578b\u5728\u56fe\u50cf-\u6587\u672c\u76f8\u4f3c\u5ea6\u8bc4\u4f30\u4e2d\u5b58\u5728\u53cd\u76f4\u89c9\u73b0\u8c61\uff1a\u7ed9\u6b63\u786e\u63cf\u8ff0\u6dfb\u52a0\u9519\u8bef\u7ec6\u8282\u53cd\u800c\u63d0\u9ad8\u76f8\u4f3c\u5ea6\u5f97\u5206\uff0c\u8fd9\u66b4\u9732\u4e86\u6a21\u578b\u5bf9\u6587\u672c\u7ec4\u4ef6\uff08\u5b9e\u4f53\u548c\u5173\u7cfb\uff09\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u76d1\u7763\u7684\u95ee\u9898", "method": "\u63d0\u51faCS-CLIP\uff08\u7ec4\u4ef6\u76d1\u7763CLIP\uff09\uff0c\u5c06\u63cf\u8ff0\u5206\u89e3\u4e3a\u5b9e\u4f53\u548c\u5173\u7cfb\u5355\u5143\uff0c\u4e3a\u6bcf\u4e2a\u5355\u5143\u6784\u5efa\u6700\u5c0f\u7f16\u8f91\u7684\u865a\u5047\u7248\u672c\uff0c\u901a\u8fc7\u5fae\u8c03\u4f7f\u6a21\u578b\u6b63\u786e\u5355\u5143\u5f97\u5206\u9ad8\u4e8e\u865a\u5047\u7248\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u6807\u51c6\u53cc\u7f16\u7801\u5668\u63a8\u7406", "result": "\u5728COCO\u6570\u636e\u96c6\u4e0a\uff0cCS-CLIP\u5c06\u534a\u771f\u51c6\u786e\u7387\u4ece40.6%\u63d0\u5347\u523069.3%\uff0c\u5728\u7ec4\u5408\u7406\u89e3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u6027\u80fd\u63d0\u53475.7\u4e2a\u767e\u5206\u70b9", "conclusion": "\u51cf\u5c11\u534a\u771f\u9519\u8bef\u4e0e\u63d0\u5347\u7ec4\u5408\u7406\u89e3\u80fd\u529b\u4e00\u81f4\uff0c\u7ec4\u4ef6\u76d1\u7763\u8bad\u7ec3\u80fd\u6709\u6548\u6539\u5584CLIP\u6a21\u578b\u5bf9\u6587\u672c\u7ec4\u4ef6\u7684\u7ec6\u7c92\u5ea6\u7406\u89e3\uff0c\u540c\u65f6\u4fdd\u6301\u6807\u51c6\u63a8\u7406\u6548\u7387"}}
{"id": "2602.23994", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23994", "abs": "https://arxiv.org/abs/2602.23994", "authors": ["Vrushank Ahire", "Yogesh Kumar", "Anouck Girard", "M. A. Ganaie"], "title": "MINT: Multimodal Imaging-to-Speech Knowledge Transfer for Early Alzheimer's Screening", "comment": null, "summary": "Alzheimer's disease is a progressive neurodegenerative disorder in which mild cognitive impairment (MCI) marks a critical transition between aging and dementia. Neuroimaging modalities, such as structural MRI, provide biomarkers of this transition; however, their high costs and infrastructure needs limit their deployment at a population scale. Speech analysis offers a non-invasive alternative, but speech-only classifiers are developed independently of neuroimaging, leaving decision boundaries biologically ungrounded and limiting reliability on the subtle CN-versus-MCI distinction. We propose MINT (Multimodal Imaging-to-Speech Knowledge Transfer), a three-stage cross-modal framework that transfers biomarker structure from MRI into a speech encoder at training time. An MRI teacher, trained on 1,228 subjects, defines a compact neuroimaging embedding space for CN-versus-MCI classification. A residual projection head aligns speech representations to this frozen imaging manifold via a combined geometric loss, adapting speech to the learned biomarker space while preserving imaging encoder fidelity. The frozen MRI classifier, which is never exposed to speech, is applied to aligned embeddings at inference and requires no scanner. Evaluation on ADNI-4 shows aligned speech achieves performance comparable to speech-only baselines (AUC 0.720 vs 0.711) while requiring no imaging at inference, demonstrating that MRI-derived decision boundaries can ground speech representations. Multimodal fusion improves over MRI alone (0.973 vs 0.958). Ablation studies identify dropout regularization and self-supervised pretraining as critical design decisions. To our knowledge, this is the first demonstration of MRI-to-speech knowledge transfer for early Alzheimer's screening, establishing a biologically grounded pathway for population-level cognitive triage without neuroimaging at inference.", "AI": {"tldr": "MINT\u6846\u67b6\u5c06MRI\u7684\u751f\u7269\u6807\u5fd7\u7269\u7ed3\u6784\u8f6c\u79fb\u5230\u8bed\u97f3\u7f16\u7801\u5668\u4e2d\uff0c\u5b9e\u73b0\u65e0\u9700\u795e\u7ecf\u5f71\u50cf\u7684\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u65e9\u671f\u7b5b\u67e5\uff0c\u6027\u80fd\u4e0e\u7eaf\u8bed\u97f3\u57fa\u7ebf\u76f8\u5f53\u3002", "motivation": "\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u7684\u65e9\u671f\u7b5b\u67e5\u9700\u8981\u751f\u7269\u6807\u5fd7\u7269\uff0c\u4f46\u795e\u7ecf\u5f71\u50cf\uff08\u5982MRI\uff09\u6210\u672c\u9ad8\u3001\u90e8\u7f72\u56f0\u96be\u3002\u8bed\u97f3\u5206\u6790\u867d\u65e0\u521b\uff0c\u4f46\u7eaf\u8bed\u97f3\u5206\u7c7b\u5668\u7f3a\u4e4f\u751f\u7269\u5b66\u57fa\u7840\uff0c\u5728CN\u4e0eMCI\u7684\u7ec6\u5fae\u533a\u5206\u4e0a\u53ef\u9760\u6027\u6709\u9650\u3002", "method": "\u63d0\u51faMINT\uff08\u591a\u6a21\u6001\u5f71\u50cf\u5230\u8bed\u97f3\u77e5\u8bc6\u8f6c\u79fb\uff09\u4e09\u9636\u6bb5\u8de8\u6a21\u6001\u6846\u67b6\uff1a1\uff09\u57281,228\u540d\u53d7\u8bd5\u8005\u4e0a\u8bad\u7ec3MRI\u6559\u5e08\u6a21\u578b\uff0c\u5b9a\u4e49\u7d27\u51d1\u7684\u795e\u7ecf\u5f71\u50cf\u5d4c\u5165\u7a7a\u95f4\u7528\u4e8eCN\u4e0eMCI\u5206\u7c7b\uff1b2\uff09\u901a\u8fc7\u6b8b\u5dee\u6295\u5f71\u5934\u5c06\u8bed\u97f3\u8868\u793a\u5bf9\u9f50\u5230\u51bb\u7ed3\u7684\u5f71\u50cf\u6d41\u5f62\uff0c\u4f7f\u7528\u51e0\u4f55\u635f\u5931\u51fd\u6570\uff1b3\uff09\u63a8\u7406\u65f6\u5e94\u7528\u51bb\u7ed3\u7684MRI\u5206\u7c7b\u5668\u5230\u5bf9\u9f50\u7684\u5d4c\u5165\uff0c\u65e0\u9700\u626b\u63cf\u4eea\u3002", "result": "\u5728ADNI-4\u6570\u636e\u96c6\u4e0a\uff0c\u5bf9\u9f50\u7684\u8bed\u97f3\u8fbe\u5230\u4e0e\u7eaf\u8bed\u97f3\u57fa\u7ebf\u76f8\u5f53\u7684\u6027\u80fd\uff08AUC 0.720 vs 0.711\uff09\uff0c\u4e14\u63a8\u7406\u65f6\u65e0\u9700\u5f71\u50cf\u3002\u591a\u6a21\u6001\u878d\u5408\u4f18\u4e8e\u5355\u72ecMRI\uff080.973 vs 0.958\uff09\u3002\u6d88\u878d\u7814\u7a76\u663e\u793adropout\u6b63\u5219\u5316\u548c\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u662f\u5173\u952e\u8bbe\u8ba1\u51b3\u7b56\u3002", "conclusion": "\u8fd9\u662f\u9996\u6b21\u5c55\u793aMRI\u5230\u8bed\u97f3\u77e5\u8bc6\u8f6c\u79fb\u7528\u4e8e\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u65e9\u671f\u7b5b\u67e5\uff0c\u5efa\u7acb\u4e86\u65e0\u9700\u63a8\u7406\u65f6\u795e\u7ecf\u5f71\u50cf\u7684\u751f\u7269\u5b66\u57fa\u7840\u901a\u8def\uff0c\u4e3a\u4eba\u7fa4\u7ea7\u8ba4\u77e5\u5206\u6d41\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2602.23997", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23997", "abs": "https://arxiv.org/abs/2602.23997", "authors": ["Florent Delgrange"], "title": "Foundation World Models for Agents that Learn, Verify, and Adapt Reliably Beyond Static Environments", "comment": "AAMAS 2026, Blue Sky Idea Track. 4 pages, 1 Figure", "summary": "The next generation of autonomous agents must not only learn efficiently but also act reliably and adapt their behavior in open worlds. Standard approaches typically assume fixed tasks and environments with little or no novelty, which limits world models' ability to support agents that must evolve their policies as conditions change. This paper outlines a vision for foundation world models: persistent, compositional representations that unify reinforcement learning, reactive/program synthesis, and abstraction mechanisms. We propose an agenda built around four components: (i) learnable reward models from specifications to support optimization with clear objectives; (ii) adaptive formal verification integrated throughout learning; (iii) online abstraction calibration to quantify the reliability of the model's predictions; and (iv) test-time synthesis and world-model generation guided by verifiers. Together, these components enable agents to synthesize verifiable programs, derive new policies from a small number of interactions, and maintain correctness while adapting to novelty. The resulting framework positions foundation world models as a substrate for learning, reasoning, and adaptation, laying the groundwork for agents that not only act well but can explain and justify the behavior they adopt.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u57fa\u7840\u4e16\u754c\u6a21\u578b\u6982\u5ff5\uff0c\u65e8\u5728\u6784\u5efa\u6301\u4e45\u3001\u7ec4\u5408\u5f0f\u7684\u8868\u793a\uff0c\u7edf\u4e00\u5f3a\u5316\u5b66\u4e60\u3001\u53cd\u5e94\u5f0f/\u7a0b\u5e8f\u7efc\u5408\u548c\u62bd\u8c61\u673a\u5236\uff0c\u4ee5\u652f\u6301\u5728\u5f00\u653e\u4e16\u754c\u4e2d\u53ef\u9760\u5b66\u4e60\u548c\u9002\u5e94\u7684\u81ea\u4e3b\u667a\u80fd\u4f53\u3002", "motivation": "\u5f53\u524d\u81ea\u4e3b\u667a\u80fd\u4f53\u65b9\u6cd5\u901a\u5e38\u5047\u8bbe\u56fa\u5b9a\u4efb\u52a1\u548c\u73af\u5883\uff0c\u7f3a\u4e4f\u5e94\u5bf9\u65b0\u9896\u6027\u7684\u80fd\u529b\uff0c\u9650\u5236\u4e86\u4e16\u754c\u6a21\u578b\u652f\u6301\u667a\u80fd\u4f53\u5728\u6761\u4ef6\u53d8\u5316\u65f6\u6f14\u5316\u7b56\u7565\u7684\u80fd\u529b\u3002\u9700\u8981\u6784\u5efa\u80fd\u591f\u652f\u6301\u5b66\u4e60\u3001\u63a8\u7406\u548c\u9002\u5e94\u7684\u57fa\u7840\u4e16\u754c\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u56f4\u7ed5\u56db\u4e2a\u7ec4\u4ef6\u7684\u8bae\u7a0b\uff1a(1)\u4ece\u89c4\u8303\u4e2d\u5b66\u4e60\u5956\u52b1\u6a21\u578b\u4ee5\u652f\u6301\u660e\u786e\u76ee\u6807\u7684\u4f18\u5316\uff1b(2)\u5728\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u96c6\u6210\u81ea\u9002\u5e94\u5f62\u5f0f\u9a8c\u8bc1\uff1b(3)\u5728\u7ebf\u62bd\u8c61\u6821\u51c6\u4ee5\u91cf\u5316\u6a21\u578b\u9884\u6d4b\u7684\u53ef\u9760\u6027\uff1b(4)\u7531\u9a8c\u8bc1\u5668\u6307\u5bfc\u7684\u6d4b\u8bd5\u65f6\u7efc\u5408\u548c\u4e16\u754c\u6a21\u578b\u751f\u6210\u3002", "result": "\u8be5\u6846\u67b6\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u7efc\u5408\u53ef\u9a8c\u8bc1\u7a0b\u5e8f\uff0c\u4ece\u5c11\u91cf\u4ea4\u4e92\u4e2d\u63a8\u5bfc\u65b0\u7b56\u7565\uff0c\u5e76\u5728\u9002\u5e94\u65b0\u9896\u6027\u65f6\u4fdd\u6301\u6b63\u786e\u6027\u3002\u57fa\u7840\u4e16\u754c\u6a21\u578b\u6210\u4e3a\u5b66\u4e60\u3001\u63a8\u7406\u548c\u9002\u5e94\u7684\u57fa\u7840\u3002", "conclusion": "\u57fa\u7840\u4e16\u754c\u6a21\u578b\u4e3a\u81ea\u4e3b\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u4e0d\u4ec5\u80fd\u826f\u597d\u884c\u52a8\uff0c\u8fd8\u80fd\u89e3\u91ca\u548c\u8bc1\u660e\u5176\u884c\u4e3a\u7684\u5e95\u5c42\u652f\u6491\uff0c\u4e3a\u5b9e\u73b0\u53ef\u9760\u3001\u9002\u5e94\u6027\u5f3a\u4e14\u53ef\u89e3\u91ca\u7684\u4e0b\u4e00\u4ee3\u81ea\u4e3b\u7cfb\u7edf\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2602.24012", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2602.24012", "abs": "https://arxiv.org/abs/2602.24012", "authors": ["Roy Betser", "Eyal Gofer", "Meir Yossef Levi", "Guy Gilboa"], "title": "InfoNCE Induces Gaussian Distribution", "comment": "Accepted to ICLR 2026, Oral", "summary": "Contrastive learning has become a cornerstone of modern representation learning, allowing training with massive unlabeled data for both task-specific and general (foundation) models. A prototypical loss in contrastive training is InfoNCE and its variants. In this work, we show that the InfoNCE objective induces Gaussian structure in representations that emerge from contrastive training. We establish this result in two complementary regimes. First, we show that under certain alignment and concentration assumptions, projections of the high-dimensional representation asymptotically approach a multivariate Gaussian distribution. Next, under less strict assumptions, we show that adding a small asymptotically vanishing regularization term that promotes low feature norm and high feature entropy leads to similar asymptotic results. We support our analysis with experiments on synthetic and CIFAR-10 datasets across multiple encoder architectures and sizes, demonstrating consistent Gaussian behavior. This perspective provides a principled explanation for commonly observed Gaussianity in contrastive representations. The resulting Gaussian model enables principled analytical treatment of learned representations and is expected to support a wide range of applications in contrastive learning.", "AI": {"tldr": "\u8bba\u6587\u8bc1\u660e\u5bf9\u6bd4\u5b66\u4e60\u4e2d\u7684InfoNCE\u635f\u5931\u4f1a\u8bf1\u5bfc\u51fa\u9ad8\u65af\u7ed3\u6784\u7684\u8868\u793a\uff0c\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8fd9\u4e00\u53d1\u73b0\u3002", "motivation": "\u5bf9\u6bd4\u5b66\u4e60\u5df2\u6210\u4e3a\u73b0\u4ee3\u8868\u793a\u5b66\u4e60\u7684\u57fa\u77f3\uff0c\u4f46\u5bf9\u5176\u8bf1\u5bfc\u7684\u8868\u793a\u7ed3\u6784\u7f3a\u4e4f\u7406\u8bba\u7406\u89e3\u3002\u672c\u6587\u65e8\u5728\u63a2\u7a76InfoNCE\u76ee\u6807\u51fd\u6570\u662f\u5426\u4f1a\u5bfc\u81f4\u8868\u793a\u5448\u73b0\u9ad8\u65af\u5206\u5e03\u7279\u6027\u3002", "method": "\u91c7\u7528\u4e24\u79cd\u4e92\u8865\u7684\u7406\u8bba\u5206\u6790\u6846\u67b6\uff1a1) \u5728\u7279\u5b9a\u5bf9\u9f50\u548c\u96c6\u4e2d\u5047\u8bbe\u4e0b\uff0c\u8bc1\u660e\u9ad8\u7ef4\u8868\u793a\u7684\u6295\u5f71\u6e10\u8fd1\u8d8b\u8fd1\u591a\u5143\u9ad8\u65af\u5206\u5e03\uff1b2) \u5728\u8f83\u5f31\u5047\u8bbe\u4e0b\uff0c\u901a\u8fc7\u6dfb\u52a0\u6e10\u8fd1\u6d88\u5931\u7684\u6b63\u5219\u5316\u9879\uff08\u4fc3\u8fdb\u4f4e\u7279\u5f81\u8303\u6570\u548c\u9ad8\u7279\u5f81\u71b5\uff09\u83b7\u5f97\u7c7b\u4f3c\u7ed3\u679c\u3002\u540c\u65f6\u5728\u5408\u6210\u6570\u636e\u548cCIFAR-10\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5bf9\u6bd4\u5b66\u4e60\u786e\u5b9e\u4f1a\u8bf1\u5bfc\u51fa\u9ad8\u65af\u7ed3\u6784\u7684\u8868\u793a\u3002\u8fd9\u4e00\u53d1\u73b0\u4e3a\u5b9e\u8df5\u4e2d\u89c2\u5bdf\u5230\u7684\u8868\u793a\u9ad8\u65af\u6027\u63d0\u4f9b\u4e86\u7406\u8bba\u89e3\u91ca\uff0c\u5e76\u652f\u6301\u4e86\u591a\u4e2a\u7f16\u7801\u5668\u67b6\u6784\u548c\u89c4\u6a21\u7684\u4e00\u81f4\u6027\u3002", "conclusion": "InfoNCE\u76ee\u6807\u51fd\u6570\u4f1a\u8bf1\u5bfc\u8868\u793a\u5448\u73b0\u9ad8\u65af\u7ed3\u6784\uff0c\u8fd9\u4e00\u9ad8\u65af\u6a21\u578b\u4e3a\u5206\u6790\u5b66\u4e60\u5230\u7684\u8868\u793a\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\uff0c\u6709\u671b\u652f\u6301\u5bf9\u6bd4\u5b66\u4e60\u7684\u5e7f\u6cdb\u5e94\u7528\u3002"}}
{"id": "2602.23945", "categories": ["cs.CV", "cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2602.23945", "abs": "https://arxiv.org/abs/2602.23945", "authors": ["Dongxu Zhang", "Yiding Sun", "Pengcheng Li", "Yumou Liu", "Hongqiang Lin", "Haoran Xu", "Xiaoxuan Mu", "Liang Lin", "Wenbiao Yan", "Ning Yang", "Chaowei Fang", "Juanjuan Zhao", "Jihua Zhu", "Conghui He", "Cheng Tan"], "title": "PointCoT: A Multi-modal Benchmark for Explicit 3D Geometric Reasoning", "comment": null, "summary": "While Multimodal Large Language Models (MLLMs) demonstrate proficiency in 2D scenes, extending their perceptual intelligence to 3D point cloud understanding remains a significant challenge. Current approaches focus primarily on aligning 3D features with pre-trained models. However, they typically treat geometric reasoning as an implicit mapping process. These methods bypass intermediate logical steps and consequently suffer from geometric hallucinations. They confidently generate plausible responses that fail to ground in precise structural details. To bridge this gap, we present PointCoT, a novel framework that empowers MLLMs with explicit Chain-of-Thought (CoT) reasoning for 3D data. We advocate for a \\textit{Look, Think, then Answer} paradigm. In this approach, the model is supervised to generate geometry-grounded rationales before predicting final answers. To facilitate this, we construct Point-Reason-Instruct, a large-scale benchmark comprising $\\sim$86k instruction-tuning samples with hierarchical CoT annotations. By leveraging a dual-stream multi-modal architecture, our method synergizes semantic appearance with geometric truth. Extensive experiments demonstrate that PointCoT achieves state-of-the-art performance on complex reasoning tasks.", "AI": {"tldr": "PointCoT\uff1a\u4e00\u79cd\u901a\u8fc7\u663e\u5f0f\u601d\u7ef4\u94fe\u63a8\u7406\u589e\u5f3aMLLMs\u57283D\u70b9\u4e91\u7406\u89e3\u80fd\u529b\u7684\u65b0\u6846\u67b6\uff0c\u91c7\u7528\"\u89c2\u5bdf-\u601d\u8003-\u56de\u7b54\"\u8303\u5f0f\uff0c\u5728\u9884\u6d4b\u7b54\u6848\u524d\u751f\u6210\u51e0\u4f55\u57fa\u7840\u63a8\u7406\uff0c\u663e\u8457\u51cf\u5c11\u51e0\u4f55\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u57282D\u573a\u666f\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u57283D\u70b9\u4e91\u7406\u89e3\u65b9\u9762\u5b58\u5728\u5c40\u9650\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce83D\u7279\u5f81\u4e0e\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u5bf9\u9f50\uff0c\u5c06\u51e0\u4f55\u63a8\u7406\u89c6\u4e3a\u9690\u5f0f\u6620\u5c04\u8fc7\u7a0b\uff0c\u5bfc\u81f4\u51e0\u4f55\u5e7b\u89c9\u95ee\u9898\u2014\u2014\u6a21\u578b\u4f1a\u751f\u6210\u770b\u4f3c\u5408\u7406\u4f46\u7f3a\u4e4f\u7cbe\u786e\u7ed3\u6784\u7ec6\u8282\u57fa\u7840\u7684\u54cd\u5e94\u3002", "method": "\u63d0\u51faPointCoT\u6846\u67b6\uff0c\u91c7\u7528\"\u89c2\u5bdf-\u601d\u8003-\u56de\u7b54\"\u8303\u5f0f\uff0c\u76d1\u7763\u6a21\u578b\u5728\u9884\u6d4b\u6700\u7ec8\u7b54\u6848\u524d\u751f\u6210\u51e0\u4f55\u57fa\u7840\u63a8\u7406\u3002\u6784\u5efaPoint-Reason-Instruct\u5927\u89c4\u6a21\u57fa\u51c6\u6570\u636e\u96c6\uff08\u7ea686k\u6307\u4ee4\u8c03\u4f18\u6837\u672c\uff0c\u5305\u542b\u5206\u5c42\u601d\u7ef4\u94fe\u6807\u6ce8\uff09\u3002\u91c7\u7528\u53cc\u6d41\u591a\u6a21\u6001\u67b6\u6784\uff0c\u534f\u540c\u6574\u5408\u8bed\u4e49\u5916\u89c2\u4e0e\u51e0\u4f55\u771f\u503c\u3002", "result": "PointCoT\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u8868\u73b0\uff0c\u901a\u8fc7\u663e\u5f0f\u601d\u7ef4\u94fe\u63a8\u7406\u663e\u8457\u63d0\u5347\u4e863D\u70b9\u4e91\u7406\u89e3\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u663e\u5f0f\u601d\u7ef4\u94fe\u63a8\u7406\u673a\u5236\uff0cPointCoT\u6210\u529f\u5c06MLLMs\u7684\u611f\u77e5\u667a\u80fd\u6269\u5c55\u52303D\u70b9\u4e91\u7406\u89e3\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u7684\u51e0\u4f55\u5e7b\u89c9\u95ee\u9898\uff0c\u4e3a3D\u573a\u666f\u7406\u89e3\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u57fa\u7840\u3002"}}
{"id": "2602.24066", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.24066", "abs": "https://arxiv.org/abs/2602.24066", "authors": ["Tobias Nygaard"], "title": "pathsig: A GPU-Accelerated Library for Truncated and Projected Path Signatures", "comment": null, "summary": "Path signatures provide a rich representation of sequential data, with strong theoretical guarantees and good performance in a variety of machine-learning tasks. While signatures have progressed from fixed feature extractors to trainable components of machine-learning models, existing libraries often lack the required scalability for large-scale, gradient-based learning. To address this gap, this paper introduces pathsig, a PyTorch-native library that computes path signatures directly in the word basis. By using CUDA kernels to update signature coefficients in parallel over prefix-closed word sets, pathsig achieves high GPU throughput and near-minimal peak memory. Compared with other libraries, pathsig achieves 10-30x speedups for computation of truncated signatures and up to 4-10x speedups in training that require backpropagation through the signature. Beyond regular truncation, pathsig supports projections of the (infinite-dimensional) signature onto user-specified sets of words and anisotropic truncation motivated by inhomogeneous path regularity, enabling more compact representations that can reduce dimensionality, redundancy, and computational cost.", "AI": {"tldr": "pathsig\u662f\u4e00\u4e2aPyTorch\u539f\u751f\u5e93\uff0c\u901a\u8fc7CUDA\u5185\u6838\u5e76\u884c\u8ba1\u7b97\u8def\u5f84\u7b7e\u540d\uff0c\u76f8\u6bd4\u73b0\u6709\u5e93\u5728\u622a\u65ad\u7b7e\u540d\u8ba1\u7b97\u4e0a\u5b9e\u73b010-30\u500d\u52a0\u901f\uff0c\u5728\u9700\u8981\u53cd\u5411\u4f20\u64ad\u7684\u8bad\u7ec3\u4e2d\u5b9e\u73b04-10\u500d\u52a0\u901f\u3002", "motivation": "\u73b0\u6709\u8def\u5f84\u7b7e\u540d\u5e93\u7f3a\u4e4f\u5927\u89c4\u6a21\u68af\u5ea6\u5b66\u4e60\u6240\u9700\u7684\u53ef\u6269\u5c55\u6027\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u9ad8\u6548\u5904\u7406GPU\u8ba1\u7b97\u548c\u53cd\u5411\u4f20\u64ad\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f00\u53d1PyTorch\u539f\u751f\u5e93pathsig\uff0c\u5728\u8bcd\u57fa\u4e0a\u76f4\u63a5\u8ba1\u7b97\u8def\u5f84\u7b7e\u540d\uff0c\u4f7f\u7528CUDA\u5185\u6838\u5728\u524d\u7f00\u5c01\u95ed\u8bcd\u96c6\u4e0a\u5e76\u884c\u66f4\u65b0\u7b7e\u540d\u7cfb\u6570\uff0c\u652f\u6301\u7528\u6237\u6307\u5b9a\u8bcd\u96c6\u7684\u6295\u5f71\u548c\u57fa\u4e8e\u975e\u5747\u5300\u8def\u5f84\u6b63\u5219\u6027\u7684\u5404\u5411\u5f02\u6027\u622a\u65ad\u3002", "result": "pathsig\u5b9e\u73b0\u4e86\u9ad8GPU\u541e\u5410\u91cf\u548c\u63a5\u8fd1\u6700\u5c0f\u7684\u5cf0\u503c\u5185\u5b58\u4f7f\u7528\uff0c\u5728\u622a\u65ad\u7b7e\u540d\u8ba1\u7b97\u4e0a\u6bd4\u73b0\u6709\u5e93\u5feb10-30\u500d\uff0c\u5728\u9700\u8981\u53cd\u5411\u4f20\u64ad\u7684\u8bad\u7ec3\u4e2d\u5feb4-10\u500d\u3002", "conclusion": "pathsig\u586b\u8865\u4e86\u73b0\u6709\u7b7e\u540d\u5e93\u5728\u5927\u89c4\u6a21\u68af\u5ea6\u5b66\u4e60\u65b9\u9762\u7684\u53ef\u6269\u5c55\u6027\u7a7a\u767d\uff0c\u901a\u8fc7\u9ad8\u6548GPU\u5b9e\u73b0\u548c\u7075\u6d3b\u7684\u7279\u5f81\u9009\u62e9\u673a\u5236\uff0c\u4e3a\u5e8f\u5217\u6570\u636e\u7684\u673a\u5668\u5b66\u4e60\u5e94\u7528\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u5de5\u5177\u3002"}}
{"id": "2602.24069", "categories": ["cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2602.24069", "abs": "https://arxiv.org/abs/2602.24069", "authors": ["Ryan DeWolfe"], "title": "Leveraging Non-linear Dimension Reduction and Random Walk Co-occurrence for Node Embedding", "comment": "13 pages, 6 figures", "summary": "Leveraging non-linear dimension reduction techniques, we remove the low dimension constraint from node embedding and propose COVE, an explainable high dimensional embedding that, when reduced to low dimension with UMAP, slightly increases performance on clustering and link prediction tasks. The embedding is inspired by neural embedding methods that use co-occurrence on a random walk as an indication of similarity, and is closely related to a diffusion process. Extending on recent community detection benchmarks, we find that a COVE UMAP HDBSCAN pipeline performs similarly to the popular Louvain algorithm.", "AI": {"tldr": "COVE\u662f\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u9ad8\u7ef4\u5d4c\u5165\u65b9\u6cd5\uff0c\u901a\u8fc7\u975e\u7ebf\u6027\u964d\u7ef4\u6280\u672f\u7a81\u7834\u8282\u70b9\u5d4c\u5165\u7684\u4f4e\u7ef4\u9650\u5236\uff0c\u7ed3\u5408UMAP\u964d\u7ef4\u540e\u5728\u805a\u7c7b\u548c\u94fe\u63a5\u9884\u6d4b\u4efb\u52a1\u4e0a\u6027\u80fd\u7565\u6709\u63d0\u5347\u3002", "motivation": "\u4f20\u7edf\u8282\u70b9\u5d4c\u5165\u65b9\u6cd5\u901a\u5e38\u53d7\u9650\u4e8e\u4f4e\u7ef4\u7a7a\u95f4\uff0c\u4f5c\u8005\u5e0c\u671b\u7a81\u7834\u8fd9\u4e00\u9650\u5236\uff0c\u5f00\u53d1\u53ef\u89e3\u91ca\u7684\u9ad8\u7ef4\u5d4c\u5165\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u5728\u793e\u533a\u68c0\u6d4b\u548c\u94fe\u63a5\u9884\u6d4b\u7b49\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51faCOVE\u9ad8\u7ef4\u5d4c\u5165\u65b9\u6cd5\uff0c\u53d7\u795e\u7ecf\u5d4c\u5165\u65b9\u6cd5\u542f\u53d1\uff0c\u5229\u7528\u968f\u673a\u6e38\u8d70\u4e2d\u7684\u5171\u73b0\u4f5c\u4e3a\u76f8\u4f3c\u6027\u6307\u6807\uff0c\u4e0e\u6269\u6563\u8fc7\u7a0b\u5bc6\u5207\u76f8\u5173\u3002\u91c7\u7528COVE-UMAP-HDBSCAN\u6d41\u7a0b\uff0c\u5148\u8fdb\u884c\u9ad8\u7ef4\u5d4c\u5165\uff0c\u518d\u7528UMAP\u964d\u7ef4\uff0c\u6700\u540e\u7528HDBSCAN\u805a\u7c7b\u3002", "result": "COVE\u7ed3\u5408UMAP\u964d\u7ef4\u540e\uff0c\u5728\u805a\u7c7b\u548c\u94fe\u63a5\u9884\u6d4b\u4efb\u52a1\u4e0a\u6027\u80fd\u7565\u6709\u63d0\u5347\u3002\u5728\u793e\u533a\u68c0\u6d4b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCOVE-UMAP-HDBSCAN\u6d41\u7a0b\u4e0e\u6d41\u884c\u7684Louvain\u7b97\u6cd5\u8868\u73b0\u76f8\u5f53\u3002", "conclusion": "COVE\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u9ad8\u7ef4\u8282\u70b9\u5d4c\u5165\u65b9\u6cd5\uff0c\u7a81\u7834\u4e86\u4f20\u7edf\u4f4e\u7ef4\u5d4c\u5165\u7684\u9650\u5236\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u56fe\u5206\u6790\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2602.23952", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23952", "abs": "https://arxiv.org/abs/2602.23952", "authors": ["Yuyang Hong", "Jiaqi Gu", "Yujin Lou", "Lubin Fan", "Qi Yang", "Ying Wang", "Kun Ding", "Yue Wu", "Shiming Xiang", "Jieping Ye"], "title": "CC-VQA: Conflict- and Correlation-Aware Method for Mitigating Knowledge Conflict in Knowledge-Based Visual Question Answering", "comment": "Accepted by CVPR2026", "summary": "Knowledge-based visual question answering (KB-VQA) demonstrates significant potential for handling knowledge-intensive tasks. However, conflicts arise between static parametric knowledge in vision language models (VLMs) and dynamically retrieved information due to the static model knowledge from pre-training. The outputs either ignore retrieved contexts or exhibit inconsistent integration with parametric knowledge, posing substantial challenges for KB-VQA. Current knowledge conflict mitigation methods primarily adapted from language-based approaches, focusing on context-level conflicts through engineered prompting strategies or context-aware decoding mechanisms. However, these methods neglect the critical role of visual information in conflicts and suffer from redundant retrieved contexts, which impair accurate conflict identification and effective mitigation. To address these limitations, we propose \\textbf{CC-VQA}: a novel training-free, conflict- and correlation-aware method for KB-VQA. Our method comprises two core components: (1) Vision-Centric Contextual Conflict Reasoning, which performs visual-semantic conflict analysis across internal and external knowledge contexts; and (2) Correlation-Guided Encoding and Decoding, featuring positional encoding compression for low-correlation statements and adaptive decoding using correlation-weighted conflict scoring. Extensive evaluations on E-VQA, InfoSeek, and OK-VQA benchmarks demonstrate that CC-VQA achieves state-of-the-art performance, yielding absolute accuracy improvements of 3.3\\% to 6.4\\% compared to existing methods. Code is available at https://github.com/cqu-student/CC-VQA.", "AI": {"tldr": "\u63d0\u51faCC-VQA\u65b9\u6cd5\uff0c\u901a\u8fc7\u89c6\u89c9\u4e2d\u5fc3\u51b2\u7a81\u63a8\u7406\u548c\u76f8\u5173\u6027\u5f15\u5bfc\u7f16\u89e3\u7801\uff0c\u89e3\u51b3KB-VQA\u4e2d\u9759\u6001\u6a21\u578b\u77e5\u8bc6\u4e0e\u52a8\u6001\u68c0\u7d22\u4fe1\u606f\u4e4b\u95f4\u7684\u51b2\u7a81\u95ee\u9898\uff0c\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u63d0\u5347\u6027\u80fd\u3002", "motivation": "KB-VQA\u4e2d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u9759\u6001\u53c2\u6570\u77e5\u8bc6\u4e0e\u52a8\u6001\u68c0\u7d22\u4fe1\u606f\u4e4b\u95f4\u5b58\u5728\u51b2\u7a81\uff0c\u5bfc\u81f4\u6a21\u578b\u8981\u4e48\u5ffd\u7565\u68c0\u7d22\u4e0a\u4e0b\u6587\uff0c\u8981\u4e48\u4e0e\u53c2\u6570\u77e5\u8bc6\u6574\u5408\u4e0d\u4e00\u81f4\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4ece\u8bed\u8a00\u89d2\u5ea6\u51fa\u53d1\uff0c\u5ffd\u7565\u4e86\u89c6\u89c9\u4fe1\u606f\u5728\u51b2\u7a81\u4e2d\u7684\u5173\u952e\u4f5c\u7528\uff0c\u4e14\u68c0\u7d22\u4e0a\u4e0b\u6587\u5197\u4f59\u5f71\u54cd\u51b2\u7a81\u8bc6\u522b\u548c\u7f13\u89e3\u6548\u679c\u3002", "method": "\u63d0\u51faCC-VQA\u65b9\u6cd5\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) \u89c6\u89c9\u4e2d\u5fc3\u4e0a\u4e0b\u6587\u51b2\u7a81\u63a8\u7406\uff0c\u5728\u5185\u90e8\u548c\u5916\u90e8\u77e5\u8bc6\u4e0a\u4e0b\u6587\u4e2d\u8fdb\u884c\u89c6\u89c9\u8bed\u4e49\u51b2\u7a81\u5206\u6790\uff1b2) \u76f8\u5173\u6027\u5f15\u5bfc\u7f16\u7801\u548c\u89e3\u7801\uff0c\u5305\u62ec\u5bf9\u4f4e\u76f8\u5173\u6027\u8bed\u53e5\u7684\u4f4d\u7f6e\u7f16\u7801\u538b\u7f29\uff0c\u4ee5\u53ca\u4f7f\u7528\u76f8\u5173\u6027\u52a0\u6743\u51b2\u7a81\u8bc4\u5206\u7684\u81ea\u9002\u5e94\u89e3\u7801\u3002", "result": "\u5728E-VQA\u3001InfoSeek\u548cOK-VQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCC-VQA\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u7edd\u5bf9\u51c6\u786e\u7387\u63d0\u53473.3%\u52306.4%\u3002", "conclusion": "CC-VQA\u901a\u8fc7\u89c6\u89c9\u4e2d\u5fc3\u51b2\u7a81\u5206\u6790\u548c\u76f8\u5173\u6027\u5f15\u5bfc\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86KB-VQA\u4e2d\u7684\u77e5\u8bc6\u51b2\u7a81\u95ee\u9898\uff0c\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u4e3a\u77e5\u8bc6\u5bc6\u96c6\u578b\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.23953", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23953", "abs": "https://arxiv.org/abs/2602.23953", "authors": ["Caner Beldek", "Emre Sariyildiz", "Son Lam Phung", "Gursel Alici"], "title": "GDA-YOLO11: Amodal Instance Segmentation for Occlusion-Robust Robotic Fruit Harvesting", "comment": "9 pages, journal pre-print", "summary": "Occlusion remains a critical challenge in robotic fruit harvesting, as undetected or inaccurately localised fruits often results in substantial crop losses. To mitigate this issue, we propose a harvesting framework using a new amodal segmentation model, GDA-YOLO11, which incorporates architectural improvements and an updated asymmetric mask loss. The proposed model is trained on a modified version of a public citrus dataset and evaluated on both the base dataset and occlusion-sensitive subsets with varying occlusion levels. Within the framework, full fruit masks, including invisible regions, are inferred by GDA-YOLO11, and picking points are subsequently estimated using the Euclidean distance transform. These points are then projected into 3D coordinates for robotic harvesting execution. Experiments were conducted using real citrus fruits in a controlled environment simulating occlusion scenarios. Notably, to the best of our knowledge, this study provides the first practical demonstration of amodal instance segmentation in robotic fruit harvesting. GDA-YOLO11 achieves a precision of 0.844, recall of 0.846, mAP@50 of 0.914, and mAP@50:95 of 0.636, outperforming YOLO11n by 5.1%, 1.3%, and 1.0% in precision, mAP@50, and mAP@50:95, respectively. The framework attains harvesting success rates of 92.59%, 85.18%, 48.14%, and 22.22% at zero to high occlusion levels, improving success by 3.5% under medium and high occlusion. These findings demonstrate that GDA-YOLO11 enhances occlusion robust segmentation and streamlines perception-to-action integration, paving the way for more reliable autonomous systems in agriculture.", "AI": {"tldr": "\u63d0\u51faGDA-YOLO11\u6a21\u578b\u7528\u4e8e\u6c34\u679c\u91c7\u6458\u4e2d\u7684\u906e\u6321\u5904\u7406\uff0c\u901a\u8fc7\u975e\u6a21\u6001\u5206\u5272\u548c\u6b27\u51e0\u91cc\u5f97\u8ddd\u79bb\u53d8\u6362\u5b9e\u73b0\u906e\u6321\u533a\u57df\u7684\u6c34\u679c\u5b9a\u4f4d\uff0c\u5728\u67d1\u6a58\u6570\u636e\u96c6\u4e0a\u6027\u80fd\u4f18\u4e8eYOLO11n\uff0c\u5728\u6a21\u62df\u906e\u6321\u73af\u5883\u4e2d\u53d6\u5f97\u826f\u597d\u91c7\u6458\u6210\u529f\u7387\u3002", "motivation": "\u906e\u6321\u662f\u673a\u5668\u4eba\u6c34\u679c\u91c7\u6458\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u672a\u68c0\u6d4b\u6216\u5b9a\u4f4d\u4e0d\u51c6\u786e\u7684\u6c34\u679c\u4f1a\u5bfc\u81f4\u5927\u91cf\u4f5c\u7269\u635f\u5931\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u906e\u6321\u6c34\u679c\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u7684\u611f\u77e5\u7cfb\u7edf\u3002", "method": "\u63d0\u51fa\u57fa\u4e8eGDA-YOLO11\u7684\u91c7\u6458\u6846\u67b6\uff0c\u8be5\u6a21\u578b\u5305\u542b\u67b6\u6784\u6539\u8fdb\u548c\u66f4\u65b0\u7684\u975e\u5bf9\u79f0\u63a9\u7801\u635f\u5931\u3002\u4f7f\u7528\u6539\u8fdb\u7684\u516c\u5171\u67d1\u6a58\u6570\u636e\u96c6\u8bad\u7ec3\uff0c\u901a\u8fc7\u975e\u6a21\u6001\u5b9e\u4f8b\u5206\u5272\u63a8\u65ad\u5b8c\u6574\u6c34\u679c\u63a9\u7801\uff08\u5305\u62ec\u4e0d\u53ef\u89c1\u533a\u57df\uff09\uff0c\u7136\u540e\u4f7f\u7528\u6b27\u51e0\u91cc\u5f97\u8ddd\u79bb\u53d8\u6362\u4f30\u8ba1\u91c7\u6458\u70b9\uff0c\u6700\u540e\u5c06\u70b9\u6295\u5f71\u52303D\u5750\u6807\u8fdb\u884c\u673a\u5668\u4eba\u91c7\u6458\u6267\u884c\u3002", "result": "GDA-YOLO11\u8fbe\u5230\u7cbe\u5ea60.844\u3001\u53ec\u56de\u73870.846\u3001mAP@50\u4e3a0.914\u3001mAP@50:95\u4e3a0.636\uff0c\u5728\u7cbe\u5ea6\u3001mAP@50\u548cmAP@50:95\u4e0a\u5206\u522b\u6bd4YOLO11n\u63d0\u53475.1%\u30011.3%\u548c1.0%\u3002\u6846\u67b6\u5728\u96f6\u5230\u9ad8\u906e\u6321\u6c34\u5e73\u4e0b\u7684\u91c7\u6458\u6210\u529f\u7387\u5206\u522b\u4e3a92.59%\u300185.18%\u300148.14%\u548c22.22%\uff0c\u5728\u4e2d\u9ad8\u906e\u6321\u4e0b\u6210\u529f\u7387\u63d0\u53473.5%\u3002", "conclusion": "GDA-YOLO11\u589e\u5f3a\u4e86\u906e\u6321\u9c81\u68d2\u6027\u5206\u5272\uff0c\u7b80\u5316\u4e86\u611f\u77e5\u5230\u52a8\u4f5c\u7684\u96c6\u6210\uff0c\u4e3a\u519c\u4e1a\u4e2d\u66f4\u53ef\u9760\u7684\u81ea\u4e3b\u7cfb\u7edf\u94fa\u5e73\u4e86\u9053\u8def\u3002\u8fd9\u662f\u9996\u4e2a\u5728\u673a\u5668\u4eba\u6c34\u679c\u91c7\u6458\u4e2d\u5b9e\u9645\u6f14\u793a\u975e\u6a21\u6001\u5b9e\u4f8b\u5206\u5272\u7684\u7814\u7a76\u3002"}}
{"id": "2602.24083", "categories": ["cs.LG", "math.PR", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.24083", "abs": "https://arxiv.org/abs/2602.24083", "authors": ["Xinlong Du", "Harsha Honnappa", "Vinayak Rao"], "title": "Neural Diffusion Intensity Models for Point Process Data", "comment": null, "summary": "Cox processes model overdispersed point process data via a latent stochastic intensity, but both nonparametric estimation of the intensity model and posterior inference over intensity paths are typically intractable, relying on expensive MCMC methods. We introduce Neural Diffusion Intensity Models, a variational framework for Cox processes driven by neural SDEs. Our key theoretical result, based on enlargement of filtrations, shows that conditioning on point process observations preserves the diffusion structure of the latent intensity with an explicit drift correction. This guarantees the variational family contains the true posterior, so that ELBO maximization coincides with maximum likelihood estimation under sufficient model capacity. We design an amortized encoder architecture that maps variable-length event sequences to posterior intensity paths by simulating the drift-corrected SDE, replacing repeated MCMC runs with a single forward pass. Experiments on synthetic and real-world data demonstrate accurate recovery of latent intensity dynamics and posterior paths, with orders-of-magnitude speedups over MCMC-based methods.", "AI": {"tldr": "\u63d0\u51faNeural Diffusion Intensity Models\uff0c\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecfSDE\u7684\u53d8\u5206\u6846\u67b6\uff0c\u7528\u4e8eCox\u8fc7\u7a0b\uff0c\u901a\u8fc7\u6269\u5927\u6ee4\u6ce2\u7406\u8bba\u4fdd\u8bc1\u53d8\u5206\u65cf\u5305\u542b\u771f\u5b9e\u540e\u9a8c\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u540e\u9a8c\u63a8\u65ad\u3002", "motivation": "Cox\u8fc7\u7a0b\u901a\u8fc7\u6f5c\u5728\u968f\u673a\u5f3a\u5ea6\u5efa\u6a21\u8fc7\u5206\u6563\u70b9\u8fc7\u7a0b\u6570\u636e\uff0c\u4f46\u5f3a\u5ea6\u6a21\u578b\u7684\u975e\u53c2\u6570\u4f30\u8ba1\u548c\u5f3a\u5ea6\u8def\u5f84\u7684\u540e\u9a8c\u63a8\u65ad\u901a\u5e38\u96be\u4ee5\u5904\u7406\uff0c\u4f9d\u8d56\u4e8e\u6602\u8d35\u7684MCMC\u65b9\u6cd5\u3002", "method": "\u5f15\u5165Neural Diffusion Intensity Models\uff0c\u57fa\u4e8e\u795e\u7ecfSDE\u7684\u53d8\u5206\u6846\u67b6\u3002\u5173\u952e\u7406\u8bba\u57fa\u4e8e\u6269\u5927\u6ee4\u6ce2\uff0c\u8bc1\u660e\u5728\u70b9\u8fc7\u7a0b\u89c2\u6d4b\u6761\u4ef6\u4e0b\uff0c\u6f5c\u5728\u5f3a\u5ea6\u4fdd\u6301\u6269\u6563\u7ed3\u6784\u5e76\u5177\u6709\u663e\u5f0f\u6f02\u79fb\u4fee\u6b63\u3002\u8bbe\u8ba1\u644a\u9500\u7f16\u7801\u5668\u67b6\u6784\uff0c\u5c06\u53d8\u957f\u4e8b\u4ef6\u5e8f\u5217\u6620\u5c04\u5230\u540e\u9a8c\u5f3a\u5ea6\u8def\u5f84\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e0a\u51c6\u786e\u6062\u590d\u6f5c\u5728\u5f3a\u5ea6\u52a8\u6001\u548c\u540e\u9a8c\u8def\u5f84\uff0c\u76f8\u6bd4\u57fa\u4e8eMCMC\u7684\u65b9\u6cd5\u83b7\u5f97\u6570\u91cf\u7ea7\u7684\u52a0\u901f\u3002", "conclusion": "\u63d0\u51fa\u7684\u53d8\u5206\u6846\u67b6\u4e3aCox\u8fc7\u7a0b\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u540e\u9a8c\u63a8\u65ad\u65b9\u6cd5\uff0c\u901a\u8fc7\u7406\u8bba\u4fdd\u8bc1\u53d8\u5206\u65cf\u5305\u542b\u771f\u5b9e\u540e\u9a8c\uff0c\u5b9e\u73b0\u5355\u6b21\u524d\u5411\u4f20\u64ad\u66ff\u4ee3\u91cd\u590dMCMC\u8fd0\u884c\u3002"}}
{"id": "2602.23956", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23956", "abs": "https://arxiv.org/abs/2602.23956", "authors": ["Qianxun Xu", "Chenxi Song", "Yujun Cai", "Chi Zhang"], "title": "SwitchCraft: Training-Free Multi-Event Video Generation with Attention Controls", "comment": "CVPR 2026", "summary": "Recent advances in text-to-video diffusion models have enabled high-fidelity and temporally coherent videos synthesis. However, current models are predominantly optimized for single-event generation. When handling multi-event prompts, without explicit temporal grounding, such models often produce blended or collapsed scenes that break the intended narrative. To address this limitation, we present SwitchCraft, a training-free framework for multi-event video generation. Our key insight is that uniform prompt injection across time ignores the correspondence between events and frames. To this end, we introduce Event-Aligned Query Steering (EAQS), which steers frame-level attention to align with relevant event prompts. Furthermore, we propose Auto-Balance Strength Solver (ABSS), which adaptively balances steering strength to preserve temporal consistency and visual fidelity. Extensive experiments demonstrate that SwitchCraft substantially improves prompt alignment, event clarity, and scene consistency compared with existing baselines, offering a simple yet effective solution for multi-event video generation.", "AI": {"tldr": "SwitchCraft\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u591a\u4e8b\u4ef6\u89c6\u9891\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u4e8b\u4ef6\u5bf9\u9f50\u67e5\u8be2\u5f15\u5bfc\u548c\u81ea\u9002\u5e94\u5e73\u8861\u5f3a\u5ea6\u6c42\u89e3\u5668\u89e3\u51b3\u73b0\u6709\u6587\u672c\u5230\u89c6\u9891\u6269\u6563\u6a21\u578b\u5728\u591a\u4e8b\u4ef6\u63d0\u793a\u4e0b\u4ea7\u751f\u6df7\u5408\u6216\u5d29\u6e83\u573a\u666f\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u6587\u672c\u5230\u89c6\u9891\u6269\u6563\u6a21\u578b\u4e3b\u8981\u9488\u5bf9\u5355\u4e8b\u4ef6\u751f\u6210\u8fdb\u884c\u4f18\u5316\uff0c\u5728\u5904\u7406\u591a\u4e8b\u4ef6\u63d0\u793a\u65f6\uff0c\u7531\u4e8e\u7f3a\u4e4f\u660e\u786e\u7684\u65f6\u95f4\u5b9a\u4f4d\uff0c\u5f80\u5f80\u4f1a\u4ea7\u751f\u6df7\u5408\u6216\u5d29\u6e83\u7684\u573a\u666f\uff0c\u7834\u574f\u4e86\u9884\u671f\u7684\u53d9\u4e8b\u7ed3\u6784\u3002", "method": "\u63d0\u51faSwitchCraft\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) \u4e8b\u4ef6\u5bf9\u9f50\u67e5\u8be2\u5f15\u5bfc(EAQS)\uff0c\u5f15\u5bfc\u5e27\u7ea7\u6ce8\u610f\u529b\u4e0e\u76f8\u5173\u4e8b\u4ef6\u63d0\u793a\u5bf9\u9f50\uff1b2) \u81ea\u9002\u5e94\u5e73\u8861\u5f3a\u5ea6\u6c42\u89e3\u5668(ABSS)\uff0c\u81ea\u9002\u5e94\u5e73\u8861\u5f15\u5bfc\u5f3a\u5ea6\u4ee5\u4fdd\u6301\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u89c6\u89c9\u4fdd\u771f\u5ea6\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cSwitchCraft\u5728\u63d0\u793a\u5bf9\u9f50\u3001\u4e8b\u4ef6\u6e05\u6670\u5ea6\u548c\u573a\u666f\u4e00\u81f4\u6027\u65b9\u9762\u76f8\u6bd4\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "SwitchCraft\u4e3a\u591a\u4e8b\u4ef6\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7b80\u5355\u800c\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u663e\u8457\u6539\u5584\u591a\u4e8b\u4ef6\u63d0\u793a\u4e0b\u7684\u89c6\u9891\u751f\u6210\u8d28\u91cf\u3002"}}
{"id": "2602.23959", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23959", "abs": "https://arxiv.org/abs/2602.23959", "authors": ["Kesen Zhao", "Beier Zhu", "Junbao Zhou", "Xingyu Zhu", "Zhongqi Yue", "Hanwang Zhang"], "title": "Thinking with Images as Continuous Actions: Numerical Visual Chain-of-Thought", "comment": null, "summary": "Recent multimodal large language models (MLLMs) increasingly rely on visual chain-of-thought to perform region-grounded reasoning over images. However, existing approaches ground regions via either textified coordinates-causing modality mismatch and semantic fragmentation or fixed-granularity patches that both limit precise region selection and often require non-trivial architectural changes. In this paper, we propose Numerical Visual Chain-of-Thought (NV-CoT), a framework that enables MLLMs to reason over images using continuous numerical coordinates. NV-CoT expands the MLLM action space from discrete vocabulary tokens to a continuous Euclidean space, allowing models to directly generate bounding-box coordinates as actions with only minimal architectural modification. The framework supports both supervised fine-tuning and reinforcement learning. In particular, we replace categorical token policies with a Gaussian (or Laplace) policy over coordinates and introduce stochasticity via reparameterized sampling, making NV-CoT fully compatible with GRPO-style policy optimization. Extensive experiments on three benchmarks against eight representative visual reasoning baselines demonstrate that NV-CoT significantly improves localization precision and final answer accuracy, while also accelerating training convergence, validating the effectiveness of continuous-action visual reasoning in MLLMs. The code is available in https://github.com/kesenzhao/NV-CoT.", "AI": {"tldr": "NV-CoT\u662f\u4e00\u4e2a\u8ba9\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4f7f\u7528\u8fde\u7eed\u6570\u503c\u5750\u6807\u8fdb\u884c\u89c6\u89c9\u63a8\u7406\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u52a8\u4f5c\u7a7a\u95f4\u4ece\u79bb\u6563\u8bcd\u6c47\u6269\u5c55\u5230\u8fde\u7eed\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u7b54\u6848\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u6a21\u6001\u4e0d\u5339\u914d\u3001\u8bed\u4e49\u788e\u7247\u5316\u6216\u7c92\u5ea6\u56fa\u5b9a\u7b49\u95ee\u9898\uff0c\u9650\u5236\u4e86\u7cbe\u786e\u533a\u57df\u9009\u62e9\u548c\u63a8\u7406\u80fd\u529b\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u81ea\u7136\u3001\u66f4\u7cbe\u786e\u7684\u89c6\u89c9\u63a8\u7406\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u6570\u503c\u89c6\u89c9\u601d\u7ef4\u94fe\u6846\u67b6\uff0c\u5c06MLLM\u52a8\u4f5c\u7a7a\u95f4\u6269\u5c55\u5230\u8fde\u7eed\u5750\u6807\u7a7a\u95f4\uff0c\u4f7f\u7528\u9ad8\u65af\u6216\u62c9\u666e\u62c9\u65af\u7b56\u7565\u751f\u6210\u8fb9\u754c\u6846\u5750\u6807\uff0c\u652f\u6301\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u4e0eGRPO\u98ce\u683c\u7b56\u7565\u4f18\u5316\u5b8c\u5168\u517c\u5bb9\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5bf9\u6bd4\u516b\u4e2a\u4ee3\u8868\u6027\u89c6\u89c9\u63a8\u7406\u57fa\u7ebf\uff0cNV-CoT\u663e\u8457\u63d0\u9ad8\u4e86\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u6700\u7ec8\u7b54\u6848\u51c6\u786e\u6027\uff0c\u540c\u65f6\u52a0\u901f\u4e86\u8bad\u7ec3\u6536\u655b\u3002", "conclusion": "\u8fde\u7eed\u52a8\u4f5c\u89c6\u89c9\u63a8\u7406\u5728\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u5177\u6709\u6709\u6548\u6027\uff0c\u4e3a\u89c6\u89c9\u63a8\u7406\u63d0\u4f9b\u4e86\u66f4\u7cbe\u786e\u3001\u66f4\u81ea\u7136\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.24146", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.24146", "abs": "https://arxiv.org/abs/2602.24146", "authors": ["Zitian Li", "Wang Chi Cheung"], "title": "Learning with a Budget: Identifying the Best Arm with Resource Constraints", "comment": "A preliminary version of this work, titled 'Best Arm Identification with Resource Constraints,' was presented at the 27th International Conference on Artificial Intelligence and Statistics (AISTATS 2024). This manuscript extends the original conference paper by providing improved theoretical results and more generalized conclusions, aiming for future journal submission. arXiv admin note: substantial text overlap with arXiv:2402.19090", "summary": "In many applications, evaluating the effectiveness of different alternatives comes with varying costs or resource usage. Motivated by such heterogeneity, we study the Best Arm Identification with Resource Constraints (BAIwRC) problem, where an agent seeks to identify the best alternative (aka arm) in the presence of resource constraints. Each arm pull consumes one or more types of limited resources. We make two key contributions. First, we propose the Successive Halving with Resource Rationing (SH-RR) algorithm, which integrates resource-aware allocation into the classical successive halving framework on best arm identification. The SH-RR algorithm unifies the theoretical analysis for both the stochastic and deterministic consumption settings, with a new \\textit{effective consumption measure", "AI": {"tldr": "\u63d0\u51fa\u4e86\u8d44\u6e90\u7ea6\u675f\u4e0b\u7684\u6700\u4f73\u81c2\u8bc6\u522b\u95ee\u9898(BAIwRC)\uff0c\u5f00\u53d1\u4e86SH-RR\u7b97\u6cd5\uff0c\u7edf\u4e00\u4e86\u968f\u673a\u548c\u786e\u5b9a\u6027\u8d44\u6e90\u6d88\u8017\u7684\u7406\u8bba\u5206\u6790\u6846\u67b6", "motivation": "\u5728\u8bb8\u591a\u5e94\u7528\u4e2d\uff0c\u8bc4\u4f30\u4e0d\u540c\u66ff\u4ee3\u65b9\u6848\u7684\u6548\u679c\u4f34\u968f\u7740\u4e0d\u540c\u7684\u6210\u672c\u6216\u8d44\u6e90\u4f7f\u7528\u3002\u53d7\u8fd9\u79cd\u5f02\u8d28\u6027\u7684\u542f\u53d1\uff0c\u7814\u7a76\u8d44\u6e90\u7ea6\u675f\u4e0b\u7684\u6700\u4f73\u81c2\u8bc6\u522b\u95ee\u9898\uff0c\u5176\u4e2d\u6bcf\u4e2a\u81c2\u62c9\u52a8\u6d88\u8017\u4e00\u79cd\u6216\u591a\u79cd\u6709\u9650\u8d44\u6e90\u7c7b\u578b", "method": "\u63d0\u51fa\u4e86Successive Halving with Resource Rationing (SH-RR)\u7b97\u6cd5\uff0c\u5c06\u8d44\u6e90\u611f\u77e5\u5206\u914d\u96c6\u6210\u5230\u7ecf\u5178\u7684\u6210\u529f\u51cf\u534a\u6846\u67b6\u4e2d\uff0c\u4f7f\u7528\u65b0\u7684\u6709\u6548\u6d88\u8017\u5ea6\u91cf\u7edf\u4e00\u968f\u673a\u548c\u786e\u5b9a\u6027\u6d88\u8017\u8bbe\u7f6e\u7684\u7406\u8bba\u5206\u6790", "result": "SH-RR\u7b97\u6cd5\u5728\u8d44\u6e90\u7ea6\u675f\u4e0b\u6709\u6548\u8bc6\u522b\u6700\u4f73\u81c2\uff0c\u901a\u8fc7\u65b0\u7684\u6709\u6548\u6d88\u8017\u5ea6\u91cf\u7edf\u4e00\u4e86\u968f\u673a\u548c\u786e\u5b9a\u6027\u8d44\u6e90\u6d88\u8017\u7684\u7406\u8bba\u5206\u6790\u6846\u67b6", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u8d44\u6e90\u7ea6\u675f\u4e0b\u7684\u6700\u4f73\u81c2\u8bc6\u522b\u95ee\u9898\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u7b97\u6cd5\u6846\u67b6\u548c\u7406\u8bba\u5206\u6790\uff0c\u9002\u7528\u4e8e\u5177\u6709\u5f02\u8d28\u6027\u8d44\u6e90\u6d88\u8017\u7684\u5b9e\u9645\u5e94\u7528\u573a\u666f"}}
{"id": "2602.23980", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23980", "abs": "https://arxiv.org/abs/2602.23980", "authors": ["Tianxiang Du", "Hulingxiao He", "Yuxin Peng"], "title": "Venus: Benchmarking and Empowering Multimodal Large Language Models for Aesthetic Guidance and Cropping", "comment": "Accepted by CVPR 2026", "summary": "The widespread use of smartphones has made photography ubiquitous, yet a clear gap remains between ordinary users and professional photographers, who can identify aesthetic issues and provide actionable shooting guidance during capture. We define this capability as aesthetic guidance (AG) -- an essential but largely underexplored domain in computational aesthetics. Existing multimodal large language models (MLLMs) primarily offer overly positive feedback, failing to identify issues or provide actionable guidance. Without AG capability, they cannot effectively identify distracting regions or optimize compositional balance, thus also struggling in aesthetic cropping, which aims to refine photo composition through reframing after capture. To address this, we introduce AesGuide, the first large-scale AG dataset and benchmark with 10,748 photos annotated with aesthetic scores, analyses, and guidance. Building upon it, we propose Venus, a two-stage framework that first empowers MLLMs with AG capability through progressively complex aesthetic questions and then activates their aesthetic cropping power via CoT-based rationales. Extensive experiments show that Venus substantially improves AG capability and achieves state-of-the-art (SOTA) performance in aesthetic cropping, enabling interpretable and interactive aesthetic refinement across both stages of photo creation. Code is available at https://github.com/PKU-ICST-MIPL/Venus_CVPR2026.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86AesGuide\u6570\u636e\u96c6\u548cVenus\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u667a\u80fd\u624b\u673a\u6444\u5f71\u4e2d\u666e\u901a\u7528\u6237\u4e0e\u4e13\u4e1a\u6444\u5f71\u5e08\u4e4b\u95f4\u7684\u7f8e\u5b66\u6307\u5bfc\u5dee\u8ddd\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u65b9\u6cd5\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7f8e\u5b66\u6307\u5bfc\u80fd\u529b\uff0c\u5e76\u5728\u7f8e\u5b66\u88c1\u526a\u4efb\u52a1\u4e0a\u53d6\u5f97SOTA\u6027\u80fd\u3002", "motivation": "\u667a\u80fd\u624b\u673a\u6444\u5f71\u666e\u53ca\uff0c\u4f46\u666e\u901a\u7528\u6237\u4e0e\u4e13\u4e1a\u6444\u5f71\u5e08\u4e4b\u95f4\u5b58\u5728\u660e\u663e\u5dee\u8ddd\uff0c\u540e\u8005\u80fd\u5728\u62cd\u6444\u8fc7\u7a0b\u4e2d\u8bc6\u522b\u7f8e\u5b66\u95ee\u9898\u5e76\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u6307\u5bfc\u3002\u73b0\u6709\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e3b\u8981\u63d0\u4f9b\u8fc7\u4e8e\u79ef\u6781\u7684\u53cd\u9988\uff0c\u65e0\u6cd5\u8bc6\u522b\u95ee\u9898\u6216\u63d0\u4f9b\u53ef\u64cd\u4f5c\u6307\u5bfc\uff0c\u7f3a\u4e4f\u7f8e\u5b66\u6307\u5bfc\u80fd\u529b\u3002", "method": "1. \u5f15\u5165AesGuide\u6570\u636e\u96c6\uff1a\u5305\u542b10,748\u5f20\u7167\u7247\uff0c\u6807\u6ce8\u4e86\u7f8e\u5b66\u8bc4\u5206\u3001\u5206\u6790\u548c\u6307\u5bfc\uff1b2. \u63d0\u51faVenus\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u901a\u8fc7\u6e10\u8fdb\u590d\u6742\u7684\u7f8e\u5b66\u95ee\u9898\u8d4b\u4e88MLLMs\u7f8e\u5b66\u6307\u5bfc\u80fd\u529b\uff1b\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u57fa\u4e8eCoT\u7684\u63a8\u7406\u6fc0\u6d3b\u5176\u7f8e\u5b66\u88c1\u526a\u80fd\u529b\u3002", "result": "Venus\u663e\u8457\u63d0\u5347\u4e86\u7f8e\u5b66\u6307\u5bfc\u80fd\u529b\uff0c\u5e76\u5728\u7f8e\u5b66\u88c1\u526a\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5b9e\u73b0\u4e86\u5728\u7167\u7247\u521b\u4f5c\u4e24\u4e2a\u9636\u6bb5\u7684\u53ef\u89e3\u91ca\u548c\u4ea4\u4e92\u5f0f\u7f8e\u5b66\u4f18\u5316\u3002", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86\u8ba1\u7b97\u7f8e\u5b66\u4e2d\u7f8e\u5b66\u6307\u5bfc\u8fd9\u4e00\u91cd\u8981\u4f46\u672a\u5145\u5206\u63a2\u7d22\u7684\u9886\u57df\uff0c\u901a\u8fc7AesGuide\u6570\u636e\u96c6\u548cVenus\u6846\u67b6\uff0c\u4f7fMLLMs\u80fd\u591f\u63d0\u4f9b\u4e13\u4e1a\u7684\u7f8e\u5b66\u6307\u5bfc\u548c\u88c1\u526a\u5efa\u8bae\uff0c\u7f29\u5c0f\u4e86\u666e\u901a\u7528\u6237\u4e0e\u4e13\u4e1a\u6444\u5f71\u5e08\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2602.24178", "categories": ["cs.LG", "cs.CC"], "pdf": "https://arxiv.org/pdf/2602.24178", "abs": "https://arxiv.org/abs/2602.24178", "authors": ["Adam R. Klivans", "Konstantinos Stavropoulos", "Arsen Vasilyan"], "title": "Sandwiching Polynomials for Geometric Concepts with Low Intrinsic Dimension", "comment": "30 pages", "summary": "Recent work has shown the surprising power of low-degree sandwiching polynomial approximators in the context of challenging learning settings such as learning with distribution shift, testable learning, and learning with contamination. A pair of sandwiching polynomials approximate a target function in expectation while also providing pointwise upper and lower bounds on the function's values. In this paper, we give a new method for constructing low-degree sandwiching polynomials that yield greatly improved degree bounds for several fundamental function classes and marginal distributions. In particular, we obtain degree $\\mathrm{poly}(k)$ sandwiching polynomials for functions of $k$ halfspaces under the Gaussian distribution, improving exponentially over the prior $2^{O(k)}$ bound. More broadly, our approach applies to function classes that are low-dimensional and have smooth boundary.\n  In contrast to prior work, our proof is relatively simple and directly uses the smoothness of the target function's boundary to construct sandwiching Lipschitz functions, which are amenable to results from high-dimensional approximation theory. For low-dimensional polynomial threshold functions (PTFs) with respect to Gaussians, we obtain doubly exponential improvements without applying the FT-mollification method of Kane used in the best previous result.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6784\u9020\u4f4e\u9636\u4e09\u660e\u6cbb\u591a\u9879\u5f0f\u7684\u65b0\u65b9\u6cd5\uff0c\u663e\u8457\u6539\u8fdb\u4e86\u591a\u4e2a\u57fa\u672c\u51fd\u6570\u7c7b\u5728\u8fb9\u9645\u5206\u5e03\u4e0b\u7684\u9636\u6570\u754c\u9650\uff0c\u7279\u522b\u662f\u5c06k\u4e2a\u534a\u7a7a\u95f4\u7684\u51fd\u6570\u4ece\u6307\u6570\u7ea7\u6539\u8fdb\u5230\u591a\u9879\u5f0f\u7ea7\u3002", "motivation": "\u5148\u524d\u7814\u7a76\u8868\u660e\u4f4e\u9636\u4e09\u660e\u6cbb\u591a\u9879\u5f0f\u8fd1\u4f3c\u5668\u5728\u5206\u5e03\u504f\u79fb\u5b66\u4e60\u3001\u53ef\u6d4b\u8bd5\u5b66\u4e60\u548c\u6c61\u67d3\u5b66\u4e60\u7b49\u6311\u6218\u6027\u5b66\u4e60\u8bbe\u7f6e\u4e2d\u5177\u6709\u5f3a\u5927\u80fd\u529b\u3002\u4e09\u660e\u6cbb\u591a\u9879\u5f0f\u901a\u8fc7\u63d0\u4f9b\u76ee\u6807\u51fd\u6570\u7684\u70b9\u503c\u4e0a\u4e0b\u754c\u6765\u8fd1\u4f3c\u671f\u671b\u503c\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u7684\u9636\u6570\u754c\u9650\u4e0d\u591f\u7406\u60f3\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u76f8\u5bf9\u7b80\u5355\u7684\u65b0\u65b9\u6cd5\uff0c\u76f4\u63a5\u5229\u7528\u76ee\u6807\u51fd\u6570\u8fb9\u754c\u7684\u5149\u6ed1\u6027\u6765\u6784\u9020\u4e09\u660e\u6cbbLipschitz\u51fd\u6570\uff0c\u8fd9\u4e9b\u51fd\u6570\u9002\u7528\u4e8e\u9ad8\u7ef4\u903c\u8fd1\u7406\u8bba\u7684\u7ed3\u679c\u3002\u8be5\u65b9\u6cd5\u7279\u522b\u9002\u7528\u4e8e\u4f4e\u7ef4\u4e14\u5177\u6709\u5149\u6ed1\u8fb9\u754c\u7684\u51fd\u6570\u7c7b\u3002", "result": "\u5bf9\u4e8e\u9ad8\u65af\u5206\u5e03\u4e0b\u7684k\u4e2a\u534a\u7a7a\u95f4\u51fd\u6570\uff0c\u83b7\u5f97\u4e86poly(k)\u9636\u7684\u4e09\u660e\u6cbb\u591a\u9879\u5f0f\uff0c\u76f8\u6bd4\u4e4b\u524d\u76842^O(k)\u6307\u6570\u7ea7\u754c\u9650\u6709\u6307\u6570\u7ea7\u6539\u8fdb\u3002\u5bf9\u4e8e\u9ad8\u65af\u5206\u5e03\u4e0b\u7684\u4f4e\u7ef4\u591a\u9879\u5f0f\u9608\u503c\u51fd\u6570\uff0c\u83b7\u5f97\u4e86\u53cc\u91cd\u6307\u6570\u7ea7\u6539\u8fdb\uff0c\u4e14\u65e0\u9700\u4f7f\u7528\u5148\u524d\u6700\u4f73\u7ed3\u679c\u4e2d\u7684FT-mollification\u65b9\u6cd5\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u901a\u8fc7\u76f4\u63a5\u5229\u7528\u51fd\u6570\u8fb9\u754c\u7684\u5149\u6ed1\u6027\uff0c\u4e3a\u6784\u9020\u4f4e\u9636\u4e09\u660e\u6cbb\u591a\u9879\u5f0f\u63d0\u4f9b\u4e86\u66f4\u7b80\u5355\u6709\u6548\u7684\u9014\u5f84\uff0c\u663e\u8457\u6539\u8fdb\u4e86\u591a\u4e2a\u91cd\u8981\u51fd\u6570\u7c7b\u7684\u903c\u8fd1\u6548\u7387\uff0c\u4e3a\u6311\u6218\u6027\u5b66\u4e60\u8bbe\u7f6e\u4e2d\u7684\u591a\u9879\u5f0f\u903c\u8fd1\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2602.23996", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23996", "abs": "https://arxiv.org/abs/2602.23996", "authors": ["Kaiwen Zhu", "Quansheng Zeng", "Yuandong Pu", "Shuo Cao", "Xiaohui Li", "Yi Xin", "Qi Qin", "Jiayang Li", "Yu Qiao", "Jinjin Gu", "Yihao Liu"], "title": "Accelerating Masked Image Generation by Learning Latent Controlled Dynamics", "comment": null, "summary": "Masked Image Generation Models (MIGMs) have achieved great success, yet their efficiency is hampered by the multiple steps of bi-directional attention. In fact, there exists notable redundancy in their computation: when sampling discrete tokens, the rich semantics contained in the continuous features are lost. Some existing works attempt to cache the features to approximate future features. However, they exhibit considerable approximation error under aggressive acceleration rates. We attribute this to their limited expressivity and the failure to account for sampling information. To fill this gap, we propose to learn a lightweight model that incorporates both previous features and sampled tokens, and regresses the average velocity field of feature evolution. The model has moderate complexity that suffices to capture the subtle dynamics while keeping lightweight compared to the original base model. We apply our method, MIGM-Shortcut, to two representative MIGM architectures and tasks. In particular, on the state-of-the-art Lumina-DiMOO, it achieves over 4x acceleration of text-to-image generation while maintaining quality, significantly pushing the Pareto frontier of masked image generation. The code and model weights are available at https://github.com/Kaiwen-Zhu/MIGM-Shortcut.", "AI": {"tldr": "\u63d0\u51faMIGM-Shortcut\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u6a21\u578b\u5b66\u4e60\u7279\u5f81\u6f14\u5316\u7684\u5e73\u5747\u901f\u5ea6\u573a\uff0c\u5728\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u5b9e\u73b04\u500d\u52a0\u901f", "motivation": "\u63a9\u7801\u56fe\u50cf\u751f\u6210\u6a21\u578b\u5b58\u5728\u8ba1\u7b97\u5197\u4f59\u95ee\u9898\uff1a\u91c7\u6837\u79bb\u6563\u4ee4\u724c\u65f6\u4f1a\u4e22\u5931\u8fde\u7eed\u7279\u5f81\u7684\u4e30\u5bcc\u8bed\u4e49\uff1b\u73b0\u6709\u7f13\u5b58\u7279\u5f81\u65b9\u6cd5\u5728\u6fc0\u8fdb\u52a0\u901f\u7387\u4e0b\u8fd1\u4f3c\u8bef\u5dee\u8f83\u5927", "method": "\u5b66\u4e60\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u6a21\u578b\uff0c\u7ed3\u5408\u5148\u524d\u7279\u5f81\u548c\u91c7\u6837\u4ee4\u724c\uff0c\u56de\u5f52\u7279\u5f81\u6f14\u5316\u7684\u5e73\u5747\u901f\u5ea6\u573a\uff1b\u6a21\u578b\u590d\u6742\u5ea6\u9002\u4e2d\uff0c\u80fd\u6355\u6349\u5fae\u5999\u52a8\u6001\u540c\u65f6\u4fdd\u6301\u8f7b\u91cf", "result": "\u5728Lumina-DiMOO\u4e0a\u5b9e\u73b0\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u8d85\u8fc74\u500d\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u751f\u6210\u8d28\u91cf\uff0c\u663e\u8457\u63a8\u8fdb\u63a9\u7801\u56fe\u50cf\u751f\u6210\u7684\u5e15\u7d2f\u6258\u524d\u6cbf", "conclusion": "MIGM-Shortcut\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u63a9\u7801\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7684\u8ba1\u7b97\u6548\u7387\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u8d28\u91cf\u7684\u540c\u65f6\u5b9e\u73b0\u663e\u8457\u52a0\u901f"}}
{"id": "2602.24182", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.24182", "abs": "https://arxiv.org/abs/2602.24182", "authors": ["Sikata Sengupta", "Guangyi Liu", "Omer Gottesman", "Joseph W Durham", "Michael Kearns", "Aaron Roth", "Michael Caldara"], "title": "Multi-Objective Reinforcement Learning for Large-Scale Tote Allocation in Human-Robot Collaborative Fulfillment Centers", "comment": null, "summary": "Optimizing the consolidation process in container-based fulfillment centers requires trading off competing objectives such as processing speed, resource usage, and space utilization while adhering to a range of real-world operational constraints. This process involves moving items between containers via a combination of human and robotic workstations to free up space for inbound inventory and increase container utilization. We formulate this problem as a large-scale Multi-Objective Reinforcement Learning (MORL) task with high-dimensional state spaces and dynamic system behavior. Our method builds on recent theoretical advances in solving constrained RL problems via best-response and no-regret dynamics in zero-sum games, enabling principled minimax policy learning. Policy evaluation on realistic warehouse simulations shows that our approach effectively trades off objectives, and we empirically observe that it learns a single policy that simultaneously satisfies all constraints, even if this is not theoretically guaranteed. We further introduce a theoretical framework to handle the problem of error cancellation, where time-averaged solutions display oscillatory behavior. This method returns a single iterate whose Lagrangian value is close to the minimax value of the game. These results demonstrate the promise of MORL in solving complex, high-impact decision-making problems in large-scale industrial systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f18\u5316\u96c6\u88c5\u7bb1\u5f0f\u5c65\u7ea6\u4e2d\u5fc3\u7684\u6574\u5408\u8fc7\u7a0b\uff0c\u5728\u6ee1\u8db3\u5b9e\u9645\u8fd0\u8425\u7ea6\u675f\u7684\u540c\u65f6\u5e73\u8861\u5904\u7406\u901f\u5ea6\u3001\u8d44\u6e90\u4f7f\u7528\u548c\u7a7a\u95f4\u5229\u7528\u7387\u7b49\u7ade\u4e89\u76ee\u6807\u3002", "motivation": "\u96c6\u88c5\u7bb1\u5f0f\u5c65\u7ea6\u4e2d\u5fc3\u7684\u6574\u5408\u8fc7\u7a0b\u9700\u8981\u5728\u5904\u7406\u901f\u5ea6\u3001\u8d44\u6e90\u4f7f\u7528\u548c\u7a7a\u95f4\u5229\u7528\u7387\u7b49\u591a\u4e2a\u7ade\u4e89\u76ee\u6807\u4e4b\u95f4\u8fdb\u884c\u6743\u8861\uff0c\u540c\u65f6\u6ee1\u8db3\u5404\u79cd\u5b9e\u9645\u8fd0\u8425\u7ea6\u675f\u3002\u8fd9\u662f\u4e00\u4e2a\u590d\u6742\u7684\u51b3\u7b56\u95ee\u9898\uff0c\u6d89\u53ca\u4eba\u673a\u534f\u4f5c\u5de5\u4f5c\u7ad9\u4e4b\u95f4\u7684\u7269\u54c1\u79fb\u52a8\uff0c\u65e8\u5728\u91ca\u653e\u5165\u5e93\u5e93\u5b58\u7a7a\u95f4\u5e76\u63d0\u9ad8\u96c6\u88c5\u7bb1\u5229\u7528\u7387\u3002", "method": "\u5c06\u95ee\u9898\u5efa\u6a21\u4e3a\u5927\u89c4\u6a21\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\uff0c\u91c7\u7528\u57fa\u4e8e\u96f6\u548c\u535a\u5f08\u4e2d\u6700\u4f73\u54cd\u5e94\u548c\u65e0\u6094\u52a8\u6001\u7684\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5b9e\u73b0\u539f\u5219\u6027\u7684\u6781\u5c0f\u6781\u5927\u7b56\u7565\u5b66\u4e60\u3002\u540c\u65f6\u5f15\u5165\u4e86\u5904\u7406\u8bef\u5dee\u6d88\u9664\u95ee\u9898\u7684\u7406\u8bba\u6846\u67b6\uff0c\u907f\u514d\u65f6\u95f4\u5e73\u5747\u89e3\u51fa\u73b0\u632f\u8361\u884c\u4e3a\u3002", "result": "\u5728\u73b0\u5b9e\u7684\u4ed3\u5e93\u6a21\u62df\u4e2d\u8fdb\u884c\u7b56\u7565\u8bc4\u4f30\uff0c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u6743\u8861\u591a\u4e2a\u76ee\u6807\u3002\u7ecf\u9a8c\u89c2\u5bdf\u5230\u5b66\u4e60\u5230\u7684\u5355\u4e00\u7b56\u7565\u80fd\u591f\u540c\u65f6\u6ee1\u8db3\u6240\u6709\u7ea6\u675f\u6761\u4ef6\uff0c\u5c3d\u7ba1\u8fd9\u5728\u7406\u8bba\u4e0a\u65e0\u6cd5\u4fdd\u8bc1\u3002\u8be5\u65b9\u6cd5\u8fd4\u56de\u7684\u8fed\u4ee3\u7ed3\u679c\u5176\u62c9\u683c\u6717\u65e5\u503c\u63a5\u8fd1\u535a\u5f08\u7684\u6781\u5c0f\u6781\u5927\u503c\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u8bc1\u660e\u4e86\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u5728\u89e3\u51b3\u5927\u89c4\u6a21\u5de5\u4e1a\u7cfb\u7edf\u4e2d\u590d\u6742\u3001\u9ad8\u5f71\u54cd\u529b\u51b3\u7b56\u95ee\u9898\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u4e3a\u96c6\u88c5\u7bb1\u5c65\u7ea6\u4e2d\u5fc3\u7684\u6574\u5408\u4f18\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.24013", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.24013", "abs": "https://arxiv.org/abs/2602.24013", "authors": ["Gustav Schmidt", "Philipp Berens", "Sarah M\u00fcller"], "title": "Ordinal Diffusion Models for Color Fundus Images", "comment": null, "summary": "It has been suggested that generative image models such as diffusion models can improve performance on clinically relevant tasks by offering deep learning models supplementary training data. However, most conditional diffusion models treat disease stages as independent classes, ignoring the continuous nature of disease progression. This mismatch is problematic in medical imaging because continuous pathological processes are typically only observed through coarse, discrete but ordered labels as in ophthalmology for diabetic retinopathy (DR). We propose an ordinal latent diffusion model for generating color fundus images that explicitly incorporates the ordered structure of DR severity into the generation process. Instead of categorical conditioning, we used a scalar disease representation, enabling a smooth transition between adjacent stages. We evaluated our approach using visual realism metrics and classification-based clinical consistency analysis on the EyePACS dataset. Compared to a standard conditional diffusion model, our model reduced the Fr\u00e9chet inception distance for four of the five DR stages and increased the quadratic weighted $\u03ba$ from 0.79 to 0.87. Furthermore, interpolation experiments showed that the model captured a continuous spectrum of disease progression learned from ordered, coarse class labels.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5e8f\u6570\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u751f\u6210\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\uff08DR\uff09\u7684\u5f69\u8272\u773c\u5e95\u56fe\u50cf\uff0c\u8be5\u6a21\u578b\u5c06DR\u4e25\u91cd\u7a0b\u5ea6\u7684\u987a\u5e8f\u7ed3\u6784\u663e\u5f0f\u5730\u6574\u5408\u5230\u751f\u6210\u8fc7\u7a0b\u4e2d\uff0c\u5b9e\u73b0\u4e86\u76f8\u90bb\u9636\u6bb5\u95f4\u7684\u5e73\u6ed1\u8fc7\u6e21\u3002", "motivation": "\u5f53\u524d\u5927\u591a\u6570\u6761\u4ef6\u6269\u6563\u6a21\u578b\u5c06\u75be\u75c5\u9636\u6bb5\u89c6\u4e3a\u72ec\u7acb\u7c7b\u522b\uff0c\u5ffd\u7565\u4e86\u75be\u75c5\u8fdb\u5c55\u7684\u8fde\u7eed\u6027\u672c\u8d28\u3002\u8fd9\u79cd\u4e0d\u5339\u914d\u5728\u533b\u5b66\u6210\u50cf\u4e2d\u5b58\u5728\u95ee\u9898\uff0c\u56e0\u4e3a\u8fde\u7eed\u7684\u75c5\u7406\u8fc7\u7a0b\u901a\u5e38\u53ea\u80fd\u901a\u8fc7\u7c97\u7c92\u5ea6\u3001\u79bb\u6563\u4f46\u6709\u5e8f\u7684\u6807\u7b7e\u6765\u89c2\u5bdf\uff0c\u5982\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\uff08DR\uff09\u7684\u5206\u7ea7\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5e8f\u6570\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff0c\u4f7f\u7528\u6807\u91cf\u75be\u75c5\u8868\u793a\u4ee3\u66ff\u5206\u7c7b\u6761\u4ef6\uff0c\u5c06DR\u4e25\u91cd\u7a0b\u5ea6\u7684\u987a\u5e8f\u7ed3\u6784\u663e\u5f0f\u5730\u6574\u5408\u5230\u751f\u6210\u8fc7\u7a0b\u4e2d\uff0c\u5b9e\u73b0\u4e86\u76f8\u90bb\u75be\u75c5\u9636\u6bb5\u95f4\u7684\u5e73\u6ed1\u8fc7\u6e21\u3002", "result": "\u5728EyePACS\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u4e0e\u6807\u51c6\u6761\u4ef6\u6269\u6563\u6a21\u578b\u76f8\u6bd4\uff0c\u8be5\u6a21\u578b\u5728\u4e94\u4e2aDR\u9636\u6bb5\u4e2d\u7684\u56db\u4e2a\u9636\u6bb5\u964d\u4f4e\u4e86Fr\u00e9chet\u8d77\u59cb\u8ddd\u79bb\uff0c\u5e76\u5c06\u4e8c\u6b21\u52a0\u6743\u03ba\u4ece0.79\u63d0\u9ad8\u52300.87\u3002\u63d2\u503c\u5b9e\u9a8c\u8868\u660e\u6a21\u578b\u80fd\u591f\u4ece\u6709\u5e8f\u7684\u7c97\u7c92\u5ea6\u7c7b\u522b\u6807\u7b7e\u4e2d\u5b66\u4e60\u5230\u8fde\u7eed\u7684\u75be\u75c5\u8fdb\u5c55\u8c31\u3002", "conclusion": "\u5e8f\u6570\u6f5c\u5728\u6269\u6563\u6a21\u578b\u80fd\u591f\u66f4\u597d\u5730\u6355\u6349\u75be\u75c5\u8fdb\u5c55\u7684\u8fde\u7eed\u6027\uff0c\u4e3a\u533b\u5b66\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u66f4\u7b26\u5408\u4e34\u5e8a\u5b9e\u9645\u7684\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u6709\u5e8f\u75be\u75c5\u9636\u6bb5\u6807\u7b7e\u65f6\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2602.24201", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.24201", "abs": "https://arxiv.org/abs/2602.24201", "authors": ["Egor Antipov", "Alessandro Palma", "Lorenzo Consoli", "Stephan G\u00fcnnemann", "Andrea Dittadi", "Fabian J. Theis"], "title": "Flow-Based Density Ratio Estimation for Intractable Distributions with Applications in Genomics", "comment": null, "summary": "Estimating density ratios between pairs of intractable data distributions is a core problem in probabilistic modeling, enabling principled comparisons of sample likelihoods under different data-generating processes across conditions and covariates. While exact-likelihood models such as normalizing flows offer a promising approach to density ratio estimation, naive flow-based evaluations are computationally expensive, as they require simulating costly likelihood integrals for each distribution separately. In this work, we leverage condition-aware flow matching to derive a single dynamical formulation for tracking density ratios along generative trajectories. We demonstrate competitive performance on simulated benchmarks for closed-form ratio estimation, and show that our method supports versatile tasks in single-cell genomics data analysis, where likelihood-based comparisons of cellular states across experimental conditions enable treatment effect estimation and batch correction evaluation.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6761\u4ef6\u611f\u77e5\u6d41\u5339\u914d\u7684\u5bc6\u5ea6\u6bd4\u4f30\u8ba1\u65b9\u6cd5\uff0c\u907f\u514d\u5206\u522b\u8ba1\u7b97\u4e24\u4e2a\u5206\u5e03\u7684\u4f3c\u7136\u79ef\u5206\uff0c\u5728\u5355\u7ec6\u80de\u57fa\u56e0\u7ec4\u5b66\u6570\u636e\u5206\u6790\u4e2d\u652f\u6301\u6cbb\u7597\u6548\u5e94\u4f30\u8ba1\u548c\u6279\u6b21\u6821\u6b63\u8bc4\u4f30\u3002", "motivation": "\u5bc6\u5ea6\u6bd4\u4f30\u8ba1\u662f\u6982\u7387\u5efa\u6a21\u7684\u6838\u5fc3\u95ee\u9898\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5982\u5f52\u4e00\u5316\u6d41\u9700\u8981\u5206\u522b\u8ba1\u7b97\u4e24\u4e2a\u5206\u5e03\u7684\u4f3c\u7136\u79ef\u5206\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u6bd4\u8f83\u4e0d\u540c\u6570\u636e\u751f\u6210\u8fc7\u7a0b\u4e0b\u7684\u6837\u672c\u4f3c\u7136\u3002", "method": "\u5229\u7528\u6761\u4ef6\u611f\u77e5\u6d41\u5339\u914d\u6280\u672f\uff0c\u63a8\u5bfc\u51fa\u5355\u4e00\u52a8\u529b\u5b66\u516c\u5f0f\u6765\u8ddf\u8e2a\u751f\u6210\u8f68\u8ff9\u4e0a\u7684\u5bc6\u5ea6\u6bd4\uff0c\u907f\u514d\u5206\u522b\u8ba1\u7b97\u4e24\u4e2a\u5206\u5e03\u7684\u4f3c\u7136\u79ef\u5206\u3002", "result": "\u5728\u6a21\u62df\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u95ed\u5f0f\u6bd4\u7387\u4f30\u8ba1\u6027\u80fd\uff0c\u5728\u5355\u7ec6\u80de\u57fa\u56e0\u7ec4\u5b66\u6570\u636e\u5206\u6790\u4e2d\u652f\u6301\u6cbb\u7597\u6548\u5e94\u4f30\u8ba1\u548c\u6279\u6b21\u6821\u6b63\u8bc4\u4f30\u7b49\u591a\u6837\u5316\u4efb\u52a1\u3002", "conclusion": "\u63d0\u51fa\u7684\u6761\u4ef6\u611f\u77e5\u6d41\u5339\u914d\u65b9\u6cd5\u4e3a\u5bc6\u5ea6\u6bd4\u4f30\u8ba1\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u8ba1\u7b97\u6846\u67b6\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9700\u8981\u6bd4\u8f83\u4e0d\u540c\u5b9e\u9a8c\u6761\u4ef6\u4e0b\u7ec6\u80de\u72b6\u6001\u7684\u751f\u7269\u4fe1\u606f\u5b66\u5e94\u7528\u3002"}}
{"id": "2602.24014", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.24014", "abs": "https://arxiv.org/abs/2602.24014", "authors": ["Na Min An", "Yoonna Jang", "Yusuke Hirota", "Ryo Hachiuma", "Isabelle Augenstein", "Hyunjung Shim"], "title": "Interpretable Debiasing of Vision-Language Models for Social Fairness", "comment": "25 pages, 30 figures, 13 Tables Accepted to CVPR 2026", "summary": "The rapid advancement of Vision-Language models (VLMs) has raised growing concerns that their black-box reasoning processes could lead to unintended forms of social bias. Current debiasing approaches focus on mitigating surface-level bias signals through post-hoc learning or test-time algorithms, while leaving the internal dynamics of the model largely unexplored. In this work, we introduce an interpretable, model-agnostic bias mitigation framework, DeBiasLens, that localizes social attribute neurons in VLMs through sparse autoencoders (SAEs) applied to multimodal encoders. Building upon the disentanglement ability of SAEs, we train them on facial image or caption datasets without corresponding social attribute labels to uncover neurons highly responsive to specific demographics, including those that are underrepresented. By selectively deactivating the social neurons most strongly tied to bias for each group, we effectively mitigate socially biased behaviors of VLMs without degrading their semantic knowledge. Our research lays the groundwork for future auditing tools, prioritizing social fairness in emerging real-world AI systems.", "AI": {"tldr": "DeBiasLens\uff1a\u4e00\u79cd\u901a\u8fc7\u7a00\u758f\u81ea\u7f16\u7801\u5668\u5b9a\u4f4d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u793e\u4f1a\u5c5e\u6027\u795e\u7ecf\u5143\uff0c\u5e76\u9009\u62e9\u6027\u53bb\u6fc0\u6d3b\u6765\u7f13\u89e3\u504f\u89c1\u7684\u53ef\u89e3\u91ca\u3001\u6a21\u578b\u65e0\u5173\u7684\u504f\u89c1\u7f13\u89e3\u6846\u67b6", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u9ed1\u76d2\u63a8\u7406\u8fc7\u7a0b\u53ef\u80fd\u5bfc\u81f4\u610f\u5916\u7684\u793e\u4f1a\u504f\u89c1\uff0c\u73b0\u6709\u65b9\u6cd5\u4ec5\u7f13\u89e3\u8868\u9762\u504f\u89c1\u4fe1\u53f7\uff0c\u672a\u6df1\u5165\u63a2\u7d22\u6a21\u578b\u5185\u90e8\u52a8\u6001", "method": "\u4f7f\u7528\u7a00\u758f\u81ea\u7f16\u7801\u5668\u5728\u591a\u6a21\u6001\u7f16\u7801\u5668\u4e0a\u5b9a\u4f4d\u793e\u4f1a\u5c5e\u6027\u795e\u7ecf\u5143\uff0c\u901a\u8fc7\u65e0\u6807\u7b7e\u7684\u9762\u90e8\u56fe\u50cf\u6216\u5b57\u5e55\u6570\u636e\u96c6\u8bad\u7ec3\uff0c\u9009\u62e9\u6027\u53bb\u6fc0\u6d3b\u4e0e\u504f\u89c1\u6700\u76f8\u5173\u7684\u795e\u7ecf\u5143", "result": "\u6709\u6548\u7f13\u89e3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u793e\u4f1a\u504f\u89c1\u884c\u4e3a\uff0c\u540c\u65f6\u4e0d\u635f\u5bb3\u5176\u8bed\u4e49\u77e5\u8bc6\uff0c\u4e3a\u672a\u6765\u5ba1\u8ba1\u5de5\u5177\u5960\u5b9a\u57fa\u7840", "conclusion": "DeBiasLens\u6846\u67b6\u4e3a\u65b0\u5174AI\u7cfb\u7edf\u7684\u793e\u4f1a\u516c\u5e73\u6027\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u504f\u89c1\u7f13\u89e3\u65b9\u6cd5\uff0c\u4e3a\u672a\u6765\u5ba1\u8ba1\u5de5\u5177\u5f00\u53d1\u5960\u5b9a\u57fa\u7840"}}
{"id": "2602.24207", "categories": ["cs.LG", "cs.CY", "cs.GT", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.24207", "abs": "https://arxiv.org/abs/2602.24207", "authors": ["Gabriele Farina", "Juan Carlos Perdomo"], "title": "The Stability of Online Algorithms in Performative Prediction", "comment": null, "summary": "The use of algorithmic predictions in decision-making leads to a feedback loop where the models we deploy actively influence the data distributions we see, and later use to retrain on. This dynamic was formalized by Perdomo et al. 2020 in their work on performative prediction. Our main result is an unconditional reduction showing that any no-regret algorithm deployed in performative settings converges to a (mixed) performatively stable equilibrium: a solution in which models actively shape data distributions in ways that their own predictions look optimal in hindsight. Prior to our work, all positive results in this area made strong restrictions on how models influenced distributions. By using a martingale argument and allowing randomization, we avoid any such assumption and sidestep recent hardness results for finding stable models. Lastly, on a more conceptual note, our connection sheds light on why common algorithms, like gradient descent, are naturally stabilizing and prevent runaway feedback loops. We hope our work enables future technical transfer of ideas between online optimization and performativity.", "AI": {"tldr": "\u8bba\u6587\u5c55\u793a\u4e86\u5728\u9884\u6d4b\u6027\u51b3\u7b56\u4e2d\uff0c\u4efb\u4f55\u65e0\u9057\u61be\u7b97\u6cd5\u90fd\u80fd\u6536\u655b\u5230\u6df7\u5408\u7684\u9884\u6d4b\u6027\u7a33\u5b9a\u5747\u8861\uff0c\u65e0\u9700\u5bf9\u6a21\u578b\u5f71\u54cd\u6570\u636e\u5206\u5e03\u7684\u65b9\u5f0f\u505a\u9650\u5236\u6027\u5047\u8bbe\u3002", "motivation": "\u7b97\u6cd5\u9884\u6d4b\u5728\u51b3\u7b56\u4e2d\u7684\u4f7f\u7528\u4f1a\u5bfc\u81f4\u53cd\u9988\u5faa\u73af\uff0c\u6a21\u578b\u90e8\u7f72\u4f1a\u5f71\u54cd\u6570\u636e\u5206\u5e03\uff0c\u8fdb\u800c\u5f71\u54cd\u540e\u7eed\u7684\u91cd\u65b0\u8bad\u7ec3\u3002Perdomo\u7b49\u4eba2020\u5e74\u5c06\u8fd9\u4e00\u52a8\u6001\u5f62\u5f0f\u5316\u4e3a\u9884\u6d4b\u6027\u9884\u6d4b\u3002\u73b0\u6709\u7814\u7a76\u5bf9\u6a21\u578b\u5982\u4f55\u5f71\u54cd\u5206\u5e03\u6709\u4e25\u683c\u9650\u5236\uff0c\u9700\u8981\u66f4\u901a\u7528\u7684\u6536\u655b\u7ed3\u679c\u3002", "method": "\u4f7f\u7528\u9785\u8bba\u8bba\u8bc1\u548c\u5141\u8bb8\u968f\u673a\u5316\uff0c\u907f\u514d\u4e86\u5bf9\u6a21\u578b\u5f71\u54cd\u6570\u636e\u5206\u5e03\u65b9\u5f0f\u7684\u4efb\u4f55\u9650\u5236\u6027\u5047\u8bbe\u3002\u901a\u8fc7\u65e0\u6761\u4ef6\u7ea6\u7b80\uff0c\u5c06\u9884\u6d4b\u6027\u8bbe\u7f6e\u4e2d\u7684\u6536\u655b\u95ee\u9898\u4e0e\u5728\u7ebf\u4f18\u5316\u8054\u7cfb\u8d77\u6765\u3002", "result": "\u4e3b\u8981\u7ed3\u679c\u662f\u4efb\u4f55\u65e0\u9057\u61be\u7b97\u6cd5\u5728\u9884\u6d4b\u6027\u8bbe\u7f6e\u4e2d\u90fd\u80fd\u6536\u655b\u5230\u6df7\u5408\u7684\u9884\u6d4b\u6027\u7a33\u5b9a\u5747\u8861\uff0c\u5373\u6a21\u578b\u4e3b\u52a8\u5851\u9020\u6570\u636e\u5206\u5e03\uff0c\u4f7f\u5176\u9884\u6d4b\u5728\u540e\u9a8c\u4e2d\u770b\u8d77\u6765\u6700\u4f18\u3002\u8fd9\u7ed5\u8fc7\u4e86\u6700\u8fd1\u5173\u4e8e\u5bfb\u627e\u7a33\u5b9a\u6a21\u578b\u7684\u786c\u5ea6\u7ed3\u679c\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5efa\u7acb\u4e86\u5728\u7ebf\u4f18\u5316\u4e0e\u9884\u6d4b\u6027\u4e4b\u95f4\u7684\u6280\u672f\u8054\u7cfb\uff0c\u89e3\u91ca\u4e86\u4e3a\u4ec0\u4e48\u5e38\u89c1\u7b97\u6cd5\uff08\u5982\u68af\u5ea6\u4e0b\u964d\uff09\u5177\u6709\u81ea\u7136\u7a33\u5b9a\u6027\u5e76\u80fd\u9632\u6b62\u5931\u63a7\u7684\u53cd\u9988\u5faa\u73af\u3002\u5e0c\u671b\u8fd9\u4e00\u8fde\u63a5\u80fd\u4fc3\u8fdb\u4e24\u4e2a\u9886\u57df\u4e4b\u95f4\u7684\u601d\u60f3\u6280\u672f\u8f6c\u79fb\u3002"}}
{"id": "2602.24209", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.24209", "abs": "https://arxiv.org/abs/2602.24209", "authors": ["Mohsen Tajgardan", "Atena Shiranzaei", "Mahdi Rabbani", "Reza Khoshkangini", "Mahtab Jamali"], "title": "An Efficient Unsupervised Federated Learning Approach for Anomaly Detection in Heterogeneous IoT Networks", "comment": null, "summary": "Federated learning (FL) is an effective paradigm for distributed environments such as the Internet of Things (IoT), where data from diverse devices with varying functionalities remains localized while contributing to a shared global model. By eliminating the need to transmit raw data, FL inherently preserves privacy. However, the heterogeneous nature of IoT data, stemming from differences in device capabilities, data formats, and communication constraints, poses significant challenges to maintaining both global model performance and privacy. In the context of IoT-based anomaly detection, unsupervised FL offers a promising means to identify abnormal behavior without centralized data aggregation. Nevertheless, feature heterogeneity across devices complicates model training and optimization, hindering effective implementation. In this study we propose an efficient unsupervised FL framework that enhances anomaly detection by leveraging shared features from two distinct IoT datasets: one focused on anomaly detection and the other on device identification, while preserving dataset-specific features. To improve transparency and interpretability, we employ explainable AI techniques, such as SHAP, to identify key features influencing local model decisions. Experiments conducted on real-world IoT datasets demonstrate that the proposed method significantly outperforms conventional FL approaches in anomaly detection accuracy. This work underscores the potential of using shared features from complementary datasets to optimize unsupervised federated learning and achieve superior anomaly detection results in decentralized IoT environments.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u9ad8\u6548\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u4e24\u4e2a\u4e0d\u540c\u7269\u8054\u7f51\u6570\u636e\u96c6\u7684\u5171\u4eab\u7279\u5f81\u6765\u589e\u5f3a\u5f02\u5e38\u68c0\u6d4b\uff0c\u540c\u65f6\u4fdd\u6301\u6570\u636e\u96c6\u7279\u5b9a\u7279\u5f81\uff0c\u5e76\u4f7f\u7528\u53ef\u89e3\u91caAI\u6280\u672f\u63d0\u9ad8\u900f\u660e\u5ea6\u3002", "motivation": "\u7269\u8054\u7f51\u73af\u5883\u4e2d\u7684\u8054\u90a6\u5b66\u4e60\u9762\u4e34\u6570\u636e\u5f02\u6784\u6027\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\u4e2d\uff0c\u8bbe\u5907\u80fd\u529b\u3001\u6570\u636e\u683c\u5f0f\u548c\u901a\u4fe1\u7ea6\u675f\u7684\u5dee\u5f02\u5f71\u54cd\u4e86\u5168\u5c40\u6a21\u578b\u6027\u80fd\u548c\u9690\u79c1\u4fdd\u62a4\u3002\u9700\u8981\u4e00\u79cd\u80fd\u6709\u6548\u5904\u7406\u7279\u5f81\u5f02\u6784\u6027\u5e76\u4fdd\u6301\u9690\u79c1\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u65e0\u76d1\u7763\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u4e24\u4e2a\u7269\u8054\u7f51\u6570\u636e\u96c6\uff08\u4e00\u4e2a\u4e13\u6ce8\u4e8e\u5f02\u5e38\u68c0\u6d4b\uff0c\u53e6\u4e00\u4e2a\u4e13\u6ce8\u4e8e\u8bbe\u5907\u8bc6\u522b\uff09\u7684\u5171\u4eab\u7279\u5f81\uff0c\u540c\u65f6\u4fdd\u7559\u6570\u636e\u96c6\u7279\u5b9a\u7279\u5f81\u3002\u4f7f\u7528\u53ef\u89e3\u91caAI\u6280\u672f\uff08\u5982SHAP\uff09\u8bc6\u522b\u5f71\u54cd\u672c\u5730\u6a21\u578b\u51b3\u7b56\u7684\u5173\u952e\u7279\u5f81\u3002", "result": "\u5728\u771f\u5b9e\u7269\u8054\u7f51\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5f02\u5e38\u68c0\u6d4b\u51c6\u786e\u7387\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5f3a\u8c03\u4e86\u5229\u7528\u4e92\u8865\u6570\u636e\u96c6\u7684\u5171\u4eab\u7279\u5f81\u6765\u4f18\u5316\u65e0\u76d1\u7763\u8054\u90a6\u5b66\u4e60\uff0c\u5728\u53bb\u4e2d\u5fc3\u5316\u7269\u8054\u7f51\u73af\u5883\u4e2d\u5b9e\u73b0\u5353\u8d8a\u5f02\u5e38\u68c0\u6d4b\u7ed3\u679c\u7684\u6f5c\u529b\u3002"}}
{"id": "2602.24021", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.24021", "abs": "https://arxiv.org/abs/2602.24021", "authors": ["Zhaolin Cai", "Fan Li", "Huiyu Duan", "Lijun He", "Guangtao Zhai"], "title": "Steering and Rectifying Latent Representation Manifolds in Frozen Multi-modal LLMs for Video Anomaly Detection", "comment": "Accepted by ICLR 2026", "summary": "Video anomaly detection (VAD) aims to identify abnormal events in videos. Traditional VAD methods generally suffer from the high costs of labeled data and full training, thus some recent works have explored leveraging frozen multi-modal large language models (MLLMs) in a tuning-free manner to perform VAD. However, their performance is limited as they directly inherit pre-training biases and cannot adapt internal representations to specific video contexts, leading to difficulties in handling subtle or ambiguous anomalies. To address these limitations, we propose a novel intervention framework, termed SteerVAD, which advances MLLM-based VAD by shifting from passively reading to actively steering and rectifying internal representations. Our approach first leverages the gradient-free representational separability analysis (RSA) to identify top attention heads as latent anomaly experts (LAEs) which are most discriminative for VAD. Then a hierarchical meta-controller (HMC) generates dynamic rectification signals by jointly conditioning on global context and these LAE outputs. The signals execute targeted, anisotropic scaling directly upon the LAE representation manifolds, amplifying anomaly-relevant dimensions while suppressing inherent biases. Extensive experiments on mainstream benchmarks demonstrate our method achieves state-of-the-art performance among tuning-free approaches requiring only 1% of training data, establishing it as a powerful new direction for video anomaly detection. The code will be released upon the publication.", "AI": {"tldr": "SteerVAD\uff1a\u4e00\u79cd\u65b0\u7684\u5e72\u9884\u6846\u67b6\uff0c\u901a\u8fc7\u4e3b\u52a8\u5f15\u5bfc\u548c\u4fee\u6b63\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5185\u90e8\u8868\u793a\u6765\u6539\u8fdb\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\uff0c\u4ec5\u97001%\u8bad\u7ec3\u6570\u636e\u5373\u53ef\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\u548c\u5b8c\u6574\u8bad\u7ec3\uff0c\u6210\u672c\u9ad8\u6602\u3002\u73b0\u6709\u7684\u514d\u8c03\u4f18MLLM\u65b9\u6cd5\u76f4\u63a5\u7ee7\u627f\u9884\u8bad\u7ec3\u504f\u89c1\uff0c\u65e0\u6cd5\u9002\u5e94\u7279\u5b9a\u89c6\u9891\u4e0a\u4e0b\u6587\uff0c\u96be\u4ee5\u5904\u7406\u7ec6\u5fae\u6216\u6a21\u7cca\u7684\u5f02\u5e38\u3002", "method": "\u63d0\u51faSteerVAD\u6846\u67b6\uff1a1\uff09\u4f7f\u7528\u65e0\u68af\u5ea6\u7684\u8868\u793a\u53ef\u5206\u6027\u5206\u6790\u8bc6\u522b\u6700\u5177\u6709\u5224\u522b\u6027\u7684\u6ce8\u610f\u529b\u5934\u4f5c\u4e3a\u6f5c\u5728\u5f02\u5e38\u4e13\u5bb6\uff1b2\uff09\u8bbe\u8ba1\u5206\u5c42\u5143\u63a7\u5236\u5668\uff0c\u57fa\u4e8e\u5168\u5c40\u4e0a\u4e0b\u6587\u548cLAE\u8f93\u51fa\u751f\u6210\u52a8\u6001\u4fee\u6b63\u4fe1\u53f7\uff1b3\uff09\u5728LAE\u8868\u793a\u6d41\u5f62\u4e0a\u6267\u884c\u6709\u9488\u5bf9\u6027\u7684\u5404\u5411\u5f02\u6027\u7f29\u653e\uff0c\u653e\u5927\u5f02\u5e38\u76f8\u5173\u7ef4\u5ea6\u5e76\u6291\u5236\u56fa\u6709\u504f\u89c1\u3002", "result": "\u5728\u4e3b\u6d41\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u4ec5\u97001%\u8bad\u7ec3\u6570\u636e\u7684\u514d\u8c03\u4f18\u65b9\u6cd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "SteerVAD\u901a\u8fc7\u4ece\u88ab\u52a8\u8bfb\u53d6\u8f6c\u5411\u4e3b\u52a8\u5f15\u5bfc\u548c\u4fee\u6b63\u5185\u90e8\u8868\u793a\uff0c\u4e3a\u89c6\u9891\u5f02\u5e38\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u65b0\u65b9\u5411\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709MLLM\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2602.24027", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2602.24027", "abs": "https://arxiv.org/abs/2602.24027", "authors": ["Xingyu Zhu", "Beier Zhu", "Junfeng Fang", "Shuo Wang", "Yin Zhang", "Xiang Wang", "Xiangnan He"], "title": "GuardAlign: Test-time Safety Alignment in Multimodal Large Language Models", "comment": "ICLR 2026", "summary": "Large vision-language models (LVLMs) have achieved remarkable progress in vision-language reasoning tasks, yet ensuring their safety remains a critical challenge. Recent input-side defenses detect unsafe images with CLIP and prepend safety prefixes to prompts, but they still suffer from inaccurate detection in complex scenes and unstable safety signals during decoding. To address these issues, we propose GuardAlign, a training-free defense framework that integrates two strategies. First, OT-enhanced safety detection leverages optimal transport to measure distribution distances between image patches and unsafe semantics, enabling accurate identification of malicious regions without additional computational cost. Second, cross-modal attentive calibration strengthens the influence of safety prefixes by adaptively reallocating attention across layers, ensuring that safety signals remain consistently activated throughout generation. Extensive evaluations on six representative MLLMs demonstrate that GuardAlign reduces unsafe response rates by up to 39% on SPA-VL, while preserving utility, achieving an improvement on VQAv2 from 78.51% to 79.21%.", "AI": {"tldr": "GuardAlign\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u9632\u5fa1\u6846\u67b6\uff0c\u901a\u8fc7\u6700\u4f18\u4f20\u8f93\u589e\u5f3a\u7684\u5b89\u5168\u68c0\u6d4b\u548c\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u6821\u51c6\uff0c\u6709\u6548\u964d\u4f4e\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u4e0d\u5b89\u5168\u54cd\u5e94\u7387\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u8bed\u8a00\u63a8\u7406\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u786e\u4fdd\u5176\u5b89\u5168\u6027\u4ecd\u7136\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\u3002\u73b0\u6709\u7684\u8f93\u5165\u4fa7\u9632\u5fa1\u65b9\u6cd5\u4f7f\u7528CLIP\u68c0\u6d4b\u4e0d\u5b89\u5168\u56fe\u50cf\u5e76\u5728\u63d0\u793a\u524d\u6dfb\u52a0\u5b89\u5168\u524d\u7f00\uff0c\u4f46\u5728\u590d\u6742\u573a\u666f\u4e2d\u68c0\u6d4b\u4e0d\u51c6\u786e\uff0c\u4e14\u5728\u89e3\u7801\u8fc7\u7a0b\u4e2d\u5b89\u5168\u4fe1\u53f7\u4e0d\u7a33\u5b9a\u3002", "method": "GuardAlign\u91c7\u7528\u4e24\u79cd\u7b56\u7565\uff1a1\uff09OT\u589e\u5f3a\u7684\u5b89\u5168\u68c0\u6d4b\uff0c\u5229\u7528\u6700\u4f18\u4f20\u8f93\u6d4b\u91cf\u56fe\u50cf\u5757\u4e0e\u4e0d\u5b89\u5168\u8bed\u4e49\u4e4b\u95f4\u7684\u5206\u5e03\u8ddd\u79bb\uff0c\u65e0\u9700\u989d\u5916\u8ba1\u7b97\u6210\u672c\u5373\u53ef\u51c6\u786e\u8bc6\u522b\u6076\u610f\u533a\u57df\uff1b2\uff09\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u6821\u51c6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5730\u5728\u5404\u5c42\u91cd\u65b0\u5206\u914d\u6ce8\u610f\u529b\u6765\u52a0\u5f3a\u5b89\u5168\u524d\u7f00\u7684\u5f71\u54cd\uff0c\u786e\u4fdd\u5b89\u5168\u4fe1\u53f7\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u6301\u7eed\u6fc0\u6d3b\u3002", "result": "\u5728\u516d\u4e2a\u4ee3\u8868\u6027\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0cGuardAlign\u5728SPA-VL\u4e0a\u5c06\u4e0d\u5b89\u5168\u54cd\u5e94\u7387\u964d\u4f4e\u4e86\u9ad8\u8fbe39%\uff0c\u540c\u65f6\u4fdd\u6301\u5b9e\u7528\u6027\uff0c\u5728VQAv2\u4e0a\u4ece78.51%\u63d0\u5347\u523079.21%\u3002", "conclusion": "GuardAlign\u662f\u4e00\u4e2a\u6709\u6548\u7684\u8bad\u7ec3\u514d\u8d39\u9632\u5fa1\u6846\u67b6\uff0c\u80fd\u591f\u51c6\u786e\u68c0\u6d4b\u590d\u6742\u573a\u666f\u4e2d\u7684\u4e0d\u5b89\u5168\u5185\u5bb9\uff0c\u5e76\u901a\u8fc7\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u6821\u51c6\u786e\u4fdd\u5b89\u5168\u4fe1\u53f7\u7684\u7a33\u5b9a\u6027\uff0c\u663e\u8457\u63d0\u5347\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u6027\u3002"}}
{"id": "2602.24231", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.24231", "abs": "https://arxiv.org/abs/2602.24231", "authors": ["Hongrui Xie", "Junyu Cao", "Kan Xu"], "title": "Adaptive Combinatorial Experimental Design: Pareto Optimality for Decision-Making and Inference", "comment": "30 pages, 3 figure, AISTATS 2026 accepted paper", "summary": "In this paper, we provide the first investigation into adaptive combinatorial experimental design, focusing on the trade-off between regret minimization and statistical power in combinatorial multi-armed bandits (CMAB). While minimizing regret requires repeated exploitation of high-reward arms, accurate inference on reward gaps requires sufficient exploration of suboptimal actions. We formalize this trade-off through the concept of Pareto optimality and establish equivalent conditions for Pareto-efficient learning in CMAB. We consider two relevant cases under different information structures, i.e., full-bandit feedback and semi-bandit feedback, and propose two algorithms MixCombKL and MixCombUCB respectively for these two cases. We provide theoretical guarantees showing that both algorithms are Pareto optimal, achieving finite-time guarantees on both regret and estimation error of arm gaps. Our results further reveal that richer feedback significantly tightens the attainable Pareto frontier, with the primary gains arising from improved estimation accuracy under our proposed methods. Taken together, these findings establish a principled framework for adaptive combinatorial experimentation in multi-objective decision-making.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7814\u7a76\u4e86\u81ea\u9002\u5e94\u7ec4\u5408\u5b9e\u9a8c\u8bbe\u8ba1\uff0c\u805a\u7126\u4e8e\u7ec4\u5408\u591a\u81c2\u8001\u864e\u673a\u4e2d\u9057\u61be\u6700\u5c0f\u5316\u4e0e\u7edf\u8ba1\u529f\u6548\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u63d0\u51fa\u4e86\u5e15\u7d2f\u6258\u6700\u4f18\u6027\u6982\u5ff5\uff0c\u5e76\u9488\u5bf9\u5168\u8001\u864e\u673a\u53cd\u9988\u548c\u534a\u8001\u864e\u673a\u53cd\u9988\u4e24\u79cd\u60c5\u51b5\u5206\u522b\u8bbe\u8ba1\u4e86MixCombKL\u548cMixCombUCB\u7b97\u6cd5\u3002", "motivation": "\u5728\u7ec4\u5408\u591a\u81c2\u8001\u864e\u673a\u4e2d\uff0c\u6700\u5c0f\u5316\u9057\u61be\u9700\u8981\u91cd\u590d\u5229\u7528\u9ad8\u5956\u52b1\u81c2\uff0c\u800c\u51c6\u786e\u63a8\u65ad\u5956\u52b1\u5dee\u8ddd\u9700\u8981\u5145\u5206\u63a2\u7d22\u6b21\u4f18\u52a8\u4f5c\u3002\u8fd9\u79cd\u6743\u8861\u5728\u81ea\u9002\u5e94\u7ec4\u5408\u5b9e\u9a8c\u8bbe\u8ba1\u4e2d\u5c1a\u672a\u5f97\u5230\u7cfb\u7edf\u7814\u7a76\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5e15\u7d2f\u6258\u6700\u4f18\u6027\u6982\u5ff5\u5f62\u5f0f\u5316\u6743\u8861\u95ee\u9898\uff0c\u9488\u5bf9\u4e24\u79cd\u4fe1\u606f\u7ed3\u6784\uff08\u5168\u8001\u864e\u673a\u53cd\u9988\u548c\u534a\u8001\u864e\u673a\u53cd\u9988\uff09\u5206\u522b\u63d0\u51faMixCombKL\u548cMixCombUCB\u7b97\u6cd5\uff0c\u5e76\u5efa\u7acb\u7406\u8bba\u4fdd\u8bc1\u3002", "result": "\u4e24\u79cd\u7b97\u6cd5\u90fd\u88ab\u8bc1\u660e\u662f\u5e15\u7d2f\u6258\u6700\u4f18\u7684\uff0c\u5728\u9057\u61be\u548c\u81c2\u5dee\u8ddd\u4f30\u8ba1\u8bef\u5dee\u65b9\u9762\u90fd\u83b7\u5f97\u4e86\u6709\u9650\u65f6\u95f4\u4fdd\u8bc1\u3002\u7814\u7a76\u8fd8\u53d1\u73b0\u66f4\u4e30\u5bcc\u7684\u53cd\u9988\u80fd\u663e\u8457\u6536\u7d27\u53ef\u8fbe\u5230\u7684\u5e15\u7d2f\u6258\u524d\u6cbf\uff0c\u4e3b\u8981\u589e\u76ca\u6765\u81ea\u6240\u63d0\u65b9\u6cd5\u4e0b\u4f30\u8ba1\u7cbe\u5ea6\u7684\u63d0\u5347\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u591a\u76ee\u6807\u51b3\u7b56\u4e2d\u7684\u81ea\u9002\u5e94\u7ec4\u5408\u5b9e\u9a8c\u5efa\u7acb\u4e86\u539f\u5219\u6027\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u53cd\u9988\u7ed3\u6784\u5bf9\u6743\u8861\u6548\u7387\u7684\u91cd\u8981\u5f71\u54cd\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u7b97\u6cd5\u652f\u6301\u3002"}}
{"id": "2602.24041", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.24041", "abs": "https://arxiv.org/abs/2602.24041", "authors": ["Xingyu Zhu", "Kesen Zhao", "Liang Yi", "Shuo Wang", "Zhicai Wang", "Beier Zhu", "Hanwang Zhang"], "title": "Look Carefully: Adaptive Visual Reinforcements in Multimodal Large Language Models for Hallucination Mitigation", "comment": "ICLR 2026", "summary": "Multimodal large language models (MLLMs) have achieved remarkable progress in vision-language reasoning, yet they remain vulnerable to hallucination, where generated content deviates from visual evidence. Existing mitigation strategies either require costly supervision during training or introduce additional latency at inference time. Recent vision enhancement methods attempt to address this issue by reinforcing visual tokens during decoding, but they typically inject all tokens indiscriminately, which causes interference from background regions and distracts the model from critical cues. To overcome this challenge, we propose Adaptive Visual Reinforcement (AIR), a training-free framework for MLLMs. AIR consists of two components. Prototype-based token reduction condenses the large pool of visual tokens into a compact subset to suppress redundancy. OT-guided patch reinforcement quantifies the alignment between hidden states and patch embeddings to selectively integrate the most consistent patches into feed-forward layers. As a result, AIR enhances the model's reliance on salient visual information and effectively mitigates hallucination. Extensive experiments across representative MLLMs demonstrate that AIR substantially reduces hallucination while preserving general capabilities, establishing it as an effective solution for building reliable MLLMs.", "AI": {"tldr": "\u63d0\u51fa\u81ea\u9002\u5e94\u89c6\u89c9\u589e\u5f3a(AIR)\u6846\u67b6\uff0c\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u51cf\u5c11\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u901a\u8fc7\u539f\u578b\u5316token\u538b\u7f29\u548c\u6700\u4f18\u4f20\u8f93\u5f15\u5bfc\u7684patch\u589e\u5f3a\u6765\u9009\u62e9\u6027\u5f3a\u5316\u5173\u952e\u89c6\u89c9\u4fe1\u606f\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u8bed\u8a00\u63a8\u7406\u65b9\u9762\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u4ecd\u5b58\u5728\u5e7b\u89c9\u95ee\u9898\uff0c\u751f\u6210\u7684\u6587\u672c\u5185\u5bb9\u504f\u79bb\u89c6\u89c9\u8bc1\u636e\u3002\u73b0\u6709\u7f13\u89e3\u7b56\u7565\u8981\u4e48\u9700\u8981\u6602\u8d35\u7684\u8bad\u7ec3\u76d1\u7763\uff0c\u8981\u4e48\u5728\u63a8\u7406\u65f6\u5f15\u5165\u989d\u5916\u5ef6\u8fdf\u3002\u6700\u8fd1\u7684\u89c6\u89c9\u589e\u5f3a\u65b9\u6cd5\u8bd5\u56fe\u901a\u8fc7\u5f3a\u5316\u89e3\u7801\u8fc7\u7a0b\u4e2d\u7684\u89c6\u89c9token\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u4f46\u5b83\u4eec\u901a\u5e38\u4e0d\u52a0\u533a\u5206\u5730\u6ce8\u5165\u6240\u6709token\uff0c\u5bfc\u81f4\u80cc\u666f\u533a\u57df\u5e72\u6270\u5e76\u5206\u6563\u6a21\u578b\u5bf9\u5173\u952e\u7ebf\u7d22\u7684\u6ce8\u610f\u529b\u3002", "method": "\u63d0\u51fa\u81ea\u9002\u5e94\u89c6\u89c9\u589e\u5f3a(AIR)\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u7ec4\u4ef6\uff1a1) \u57fa\u4e8e\u539f\u578b\u7684token\u538b\u7f29\uff1a\u5c06\u5927\u91cf\u89c6\u89c9token\u538b\u7f29\u4e3a\u7d27\u51d1\u5b50\u96c6\u4ee5\u6291\u5236\u5197\u4f59\uff1b2) OT\u5f15\u5bfc\u7684patch\u589e\u5f3a\uff1a\u91cf\u5316\u9690\u85cf\u72b6\u6001\u4e0epatch\u5d4c\u5165\u4e4b\u95f4\u7684\u5bf9\u9f50\u7a0b\u5ea6\uff0c\u9009\u62e9\u6027\u5730\u5c06\u6700\u4e00\u81f4\u7684patch\u96c6\u6210\u5230\u524d\u9988\u5c42\u4e2d\u3002", "result": "\u5728\u4ee3\u8868\u6027\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cAIR\u663e\u8457\u51cf\u5c11\u4e86\u5e7b\u89c9\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u4e00\u822c\u80fd\u529b\uff0c\u8bc1\u660e\u5176\u662f\u6784\u5efa\u53ef\u9760\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "AIR\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u589e\u5f3a\u5173\u952e\u89c6\u89c9\u4fe1\u606f\u6765\u6709\u6548\u7f13\u89e3\u5e7b\u89c9\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u901a\u7528\u80fd\u529b\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u53ef\u9760\u6027\u3002"}}
{"id": "2602.24043", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.24043", "abs": "https://arxiv.org/abs/2602.24043", "authors": ["Yingxuan You", "Ren Li", "Corentin Dumery", "Cong Cao", "Hao Li", "Pascal Fua"], "title": "Spatio-Temporal Garment Reconstruction Using Diffusion Mapping via Pattern Coordinates", "comment": "arXiv admin note: text overlap with arXiv:2504.08353", "summary": "Reconstructing 3D clothed humans from monocular images and videos is a fundamental problem with applications in virtual try-on, avatar creation, and mixed reality. Despite significant progress in human body recovery, accurately reconstructing garment geometry, particularly for loose-fitting clothing, remains an open challenge. We propose a unified framework for high-fidelity 3D garment reconstruction from both single images and video sequences. Our approach combines Implicit Sewing Patterns (ISP) with a generative diffusion model to learn expressive garment shape priors in 2D UV space. Leveraging these priors, we introduce a mapping model that establishes correspondences between image pixels, UV pattern coordinates, and 3D geometry, enabling accurate and detailed garment reconstruction from single images. We further extend this formulation to dynamic reconstruction by introducing a spatio-temporal diffusion scheme with test-time guidance to enforce long-range temporal consistency. We also develop analytic projection-based constraints that preserve image-aligned geometry in visible regions while enforcing coherent completion in occluded areas over time. Although trained exclusively on synthetically simulated cloth data, our method generalizes well to real-world imagery and consistently outperforms existing approaches on both tight- and loose-fitting garments. The reconstructed garments preserve fine geometric detail while exhibiting realistic dynamic motion, supporting downstream applications such as texture editing, garment retargeting, and animation.", "AI": {"tldr": "\u63d0\u51fa\u7edf\u4e00\u6846\u67b6\uff0c\u4ece\u5355\u5f20\u56fe\u50cf\u548c\u89c6\u9891\u5e8f\u5217\u91cd\u5efa\u9ad8\u4fdd\u771f3D\u670d\u88c5\uff0c\u7ed3\u5408\u9690\u5f0f\u7f1d\u7eab\u6a21\u5f0f\u548c\u751f\u6210\u6269\u6563\u6a21\u578b\uff0c\u57282D UV\u7a7a\u95f4\u5b66\u4e60\u670d\u88c5\u5f62\u72b6\u5148\u9a8c\uff0c\u5b9e\u73b0\u51c6\u786e\u8be6\u7ec6\u7684\u670d\u88c5\u91cd\u5efa\u3002", "motivation": "\u4ece\u5355\u76ee\u56fe\u50cf\u548c\u89c6\u9891\u91cd\u5efa3D\u7740\u88c5\u4eba\u4f53\u662f\u865a\u62df\u8bd5\u7a7f\u3001\u865a\u62df\u4eba\u521b\u5efa\u548c\u6df7\u5408\u73b0\u5b9e\u5e94\u7528\u7684\u57fa\u7840\u95ee\u9898\u3002\u5c3d\u7ba1\u4eba\u4f53\u6062\u590d\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u51c6\u786e\u91cd\u5efa\u670d\u88c5\u51e0\u4f55\uff08\u7279\u522b\u662f\u5bbd\u677e\u670d\u88c5\uff09\u4ecd\u7136\u662f\u4e00\u4e2a\u5f00\u653e\u6311\u6218\u3002", "method": "\u7ed3\u5408\u9690\u5f0f\u7f1d\u7eab\u6a21\u5f0f\uff08ISP\uff09\u4e0e\u751f\u6210\u6269\u6563\u6a21\u578b\uff0c\u57282D UV\u7a7a\u95f4\u5b66\u4e60\u8868\u8fbe\u6027\u670d\u88c5\u5f62\u72b6\u5148\u9a8c\uff1b\u5f15\u5165\u6620\u5c04\u6a21\u578b\u5efa\u7acb\u56fe\u50cf\u50cf\u7d20\u3001UV\u56fe\u6848\u5750\u6807\u548c3D\u51e0\u4f55\u4e4b\u95f4\u7684\u5bf9\u5e94\u5173\u7cfb\uff1b\u6269\u5c55\u5230\u52a8\u6001\u91cd\u5efa\u65f6\uff0c\u5f15\u5165\u65f6\u7a7a\u6269\u6563\u65b9\u6848\u548c\u6d4b\u8bd5\u65f6\u6307\u5bfc\u4ee5\u4fdd\u6301\u957f\u671f\u65f6\u95f4\u4e00\u81f4\u6027\uff1b\u5f00\u53d1\u57fa\u4e8e\u5206\u6790\u7684\u6295\u5f71\u7ea6\u675f\uff0c\u5728\u53ef\u89c1\u533a\u57df\u4fdd\u6301\u56fe\u50cf\u5bf9\u9f50\u51e0\u4f55\uff0c\u540c\u65f6\u5728\u906e\u6321\u533a\u57df\u968f\u65f6\u95f4\u5f3a\u5236\u6267\u884c\u4e00\u81f4\u8865\u5168\u3002", "result": "\u5c3d\u7ba1\u4ec5\u5728\u5408\u6210\u6a21\u62df\u5e03\u6599\u6570\u636e\u4e0a\u8bad\u7ec3\uff0c\u4f46\u65b9\u6cd5\u80fd\u5f88\u597d\u5730\u6cdb\u5316\u5230\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\uff0c\u5728\u7d27\u8eab\u548c\u5bbd\u677e\u670d\u88c5\u4e0a\u90fd\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u91cd\u5efa\u7684\u670d\u88c5\u4fdd\u7559\u7cbe\u7ec6\u51e0\u4f55\u7ec6\u8282\uff0c\u540c\u65f6\u5c55\u73b0\u903c\u771f\u7684\u52a8\u6001\u8fd0\u52a8\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u652f\u6301\u7eb9\u7406\u7f16\u8f91\u3001\u670d\u88c5\u91cd\u5b9a\u5411\u548c\u52a8\u753b\u7b49\u4e0b\u6e38\u5e94\u7528\uff0c\u4e3a\u4ece\u5355\u76ee\u56fe\u50cf\u548c\u89c6\u9891\u91cd\u5efa\u9ad8\u4fdd\u771f3D\u670d\u88c5\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u7edf\u4e00\u6846\u67b6\u3002"}}
{"id": "2602.24059", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.24059", "abs": "https://arxiv.org/abs/2602.24059", "authors": ["Chenwei Jia", "Baoting Li", "Xuchong Zhang", "Mingzhuo Wei", "Bochen Lin", "Hongbin Sun"], "title": "Quant Experts: Token-aware Adaptive Error Reconstruction with Mixture of Experts for Large Vision-Language Models Quantization", "comment": "13 pages, 6 figures, including appendix, Accepted at CVPR 2026", "summary": "Post-Training Quantization (PTQ) has emerged as an effective technique for alleviating the substantial computational and memory overheads of Vision-Language Models (VLMs) by compressing both weights and activations without retraining the full model. Existing PTQ methods primarily rely on static identification and global compensation of sensitive or outlier channels, yet they often overlook the distributional differences of these important channels across inputs, leading to unsatisfactory quantization. In this work, we observe that the distributions and occurrence frequencies of important channels vary significantly both across modalities and among tokens, even within the same modality. Accordingly, we propose \\textbf{Quant Experts (QE)}, a token-aware adaptive error compensation with mixture-of-experts for VLMs quantization. QE divides the important channels into token-independent and token-dependent groups. For the former, a shared expert is designed for most tokens to compensate for global quantization error using a low-rank adapter. For the latter, routed experts including multiple routed low-rank adapters are elaborated to compensate for local quantization error related to specific tokens. Extensive experiments demonstrate that QE consistently enhances task accuracy across various quantization settings and model scales, ranging from 2B to 70B parameters, while maintaining performance comparable to full-precision models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faQE\u65b9\u6cd5\uff0c\u901a\u8fc7token\u611f\u77e5\u7684\u81ea\u9002\u5e94\u8bef\u5dee\u8865\u507f\u548c\u4e13\u5bb6\u6df7\u5408\u673a\u5236\u6765\u6539\u8fdb\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u8bad\u7ec3\u540e\u91cf\u5316\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u91cd\u8981\u901a\u9053\u5206\u5e03\u5dee\u5f02\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709PTQ\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u9759\u6001\u8bc6\u522b\u548c\u5168\u5c40\u8865\u507f\u654f\u611f\u6216\u5f02\u5e38\u901a\u9053\uff0c\u4f46\u5ffd\u89c6\u4e86\u8fd9\u4e9b\u91cd\u8981\u901a\u9053\u5728\u4e0d\u540c\u8f93\u5165\u95f4\u7684\u5206\u5e03\u5dee\u5f02\uff0c\u5bfc\u81f4\u91cf\u5316\u6548\u679c\u4e0d\u7406\u60f3\u3002\u7814\u7a76\u53d1\u73b0\u91cd\u8981\u901a\u9053\u7684\u5206\u5e03\u548c\u51fa\u73b0\u9891\u7387\u5728\u4e0d\u540c\u6a21\u6001\u548ctoken\u95f4\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "method": "\u63d0\u51faQuant Experts (QE)\u65b9\u6cd5\uff1a1\uff09\u5c06\u91cd\u8981\u901a\u9053\u5206\u4e3atoken\u65e0\u5173\u548ctoken\u76f8\u5173\u4e24\u7ec4\uff1b2\uff09\u5bf9\u524d\u8005\u8bbe\u8ba1\u5171\u4eab\u4e13\u5bb6\uff0c\u4f7f\u7528\u4f4e\u79e9\u9002\u914d\u5668\u8865\u507f\u5168\u5c40\u91cf\u5316\u8bef\u5dee\uff1b3\uff09\u5bf9\u540e\u8005\u8bbe\u8ba1\u8def\u7531\u4e13\u5bb6\uff0c\u5305\u542b\u591a\u4e2a\u8def\u7531\u4f4e\u79e9\u9002\u914d\u5668\u8865\u507f\u4e0e\u7279\u5b9atoken\u76f8\u5173\u7684\u5c40\u90e8\u91cf\u5316\u8bef\u5dee\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cQE\u5728\u5404\u79cd\u91cf\u5316\u8bbe\u7f6e\u548c\u6a21\u578b\u89c4\u6a21\uff08\u4ece2B\u523070B\u53c2\u6570\uff09\u4e0b\u90fd\u80fd\u6301\u7eed\u63d0\u5347\u4efb\u52a1\u7cbe\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u5168\u7cbe\u5ea6\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\u3002", "conclusion": "QE\u901a\u8fc7token\u611f\u77e5\u7684\u81ea\u9002\u5e94\u8bef\u5dee\u8865\u507f\u548c\u4e13\u5bb6\u6df7\u5408\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u91cf\u5316\u4e2d\u91cd\u8981\u901a\u9053\u5206\u5e03\u5dee\u5f02\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91cf\u5316\u6548\u679c\u3002"}}
{"id": "2602.24065", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.24065", "abs": "https://arxiv.org/abs/2602.24065", "authors": ["Zaiyan Yang", "Jieji Ren", "Xiangyi Wang", "zonglin li", "Xu Cao", "Heng Guo", "Zhanyu Ma", "Boxin Shi"], "title": "EvalMVX: A Unified Benchmarking for Neural 3D Reconstruction under Diverse Multiview Setups", "comment": null, "summary": "Recent advancements in neural surface reconstruction have significantly enhanced 3D reconstruction. However, current real world datasets mainly focus on benchmarking multiview stereo (MVS) based on RGB inputs. Multiview photometric stereo (MVPS) and multiview shape from polarization (MVSfP), though indispensable on high-fidelity surface reconstruction and sparse inputs, have not been quantitatively assessed together with MVS. To determine the working range of different MVX (MVS, MVSfP, and MVPS) techniques, we propose EvalMVX, a real-world dataset containing $25$ objects, each captured with a polarized camera under $20$ varying views and $17$ light conditions including OLAT and natural illumination, leading to $8,500$ images. Each object includes aligned ground-truth 3D mesh, facilitating quantitative benchmarking of MVX methods simultaneously. Based on our EvalMVX, we evaluate $13$ MVX methods published in recent years, record the best-performing methods, and identify open problems under diverse geometric details and reflectance types. We hope EvalMVX and the benchmarking results can inspire future research on multiview 3D reconstruction.", "AI": {"tldr": "EvalMVX\u662f\u4e00\u4e2a\u5305\u542b25\u4e2a\u7269\u4f53\u30018500\u5f20\u56fe\u50cf\u7684\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u540c\u65f6\u8bc4\u4f30\u591a\u89c6\u89d2\u7acb\u4f53\u89c6\u89c9(MVS)\u3001\u591a\u89c6\u89d2\u504f\u632f\u5f62\u72b6\u91cd\u5efa(MVSfP)\u548c\u591a\u89c6\u89d2\u5149\u5ea6\u7acb\u4f53\u89c6\u89c9(MVPS)\u4e09\u79cd\u6280\u672f\uff0c\u586b\u8865\u4e86\u73b0\u6709\u6570\u636e\u96c6\u4ec5\u5173\u6ce8RGB\u8f93\u5165\u7684\u7a7a\u767d\u3002", "motivation": "\u5f53\u524d\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e3b\u8981\u5173\u6ce8\u57fa\u4e8eRGB\u8f93\u5165\u7684\u591a\u89c6\u89d2\u7acb\u4f53\u89c6\u89c9(MVS)\u57fa\u51c6\u6d4b\u8bd5\uff0c\u800c\u591a\u89c6\u89d2\u5149\u5ea6\u7acb\u4f53\u89c6\u89c9(MVPS)\u548c\u591a\u89c6\u89d2\u504f\u632f\u5f62\u72b6\u91cd\u5efa(MVSfP)\u867d\u7136\u5728\u9ad8\u4fdd\u771f\u8868\u9762\u91cd\u5efa\u548c\u7a00\u758f\u8f93\u5165\u65b9\u9762\u4e0d\u53ef\u6216\u7f3a\uff0c\u4f46\u5c1a\u672a\u4e0eMVS\u4e00\u8d77\u8fdb\u884c\u5b9a\u91cf\u8bc4\u4f30\u3002\u9700\u8981\u786e\u5b9a\u4e0d\u540cMVX\u6280\u672f\u7684\u5de5\u4f5c\u8303\u56f4\u3002", "method": "\u63d0\u51faEvalMVX\u6570\u636e\u96c6\uff0c\u5305\u542b25\u4e2a\u7269\u4f53\uff0c\u6bcf\u4e2a\u7269\u4f53\u4f7f\u7528\u504f\u632f\u76f8\u673a\u572820\u4e2a\u4e0d\u540c\u89c6\u89d2\u548c17\u79cd\u5149\u7167\u6761\u4ef6\uff08\u5305\u62ecOLAT\u548c\u81ea\u7136\u5149\u7167\uff09\u4e0b\u62cd\u6444\uff0c\u51718500\u5f20\u56fe\u50cf\u3002\u6bcf\u4e2a\u7269\u4f53\u90fd\u5305\u542b\u5bf9\u9f50\u7684\u771f\u5b9e3D\u7f51\u683c\uff0c\u4fbf\u4e8e\u540c\u65f6\u5b9a\u91cf\u8bc4\u4f30MVX\u65b9\u6cd5\u3002", "result": "\u57fa\u4e8eEvalMVX\u8bc4\u4f30\u4e86\u8fd1\u5e74\u6765\u53d1\u8868\u768413\u79cdMVX\u65b9\u6cd5\uff0c\u8bb0\u5f55\u4e86\u6027\u80fd\u6700\u4f73\u7684\u65b9\u6cd5\uff0c\u5e76\u5728\u4e0d\u540c\u51e0\u4f55\u7ec6\u8282\u548c\u53cd\u5c04\u7c7b\u578b\u4e0b\u8bc6\u522b\u4e86\u5f00\u653e\u6027\u95ee\u9898\u3002", "conclusion": "EvalMVX\u6570\u636e\u96c6\u548c\u57fa\u51c6\u6d4b\u8bd5\u7ed3\u679c\u6709\u671b\u6fc0\u53d1\u591a\u89c6\u89d23D\u91cd\u5efa\u9886\u57df\u7684\u672a\u6765\u7814\u7a76\uff0c\u4e3a\u540c\u65f6\u8bc4\u4f30MVS\u3001MVSfP\u548cMVPS\u6280\u672f\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u5e73\u53f0\u3002"}}
{"id": "2602.24096", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.24096", "abs": "https://arxiv.org/abs/2602.24096", "authors": ["Yuxuan Zhang", "Katar\u00edna T\u00f3thov\u00e1", "Zian Wang", "Kangxue Yin", "Haithem Turki", "Riccardo de Lutio", "Yen-Yu Chang", "Or Litany", "Sanja Fidler", "Zan Gojcic"], "title": "DiffusionHarmonizer: Bridging Neural Reconstruction and Photorealistic Simulation with Online Diffusion Enhancer", "comment": "For more details and updates, please visit our project website: https://research.nvidia.com/labs/sil/projects/diffusion-harmonizer", "summary": "Simulation is essential to the development and evaluation of autonomous robots such as self-driving vehicles. Neural reconstruction is emerging as a promising solution as it enables simulating a wide variety of scenarios from real-world data alone in an automated and scalable way. However, while methods such as NeRF and 3D Gaussian Splatting can produce visually compelling results, they often exhibit artifacts particularly when rendering novel views, and fail to realistically integrate inserted dynamic objects, especially when they were captured from different scenes. To overcome these limitations, we introduce DiffusionHarmonizer, an online generative enhancement framework that transforms renderings from such imperfect scenes into temporally consistent outputs while improving their realism. At its core is a single-step temporally-conditioned enhancer that is converted from a pretrained multi-step image diffusion model, capable of running in online simulators on a single GPU. The key to training it effectively is a custom data curation pipeline that constructs synthetic-real pairs emphasizing appearance harmonization, artifact correction, and lighting realism. The result is a scalable system that significantly elevates simulation fidelity in both research and production environments.", "AI": {"tldr": "DiffusionHarmonizer\u662f\u4e00\u4e2a\u5728\u7ebf\u751f\u6210\u589e\u5f3a\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u795e\u7ecf\u91cd\u5efa\u573a\u666f\u7684\u6e32\u67d3\u8d28\u91cf\uff0c\u89e3\u51b3\u65b0\u89c6\u89d2\u6e32\u67d3\u4e2d\u7684\u4f2a\u5f71\u95ee\u9898\uff0c\u5e76\u6539\u5584\u52a8\u6001\u7269\u4f53\u63d2\u5165\u7684\u771f\u5b9e\u611f\u3002", "motivation": "\u795e\u7ecf\u91cd\u5efa\u65b9\u6cd5\uff08\u5982NeRF\u548c3D\u9ad8\u65af\u6e85\u5c04\uff09\u867d\u7136\u80fd\u4ece\u771f\u5b9e\u4e16\u754c\u6570\u636e\u81ea\u52a8\u751f\u6210\u5404\u79cd\u573a\u666f\uff0c\u4f46\u5728\u6e32\u67d3\u65b0\u89c6\u89d2\u65f6\u7ecf\u5e38\u51fa\u73b0\u4f2a\u5f71\uff0c\u5e76\u4e14\u96be\u4ee5\u771f\u5b9e\u5730\u96c6\u6210\u6765\u81ea\u4e0d\u540c\u573a\u666f\u7684\u52a8\u6001\u7269\u4f53\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5728\u81ea\u4e3b\u673a\u5668\u4eba\uff08\u5982\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\uff09\u4eff\u771f\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51faDiffusionHarmonizer\u6846\u67b6\uff0c\u6838\u5fc3\u662f\u4e00\u4e2a\u5355\u6b65\u65f6\u95f4\u6761\u4ef6\u589e\u5f3a\u5668\uff0c\u4ece\u9884\u8bad\u7ec3\u7684\u591a\u6b65\u56fe\u50cf\u6269\u6563\u6a21\u578b\u8f6c\u6362\u800c\u6765\uff0c\u80fd\u591f\u5728\u5355GPU\u4e0a\u5728\u7ebf\u8fd0\u884c\u3002\u901a\u8fc7\u5b9a\u5236\u6570\u636e\u7ba1\u7406\u6d41\u7a0b\u6784\u5efa\u5408\u6210-\u771f\u5b9e\u5bf9\uff0c\u5f3a\u8c03\u5916\u89c2\u534f\u8c03\u3001\u4f2a\u5f71\u6821\u6b63\u548c\u5149\u7167\u771f\u5b9e\u611f\u3002", "result": "\u8be5\u7cfb\u7edf\u80fd\u591f\u5c06\u4e0d\u5b8c\u7f8e\u573a\u666f\u7684\u6e32\u67d3\u8f6c\u6362\u4e3a\u65f6\u95f4\u4e00\u81f4\u4e14\u66f4\u771f\u5b9e\u7684\u8f93\u51fa\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4eff\u771f\u4fdd\u771f\u5ea6\uff0c\u9002\u7528\u4e8e\u7814\u7a76\u548c\u751f\u4ea7\u73af\u5883\u3002", "conclusion": "DiffusionHarmonizer\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u7cfb\u7edf\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u795e\u7ecf\u91cd\u5efa\u573a\u666f\u4e2d\u7684\u6e32\u67d3\u8d28\u91cf\u95ee\u9898\uff0c\u4e3a\u81ea\u4e3b\u673a\u5668\u4eba\u4eff\u771f\u63d0\u4f9b\u4e86\u66f4\u9ad8\u8d28\u91cf\u7684\u89c6\u89c9\u73af\u5883\u3002"}}
{"id": "2602.24133", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.24133", "abs": "https://arxiv.org/abs/2602.24133", "authors": ["Sifan Zhou", "Jiahao Nie", "Ziyu Zhao", "Yichao Cao", "Xiaobo Lu"], "title": "FocusTrack: One-Stage Focus-and-Suppress Framework for 3D Point Cloud Object Tracking", "comment": "Acceptted in ACM MM 2025", "summary": "In 3D point cloud object tracking, the motion-centric methods have emerged as a promising avenue due to its superior performance in modeling inter-frame motion. However, existing two-stage motion-based approaches suffer from fundamental limitations: (1) error accumulation due to decoupled optimization caused by explicit foreground segmentation prior to motion estimation, and (2) computational bottlenecks from sequential processing. To address these challenges, we propose FocusTrack, a novel one-stage paradigms tracking framework that unifies motion-semantics co-modeling through two core innovations: Inter-frame Motion Modeling (IMM) and Focus-and-Suppress Attention. The IMM module employs a temp-oral-difference siamese encoder to capture global motion patterns between adjacent frames. The Focus-and-Suppress attention that enhance the foreground semantics via motion-salient feature gating and suppress the background noise based on the temporal-aware motion context from IMM without explicit segmentation. Based on above two designs, FocusTrack enables end-to-end training with compact one-stage pipeline. Extensive experiments on prominent 3D tracking benchmarks, such as KITTI, nuScenes, and Waymo, demonstrate that the FocusTrack achieves new SOTA performance while running at a high speed with 105 FPS.", "AI": {"tldr": "FocusTrack\u63d0\u51fa\u4e86\u4e00\u79cd\u5355\u9636\u6bb53D\u70b9\u4e91\u76ee\u6807\u8ddf\u8e2a\u6846\u67b6\uff0c\u901a\u8fc7\u5e27\u95f4\u8fd0\u52a8\u5efa\u6a21\u548c\u805a\u7126-\u6291\u5236\u6ce8\u610f\u529b\u673a\u5236\u7edf\u4e00\u8fd0\u52a8-\u8bed\u4e49\u534f\u540c\u5efa\u6a21\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u4e24\u9636\u6bb5\u65b9\u6cd5\u4e2d\u7684\u8bef\u5dee\u7d2f\u79ef\u548c\u8ba1\u7b97\u74f6\u9888\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\u4e14\u8fd0\u884c\u901f\u5ea6\u8fbe105 FPS\u3002", "motivation": "\u73b0\u6709\u4e24\u9636\u6bb5\u8fd0\u52a8\u4e2d\u5fc3\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u57fa\u672c\u9650\u5236\uff1a1) \u7531\u4e8e\u5728\u8fd0\u52a8\u4f30\u8ba1\u524d\u8fdb\u884c\u663e\u5f0f\u524d\u666f\u5206\u5272\u5bfc\u81f4\u7684\u89e3\u8026\u4f18\u5316\u5f15\u8d77\u7684\u8bef\u5dee\u7d2f\u79ef\uff1b2) \u987a\u5e8f\u5904\u7406\u5e26\u6765\u7684\u8ba1\u7b97\u74f6\u9888\u3002\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u66f4\u51c6\u786e\u76843D\u70b9\u4e91\u8ddf\u8e2a\u65b9\u6cd5\u3002", "method": "\u63d0\u51faFocusTrack\u5355\u9636\u6bb5\u8ddf\u8e2a\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u521b\u65b0\uff1a1) \u5e27\u95f4\u8fd0\u52a8\u5efa\u6a21(IMM)\u6a21\u5757\uff0c\u4f7f\u7528\u65f6\u5dee\u5b6a\u751f\u7f16\u7801\u5668\u6355\u6349\u76f8\u90bb\u5e27\u95f4\u7684\u5168\u5c40\u8fd0\u52a8\u6a21\u5f0f\uff1b2) \u805a\u7126-\u6291\u5236\u6ce8\u610f\u529b\u673a\u5236\uff0c\u901a\u8fc7\u8fd0\u52a8\u663e\u8457\u7279\u5f81\u95e8\u63a7\u589e\u5f3a\u524d\u666f\u8bed\u4e49\uff0c\u5e76\u57fa\u4e8eIMM\u7684\u65f6\u5e8f\u611f\u77e5\u8fd0\u52a8\u4e0a\u4e0b\u6587\u6291\u5236\u80cc\u666f\u566a\u58f0\uff0c\u65e0\u9700\u663e\u5f0f\u5206\u5272\u3002", "result": "\u5728KITTI\u3001nuScenes\u548cWaymo\u7b49\u4e3b\u89813D\u8ddf\u8e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFocusTrack\u5b9e\u73b0\u4e86\u65b0\u7684SOTA\u6027\u80fd\uff0c\u540c\u65f6\u4ee5105 FPS\u7684\u9ad8\u901f\u5ea6\u8fd0\u884c\u3002", "conclusion": "FocusTrack\u901a\u8fc7\u7edf\u4e00\u8fd0\u52a8-\u8bed\u4e49\u534f\u540c\u5efa\u6a21\u7684\u5355\u9636\u6bb5\u8303\u5f0f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u4e24\u9636\u6bb5\u65b9\u6cd5\u7684\u8bef\u5dee\u7d2f\u79ef\u548c\u8ba1\u7b97\u74f6\u9888\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u9ad8\u8fd0\u884c\u901f\u5ea6\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u8ddf\u8e2a\u6027\u80fd\u3002"}}
{"id": "2602.24136", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.24136", "abs": "https://arxiv.org/abs/2602.24136", "authors": ["Haoran Wang", "Guoxi Huang", "Fan Zhang", "David Bull", "Nantheera Anantrasirichai"], "title": "Prune Wisely, Reconstruct Sharply: Compact 3D Gaussian Splatting via Adaptive Pruning and Difference-of-Gaussian Primitives", "comment": "CVPR2026", "summary": "Recent significant advances in 3D scene representation have been driven by 3D Gaussian Splatting (3DGS), which has enabled real-time rendering with photorealistic quality. 3DGS often requires a large number of primitives to achieve high fidelity, leading to redundant representations and high resource consumption, thereby limiting its scalability for complex or large-scale scenes. Consequently, effective pruning strategies and more expressive primitives that can reduce redundancy while preserving visual quality are crucial for practical deployment. We propose an efficient, integrated reconstruction-aware pruning strategy that adaptively determines pruning timing and refining intervals based on reconstruction quality, thus reducing model size while enhancing rendering quality. Moreover, we introduce a 3D Difference-of-Gaussians primitive that jointly models both positive and negative densities in a single primitive, improving the expressiveness of Gaussians under compact configurations. Our method significantly improves model compactness, achieving up to 90\\% reduction in Gaussian-count while delivering visual quality that is similar to, or in some cases better than, that produced by state-of-the-art methods. Code will be made publicly available.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u9ad8\u6548\u76843D\u9ad8\u65af\u6cfc\u6e85\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u5efa\u611f\u77e5\u7684\u526a\u679d\u7b56\u7565\u548c\u65b0\u578b3D\u5dee\u5206\u9ad8\u65af\u57fa\u5143\uff0c\u5728\u4fdd\u6301\u89c6\u89c9\u8d28\u91cf\u7684\u540c\u65f6\u5c06\u9ad8\u65af\u6570\u91cf\u51cf\u5c11\u9ad8\u8fbe90%\u3002", "motivation": "3D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u867d\u7136\u5b9e\u73b0\u4e86\u5b9e\u65f6\u6e32\u67d3\u548c\u7167\u7247\u7ea7\u771f\u5b9e\u611f\uff0c\u4f46\u901a\u5e38\u9700\u8981\u5927\u91cf\u57fa\u5143\u624d\u80fd\u8fbe\u5230\u9ad8\u4fdd\u771f\u5ea6\uff0c\u5bfc\u81f4\u5197\u4f59\u8868\u793a\u548c\u9ad8\u8d44\u6e90\u6d88\u8017\uff0c\u9650\u5236\u4e86\u5176\u5728\u590d\u6742\u6216\u5927\u89c4\u6a21\u573a\u666f\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u3002", "method": "1) \u63d0\u51fa\u9ad8\u6548\u3001\u96c6\u6210\u7684\u91cd\u5efa\u611f\u77e5\u526a\u679d\u7b56\u7565\uff0c\u6839\u636e\u91cd\u5efa\u8d28\u91cf\u81ea\u9002\u5e94\u786e\u5b9a\u526a\u679d\u65f6\u673a\u548c\u7ec6\u5316\u95f4\u9694\uff1b2) \u5f15\u51653D\u5dee\u5206\u9ad8\u65af\u57fa\u5143\uff0c\u5728\u5355\u4e2a\u57fa\u5143\u4e2d\u8054\u5408\u5efa\u6a21\u6b63\u8d1f\u5bc6\u5ea6\uff0c\u63d0\u9ad8\u7d27\u51d1\u914d\u7f6e\u4e0b\u9ad8\u65af\u7684\u8868\u8fbe\u80fd\u529b\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u7d27\u51d1\u6027\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe90%\u7684\u9ad8\u65af\u6570\u91cf\u51cf\u5c11\uff0c\u540c\u65f6\u63d0\u4f9b\u7684\u89c6\u89c9\u8d28\u91cf\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u4f3c\uff0c\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u751a\u81f3\u66f4\u597d\u3002", "conclusion": "\u901a\u8fc7\u91cd\u5efa\u611f\u77e5\u526a\u679d\u548c\u5dee\u5206\u9ad8\u65af\u57fa\u5143\u7684\u7ed3\u5408\uff0c\u6709\u6548\u89e3\u51b3\u4e863D\u9ad8\u65af\u6cfc\u6e85\u7684\u5197\u4f59\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.24138", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.24138", "abs": "https://arxiv.org/abs/2602.24138", "authors": ["Omar Mohamed", "Edoardo Fazzari", "Ayah Al-Naji", "Hamdan Alhadhrami", "Khalfan Hableel", "Saif Alkindi", "Cesare Stefanini"], "title": "Multimodal Optimal Transport for Unsupervised Temporal Segmentation in Surgical Robotics", "comment": null, "summary": "Recognizing surgical phases and steps from video is a fundamental problem in computer-assisted interventions. Recent approaches increasingly rely on large-scale pre-training on thousands of labeled surgical videos, followed by zero-shot transfer to specific procedures. While effective, this strategy incurs substantial computational and data collection costs. In this work, we question whether such heavy pre-training is truly necessary. We propose Text-Augmented Action Segmentation Optimal Transport (TASOT), an unsupervised method for surgical phase and step recognition that extends Action Segmentation Optimal Transport (ASOT) by incorporating textual information generated directly from the videos. TASOT formulates temporal action segmentation as a multimodal optimal transport problem, where the matching cost is defined as a weighted combination of visual and text-based costs. The visual term captures frame-level appearance similarity, while the text term provides complementary semantic cues, and both are jointly regularized through a temporally consistent unbalanced Gromov-Wasserstein formulation. This design enables effective alignment between video frames and surgical actions without surgical-specific pretraining or external web-scale supervision. We evaluate TASOT on multiple benchmark surgical datasets and observe consistent and substantial improvements over existing zero-shot methods, including StrasBypass70 (+23.7), BernBypass70 (+4.5), Cholec80 (+16.5), and AutoLaparo (+19.6). These results demonstrate that fine-grained surgical understanding can be achieved by exploiting information already present in standard visual and textual representations, without resorting to increasingly complex pre-training pipelines. The code will be available at https://github.com/omar8ahmed9/TASOT.", "AI": {"tldr": "TASOT\u662f\u4e00\u79cd\u65e0\u76d1\u7763\u7684\u624b\u672f\u9636\u6bb5\u548c\u6b65\u9aa4\u8bc6\u522b\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u89c6\u9891\u751f\u6210\u7684\u6587\u672c\u4fe1\u606f\uff0c\u907f\u514d\u4e86\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u7684\u9700\u6c42\uff0c\u5728\u591a\u4e2a\u624b\u672f\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u96f6\u6837\u672c\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u624b\u672f\u89c6\u9891\u8bc6\u522b\u65b9\u6cd5\u4f9d\u8d56\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\uff0c\u8ba1\u7b97\u548c\u6570\u636e\u6536\u96c6\u6210\u672c\u9ad8\u6602\u3002\u672c\u6587\u8d28\u7591\u8fd9\u79cd\u91cd\u578b\u9884\u8bad\u7ec3\u7684\u5fc5\u8981\u6027\uff0c\u63a2\u7d22\u662f\u5426\u53ef\u4ee5\u5229\u7528\u89c6\u9891\u4e2d\u5df2\u6709\u7684\u89c6\u89c9\u548c\u6587\u672c\u4fe1\u606f\u5b9e\u73b0\u6709\u6548\u7684\u624b\u672f\u7406\u89e3\u3002", "method": "\u63d0\u51fa\u6587\u672c\u589e\u5f3a\u7684\u52a8\u4f5c\u5206\u5272\u6700\u4f18\u4f20\u8f93\uff08TASOT\uff09\uff0c\u5c06\u65f6\u5e8f\u52a8\u4f5c\u5206\u5272\u5efa\u6a21\u4e3a\u591a\u6a21\u6001\u6700\u4f18\u4f20\u8f93\u95ee\u9898\u3002\u5339\u914d\u6210\u672c\u5b9a\u4e49\u4e3a\u89c6\u89c9\u548c\u6587\u672c\u6210\u672c\u7684\u52a0\u6743\u7ec4\u5408\uff1a\u89c6\u89c9\u9879\u6355\u6349\u5e27\u7ea7\u5916\u89c2\u76f8\u4f3c\u6027\uff0c\u6587\u672c\u9879\u63d0\u4f9b\u8865\u5145\u8bed\u4e49\u7ebf\u7d22\uff0c\u4e24\u8005\u901a\u8fc7\u65f6\u95f4\u4e00\u81f4\u7684\u4e0d\u5e73\u8861Gromov-Wasserstein\u516c\u5f0f\u8fdb\u884c\u8054\u5408\u6b63\u5219\u5316\u3002", "result": "\u5728\u591a\u4e2a\u624b\u672f\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0cTASOT\u76f8\u6bd4\u73b0\u6709\u96f6\u6837\u672c\u65b9\u6cd5\u53d6\u5f97\u663e\u8457\u6539\u8fdb\uff1aStrasBypass70 (+23.7)\u3001BernBypass70 (+4.5)\u3001Cholec80 (+16.5)\u3001AutoLaparo (+19.6)\u3002", "conclusion": "\u901a\u8fc7\u5229\u7528\u6807\u51c6\u89c6\u89c9\u548c\u6587\u672c\u8868\u793a\u4e2d\u5df2\u6709\u7684\u4fe1\u606f\uff0c\u65e0\u9700\u590d\u6742\u9884\u8bad\u7ec3\u7ba1\u9053\u5373\u53ef\u5b9e\u73b0\u7cbe\u7ec6\u7684\u624b\u672f\u7406\u89e3\u3002\u8fd9\u6311\u6218\u4e86\u5f53\u524d\u4f9d\u8d56\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u7684\u4e3b\u6d41\u65b9\u6cd5\uff0c\u4e3a\u624b\u672f\u89c6\u9891\u5206\u6790\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u3001\u6210\u672c\u66f4\u4f4e\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2602.24144", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.24144", "abs": "https://arxiv.org/abs/2602.24144", "authors": ["Muquan Li", "Hang Gou", "Yingyi Ma", "Rongzheng Wang", "Ke Qin", "Tao He"], "title": "Fixed Anchors Are Not Enough: Dynamic Retrieval and Persistent Homology for Dataset Distillation", "comment": "Accepted by CVPR 2026", "summary": "Decoupled dataset distillation (DD) compresses large corpora into a few synthetic images by matching a frozen teacher's statistics. However, current residual-matching pipelines rely on static real patches, creating a fit-complexity gap and a pull-to-anchor effect that reduce intra-class diversity and hurt generalization. To address these issues, we introduce RETA -- a Retrieval and Topology Alignment framework for decoupled DD. First, Dynamic Retrieval Connection (DRC) selects a real patch from a prebuilt pool by minimizing a fit-complexity score in teacher feature space; the chosen patch is injected via a residual connection to tighten feature fit while controlling injected complexity. Second, Persistent Topology Alignment (PTA) regularizes synthesis with persistent homology: we build a mutual k-NN feature graph, compute persistence images of components and loops, and penalize topology discrepancies between real and synthetic sets, mitigating pull-to-anchor effect. Across CIFAR-100, Tiny-ImageNet, ImageNet-1K, and multiple ImageNet subsets, RETA consistently outperforms various baselines under comparable time and memory, especially reaching 64.3% top-1 accuracy on ImageNet-1K with ResNet-18 at 50 images per class, +3.1% over the best prior.", "AI": {"tldr": "RETA\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u68c0\u7d22\u8fde\u63a5\u548c\u6301\u7eed\u62d3\u6251\u5bf9\u9f50\u6539\u8fdb\u89e3\u8026\u6570\u636e\u96c6\u84b8\u998f\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u7684\u62df\u5408\u590d\u6742\u5ea6\u5dee\u8ddd\u548c\u951a\u70b9\u7275\u5f15\u6548\u5e94\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u89e3\u8026\u6570\u636e\u96c6\u84b8\u998f\u65b9\u6cd5\u4f7f\u7528\u9759\u6001\u771f\u5b9e\u56fe\u50cf\u5757\uff0c\u5bfc\u81f4\u62df\u5408\u590d\u6742\u5ea6\u5dee\u8ddd\u548c\u951a\u70b9\u7275\u5f15\u6548\u5e94\uff0c\u964d\u4f4e\u4e86\u7c7b\u5185\u591a\u6837\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faRETA\u6846\u67b6\uff1a1) \u52a8\u6001\u68c0\u7d22\u8fde\u63a5(DRC)\u4ece\u9884\u5efa\u6c60\u4e2d\u9009\u62e9\u771f\u5b9e\u56fe\u50cf\u5757\uff0c\u6700\u5c0f\u5316\u6559\u5e08\u7279\u5f81\u7a7a\u95f4\u7684\u62df\u5408\u590d\u6742\u5ea6\u5206\u6570\uff1b2) \u6301\u7eed\u62d3\u6251\u5bf9\u9f50(PTA)\u4f7f\u7528\u6301\u7eed\u540c\u8c03\u6b63\u5219\u5316\u5408\u6210\u8fc7\u7a0b\uff0c\u6784\u5efa\u4e92k-NN\u7279\u5f81\u56fe\u5e76\u8ba1\u7b97\u6301\u4e45\u6027\u56fe\u50cf\uff0c\u60e9\u7f5a\u771f\u5b9e\u4e0e\u5408\u6210\u96c6\u4e4b\u95f4\u7684\u62d3\u6251\u5dee\u5f02\u3002", "result": "\u5728CIFAR-100\u3001Tiny-ImageNet\u3001ImageNet-1K\u7b49\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cRETA\u59cb\u7ec8\u4f18\u4e8e\u5404\u79cd\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728ImageNet-1K\u4e0a\u8fbe\u523064.3%\u7684top-1\u51c6\u786e\u7387\uff08\u6bcf\u7c7b50\u5f20\u56fe\u50cf\uff0cResNet-18\uff09\uff0c\u6bd4\u4e4b\u524d\u6700\u4f73\u65b9\u6cd5\u63d0\u53473.1%\u3002", "conclusion": "RETA\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u68c0\u7d22\u8fde\u63a5\u548c\u6301\u7eed\u62d3\u6251\u5bf9\u9f50\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u89e3\u8026\u6570\u636e\u96c6\u84b8\u998f\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5408\u6210\u6570\u636e\u96c6\u7684\u591a\u6837\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2602.24148", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.24148", "abs": "https://arxiv.org/abs/2602.24148", "authors": ["Keito Suzuki", "Kunyao Chen", "Lei Wang", "Bang Du", "Runfa Blark Li", "Peng Liu", "Ning Bi", "Truong Nguyen"], "title": "HumanOrbit: 3D Human Reconstruction as 360\u00b0 Orbit Generation", "comment": "CVPR 2026 Findings", "summary": "We present a method for generating a full 360\u00b0 orbit video around a person from a single input image. Existing methods typically adapt image-based diffusion models for multi-view synthesis, but yield inconsistent results across views and with the original identity. In contrast, recent video diffusion models have demonstrated their ability in generating photorealistic results that align well with the given prompts. Inspired by these results, we propose HumanOrbit, a video diffusion model for multi-view human image generation. Our approach enables the model to synthesize continuous camera rotations around the subject, producing geometrically consistent novel views while preserving the appearance and identity of the person. Using the generated multi-view frames, we further propose a reconstruction pipeline that recovers a textured mesh of the subject. Experimental results validate the effectiveness of HumanOrbit for multi-view image generation and that the reconstructed 3D models exhibit superior completeness and fidelity compared to those from state-of-the-art baselines.", "AI": {"tldr": "HumanOrbit\uff1a\u4ece\u5355\u5f20\u56fe\u50cf\u751f\u6210360\u5ea6\u73af\u7ed5\u4eba\u7269\u89c6\u9891\u7684\u6269\u6563\u6a21\u578b\u65b9\u6cd5", "motivation": "\u73b0\u6709\u65b9\u6cd5\u57fa\u4e8e\u56fe\u50cf\u6269\u6563\u6a21\u578b\u8fdb\u884c\u591a\u89c6\u89d2\u5408\u6210\uff0c\u4f46\u5b58\u5728\u89c6\u89d2\u95f4\u4e0d\u4e00\u81f4\u548c\u8eab\u4efd\u4fdd\u6301\u95ee\u9898\u3002\u89c6\u9891\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u903c\u771f\u7ed3\u679c\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u56e0\u6b64\u63a2\u7d22\u5c06\u5176\u7528\u4e8e\u591a\u89c6\u89d2\u4eba\u7269\u56fe\u50cf\u751f\u6210\u3002", "method": "\u63d0\u51faHumanOrbit\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u80fd\u591f\u5408\u6210\u56f4\u7ed5\u4eba\u7269\u7684\u8fde\u7eed\u76f8\u673a\u65cb\u8f6c\uff0c\u751f\u6210\u51e0\u4f55\u4e00\u81f4\u7684\u65b0\u89c6\u89d2\u540c\u65f6\u4fdd\u6301\u4eba\u7269\u5916\u89c2\u548c\u8eab\u4efd\u3002\u8fdb\u4e00\u6b65\u63d0\u51fa\u91cd\u5efa\u6d41\u7a0b\uff0c\u4ece\u751f\u6210\u7684\u591a\u89c6\u89d2\u5e27\u4e2d\u6062\u590d\u5e26\u7eb9\u7406\u7684\u7f51\u683c\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86HumanOrbit\u5728\u591a\u89c6\u89d2\u56fe\u50cf\u751f\u6210\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u91cd\u5efa\u76843D\u6a21\u578b\u5728\u5b8c\u6574\u6027\u548c\u4fdd\u771f\u5ea6\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "HumanOrbit\u80fd\u591f\u4ece\u5355\u5f20\u8f93\u5165\u56fe\u50cf\u751f\u6210360\u5ea6\u73af\u7ed5\u4eba\u7269\u89c6\u9891\uff0c\u5e76\u5728\u591a\u89c6\u89d2\u4e00\u81f4\u6027\u548c\u8eab\u4efd\u4fdd\u6301\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u540c\u65f6\u80fd\u591f\u91cd\u5efa\u9ad8\u8d28\u91cf\u76843D\u6a21\u578b\u3002"}}
{"id": "2602.24160", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.24160", "abs": "https://arxiv.org/abs/2602.24160", "authors": ["Alexander Vieth", "Boudewijn Lelieveldt", "Elmar Eisemann", "Anna Vilanova", "Thomas H\u00f6llt"], "title": "Manifold-Preserving Superpixel Hierarchies and Embeddings for the Exploration of High-Dimensional Images", "comment": "12 pages main paper, 8 pages supplemental material", "summary": "High-dimensional images, or images with a high-dimensional attribute vector per pixel, are commonly explored with coordinated views of a low-dimensional embedding of the attribute space and a conventional image representation. Nowadays, such images can easily contain several million pixels. For such large datasets, hierarchical embedding techniques are better suited to represent the high-dimensional attribute space than flat dimensionality reduction methods. However, available hierarchical dimensionality reduction methods construct the hierarchy purely based on the attribute information and ignore the spatial layout of pixels in the images. This impedes the exploration of regions of interest in the image space, since there is no congruence between a region of interest in image space and the associated attribute abstractions in the hierarchy. In this paper, we present a superpixel hierarchy for high-dimensional images that takes the high-dimensional attribute manifold into account during construction. Through this, our method enables consistent exploration of high-dimensional images in both image and attribute space. We show the effectiveness of this new image-guided hierarchy in the context of embedding exploration by comparing it with classical hierarchical embedding-based image exploration in two use cases.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8003\u8651\u9ad8\u7ef4\u5c5e\u6027\u6d41\u5f62\u548c\u56fe\u50cf\u7a7a\u95f4\u5e03\u5c40\u7684\u8d85\u50cf\u7d20\u5c42\u6b21\u7ed3\u6784\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u9ad8\u7ef4\u56fe\u50cf\u7684\u53ef\u89c6\u5316\u63a2\u7d22\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u5c42\u6b21\u5d4c\u5165\u65b9\u6cd5\u5ffd\u7565\u56fe\u50cf\u7a7a\u95f4\u5e03\u5c40\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u9ad8\u7ef4\u56fe\u50cf\uff08\u6bcf\u50cf\u7d20\u5177\u6709\u9ad8\u7ef4\u5c5e\u6027\u5411\u91cf\uff09\u901a\u5e38\u901a\u8fc7\u5c5e\u6027\u7a7a\u95f4\u7684\u4f4e\u7ef4\u5d4c\u5165\u548c\u4f20\u7edf\u56fe\u50cf\u8868\u793a\u8fdb\u884c\u534f\u8c03\u89c6\u56fe\u63a2\u7d22\u3002\u5bf9\u4e8e\u6570\u767e\u4e07\u50cf\u7d20\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u5c42\u6b21\u5d4c\u5165\u6280\u672f\u6bd4\u5e73\u9762\u964d\u7ef4\u65b9\u6cd5\u66f4\u9002\u5408\u8868\u793a\u9ad8\u7ef4\u5c5e\u6027\u7a7a\u95f4\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u5c42\u6b21\u964d\u7ef4\u65b9\u6cd5\u4ec5\u57fa\u4e8e\u5c5e\u6027\u4fe1\u606f\u6784\u5efa\u5c42\u6b21\u7ed3\u6784\uff0c\u5ffd\u7565\u4e86\u50cf\u7d20\u5728\u56fe\u50cf\u4e2d\u7684\u7a7a\u95f4\u5e03\u5c40\uff0c\u8fd9\u963b\u788d\u4e86\u56fe\u50cf\u7a7a\u95f4\u4e2d\u611f\u5174\u8da3\u533a\u57df\u7684\u63a2\u7d22\uff0c\u56e0\u4e3a\u56fe\u50cf\u7a7a\u95f4\u4e2d\u7684\u611f\u5174\u8da3\u533a\u57df\u4e0e\u5c42\u6b21\u7ed3\u6784\u4e2d\u7684\u76f8\u5173\u5c5e\u6027\u62bd\u8c61\u4e4b\u95f4\u7f3a\u4e4f\u4e00\u81f4\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8d85\u50cf\u7d20\u5c42\u6b21\u7ed3\u6784\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u6784\u5efa\u8fc7\u7a0b\u4e2d\u540c\u65f6\u8003\u8651\u9ad8\u7ef4\u5c5e\u6027\u6d41\u5f62\u548c\u56fe\u50cf\u7a7a\u95f4\u5e03\u5c40\u3002\u901a\u8fc7\u8fd9\u79cd\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7ef4\u56fe\u50cf\u5728\u56fe\u50cf\u7a7a\u95f4\u548c\u5c5e\u6027\u7a7a\u95f4\u4e2d\u7684\u4e00\u81f4\u63a2\u7d22\u3002", "result": "\u5728\u4e24\u4e2a\u4f7f\u7528\u6848\u4f8b\u4e2d\uff0c\u901a\u8fc7\u4e0e\u4f20\u7edf\u57fa\u4e8e\u5c42\u6b21\u5d4c\u5165\u7684\u56fe\u50cf\u63a2\u7d22\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\uff0c\u5c55\u793a\u4e86\u8fd9\u79cd\u65b0\u7684\u56fe\u50cf\u5f15\u5bfc\u5c42\u6b21\u7ed3\u6784\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u8d85\u50cf\u7d20\u5c42\u6b21\u7ed3\u6784\u65b9\u6cd5\u80fd\u591f\u66f4\u597d\u5730\u652f\u6301\u5927\u89c4\u6a21\u9ad8\u7ef4\u56fe\u50cf\u7684\u53ef\u89c6\u5316\u63a2\u7d22\uff0c\u901a\u8fc7\u5728\u5c42\u6b21\u6784\u5efa\u4e2d\u540c\u65f6\u8003\u8651\u5c5e\u6027\u4fe1\u606f\u548c\u7a7a\u95f4\u5e03\u5c40\uff0c\u5b9e\u73b0\u4e86\u56fe\u50cf\u7a7a\u95f4\u548c\u5c5e\u6027\u7a7a\u95f4\u4e4b\u95f4\u66f4\u4e00\u81f4\u7684\u63a2\u7d22\u4f53\u9a8c\u3002"}}
{"id": "2602.24161", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.24161", "abs": "https://arxiv.org/abs/2602.24161", "authors": ["Chao Xu", "Xiaochen Zhao", "Xiang Deng", "Jingxiang Sun", "Zhuo Su", "Donglin Di", "Yebin Liu"], "title": "GeoDiff4D: Geometry-Aware Diffusion for 4D Head Avatar Reconstruction", "comment": "17 pages", "summary": "Reconstructing photorealistic and animatable 4D head avatars from a single portrait image remains a fundamental challenge in computer vision. While diffusion models have enabled remarkable progress in image and video generation for avatar reconstruction, existing methods primarily rely on 2D priors and struggle to achieve consistent 3D geometry. We propose a novel framework that leverages geometry-aware diffusion to learn strong geometry priors for high-fidelity head avatar reconstruction. Our approach jointly synthesizes portrait images and corresponding surface normals, while a pose-free expression encoder captures implicit expression representations. Both synthesized images and expression latents are incorporated into 3D Gaussian-based avatars, enabling photorealistic rendering with accurate geometry. Extensive experiments demonstrate that our method substantially outperforms state-of-the-art approaches in visual quality, expression fidelity, and cross-identity generalization, while supporting real-time rendering.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u51e0\u4f55\u611f\u77e5\u6269\u6563\u6a21\u578b\u7684\u5355\u56fe\u50cf4D\u5934\u90e8\u5316\u8eab\u91cd\u5efa\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u5408\u6210\u8096\u50cf\u56fe\u50cf\u548c\u8868\u9762\u6cd5\u7ebf\uff0c\u7ed3\u54083D\u9ad8\u65af\u8868\u793a\u5b9e\u73b0\u9ad8\u8d28\u91cf\u3001\u53ef\u52a8\u753b\u7684\u5934\u90e8\u91cd\u5efa\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u5316\u8eab\u91cd\u5efa\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d562D\u5148\u9a8c\uff0c\u96be\u4ee5\u5b9e\u73b0\u4e00\u81f4\u76843D\u51e0\u4f55\u7ed3\u6784\u3002\u4ece\u5355\u5f20\u8096\u50cf\u56fe\u50cf\u91cd\u5efa\u903c\u771f\u4e14\u53ef\u52a8\u753b\u76844D\u5934\u90e8\u5316\u8eab\u4ecd\u7136\u662f\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u57fa\u672c\u6311\u6218\u3002", "method": "\u63d0\u51fa\u51e0\u4f55\u611f\u77e5\u6269\u6563\u6846\u67b6\uff0c\u8054\u5408\u5408\u6210\u8096\u50cf\u56fe\u50cf\u548c\u5bf9\u5e94\u7684\u8868\u9762\u6cd5\u7ebf\uff1b\u4f7f\u7528\u65e0\u59ff\u6001\u8868\u60c5\u7f16\u7801\u5668\u6355\u6349\u9690\u5f0f\u8868\u60c5\u8868\u793a\uff1b\u5c06\u5408\u6210\u56fe\u50cf\u548c\u8868\u60c5\u6f5c\u5728\u7f16\u7801\u6574\u5408\u5230\u57fa\u4e8e3D\u9ad8\u65af\u7684\u5316\u8eab\u4e2d\u3002", "result": "\u5728\u89c6\u89c9\u8d28\u91cf\u3001\u8868\u60c5\u4fdd\u771f\u5ea6\u548c\u8de8\u8eab\u4efd\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u540c\u65f6\u652f\u6301\u5b9e\u65f6\u6e32\u67d3\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u51e0\u4f55\u611f\u77e5\u6269\u6563\u5b66\u4e60\u5f3a\u5927\u7684\u51e0\u4f55\u5148\u9a8c\uff0c\u5b9e\u73b0\u4e86\u4ece\u5355\u5f20\u8096\u50cf\u56fe\u50cf\u91cd\u5efa\u9ad8\u8d28\u91cf\u3001\u53ef\u52a8\u753b\u76844D\u5934\u90e8\u5316\u8eab\uff0c\u5728\u591a\u4e2a\u8bc4\u4f30\u6307\u6807\u4e0a\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2602.24181", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.24181", "abs": "https://arxiv.org/abs/2602.24181", "authors": ["Rishabh Kabra", "Maks Ovsjanikov", "Drew A. Hudson", "Ye Xia", "Skanda Koppula", "Andre Araujo", "Joao Carreira", "Niloy J. Mitra"], "title": "A Mixed Diet Makes DINO An Omnivorous Vision Encoder", "comment": "CVPR 2026", "summary": "Pre-trained vision encoders like DINOv2 have demonstrated exceptional performance on unimodal tasks. However, we observe that their feature representations are poorly aligned across different modalities. For instance, the feature embedding for an RGB image and its corresponding depth map of the same scene exhibit a cosine similarity that is nearly identical to that of two random, unrelated images. To address this, we propose the Omnivorous Vision Encoder, a novel framework that learns a modality-agnostic feature space. We train the encoder with a dual objective: first, to maximize the feature alignment between different modalities of the same scene; and second, a distillation objective that anchors the learned representations to the output of a fully frozen teacher such as DINOv2. The resulting student encoder becomes \"omnivorous\" by producing a consistent, powerful embedding for a given scene, regardless of the input modality (RGB, Depth, Segmentation, etc.). This approach enables robust cross-modal understanding while retaining the discriminative semantics of the original foundation model.", "AI": {"tldr": "\u63d0\u51faOmnivorous Vision Encoder\u6846\u67b6\uff0c\u89e3\u51b3\u9884\u8bad\u7ec3\u89c6\u89c9\u7f16\u7801\u5668\uff08\u5982DINOv2\uff09\u5728\u4e0d\u540c\u6a21\u6001\u7279\u5f81\u8868\u793a\u5bf9\u9f50\u4e0d\u4f73\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u5b66\u4e60\u6a21\u6001\u65e0\u5173\u7684\u7279\u5f81\u7a7a\u95f4\uff0c\u4f7f\u7f16\u7801\u5668\u80fd\u591f\u4e3a\u540c\u4e00\u573a\u666f\u7684\u4e0d\u540c\u6a21\u6001\u8f93\u5165\uff08RGB\u3001\u6df1\u5ea6\u3001\u5206\u5272\u7b49\uff09\u751f\u6210\u4e00\u81f4\u4e14\u5f3a\u5927\u7684\u5d4c\u5165\u3002", "motivation": "\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u7f16\u7801\u5668\uff08\u5982DINOv2\uff09\u5728\u5355\u6a21\u6001\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7814\u7a76\u53d1\u73b0\u5b83\u4eec\u7684\u7279\u5f81\u8868\u793a\u5728\u4e0d\u540c\u6a21\u6001\u4e4b\u95f4\u5bf9\u9f50\u6548\u679c\u5f88\u5dee\u3002\u4f8b\u5982\uff0c\u540c\u4e00\u573a\u666f\u7684RGB\u56fe\u50cf\u548c\u5176\u5bf9\u5e94\u7684\u6df1\u5ea6\u56fe\u4e4b\u95f4\u7684\u7279\u5f81\u5d4c\u5165\u4f59\u5f26\u76f8\u4f3c\u5ea6\u51e0\u4e4e\u4e0e\u4e24\u4e2a\u968f\u673a\u4e0d\u76f8\u5173\u56fe\u50cf\u76f8\u540c\uff0c\u8fd9\u8868\u660e\u73b0\u6709\u6a21\u578b\u7f3a\u4e4f\u8de8\u6a21\u6001\u4e00\u81f4\u6027\u3002", "method": "\u63d0\u51faOmnivorous Vision Encoder\u6846\u67b6\uff0c\u91c7\u7528\u53cc\u91cd\u8bad\u7ec3\u76ee\u6807\uff1a1\uff09\u6700\u5927\u5316\u540c\u4e00\u573a\u666f\u4e0d\u540c\u6a21\u6001\u4e4b\u95f4\u7684\u7279\u5f81\u5bf9\u9f50\uff1b2\uff09\u4f7f\u7528\u84b8\u998f\u76ee\u6807\u5c06\u5b66\u4e60\u5230\u7684\u8868\u793a\u951a\u5b9a\u5230\u5b8c\u5168\u51bb\u7ed3\u7684\u6559\u5e08\u6a21\u578b\uff08\u5982DINOv2\uff09\u7684\u8f93\u51fa\u3002\u8fd9\u79cd\u65b9\u6cd5\u4f7f\u5b66\u751f\u7f16\u7801\u5668\u80fd\u591f\u4e3a\u7ed9\u5b9a\u573a\u666f\u751f\u6210\u4e00\u81f4\u7684\u5d4c\u5165\uff0c\u65e0\u8bba\u8f93\u5165\u6a21\u6001\u5982\u4f55\u3002", "result": "\u8bad\u7ec3\u5f97\u5230\u7684\"\u5168\u80fd\"\u5b66\u751f\u7f16\u7801\u5668\u80fd\u591f\u4e3a\u540c\u4e00\u573a\u666f\u7684\u4e0d\u540c\u6a21\u6001\u8f93\u5165\uff08RGB\u3001\u6df1\u5ea6\u3001\u5206\u5272\u7b49\uff09\u4ea7\u751f\u4e00\u81f4\u4e14\u5f3a\u5927\u7684\u7279\u5f81\u5d4c\u5165\uff0c\u5b9e\u73b0\u4e86\u7a33\u5065\u7684\u8de8\u6a21\u6001\u7406\u89e3\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u539f\u59cb\u57fa\u7840\u6a21\u578b\u7684\u5224\u522b\u8bed\u4e49\u80fd\u529b\u3002", "conclusion": "Omnivorous Vision Encoder\u901a\u8fc7\u53cc\u91cd\u8bad\u7ec3\u76ee\u6807\u6210\u529f\u89e3\u51b3\u4e86\u9884\u8bad\u7ec3\u89c6\u89c9\u7f16\u7801\u5668\u8de8\u6a21\u6001\u7279\u5f81\u5bf9\u9f50\u95ee\u9898\uff0c\u521b\u5efa\u4e86\u4e00\u4e2a\u6a21\u6001\u65e0\u5173\u7684\u7279\u5f81\u7a7a\u95f4\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u5904\u7406\u591a\u79cd\u8f93\u5165\u6a21\u6001\u5e76\u4fdd\u6301\u7279\u5f81\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u7ee7\u627f\u57fa\u7840\u6a21\u578b\u7684\u5f3a\u5927\u8bed\u4e49\u8868\u793a\u80fd\u529b\u3002"}}
{"id": "2602.24183", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.24183", "abs": "https://arxiv.org/abs/2602.24183", "authors": ["Yixuan Liu", "Kanwal K. Bhatia", "Ahmed E. Fetit"], "title": "A multimodal slice discovery framework for systematic failure detection and explanation in medical image classification", "comment": null, "summary": "Despite advances in machine learning-based medical image classifiers, the safety and reliability of these systems remain major concerns in practical settings. Existing auditing approaches mainly rely on unimodal features or metadata-based subgroup analyses, which are limited in interpretability and often fail to capture hidden systematic failures. To address these limitations, we introduce the first automated auditing framework that extends slice discovery methods to multimodal representations specifically for medical applications. Comprehensive experiments were conducted under common failure scenarios using the MIMIC-CXR-JPG dataset, demonstrating the framework's strong capability in both failure discovery and explanation generation. Our results also show that multimodal information generally allows more comprehensive and effective auditing of classifiers, while unimodal variants beyond image-only inputs exhibit strong potential in scenarios where resources are constrained.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u5668\u7684\u591a\u6a21\u6001\u81ea\u52a8\u5ba1\u8ba1\u6846\u67b6\uff0c\u6269\u5c55\u4e86\u5207\u7247\u53d1\u73b0\u65b9\u6cd5\uff0c\u5728MIMIC-CXR-JPG\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u5728\u6545\u969c\u53d1\u73b0\u548c\u89e3\u91ca\u751f\u6210\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5c3d\u7ba1\u673a\u5668\u5b66\u4e60\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u5668\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u5176\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u4ecd\u662f\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u4e3b\u8981\u95ee\u9898\u3002\u73b0\u6709\u5ba1\u8ba1\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u5355\u6a21\u6001\u7279\u5f81\u6216\u57fa\u4e8e\u5143\u6570\u636e\u7684\u5b50\u7ec4\u5206\u6790\uff0c\u89e3\u91ca\u6027\u6709\u9650\u4e14\u96be\u4ee5\u53d1\u73b0\u9690\u85cf\u7684\u7cfb\u7edf\u6027\u6545\u969c\u3002", "method": "\u5f15\u5165\u4e86\u9996\u4e2a\u81ea\u52a8\u5ba1\u8ba1\u6846\u67b6\uff0c\u5c06\u5207\u7247\u53d1\u73b0\u65b9\u6cd5\u6269\u5c55\u5230\u591a\u6a21\u6001\u8868\u793a\uff0c\u4e13\u95e8\u9488\u5bf9\u533b\u5b66\u5e94\u7528\u3002\u8be5\u6846\u67b6\u5229\u7528\u591a\u6a21\u6001\u4fe1\u606f\u8fdb\u884c\u66f4\u5168\u9762\u7684\u5206\u7c7b\u5668\u5ba1\u8ba1\u3002", "result": "\u5728MIMIC-CXR-JPG\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u7efc\u5408\u5b9e\u9a8c\uff0c\u5c55\u793a\u4e86\u6846\u67b6\u5728\u6545\u969c\u53d1\u73b0\u548c\u89e3\u91ca\u751f\u6210\u65b9\u9762\u7684\u5f3a\u5927\u80fd\u529b\u3002\u7ed3\u679c\u8868\u660e\u591a\u6a21\u6001\u4fe1\u606f\u901a\u5e38\u80fd\u5b9e\u73b0\u66f4\u5168\u9762\u6709\u6548\u7684\u5206\u7c7b\u5668\u5ba1\u8ba1\uff0c\u800c\u56fe\u50cf\u4e4b\u5916\u7684\u5355\u6a21\u6001\u53d8\u4f53\u5728\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e2d\u663e\u793a\u51fa\u5f3a\u5927\u6f5c\u529b\u3002", "conclusion": "\u591a\u6a21\u6001\u5ba1\u8ba1\u6846\u67b6\u80fd\u591f\u66f4\u6709\u6548\u5730\u53d1\u73b0\u548c\u89e3\u91ca\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u5668\u7684\u7cfb\u7edf\u6027\u6545\u969c\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u91cd\u8981\u4fdd\u969c\u3002"}}
{"id": "2602.24208", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.24208", "abs": "https://arxiv.org/abs/2602.24208", "authors": ["Yasaman Haghighi", "Alexandre Alahi"], "title": "SenCache: Accelerating Diffusion Model Inference via Sensitivity-Aware Caching", "comment": null, "summary": "Diffusion models achieve state-of-the-art video generation quality, but their inference remains expensive due to the large number of sequential denoising steps. This has motivated a growing line of research on accelerating diffusion inference. Among training-free acceleration methods, caching reduces computation by reusing previously computed model outputs across timesteps. Existing caching methods rely on heuristic criteria to choose cache/reuse timesteps and require extensive tuning. We address this limitation with a principled sensitivity-aware caching framework. Specifically, we formalize the caching error through an analysis of the model output sensitivity to perturbations in the denoising inputs, i.e., the noisy latent and the timestep, and show that this sensitivity is a key predictor of caching error. Based on this analysis, we propose Sensitivity-Aware Caching (SenCache), a dynamic caching policy that adaptively selects caching timesteps on a per-sample basis. Our framework provides a theoretical basis for adaptive caching, explains why prior empirical heuristics can be partially effective, and extends them to a dynamic, sample-specific approach. Experiments on Wan 2.1, CogVideoX, and LTX-Video show that SenCache achieves better visual quality than existing caching methods under similar computational budgets.", "AI": {"tldr": "SenCache\uff1a\u57fa\u4e8e\u654f\u611f\u5ea6\u611f\u77e5\u7684\u52a8\u6001\u7f13\u5b58\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790\u6269\u6563\u6a21\u578b\u8f93\u51fa\u5bf9\u53bb\u566a\u8f93\u5165\u7684\u654f\u611f\u6027\uff0c\u81ea\u9002\u5e94\u9009\u62e9\u7f13\u5b58\u65f6\u95f4\u6b65\uff0c\u5728\u4fdd\u6301\u89c6\u89c9\u8d28\u91cf\u7684\u540c\u65f6\u52a0\u901f\u89c6\u9891\u751f\u6210\u63a8\u7406\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u89c6\u9891\u751f\u6210\u65b9\u9762\u8fbe\u5230\u6700\u5148\u8fdb\u8d28\u91cf\uff0c\u4f46\u5176\u63a8\u7406\u8fc7\u7a0b\u56e0\u9700\u8981\u5927\u91cf\u987a\u5e8f\u53bb\u566a\u6b65\u9aa4\u800c\u8ba1\u7b97\u6602\u8d35\u3002\u73b0\u6709\u7f13\u5b58\u65b9\u6cd5\u4f9d\u8d56\u542f\u53d1\u5f0f\u6807\u51c6\u9009\u62e9\u7f13\u5b58/\u91cd\u7528\u65f6\u95f4\u6b65\uff0c\u9700\u8981\u5927\u91cf\u8c03\u4f18\uff0c\u7f3a\u4e4f\u7406\u8bba\u4f9d\u636e\u3002", "method": "\u63d0\u51fa\u654f\u611f\u6027\u611f\u77e5\u7f13\u5b58\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790\u6a21\u578b\u8f93\u51fa\u5bf9\u566a\u58f0\u6f5c\u53d8\u91cf\u548c\u65f6\u95f4\u6b65\u6270\u52a8\u7684\u654f\u611f\u6027\uff0c\u5c06\u654f\u611f\u6027\u4f5c\u4e3a\u7f13\u5b58\u8bef\u5dee\u7684\u5173\u952e\u9884\u6d4b\u6307\u6807\u3002\u57fa\u4e8e\u6b64\u63d0\u51faSenCache\u52a8\u6001\u7f13\u5b58\u7b56\u7565\uff0c\u5728\u6bcf\u6837\u672c\u57fa\u7840\u4e0a\u81ea\u9002\u5e94\u9009\u62e9\u7f13\u5b58\u65f6\u95f4\u6b65\u3002", "result": "\u5728Wan 2.1\u3001CogVideoX\u548cLTX-Video\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u76f8\u4f3c\u8ba1\u7b97\u9884\u7b97\u4e0b\uff0cSenCache\u6bd4\u73b0\u6709\u7f13\u5b58\u65b9\u6cd5\u83b7\u5f97\u66f4\u597d\u7684\u89c6\u89c9\u8d28\u91cf\u3002", "conclusion": "SenCache\u4e3a\u81ea\u9002\u5e94\u7f13\u5b58\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u89e3\u91ca\u4e86\u5148\u524d\u7ecf\u9a8c\u542f\u53d1\u5f0f\u65b9\u6cd5\u4e3a\u4f55\u90e8\u5206\u6709\u6548\uff0c\u5e76\u5c06\u5176\u6269\u5c55\u4e3a\u52a8\u6001\u3001\u6837\u672c\u7279\u5b9a\u7684\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u89c6\u89c9\u8d28\u91cf\u7684\u540c\u65f6\u52a0\u901f\u6269\u6563\u6a21\u578b\u63a8\u7406\u3002"}}
{"id": "2602.24222", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.24222", "abs": "https://arxiv.org/abs/2602.24222", "authors": ["Albert Dominguez Mantes", "Gioele La Manno", "Martin Weigert"], "title": "MuViT: Multi-Resolution Vision Transformers for Learning Across Scales in Microscopy", "comment": "Accepted at CVPR 2026", "summary": "Modern microscopy routinely produces gigapixel images that contain structures across multiple spatial scales, from fine cellular morphology to broader tissue organization. Many analysis tasks require combining these scales, yet most vision models operate at a single resolution or derive multi-scale features from one view, limiting their ability to exploit the inherently multi-resolution nature of microscopy data. We introduce MuViT, a transformer architecture built to fuse true multi-resolution observations from the same underlying image. MuViT embeds all patches into a shared world-coordinate system and extends rotary positional embeddings to these coordinates, enabling attention to integrate wide-field context with high-resolution detail within a single encoder. Across synthetic benchmarks, kidney histopathology, and high-resolution mouse-brain microscopy, MuViT delivers consistent improvements over strong ViT and CNN baselines. Multi-resolution MAE pretraining further produces scale-consistent representations that enhance downstream tasks. These results demonstrate that explicit world-coordinate modelling provides a simple yet powerful mechanism for leveraging multi-resolution information in large-scale microscopy analysis.", "AI": {"tldr": "MuViT\u662f\u4e00\u79cd\u7528\u4e8e\u591a\u5206\u8fa8\u7387\u663e\u5fae\u955c\u56fe\u50cf\u5206\u6790\u7684Transformer\u67b6\u6784\uff0c\u901a\u8fc7\u5171\u4eab\u4e16\u754c\u5750\u6807\u7cfb\u5d4c\u5165\u548c\u65cb\u8f6c\u4f4d\u7f6e\u7f16\u7801\uff0c\u80fd\u591f\u878d\u5408\u4e0d\u540c\u5206\u8fa8\u7387\u7684\u89c2\u5bdf\u4fe1\u606f\uff0c\u5728\u591a\u79cd\u751f\u7269\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4efb\u52a1\u4e2d\u4f18\u4e8e\u4f20\u7edf\u5355\u5206\u8fa8\u7387\u6a21\u578b\u3002", "motivation": "\u73b0\u4ee3\u663e\u5fae\u955c\u4ea7\u751f\u5305\u542b\u591a\u5c3a\u5ea6\u7ed3\u6784\uff08\u4ece\u7ec6\u80de\u5f62\u6001\u5230\u7ec4\u7ec7\u7ec4\u7ec7\uff09\u7684\u5343\u5146\u50cf\u7d20\u56fe\u50cf\uff0c\u4f46\u73b0\u6709\u89c6\u89c9\u6a21\u578b\u901a\u5e38\u53ea\u5904\u7406\u5355\u4e00\u5206\u8fa8\u7387\u6216\u4ece\u5355\u4e00\u89c6\u56fe\u63d0\u53d6\u591a\u5c3a\u5ea6\u7279\u5f81\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u663e\u5fae\u955c\u6570\u636e\u56fa\u6709\u7684\u591a\u5206\u8fa8\u7387\u7279\u6027\u3002", "method": "\u63d0\u51faMuViT Transformer\u67b6\u6784\uff0c\u5c06\u6240\u6709\u56fe\u50cf\u5757\u5d4c\u5165\u5171\u4eab\u7684\u4e16\u754c\u5750\u6807\u7cfb\uff0c\u5e76\u5c06\u65cb\u8f6c\u4f4d\u7f6e\u7f16\u7801\u6269\u5c55\u5230\u8fd9\u4e9b\u5750\u6807\uff0c\u4f7f\u6ce8\u610f\u529b\u673a\u5236\u80fd\u591f\u5728\u5355\u4e2a\u7f16\u7801\u5668\u4e2d\u6574\u5408\u5e7f\u57df\u4e0a\u4e0b\u6587\u548c\u9ad8\u5206\u8fa8\u7387\u7ec6\u8282\u3002\u91c7\u7528\u591a\u5206\u8fa8\u7387MAE\u9884\u8bad\u7ec3\u65b9\u6cd5\u3002", "result": "\u5728\u5408\u6210\u57fa\u51c6\u6d4b\u8bd5\u3001\u80be\u810f\u7ec4\u7ec7\u75c5\u7406\u5b66\u548c\u9ad8\u5206\u8fa8\u7387\u5c0f\u9f20\u8111\u663e\u5fae\u955c\u56fe\u50cf\u5206\u6790\u4e2d\uff0cMuViT\u76f8\u6bd4\u5f3a\u5927\u7684ViT\u548cCNN\u57fa\u7ebf\u6a21\u578b\u53d6\u5f97\u4e86\u6301\u7eed\u6539\u8fdb\u3002\u591a\u5206\u8fa8\u7387MAE\u9884\u8bad\u7ec3\u4ea7\u751f\u4e86\u5c3a\u5ea6\u4e00\u81f4\u7684\u8868\u5f81\uff0c\u589e\u5f3a\u4e86\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "\u663e\u5f0f\u7684\u4e16\u754c\u5750\u6807\u5efa\u6a21\u4e3a\u5927\u89c4\u6a21\u663e\u5fae\u955c\u5206\u6790\u4e2d\u7684\u591a\u5206\u8fa8\u7387\u4fe1\u606f\u5229\u7528\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u800c\u5f3a\u5927\u7684\u673a\u5236\uff0cMuViT\u67b6\u6784\u80fd\u591f\u6709\u6548\u878d\u5408\u591a\u5206\u8fa8\u7387\u89c2\u5bdf\uff0c\u63d0\u5347\u751f\u7269\u533b\u5b66\u56fe\u50cf\u5206\u6790\u6027\u80fd\u3002"}}
{"id": "2602.24233", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.24233", "abs": "https://arxiv.org/abs/2602.24233", "authors": ["Zhenyu Tang", "Chaoran Feng", "Yufan Deng", "Jie Wu", "Xiaojie Li", "Rui Wang", "Yunpeng Chen", "Daquan Zhou"], "title": "Enhancing Spatial Understanding in Image Generation via Reward Modeling", "comment": "Accepted at CVPR 2026. Github: https://github.com/DAGroup-PKU/SpatialT2I Project website: https://dagroup-pku.github.io/SpatialT2I/", "summary": "Recent progress in text-to-image generation has greatly advanced visual fidelity and creativity, but it has also imposed higher demands on prompt complexity-particularly in encoding intricate spatial relationships. In such cases, achieving satisfactory results often requires multiple sampling attempts. To address this challenge, we introduce a novel method that strengthens the spatial understanding of current image generation models. We first construct the SpatialReward-Dataset with over 80k preference pairs. Building on this dataset, we build SpatialScore, a reward model designed to evaluate the accuracy of spatial relationships in text-to-image generation, achieving performance that even surpasses leading proprietary models on spatial evaluation. We further demonstrate that this reward model effectively enables online reinforcement learning for the complex spatial generation. Extensive experiments across multiple benchmarks show that our specialized reward model yields significant and consistent gains in spatial understanding for image generation.", "AI": {"tldr": "\u63d0\u51faSpatialScore\u5956\u52b1\u6a21\u578b\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u63d0\u5347\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7684\u7a7a\u95f4\u5173\u7cfb\u7406\u89e3\u80fd\u529b", "motivation": "\u5f53\u524d\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u867d\u7136\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u521b\u9020\u529b\u6709\u6240\u63d0\u5347\uff0c\u4f46\u5bf9\u590d\u6742\u7a7a\u95f4\u5173\u7cfb\u7684\u7f16\u7801\u8981\u6c42\u66f4\u9ad8\uff0c\u5f80\u5f80\u9700\u8981\u591a\u6b21\u91c7\u6837\u624d\u80fd\u83b7\u5f97\u6ee1\u610f\u7ed3\u679c", "method": "\u6784\u5efa\u5305\u542b8\u4e07\u591a\u4e2a\u504f\u597d\u5bf9\u7684SpatialReward-Dataset\uff0c\u57fa\u4e8e\u6b64\u6570\u636e\u96c6\u5f00\u53d1SpatialScore\u5956\u52b1\u6a21\u578b\uff0c\u7528\u4e8e\u8bc4\u4f30\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u7a7a\u95f4\u5173\u7cfb\u51c6\u786e\u6027\uff0c\u5e76\u5229\u7528\u8be5\u6a21\u578b\u8fdb\u884c\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60", "result": "SpatialScore\u5728\u7a7a\u95f4\u5173\u7cfb\u8bc4\u4f30\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u751a\u81f3\u8d85\u8fc7\u9886\u5148\u7684\u4e13\u6709\u6a21\u578b\uff1b\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u751f\u6210\u6a21\u578b\u7684\u7a7a\u95f4\u7406\u89e3\u80fd\u529b", "conclusion": "\u4e13\u95e8\u8bbe\u8ba1\u7684\u5956\u52b1\u6a21\u578b\u80fd\u591f\u6709\u6548\u63d0\u5347\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\u5bf9\u590d\u6742\u7a7a\u95f4\u5173\u7cfb\u7684\u7406\u89e3\u548c\u751f\u6210\u80fd\u529b"}}
{"id": "2602.24240", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.24240", "abs": "https://arxiv.org/abs/2602.24240", "authors": ["Chengyan Deng", "Zhangquan Chen", "Li Yu", "Kai Zhang", "Xue Zhou", "Wang Zhang"], "title": "Joint Geometric and Trajectory Consistency Learning for One-Step Real-World Super-Resolution", "comment": null, "summary": "Diffusion-based Real-World Image Super-Resolution (Real-ISR) achieves impressive perceptual quality but suffers from high computational costs due to iterative sampling. While recent distillation approaches leveraging large-scale Text-to-Image (T2I) priors have enabled one-step generation, they are typically hindered by prohibitive parameter counts and the inherent capability bounds imposed by teacher models. As a lightweight alternative, Consistency Models offer efficient inference but struggle with two critical limitations: the accumulation of consistency drift inherent to transitive training, and a phenomenon we term \"Geometric Decoupling\" - where the generative trajectory achieves pixel-wise alignment yet fails to preserve structural coherence. To address these challenges, we propose GTASR (Geometric Trajectory Alignment Super-Resolution), a simple yet effective consistency training paradigm for Real-ISR. Specifically, we introduce a Trajectory Alignment (TA) strategy to rectify the tangent vector field via full-path projection, and a Dual-Reference Structural Rectification (DRSR) mechanism to enforce strict structural constraints. Extensive experiments verify that GTASR delivers superior performance over representative baselines while maintaining minimal latency. The code and model will be released at https://github.com/Blazedengcy/GTASR.", "AI": {"tldr": "GTASR\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u7684\u51e0\u4f55\u8f68\u8ff9\u5bf9\u9f50\u4e00\u81f4\u6027\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f68\u8ff9\u5bf9\u9f50\u548c\u53cc\u53c2\u8003\u7ed3\u6784\u6821\u6b63\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u4e00\u81f4\u6027\u6a21\u578b\u4e2d\u7684\u4e00\u81f4\u6027\u6f02\u79fb\u548c\u51e0\u4f55\u89e3\u8026\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u4f4e\u5ef6\u8fdf\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u867d\u7136\u611f\u77e5\u8d28\u91cf\u51fa\u8272\uff0c\u4f46\u5b58\u5728\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002\u4e00\u81f4\u6027\u6a21\u578b\u867d\u7136\u63a8\u7406\u9ad8\u6548\uff0c\u4f46\u9762\u4e34\u4e00\u81f4\u6027\u6f02\u79fb\u7d2f\u79ef\u548c\"\u51e0\u4f55\u89e3\u8026\"\u95ee\u9898\uff08\u50cf\u7d20\u5bf9\u9f50\u4f46\u7ed3\u6784\u4e0d\u8fde\u8d2f\uff09\u3002", "method": "\u63d0\u51faGTASR\u65b9\u6cd5\uff1a1\uff09\u8f68\u8ff9\u5bf9\u9f50\u7b56\u7565\u901a\u8fc7\u5168\u8def\u5f84\u6295\u5f71\u4fee\u6b63\u5207\u5411\u91cf\u573a\uff1b2\uff09\u53cc\u53c2\u8003\u7ed3\u6784\u6821\u6b63\u673a\u5236\u65bd\u52a0\u4e25\u683c\u7684\u7ed3\u6784\u7ea6\u675f\uff0c\u786e\u4fdd\u751f\u6210\u8f68\u8ff9\u4fdd\u6301\u7ed3\u6784\u8fde\u8d2f\u6027\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1GTASR\u5728\u4fdd\u6301\u6700\u5c0f\u5ef6\u8fdf\u7684\u540c\u65f6\uff0c\u6027\u80fd\u4f18\u4e8e\u4ee3\u8868\u6027\u57fa\u7ebf\u65b9\u6cd5\u3002\u4ee3\u7801\u548c\u6a21\u578b\u5c06\u5728GitHub\u4e0a\u53d1\u5e03\u3002", "conclusion": "GTASR\u4e3a\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u4e00\u81f4\u6027\u6a21\u578b\u7684\u5173\u952e\u9650\u5236\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\u91cd\u5efa\u3002"}}
{"id": "2602.24264", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.24264", "abs": "https://arxiv.org/abs/2602.24264", "authors": ["Arnas Uselis", "Andrea Dittadi", "Seong Joon Oh"], "title": "Compositional Generalization Requires Linear, Orthogonal Representations in Vision Embedding Models", "comment": null, "summary": "Compositional generalization, the ability to recognize familiar parts in novel contexts, is a defining property of intelligent systems. Although modern models are trained on massive datasets, they still cover only a tiny fraction of the combinatorial space of possible inputs, raising the question of what structure representations must have to support generalization to unseen combinations. We formalize three desiderata for compositional generalization under standard training (divisibility, transferability, stability) and show they impose necessary geometric constraints: representations must decompose linearly into per-concept components, and these components must be orthogonal across concepts. This provides theoretical grounding for the Linear Representation Hypothesis: the linear structure widely observed in neural representations is a necessary consequence of compositional generalization. We further derive dimension bounds linking the number of composable concepts to the embedding geometry. Empirically, we evaluate these predictions across modern vision models (CLIP, SigLIP, DINO) and find that representations exhibit partial linear factorization with low-rank, near-orthogonal per-concept factors, and that the degree of this structure correlates with compositional generalization on unseen combinations. As models continue to scale, these conditions predict the representational geometry they may converge to. Code is available at https://github.com/oshapio/necessary-compositionality.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u7ec4\u5408\u6cdb\u5316\u7684\u4e09\u4e2a\u5fc5\u8981\u6761\u4ef6\uff08\u53ef\u5206\u6027\u3001\u53ef\u8fc1\u79fb\u6027\u3001\u7a33\u5b9a\u6027\uff09\uff0c\u8bc1\u660e\u8fd9\u4e9b\u6761\u4ef6\u8981\u6c42\u795e\u7ecf\u8868\u793a\u5fc5\u987b\u7ebf\u6027\u5206\u89e3\u4e3a\u6982\u5ff5\u7ec4\u4ef6\u4e14\u7ec4\u4ef6\u95f4\u6b63\u4ea4\uff0c\u4e3a\u7ebf\u6027\u8868\u793a\u5047\u8bf4\u63d0\u4f9b\u7406\u8bba\u4f9d\u636e\uff0c\u5e76\u901a\u8fc7\u89c6\u89c9\u6a21\u578b\u9a8c\u8bc1\u4e86\u8be5\u7ed3\u6784\u7684\u5b58\u5728\u53ca\u5176\u4e0e\u7ec4\u5408\u6cdb\u5316\u7684\u76f8\u5173\u6027\u3002", "motivation": "\u5c3d\u7ba1\u73b0\u4ee3\u6a21\u578b\u5728\u5927\u89c4\u6a21\u6570\u636e\u4e0a\u8bad\u7ec3\uff0c\u4f46\u53ea\u80fd\u8986\u76d6\u7ec4\u5408\u7a7a\u95f4\u4e2d\u6781\u5c0f\u90e8\u5206\u8f93\u5165\uff0c\u9700\u8981\u63a2\u7a76\u652f\u6301\u672a\u89c1\u7ec4\u5408\u6cdb\u5316\u7684\u8868\u793a\u7ed3\u6784\u3002\u7814\u7a76\u65e8\u5728\u5f62\u5f0f\u5316\u7ec4\u5408\u6cdb\u5316\u7684\u5fc5\u8981\u6761\u4ef6\uff0c\u5e76\u63ed\u793a\u8fd9\u4e9b\u6761\u4ef6\u5bf9\u8868\u793a\u51e0\u4f55\u7ed3\u6784\u7684\u7ea6\u675f\u3002", "method": "\u5f62\u5f0f\u5316\u7ec4\u5408\u6cdb\u5316\u7684\u4e09\u4e2a\u5fc5\u8981\u6761\u4ef6\uff08\u53ef\u5206\u6027\u3001\u53ef\u8fc1\u79fb\u6027\u3001\u7a33\u5b9a\u6027\uff09\uff0c\u8bc1\u660e\u8fd9\u4e9b\u6761\u4ef6\u5bfc\u81f4\u8868\u793a\u5fc5\u987b\u7ebf\u6027\u5206\u89e3\u4e3a\u6982\u5ff5\u7ec4\u4ef6\u4e14\u7ec4\u4ef6\u6b63\u4ea4\u7684\u51e0\u4f55\u7ea6\u675f\u3002\u63a8\u5bfc\u7ef4\u5ea6\u754c\u9650\uff0c\u5c06\u53ef\u7ec4\u5408\u6982\u5ff5\u6570\u91cf\u4e0e\u5d4c\u5165\u51e0\u4f55\u8054\u7cfb\u8d77\u6765\u3002\u5728CLIP\u3001SigLIP\u3001DINO\u7b49\u73b0\u4ee3\u89c6\u89c9\u6a21\u578b\u4e0a\u8fdb\u884c\u5b9e\u8bc1\u8bc4\u4f30\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\u7ec4\u5408\u6cdb\u5316\u8981\u6c42\u8868\u793a\u7ebf\u6027\u5206\u89e3\u4e14\u6982\u5ff5\u7ec4\u4ef6\u6b63\u4ea4\u3002\u5b9e\u8bc1\u53d1\u73b0\u73b0\u4ee3\u89c6\u89c9\u6a21\u578b\u8868\u793a\u5448\u73b0\u90e8\u5206\u7ebf\u6027\u5206\u89e3\uff0c\u5177\u6709\u4f4e\u79e9\u3001\u8fd1\u6b63\u4ea4\u7684\u6982\u5ff5\u56e0\u5b50\uff0c\u4e14\u8fd9\u79cd\u7ed3\u6784\u7a0b\u5ea6\u4e0e\u672a\u89c1\u7ec4\u5408\u4e0a\u7684\u7ec4\u5408\u6cdb\u5316\u80fd\u529b\u76f8\u5173\u3002", "conclusion": "\u7ebf\u6027\u8868\u793a\u5047\u8bf4\u5f97\u5230\u7406\u8bba\u652f\u6301\uff1a\u795e\u7ecf\u8868\u793a\u4e2d\u5e7f\u6cdb\u89c2\u5bdf\u5230\u7684\u7ebf\u6027\u7ed3\u6784\u662f\u7ec4\u5408\u6cdb\u5316\u7684\u5fc5\u7136\u7ed3\u679c\u3002\u968f\u7740\u6a21\u578b\u89c4\u6a21\u6269\u5927\uff0c\u8fd9\u4e9b\u6761\u4ef6\u9884\u6d4b\u4e86\u5b83\u4eec\u53ef\u80fd\u6536\u655b\u7684\u8868\u793a\u51e0\u4f55\u7ed3\u6784\u3002"}}
{"id": "2602.24275", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.24275", "abs": "https://arxiv.org/abs/2602.24275", "authors": ["Junxian Huang", "Ruichu Cai", "Hao Zhu", "Juntao Fang", "Boyan Xu", "Weilin Chen", "Zijian Li", "Shenghua Gao"], "title": "Hierarchical Action Learning for Weakly-Supervised Action Segmentation", "comment": null, "summary": "Humans perceive actions through key transitions that structure actions across multiple abstraction levels, whereas machines, relying on visual features, tend to over-segment. This highlights the difficulty of enabling hierarchical reasoning in video understanding. Interestingly, we observe that lower-level visual and high-level action latent variables evolve at different rates, with low-level visual variables changing rapidly, while high-level action variables evolve more slowly, making them easier to identify. Building on this insight, we propose the Hierarchical Action Learning (\\textbf{HAL}) model for weakly-supervised action segmentation. Our approach introduces a hierarchical causal data generation process, where high-level latent action governs the dynamics of low-level visual features. To model these varying timescales effectively, we introduce deterministic processes to align these latent variables over time. The \\textbf{HAL} model employs a hierarchical pyramid transformer to capture both visual features and latent variables, and a sparse transition constraint is applied to enforce the slower dynamics of high-level action variables. This mechanism enhances the identification of these latent variables over time. Under mild assumptions, we prove that these latent action variables are strictly identifiable. Experimental results on several benchmarks show that the \\textbf{HAL} model significantly outperforms existing methods for weakly-supervised action segmentation, confirming its practical effectiveness in real-world applications.", "AI": {"tldr": "HAL\u6a21\u578b\u901a\u8fc7\u5206\u5c42\u56e0\u679c\u751f\u6210\u8fc7\u7a0b\u548c\u786e\u5b9a\u6027\u65f6\u95f4\u5bf9\u9f50\uff0c\u5229\u7528\u9ad8\u4f4e\u5c42\u6f5c\u5728\u53d8\u91cf\u4e0d\u540c\u6f14\u5316\u901f\u7387\uff0c\u5728\u5f31\u76d1\u7763\u52a8\u4f5c\u5206\u5272\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4eba\u7c7b\u901a\u8fc7\u5173\u952e\u8f6c\u6362\u70b9\u5728\u4e0d\u540c\u62bd\u8c61\u5c42\u6b21\u4e0a\u611f\u77e5\u52a8\u4f5c\uff0c\u800c\u673a\u5668\u4f9d\u8d56\u89c6\u89c9\u7279\u5f81\u5bb9\u6613\u8fc7\u5206\u5272\u3002\u9ad8\u4f4e\u5c42\u6f5c\u5728\u53d8\u91cf\u6f14\u5316\u901f\u7387\u4e0d\u540c\uff1a\u4f4e\u5c42\u89c6\u89c9\u53d8\u91cf\u53d8\u5316\u5feb\uff0c\u9ad8\u5c42\u52a8\u4f5c\u53d8\u91cf\u53d8\u5316\u6162\uff0c\u8fd9\u4e3a\u8bc6\u522b\u9ad8\u5c42\u52a8\u4f5c\u53d8\u91cf\u63d0\u4f9b\u4e86\u673a\u4f1a\u3002", "method": "\u63d0\u51fa\u5206\u5c42\u52a8\u4f5c\u5b66\u4e60\uff08HAL\uff09\u6a21\u578b\uff0c\u5f15\u5165\u5206\u5c42\u56e0\u679c\u6570\u636e\u751f\u6210\u8fc7\u7a0b\uff0c\u9ad8\u5c42\u6f5c\u5728\u52a8\u4f5c\u63a7\u5236\u4f4e\u5c42\u89c6\u89c9\u7279\u5f81\u52a8\u6001\u3002\u4f7f\u7528\u786e\u5b9a\u6027\u8fc7\u7a0b\u5bf9\u9f50\u65f6\u95f4\u5c3a\u5ea6\uff0c\u91c7\u7528\u5206\u5c42\u91d1\u5b57\u5854transformer\u6355\u83b7\u89c6\u89c9\u7279\u5f81\u548c\u6f5c\u5728\u53d8\u91cf\uff0c\u5e94\u7528\u7a00\u758f\u8f6c\u6362\u7ea6\u675f\u5f3a\u5236\u9ad8\u5c42\u52a8\u4f5c\u53d8\u91cf\u7684\u6162\u52a8\u6001\u3002", "result": "\u5728\u6e29\u548c\u5047\u8bbe\u4e0b\u8bc1\u660e\u6f5c\u5728\u52a8\u4f5c\u53d8\u91cf\u4e25\u683c\u53ef\u8bc6\u522b\u3002\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cHAL\u6a21\u578b\u5728\u5f31\u76d1\u7763\u52a8\u4f5c\u5206\u5272\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "HAL\u6a21\u578b\u901a\u8fc7\u5efa\u6a21\u9ad8\u4f4e\u5c42\u6f5c\u5728\u53d8\u91cf\u7684\u4e0d\u540c\u6f14\u5316\u901f\u7387\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5f31\u76d1\u7763\u52a8\u4f5c\u5206\u5272\u95ee\u9898\uff0c\u5728\u7406\u8bba\u548c\u5b9e\u8df5\u4e0a\u90fd\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u3002"}}
{"id": "2602.24289", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.24289", "abs": "https://arxiv.org/abs/2602.24289", "authors": ["Shengqu Cai", "Weili Nie", "Chao Liu", "Julius Berner", "Lvmin Zhang", "Nanye Ma", "Hansheng Chen", "Maneesh Agrawala", "Leonidas Guibas", "Gordon Wetzstein", "Arash Vahdat"], "title": "Mode Seeking meets Mean Seeking for Fast Long Video Generation", "comment": "Project website: https://primecai.github.io/mmm/", "summary": "Scaling video generation from seconds to minutes faces a critical bottleneck: while short-video data is abundant and high-fidelity, coherent long-form data is scarce and limited to narrow domains. To address this, we propose a training paradigm where Mode Seeking meets Mean Seeking, decoupling local fidelity from long-term coherence based on a unified representation via a Decoupled Diffusion Transformer. Our approach utilizes a global Flow Matching head trained via supervised learning on long videos to capture narrative structure, while simultaneously employing a local Distribution Matching head that aligns sliding windows to a frozen short-video teacher via a mode-seeking reverse-KL divergence. This strategy enables the synthesis of minute-scale videos that learns long-range coherence and motions from limited long videos via supervised flow matching, while inheriting local realism by aligning every sliding-window segment of the student to a frozen short-video teacher, resulting in a few-step fast long video generator. Evaluations show that our method effectively closes the fidelity-horizon gap by jointly improving local sharpness, motion and long-range consistency. Project website: https://primecai.github.io/mmm/.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u6a21\u5f0f\u5bfb\u6c42\u4e0e\u5747\u503c\u5bfb\u6c42\u7684\u8bad\u7ec3\u8303\u5f0f\uff0c\u901a\u8fc7\u89e3\u8026\u6269\u6563Transformer\u7edf\u4e00\u8868\u793a\uff0c\u5206\u79bb\u5c40\u90e8\u4fdd\u771f\u5ea6\u4e0e\u957f\u671f\u8fde\u8d2f\u6027\uff0c\u5b9e\u73b0\u5206\u949f\u7ea7\u89c6\u9891\u751f\u6210\u3002", "motivation": "\u89c6\u9891\u751f\u6210\u4ece\u79d2\u7ea7\u6269\u5c55\u5230\u5206\u949f\u7ea7\u9762\u4e34\u5173\u952e\u74f6\u9888\uff1a\u77ed\u89c6\u9891\u6570\u636e\u4e30\u5bcc\u4e14\u4fdd\u771f\u5ea6\u9ad8\uff0c\u4f46\u8fde\u8d2f\u7684\u957f\u89c6\u9891\u6570\u636e\u7a00\u7f3a\u4e14\u5c40\u9650\u4e8e\u72ed\u7a84\u9886\u57df\u3002", "method": "\u91c7\u7528\u89e3\u8026\u6269\u6563Transformer\u7edf\u4e00\u8868\u793a\uff0c\u4f7f\u7528\u5168\u5c40\u6d41\u5339\u914d\u5934\u901a\u8fc7\u957f\u89c6\u9891\u76d1\u7763\u5b66\u4e60\u6355\u6349\u53d9\u4e8b\u7ed3\u6784\uff0c\u540c\u65f6\u4f7f\u7528\u5c40\u90e8\u5206\u5e03\u5339\u914d\u5934\u901a\u8fc7\u6a21\u5f0f\u5bfb\u6c42\u7684\u53cd\u5411KL\u6563\u5ea6\u5c06\u6ed1\u52a8\u7a97\u53e3\u4e0e\u51bb\u7ed3\u7684\u77ed\u89c6\u9891\u6559\u5e08\u5bf9\u9f50\u3002", "result": "\u8be5\u65b9\u6cd5\u6709\u6548\u7f29\u5c0f\u4e86\u4fdd\u771f\u5ea6-\u65f6\u95f4\u8de8\u5ea6\u5dee\u8ddd\uff0c\u8054\u5408\u63d0\u5347\u4e86\u5c40\u90e8\u6e05\u6670\u5ea6\u3001\u8fd0\u52a8\u6548\u679c\u548c\u957f\u671f\u4e00\u81f4\u6027\uff0c\u5b9e\u73b0\u4e86\u5feb\u901f\u751f\u6210\u5206\u949f\u7ea7\u89c6\u9891\u3002", "conclusion": "\u901a\u8fc7\u6a21\u5f0f\u5bfb\u6c42\u4e0e\u5747\u503c\u5bfb\u6c42\u7684\u89e3\u8026\u8bad\u7ec3\u8303\u5f0f\uff0c\u80fd\u591f\u5728\u6709\u9650\u957f\u89c6\u9891\u6570\u636e\u4e0b\u5b66\u4e60\u957f\u671f\u8fde\u8d2f\u6027\uff0c\u540c\u65f6\u7ee7\u627f\u77ed\u89c6\u9891\u6559\u5e08\u7684\u5c40\u90e8\u771f\u5b9e\u611f\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u5206\u949f\u7ea7\u89c6\u9891\u751f\u6210\u3002"}}
{"id": "2602.24290", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.24290", "abs": "https://arxiv.org/abs/2602.24290", "authors": ["Junhwa Hur", "Charles Herrmann", "Songyou Peng", "Philipp Henzler", "Zeyu Ma", "Todd Zickler", "Deqing Sun"], "title": "UFO-4D: Unposed Feedforward 4D Reconstruction from Two Images", "comment": "ICLR 2026, Project page: https://ufo-4d.github.io/", "summary": "Dense 4D reconstruction from unposed images remains a critical challenge, with current methods relying on slow test-time optimization or fragmented, task-specific feedforward models. We introduce UFO-4D, a unified feedforward framework to reconstruct a dense, explicit 4D representation from just a pair of unposed images. UFO-4D directly estimates dynamic 3D Gaussian Splats, enabling the joint and consistent estimation of 3D geometry, 3D motion, and camera pose in a feedforward manner. Our core insight is that differentiably rendering multiple signals from a single Dynamic 3D Gaussian representation offers major training advantages. This approach enables a self-supervised image synthesis loss while tightly coupling appearance, depth, and motion. Since all modalities share the same geometric primitives, supervising one inherently regularizes and improves the others. This synergy overcomes data scarcity, allowing UFO-4D to outperform prior work by up to 3 times in joint geometry, motion, and camera pose estimation. Our representation also enables high-fidelity 4D interpolation across novel views and time. Please visit our project page for visual results: https://ufo-4d.github.io/", "AI": {"tldr": "UFO-4D\u662f\u4e00\u4e2a\u7edf\u4e00\u7684feedforward\u6846\u67b6\uff0c\u4ec5\u9700\u4e00\u5bf9\u672a\u6807\u5b9a\u56fe\u50cf\u5373\u53ef\u91cd\u5efa\u5bc6\u96c6\u76844D\u8868\u793a\uff0c\u76f4\u63a5\u4f30\u8ba1\u52a8\u60013D\u9ad8\u65af\u6cfc\u6e85\uff0c\u5b9e\u73b0\u51e0\u4f55\u3001\u8fd0\u52a8\u548c\u76f8\u673a\u59ff\u6001\u7684\u8054\u5408\u4e00\u81f4\u4f30\u8ba1\u3002", "motivation": "\u5f53\u524d\u5bc6\u96c64D\u91cd\u5efa\u65b9\u6cd5\u4f9d\u8d56\u7f13\u6162\u7684\u6d4b\u8bd5\u65f6\u4f18\u5316\u6216\u788e\u7247\u5316\u7684\u4efb\u52a1\u7279\u5b9a\u524d\u9988\u6a21\u578b\uff0c\u9700\u8981\u4e00\u79cd\u7edf\u4e00\u7684feedforward\u6846\u67b6\u6765\u5904\u7406\u672a\u6807\u5b9a\u56fe\u50cf\u3002", "method": "\u4f7f\u7528\u52a8\u60013D\u9ad8\u65af\u8868\u793a\uff0c\u901a\u8fc7\u53ef\u5fae\u5206\u6e32\u67d3\u591a\u4e2a\u4fe1\u53f7\u5b9e\u73b0\u81ea\u76d1\u7763\u56fe\u50cf\u5408\u6210\u635f\u5931\uff0c\u7d27\u5bc6\u8026\u5408\u5916\u89c2\u3001\u6df1\u5ea6\u548c\u8fd0\u52a8\uff0c\u6240\u6709\u6a21\u6001\u5171\u4eab\u76f8\u540c\u51e0\u4f55\u57fa\u5143\u3002", "result": "\u5728\u8054\u5408\u51e0\u4f55\u3001\u8fd0\u52a8\u548c\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u65b9\u9762\u6bd4\u5148\u524d\u5de5\u4f5c\u63d0\u5347\u9ad8\u8fbe3\u500d\uff0c\u652f\u6301\u8de8\u65b0\u89c6\u89d2\u548c\u65f6\u95f4\u7684\u9ad8\u4fdd\u771f4D\u63d2\u503c\u3002", "conclusion": "UFO-4D\u901a\u8fc7\u7edf\u4e00\u7684feedforward\u6846\u67b6\u548c\u52a8\u60013D\u9ad8\u65af\u8868\u793a\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u672a\u6807\u5b9a\u56fe\u50cf\u7684\u5bc6\u96c64D\u91cd\u5efa\u95ee\u9898\uff0c\u514b\u670d\u4e86\u6570\u636e\u7a00\u7f3a\u6027\uff0c\u5b9e\u73b0\u4e86\u591a\u6a21\u6001\u7684\u534f\u540c\u63d0\u5347\u3002"}}
